 6.4 Hierarchy of Clusters  Hierarchical clustering in Information Retrieval focuses on the area of hierarchical agglomerative clustering methods (HACM) (Willet-88).    The term  agglomerative means the clustering process starts with unclustered items and performs pairwise similarity measures to determine the clusters. Divisive is the term applied to starting with a cluster and breaking it down into smaller clusters. The objectives of creating a hierarchy of clusters are to:  Item I 33/2 17/2  Item 2 23/2 51/2  Item 3 30/2 18/2  Item 4 8/2 24/2  Item 5 31/2 47/2  Document and Term Clustering  157  Reduce the overhead of search  Provide for a visual representation of the information space  Expand the retrieval of relevant items.  Search overhead is reduced by performing top-down searches of the centroids of the clusters in the hierarchy and trimming those branches that are not relevant (discussed in greater depth in Chapter 7). It is difficult to create a visual display of the total item space. Use of dendograms along with visual cues on the size of clusters (e.g., size of the ellipse) and strengths of the linkages between clusters (e.g., dashed lines indicate reduced similarities) allows a user to determine alternate paths of browsing the database (see Figure 6.12). The dendogram allows the user to determine which clusters to be reviewed are likely to have items of interest. Even without the visual display of the hierarchy, a user can use the  Figure 6.12 Dendogram  logical hierarchy to browse items of interest. A user, once having identified an item of interest, can request to see other items in the cluster. The user can increase the specificity of items by going to children clusters or by increasing the generality of items being reviewed by going to a parent cluster.  Most of the existing HACM approaches can be defined in terms of the Lance-Williams dissimilarity update formula (Lance-66). It defines a general formula for calculating the dissimilarity D between any existing cluster Ck and a new cluster Cy created by combining clusters C, and Cr 158                                                                                                 Chapter 6  D(CijsCk) = aiD(Ci,Ck) + ajD(CJ9Ck) + PD(CISCj) + y| D(C,,Ck) - D(Cj?Ck)|  By proper selection of a, p, and y, the current techniques for HACM can be represented (Frakes-92). In comparing the various methods of creating hierarchical clusters Voorhees and later El-Hamdouchi and Willet determined that the group average method produced the best results on document collections (Voorhees-86, El-Hamdouchi-89).  The similarity between two clusters can be treated as the similarity between all objects in one cluster and all objects in the other cluster. Voorhees showed that the similarity between a cluster centroid and any item is equal to the mean similarity between the item and all items in the cluster. Since the centroid is the average of all items in the cluster, this means that similarities between centroids can be used to calculate the similarities between clusters.  Ward's Method (Ward-63) chooses the minimum square Euclidean distance between points (e.g., centroids in this case) normalized by the number of objects in each cluster. He uses the formula for the variance I, choosing the minimum variance:  I,j = ((m1mJ)/(m1 + m }))dh}2 di/ = Zk=l (xI(k - xhkf  where m, is the number of objects in Classs and dj?J2 is the squared Euclidean distance. The process of selection of centroids can be improved by using the reciprocal nearest neighbor algorithm (Murtaugh-83, Murtaugh-85).  The techniques discribed in Section 6.2 created independent sets of classes. The automatic clustering techniques can also be used to create a hierarchy of objects (items or terms). The automatic approach has been applied to creating  item hierarchies more than in hierarchical statistical thesaurus generation.  In the  manual creation of thesauri, network relationships are frequently allowed between terms and classes creating an expanded thesaurus called semantic networks (e.g., in TOPIC and RetrievalWare). Hierarchies have also been created going from general categories to more specific classes of terms. The human creator ensures that the generalization or specification as the hierarchy is created makes semantic sense. Automatic creation of a hierarchy for a statistical thesaurus introduces too many errors to be productive.  But for item hierarchies the algorithms can also be applied. Centroids were used to reduce computation required for adjustments in term assignments to classes. For both terms and items, the centroid has the same structure as any of the items or terms when viewed as a vector from the Item/Term matrix (see Figure 6.2). A term is a vector composed of a column whereas an item is a vector composed of a row. The Scatter/Gather system (Hearst-96) is an example of this technique. In the Scatter/Gather system an initial set of clusters was generated. Document and Term Clustering                                                                 159  Each of these clusters was re-clustered to produce a second level. This process iterated until individual items were left at the lowest level.  When the creation of the classes is complete, a centroid can be calculated for each class. When there are a large number of classes, the next higher level in the hierarchy can be created by using the same algorithms used in the initial clustering to cluster the centroids. The only change required may be in the thresholds used. When this process is complete, if there are still too many of these higher level clusters, an additional iteration of clustering can be applied to their centroids. This process will continue until the desired number of clusters at the highest level is achieved.  A cluster can be represented by a category if the clusters were monolithic (membership is based upon a specific attribute). If the cluster is polythetic, generated by allowing for multiple attributes (e.g., words/concepts), then it can best be represented by using a list of the most significant words in the cluster. An alternative is to show a two or three-dimensional space where the clusters are represented by clusters of points. Monolithic clusters have two advantages over polythetic (Sanderson-99): how easy it is for a user to understand the topic of the cluster and the confidence that every item within the cluster will have a significant focus on the topic. For example, YAHOO is a good example of a monolithic cluster environment.  Sanderson and Croft proposed the following methodology to building a concept hierarchy. Rather than just focusing the construction of the hierarchy, they looked at ways of extracting terms from the documents to represent the hierarchy. The terms had the following characteristics:  Terms had to best reflect the topics  A parent term would refer to a more general concept then its child  A child would cover a related subtopic of the parent  A directed acyclic graph would represent relationships versus a pure hierarchy.  Ambiguous terms would have separate entries in the hierarchy for each meaning.  As a concept hierarchy, it should be represented similar to WordNet (Miller-95)  which uses synonyms, antonyms, hyponyrn/hypernym (is-a/is-a-type-of)* an^ meronym/holonym (has-part/is-a-part-of). Some techniques for generating hierarchies are Grefenstette's use of the similarity of contexts for locating synonyms (Grefenstette-94), use of key phrases (e.g., "such as", "and other") as an indicator of hyponym/hypernym relationships (Hearst-98), use of head and modifier noun and verb phrases to determine hierarchies (Woods-97) and use of a cohesion statistic to measure the degree of association between terms (Forsyth-86). 160                                                                                                Chapter 6  Sanderson and Croft used a test based upon subsumption. It is defined given two terms X and Y, X subsumes Y if:  P(X/Y)gt;.8,P(Y/X)lt;1.  X subsumes Y if the documents which Y occurs in are almost (.8) a subset of the documents that X occurs in. The factor of .8 was heuristically used because an absolute condition was eliminating too many useful relationships. X is thus a parent of Y.  The set of documents to be clustered was determined by a query and the query terms were used as the initial set of terms for the monolithic cluster. This set was expanded by adding more terms via query expansion using peudorelevance feedback (Blind feedback, Local Context Analysis) which is described in Chapter 7. They then used the terms and the formula above to create the hierarchies.   
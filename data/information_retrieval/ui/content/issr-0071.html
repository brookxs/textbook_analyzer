 5.2.1 Probabilistic Weighting  The probabilistic approach is based upon direct application of the theory of probability to information retrieval systems. This has the advantage of being able to use the developed formal theory of probability to direct the algorithmic development. It also leads to an invariant result that facilitates integration of results from different databases. The use of probability theory is a natural choice because it is the basis of evidential reasoning (i.e., drawing conclusions from evidence). This is summarized by the Probability Ranking Principle (PRP) and its Plausible Corollary (Cooper-94): Automatic Indexing                                                                                    109  HYPOTHESIS: If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of usefulness to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data is available for this purpose, then the overall effectiveness of the system to its users is the best obtainable on the basis of that data.  PLAUSIBLE COROLLARY: The most promising source of techniques for estimating the probabilities of usefulness for output ranking in IR is standard probability theory and statistics.  There are several factors that make this hypothesis and its corollary difficult (Gordon-92, Gordon-91, Robertson-77). Probabilities are usually based upon a binary condition; an item is relevant or not. But in information systems the relevance of an item is a continuous function from non-relevant to absolutely useful. A more complex theory of expected utility (Cooper-78) is needed to address this characteristic. Additionally, the output ordering by rank of items based upon probabilities, even if accurately calculated, may not be as optimal as that defined by some domain specific heuristic (Stirling-77). The domains in which probabilistic ranking are suboptimal are so narrowly focused as to make this a minor issue. But these issues mentioned are not as compelling as the benefit of a good probability value for ranking that would allow integration of results from multiple sources.  The source of the problems that arise in application of probability theory come from a lack of accurate data and simplifying assumptions that are applied to the mathematical model. If nothing else, these simplifying assumptions cause the results of probabilistic approaches in ranking items to be less accurate than other approaches. The advantage of the probabilistic approach is that it can accurately identify its weak assumptions and work to strengthen them. In many other approaches, the underlying weaknesses in assumptions are less obvious and harder to identify and correct. Even with the simplifying assumption, results from comparisons of approaches in the TREC conferences have shown that the probabilistic approaches, while not scoring highest, are competitive against all other approaches.  There are many different areas in which the probabilistic approach may be applied. The method of logistic regression is described as an example of how a probabilistic approach is applied to information retrieval (Gey-94). The approach starts by defining a "Model 0" system which exists before specific probabilistic models are applied. In a retrieval system there exist query terms q, and document terms djª which have a set of attributes (vi,. . ., vn) from the query (e.g., counts of term frequency in the query), from the document (e.g., counts of term frequency in the document ) and from the database (e.g., total number of documents in the database divided by the number of documents indexed by the term).  The logistic reference model uses a random sample of query-documcutterm triples for which binary relevance judgments have been made from a training 110                                                                                              Chapters  sample.   Log 0 is the logarithm of the odds (logodds) of relevance for term tk which is present in document Dj and query Q,:  log(0(R | Qi, Dj, t0) = Co + c,v, + ... + cnvn  The logarithm that the ith Query is relevant to the jth Document is the sum of the logodds for all terms:  log(0(R | Q,, Dj)) = J   tlog(∞(R I Q^ djgt; W) - log(O(R))]  k=\  where O(R) is the odds that a document chosen at random from the database is relevant to query Qx. The coefficients c are derived using logistic regression which fits an equation to predict a dichotomous independent variable as a function of independent variables that show statistical variation (Hosmer-89). The inverse logistic transformation is applied to obtain the probability of relevance of a document to a query:  P(R|QisDj)= i\(i+e-log(O(RiQi'Di)))  The coefficients of the equation for logodds is derived for a particular database using a random sample of query-document-term-relevance quadruples and used to predict odds of relevance for other query-document pairs.  Gey applied this methodology to the Cranfield Collection (Gey-94). The collection has 1400 items and 225 queries with known results. Additional attributes of relative frequency in the query (QRF), relative frequency in the document (DRF) and relative frequency of the term in all the documents (RFAD) were included, producing the following logodds formula:  Z} , log(O(R | tj)) = cQ+ Cilog(QAF) + c2log(QRF) + c3log(DAF) +  c4log(DRF)  + c5Iog(IDF) + c6log(RFAD)  where QAF, DAF, and IDF were previously defined, QRF = QAF\ (total number of terms in the query), DRF = DAF\(total number of words in the document) and RFAD = (total number of term occurrences in the c!atabase)\ (total number of all words in the database). Logs are used to reduce the impact of frequency information; then smooth out skewed distributions. A higher maximum likelihood is attained for logged attributes.  The coefficients and log (O(R)) were calculated creating the final formula  for ranking for query vector Q, which contains q terms: Automatic Indexing                                                                                   111  log(O(R |0)) = -5.138+ 2,   (Zj + 5.138)  k=\  The logistic inference method was applied to the test database along with the Cornell SMART vector system which uses traditional term frequency, inverse document frequency and cosine relevance weighting formulas (see Section 5.2.2). The logistic inference method outperformed the vector method.  Thus the index that supports the calculations for the logistic reference model contains the O(R) constant value (e.g., -5.138) along with the coefficients c0 through c6. Additionally, it needs to maintain the data to support DAF, DRF, IDF and RFAD. The values for QAF and QRF are derived from the query.  Attempts have been made to combine the results of different probabilistic techniques to get a more accurate value. The objective is to have the strong points of different techniques compensate for weaknesses. To date this combination of probabilities using averages of Log-Odds has not produced better results and in many cases produced worse results (Hull-96).   
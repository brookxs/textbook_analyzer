 Basic probabilistic model*  Since we are assuming that each document is described by the presence/absence of index terms any document can be represented by a binary vector,  x = (x1,x2, . . ., xn)  where xi = 0 or 1 indicates absence or presence of the ith index term. We also assume that there are two mutually exclusive events,  w1 = document is relevant  w2 = document is non-relevant.  * The theory that follows is at first rather abstract, the reader is asked to bear with it, since we soon return to the  nuts and bolts of retrieval.  So, in terms of these symbols, what we wish to calculate for each document is P(w1/x) and perhaps P(w2/x) so that we may decide which is relevant and which is non-relevant. This is a slight change in objective from simply producing a ranking, we also wish the theory to tell us how to cut off the ranking. Therefore we formulate the problem as a decision problem. Of course we cannot estimate P(wi/x) directly so we must find a way of estimating it in terms of quantities we do know something about. Bayes' Theorem tells us that for discrete distributions  Here P(wi) is the prior probability of relevance (i=1) or non-relevance (i=2), P(x/wi) is proportional to what is commonly known as the likelihood of relevance or non-relevance given x; in the continuous case this would be a density function and we would write p(x/wi). Finally,  which is the probability of observing x on a random basis given that it may be either relevant or non-relevant. Again this would be written as a density function p(x) in the continuous case. Although P(x) (or p(x) ) will mostly appear as a normalising factor (i.e. ensuring that P(w1/x) + P(w2/x) = 1) it is in some ways the function we know most about, it does not require a knowledge of relevance for it to be specified. Before I discuss how we go about estimating the right hand side of Bayes' Theorem I will show how the decision for or against relevance is made.  The decision rule we use is in fact well known as Bayes' Decision Rule. It is  [P (w1/x) gt; P(w2/x) -gt; x is relevant, x is non-relevant] * D1  The expression D1 is a short hand notation for the following: compare P (w1/x) with P(w2/x) if the first is greater than the second then decide that x is relevant otherwise decide x is non-relevant. The case P(w1/x) = P(w2/x) is arbitrarily dealt with by deciding non-relevance. The basis for the rule D1 is simply that it minimises the average probability of error, the error of assigning a relevant document as non-relevant or vice versa. To see this note that for any x the probability of error is  * The meaning of [E -gt; p,q] is that if E is true then decide p, otherwise decide q.  In other words once we have decided one way (e.g. relevant) then the probability of having made an error is clearly given by the probability of the opposite way being the case (e.g. non-relevant). So to make this error as small as possible for any given x we must always pick that wi for which P (w1/x) is largest and by implication for which the probability of error is the smallest. To minimise the average probability of error we must minimise  This sum will be minimised by making P (error/x) as small as possible for each x since P(error/x) and P(x) are always positive. This is accomplished by the decision rule D1 which now stands as justified.  Of course average error is not the only sensible quantity worth minimising. If we associate with each type of error a cost we can derive a decision rule which will minimise the overall risk. The overall risk is an average of the conditional risks R(wi/x) which itself in turn is defined in terms of a cost function lij. More specifically lij is the loss incurred for deciding wi when wj is the case. Now the associated expected loss when deciding wi is called the conditional risk and is given by  R (wi/x) - li1P(w1/x) + li2P(w2/x) i = 1, 2  The overall risk is a sum in the same way that the average probability of error was, R (wi/x) now playing the role of P(wi/x). The overall risk is minimised by  [R (w1/x) lt; R (w2/x) -gt; x is relevant, x is non-relevant] D2  D1 and D2 can be shown to be equivalent under certain conditions. First we rewrite D1, using Bayes' Theorem, in a form in which it will be used subsequently, viz.  [P( x/w1) P (w1) gt; P( x/w2) P(w2) -gt; x is relevant, x is non-relevant] D3  Notice that P(x) has disappeared from the equation since it does not affect the outcome of the decision. Now, using the definition R (wi/x) it is easy to show that  [R (w1/x) lt; R (w2/x) ] [[equivalence]] [(l21 - l11) P( x/w1) P(w1) gt; (l12 - l22) P( x/w2) P(w2)]  When a special loss function is chosen, namely,  which implies that no loss is assigned to a correct decision (quite reasonable) and unit loss to any error (not so reasonable), then we have  [R (w1/x) lt; R (w2/x) [[equivalence]] P(x/w1) P (w1) gt; P(x/w2) P(w2)]  which shows the equivalence of D2 and D3, and hence of D1 and D2 under a binary loss function.  This completes the derivation of the decision rule to be used to decide relevance or non-relevance, or to put it differently to retrieve or not to retrieve. So far no constraints have been put on the form of P(x/w1), therefore the decision rule is quite general. I have set up the problem as one of deciding between two classes thereby ignoring the problem of ranking for the moment. One reason for this is that the analysis is simpler, the other is that I want the analysis to say as much as possible about the cut-off value. When ranking, the cut-off value is usually left to the user; within the model so far one can still rank, but the cut-off value will have an interpretation in terms of prior probabilities and cost functions. The optimality of the probability ranking principle follows immediately from the optimality of the decision rule at any cut-off. I shall now go on to be more precise about the exact form of the probability functions in the decision rule.   
 3.4 Information Extraction  There are two processes associated with information extraction: determination of facts to go into structured fields in a database and extraction of  text that can be used to summarize an item. In the first case only a subset of the important facts in an item may be identified and extracted. In summarization all of the major concepts in the item should be represented in the summary.  The process of extracting facts to go into indexes is called Automatic File Build in Chapter 1. Its goal is to process incoming items and extract index terms that will go into a structured database. This differs from indexing in that its objective is to extract specific types of information versus understanding all of the t^xt of the document. An Information Retrieval System's goal is to provide an indepth   representation   of the  total   contents   of an   item   (Sundheim-92).   An 66                                                                                                Chapter 3  Information Extraction system only analyzes those portions of a document that potentially contain information relevant to the extraction criteria. The objective of the data extraction is in most cases to update a structured database with additional facts. The updates may be from a controlled vocabulary or substrings from the item as defined by the extraction rules. The term "slot" is used to define a particular category of information to be extracted. Slots are organized into templates or semantic frames. Information extraction requires multiple levels of analysis of the text of an item. It must understand the words and their context (discourse analysis). The processing is very similar to the natural language processing described under indexing.  In establishing metrics to compare information extraction, the previously defined measures of precision and recall are applied with slight modifications to their meaning. Recall refers to how much information was extracted from an item versus how much should have been extracted from the item. It shows the amount of correct and relevant data extracted versus the correct and relevant data in the item. Precision refers to how much information was extracted accurately versus the total information extracted.  Additional metrics used are overgeneration and fallout. Overgeneration measures the amount of irrelevant information that is extracted. This could be caused by templates filled on topics that are not intended to be extracted or slots that get filled with non-relevant data. Fallout measures how much a system assigns incorrect slot fillers as the number of potential incorrect slot fillers increases (Lehnert-91).  These measures are applicable to both human and automated extraction processes. Human beings fall short of perfection in data extraction as well as automated systems. The best source of analysis of data extraction is from the Message Understanding Conference Proceedings. Conferences (similar to TREC) were held in 1991, 1992, 1993 and 1995. The conferences are sponsored by the Advanced Research Project Agency/Software and Intelligent Systems Technology Office of the Department of Defense. Large test databases are made available to any organization interested in participating in evaluation of their algorithms. In MUC-5 (1993), four experienced human analysts performed detailed extraction against 120 documents and their performance was compared against the top three information extraction systems. The humans achieved a 79 per cent recall with 82 per cent precision. That is, they extracted 79 per cent of the data they could have found and 18 per cent of what they extracted was erroneous. The automated programs achieved 53 per cent recall and 57 per cent precision. The other mediating factor is the costs associated with information extraction. The humans required between 15 and 60 minutes to process a single item versus the 30 seconds to three minutes required by the computers. Thus the existing algorithms are not operating close to what a human can achieve, but they are significantly cheaper. A combination of the two in a computer-assisted information extraction system appears the most reasonable solution in the foreseeable future.  Another related information technology is document summarization. Rather than trying to determine specific facts, the goal of document summarization Cataloging and Indexing                                                                              67  is to extract a summary of an item maintaining the most important ideas while significantly reducing the size. Examples of summaries that are often part of any item are titles, table of contents, and abstracts with the abstract being the closest. The abstract can be used to represent the item for search purposes or as a way for a user to determine the utility of an item without having to read the complete item. It is not feasible to automatically generate a coherent narrative summary of an item with proper discourse, abstraction and language usage (Sparck Jones-93). Restricting the domain of the item can significantly improve the quality of the output (Paice-93, Reimer-88). The more restricted goals for much of the research is in finding subsets of the item that can be extracted and concatenated (usually extracting at the sentence level) and represents the most important concepts in the item. There is no guarantee of readability as a narrative abstract and it is seldom achieved. It has been shown that extracts of approximately 20 per cent of the complete item can represent the majority of significant concepts (Morris-92). Different algorithms produces different summaries. Just as different humans create different abstracts for the same item, automated techniques that generate different summaries does not intrinsically imply major deficiencies between the summaries. Most automated algorithms approach summarization by calculating a score for each sentence and then extracting the sentences with the highest scores. Some examples of the scoring techniques are use of rhetorical relations (e.g., reason, direction, contrast: see Miike-94 for experiments in Japanese), contextual inference and syntactic coherence using cue words (Rush-71), term location (Salton-83), and statistical weighting properties discussed in Chapter 5. There is no overall theoretic basis for the approaches leading to many heuristic algorithms. Kupiec et al. are pursuing statistical classification approach based upon a training set reducing the heuristics by focusing on a weighted combination of criteria to produce "optimal" scoring scheme (Kupiec-95). They selected the following five feature sets as a basis for their algorithm:  Sentence Length Feature that requires sentence to be over five words in  length  Fixed Phrase Feature that looks for the existence of phrase "cues" (e.g., "in conclusion)  Paragraph Feature that places emphasis on the first tee and last five paragraphs in an item and also the location of the sentences within the paragraph  Thematic Word Feature that uses word frequency  Uppercase Word Feature that places emphasis on proper names and acronvms. 68                                                                                               Chapter 3  As with previous experiments by Edmundson, Kupiec et al. discovered that location based heuristics gives better results than the frequency based features (Edmundson-69).  Although there is significant overlap in the algorithms and techniques for information extraction and indexing items for information retrieval, this text does not present more detail on information extraction. For additional information, the MUC proceedings from Morgan Kaufman Publishers, Inc. in San Francisco is one source of the latest detailed information on information extraction.   
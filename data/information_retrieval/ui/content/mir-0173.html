 9.3.4    Query Processing Query processing in a distributed IR system proceeds as follows: (1)  Select collections to search. (2)  Distribute query to selected collections. (3)  Evaluate query at distributed collections in parallel. (4)  Combine results from distributed collections into final result. As described in the previous section, Step 1 may be eliminated if the query is always broadcast to every document collection in the system. Otherwise, one of the previously described selection algorithms is used and the query is distributed to the selected collections. Each of the participating search servers then evaluates the query on the selected collections using its own local search algorithm. Finally, the results are merged. At this point we have covered everything except how to merge the results. There are a number of scenarios. First, if the query is Boolean and the search servers return Boolean result sets, all of the sets are simply unioned to create the final result set. If the query involves free-text ranking, a number of techniques are available ranging from simple/naive to complex/accurate. The simplest approach is to combine the ranked hit-lists using round robin interleaving. This is likely to produce poor quality results since hits from irrelevant collections are given status equal to that of hits from highly relevant collections. An improvement on this process is to merge the hit-lists based on relevance score. As with the parallel process described for Document Partitioning in section 9.2.2, unless proper global term statistics are used to compute the document scores, we may get incorrect results. If documents are randomly distributed such that global term statistics are consistent across all of the distributed collections, the merging based on relevance score is sufficient to maintain retrieval effectiveness. If, however, the distributed document collections are 254        PARALLEL AND DISTRIBUTED IR semantically partitioned or maintained by independent parties, then reranking must be performed. Callan [139] proposes reranking documents by weighting document scores based on their collection similarity computed during the source selection step. The weight for a collection is computed as w = 1+ | C \ -(s รณ s)/s, where | C | is the number of collections searched, s is the collection's score, and s is the mean of the collection scores. The most accurate technique for merging ranked hit-lists is to use accurate global term statistics. This can be accomplished in one of a variety of ways. First, if the collections have been indexed for source selection, that index will contain global term statistics across all of the distributed collections. The broker can include these statistics in the query when it distributes the query to the remote search servers. The servers can then account for these statistics in their processing and produce relevance scores that can be merged directly. If a collection index is unavailable, query distribution can proceed in two rounds of communication. In the first round, the broker distributes the query and gathers collection statistics from each of the search servers. These statistics are combined by the broker and distributed back to the search servers in the second round. Finally, the search protocol can require that search servers return global query term statistics and per-document query term statistics [317, 441]. The broker is then free to rerank every document using the query term statistics and a ranking algorithm of its choice. The end result is a hit-list that contains documents from the distributed collections ranked in the same order as if all of the documents had been indexed in a single collection.  
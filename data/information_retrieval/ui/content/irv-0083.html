 The Swets model*  As early as 1963 Swets[12] expressed dissatisfaction with existing methods of measuring retrieval effectiveness. His background in signal detection led him to formulate an evaluation model based on statistical decision theory. In 1967 he evaluated some fifty different retrieval methods from the point of view of his model[13]. The results of his evaluation were encouraging but not conclusive. Subsequently, Brookes[14] suggested some reasonable modifications to Swets' measure of effectiveness, and Robertson[15] showed that the suggested modifications were in fact simply related to an alternative measure already suggested by Swets. * Bookstein[16] has recently re-examined this model showing how Swets implicitly relied on an 'equal variance' assumption.  It is interesting that although the Swets model is theoretically attractive and links IR measurements to a ready made and well-developed statistical theory, it has not found general acceptance amongst workers in the field.  Before proceeding to an explanation of the Swets model, it is as well to quote in full the conditions that the desired measure of effectiveness is designed to meet. At the beginning of his 1967 report Swets states:  'A desirable measure of retrieval performance would have the following properties: First, it would express solely the ability of a retrieval system to distinguish between wanted and unwanted items - that is, it would be a measure of "effectiveness" only, leaving for separate consideration factors related to cost or "efficiency". Second, the desired measure would not be confounded by the relative willingness of the system to emit items - it would express discrimination power independent of any "acceptance criterion" employed, whether the criterion is characteristic of the system or adjusted by the user. Third, the measure would be a single number - in preference, for example, to a pair of numbers which may co-vary in a loosely specified way, or a curve representing a table of several pairs of numbers - so that it could be transmitted simply and immediately apprehended. Fourth, and finally, the measure would allow complete ordering of different performances, and assess the performance of any one system in absolute terms - that is, the metric would be a scale with a unit, a true zero, and a maximum value. Given a measure with these properties, we could be confident of having a pure and valid index of how well a retrieval system (or method) were performing the function it was primarily designed to accomplish, and we could reasonably ask questions of the form "Shall we pay X dollars for Y units of effectiveness?".'  He then goes on to claim that 'The measure I proposed [in 1963], one drawn from statistical decision theory, has the potential [my italics] to satisfy all four desiderata'. So, what is this measure?  To arrive at the measure, we must first discuss the underlying model. Swets defines the basic variables Precision, Recall, and Fallout in probabilistic terms.  Recall = an estimate of the conditional probability that an item will be  retrieved given that it is relevant [we denote this P(B/A)].  Precision = an estimate of the conditional probability that an item will be  relevant given that it is retrieved [i.e. P(A/B)].  Fallout = an estimate of the conditional probability that an item will be  retrieved given that it is non-relevant [i.e. P(B/`A].  He accepts the validity of measuring the effectiveness of retrieval by a curve either precision-recall or recall-fallout generated by the variation of some control variable [[lambda]] (e.g. co-ordination level). He seeks to characterise each curve by a single number. He rejects precision-recall in favour of recall-fallout since he is unable to do it for the former but achieves limited success with the latter.  In the simplest case we assume that the variable [[lambda]] is distributed normally on the set of relevant and non-relevant documents. The two distributions are given respectively by N(u1, [[sigma]]1) and N(u2, [[sigma]]2). The density functions are given by [[florin]]1 ([[lambda]]|A) and [[florin]]2 ([[lambda]]|`A). We may picture the distribution as shown in Figure 7.5.  The usual set-up in IR is now to define a decision rule in terms of [[lambda]], to determine which documents are retrieved (the acceptance criterion). In other words we specify [[lambda]]c such that a document for which the associated [[lambda]] exceeds [[lambda]]c is retrieved. We now measure the effectiveness of a retrieval strategy by measuring some appropriate variables (such as R and P, or R and F) at various values of [[lambda]]c. It turns out that the differently shaded areas under the curves in Figure 7.5 correspond to recall and fallout. Moreover, we find the operating characteristic (OC) traced out by the point (F[[lambda]], R[[lambda]]) due to variation in [[lambda]]c is a smooth curve fully determined by two points, in the general case of unequal variance, and by one point in the special case of equal variance. To see this one only needs to plot the (F[[lambda]], R[[lambda]]) points on double probability paper (scaled linearly for the normal deviate) to find that the points lie on a straight line. A slope of 45deg. corresponds to equal variance, and otherwise the slope is given by the ratio of [[sigma]]1 and [[sigma]]2. Figure 7.6 shows the two cases. Swets now suggests, regardless of  slope, that the distance 0I (actually [[radical]]20I) be used as a measure of effectiveness. This amounts to using:  which is simply the difference between the means of the distribution normalised by the average standard deviation. Unfortunately this measure does rather hide the fact that a high S1 value may be due to a steep slope. The slope, and S1, would have to be given which fails to meet Swets' second condition. We, also, still have the problem of deciding between two strategies whose OC's intersect and hence have different S1 values and slopes.  Brookes[14] in an attempt to correct for the S1 bias towards systems with slopes much greater than unity suggested a modification to S1. Mathematically Brookes's measure is  Brookes also gives statistical reasons for preferring S2 to S1 which need not concern us here. Geometrically S2 is the perpendicular distance from 0 to OC (see Figure 7.6).  Interestingly enough, Robertson[15] showed that S2 is simply related to the area under the Recall-Fallout curve. In fact, the area is a strictly increasing function of S2. It also has the appealing interpretation that it is equal to the percentage of correct choices a strategy will make when attempting to select from a pair of items, one drawn at random from the non-relevant set and one drawn from the relevant set. It does seem therefore that S2 goes a long way to meeting the requirements laid down by Swets. However, the appropriateness of the model is questionable on a number of grounds. Firstly, the linearity of the OC curve does not necessarily imply that [[lambda]] is normally distributed in both populations, although they will be 'similarly' distributed. Secondly, [[lambda]] is assumed to be continuous which certainly is not the case for the data checked out both by Swets and Brookes, in which the co-ordination level used assumed only integer values. Thirdly, there is no evidence to suggest that in the case of more sophisticated matching functions, as used by the SMART system, that the distributions will be similarly distributed let alone normally. Finally the choice of fallout rather than precision as second variable is hard to justify. The reason is that the proportion of non-relevant retrieved for large systems is going to behave much like the ratio of 'non-relevant' retrieved to 'total documents in system'. For comparative purposes 'total document' may be ignored leaving us with 'non-relevant retrieved' which is complementary to 'relevant retrieved'. But now we may as well use precision instead of fallout.   
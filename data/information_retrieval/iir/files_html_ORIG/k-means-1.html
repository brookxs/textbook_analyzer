<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>K-means</title> 
  <meta name="description" content="K-means" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="model-based-clustering-1.html" /> 
  <link rel="previous" href="evaluation-of-clustering-1.html" /> 
  <link rel="up" href="flat-clustering-1.html" /> 
  <link rel="next" href="cluster-cardinality-in-k-means-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html4225" href="cluster-cardinality-in-k-means-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4219" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4213" href="evaluation-of-clustering-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4221" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4223" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4226" href="cluster-cardinality-in-k-means-1.html">Cluster cardinality in K-means</a> 
  <b> Up:</b> 
  <a name="tex2html4220" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4214" href="evaluation-of-clustering-1.html">Evaluation of clustering</a> &nbsp; 
  <b> <a name="tex2html4222" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4224" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION002140000000000000000"></a> <a name="sec:kmeans"></a> <a name="p:kmeans"></a> <br /> K-means </h1> 
  <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is the most important flat clustering algorithm. Its objective is to minimize the average squared Euclidean distance (Chapter 
  <a href="scoring-term-weighting-and-the-vector-space-model-1.html#ch:termvspace">6</a> , page 
  <a href="pivoted-normalized-document-length-1.html#p:euclideandistance">6.4.4</a> ) of documents from their cluster centers where a cluster center is defined as the mean or 
  <a name="24471"></a> 
  <i>centroid</i> 
  <img width="13" height="32" align="MIDDLE" border="0" src="img1436.png" alt="$\vec{\mu}$" /> of the documents in a cluster 
  <img width="17" height="32" align="MIDDLE" border="0" src="img507.png" alt="$\omega$" />: 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\vec{\mu} (\omega) = \frac{1}{|\omega|} \sum_{\vec{x} \in \omega} \vec{x}
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="124" height="48" border="0" src="img1437.png" alt="\begin{displaymath}
\vec{\mu} (\omega) = \frac{1}{\vert\omega\vert} \sum_{\vec{x} \in \omega} \vec{x}
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (190)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> 
  <p> The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way. We used centroids for Rocchio classification in Chapter <a href="vector-space-classification-1.html#ch:vectorclass">14</a> (page <a href="rocchio-classification-1.html#p:centroid">14.2</a> ). They play a similar role here. The ideal cluster in <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is a sphere with the centroid as its center of gravity. Ideally, the clusters should not overlap. Our desiderata for classes in Rocchio classification were the same. The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster. </p>
  <p> A measure of how well the centroids represent the members of their clusters is the <a name="24484"></a> <i>residual sum of squares</i> or <a name="24486"></a> <i>RSS</i> , the squared distance of each vector from its centroid summed over all vectors: </p>
  <p></p> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray*}
\mbox{RSS}_k = \sum_{\vec{x} \in \omega_k} |
\vec{x}-\vec{\mu}(\omega_k)|^2
\end{eqnarray*}
 --> 
   <a name="p:rss"></a>
   <img width="175" height="46" border="0" src="img1438.png" alt="\begin{eqnarray*}
\mbox{RSS}_k = \sum_{\vec{x} \in \omega_k} \vert
\vec{x}-\vec{\mu}(\omega_k)\vert^2
\end{eqnarray*}" />
  </div> 
  <br clear="ALL" />
  <p></p> 
  <br /> 
  <div align="CENTER">
   <a name="eqn:rss"></a> 
   <!-- MATH
 \begin{eqnarray}
\mbox{RSS} = \sum_{k=1}^K \mbox{RSS}_k
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="112" height="64" align="MIDDLE" border="0" src="img1439.png" alt="$\displaystyle \mbox{RSS} = \sum_{k=1}^K \mbox{RSS}_k$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (191)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> RSS is the 
  <a name="24501"></a> 
  <i>objective function</i> in 
  <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means and our goal is to minimize it. Since 
  <img width="17" height="32" align="MIDDLE" border="0" src="img62.png" alt="$N$" /> is fixed, minimizing RSS is equivalent to minimizing the average squared distance, a measure of how well centroids represent their documents. 
  <p> </p>
  <div align="CENTER"> 
   <p><a name="fig:clusttb2"></a><a name="p:clusttb2"></a></p>
   <img width="555" height="345" border="0" src="img1440.png" alt="\begin{figure}
% latex2html id marker 24504
\begin{algorithm}{K-means}{\{\vec{x}...
...n and initialization are discussed on page \ref{p:seedselection} .}
\end{figure}" /> 
  </div> 
  <p> </p>
  <div align="CENTER"> 
   <p><a name="fig:clustfg4"></a><a name="p:clustfg4"></a></p>
   <img width="554" height="743" border="0" src="img1441.png" alt="\begin{figure}
% latex2html id marker 24554
\par
\psset{unit=0.75cm}
\par
\begin...
...n
as X's in the top four panels) converges after nine iterations. }
\end{figure}" /> 
  </div> The first step of 
  <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is to select as initial cluster centers 
  <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" /> randomly selected documents, the 
  <a name="24716"></a> 
  <i>seeds</i> . The algorithm then moves the cluster centers around in space in order to minimize RSS. As shown in Figure 
  <a href="#fig:clusttb2">16.5</a> , this is done iteratively by repeating two steps until a stopping criterion is met: reassigning documents to the cluster with the closest centroid; and recomputing each centroid based on the current members of its cluster. Figure 
  <a href="#fig:clustfg4">16.6</a> shows snapshots from nine iterations of the 
  <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means algorithm for a set of points. The ``centroid'' column of Table 
  <a href="cluster-labeling-1.html#tab:clabels">17.2</a> (page 
  <a href="cluster-labeling-1.html#p:clabels">17.2</a> ) shows examples of centroids. 
  <p> <a name="p:kmeanstermination"></a> We can apply one of the following termination conditions. </p>
  <ul> 
   <li>A fixed number of iterations <img width="11" height="32" align="MIDDLE" border="0" src="img1399.png" alt="$I$" /> has been completed. This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations. </li> 
   <li>Assignment of documents to clusters (the partitioning function <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" />) does not change between iterations. Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long. </li> 
   <li>Centroids <img width="20" height="32" align="MIDDLE" border="0" src="img1442.png" alt="$\vec{\mu}_k$" /> do not change between iterations. This is equivalent to <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> not changing (Exercise <a href="cluster-cardinality-in-k-means-1.html#ex:kmeansconvergence">16.4.1</a> ). </li> 
   <li><a name="p:kneecrit"></a> Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. In practice, we need to combine it with a bound on the number of iterations to guarantee termination. </li> 
   <li>Terminate when the decrease in RSS falls below a threshold <img width="12" height="31" align="MIDDLE" border="0" src="img425.png" alt="$\theta$" />. For small <img width="12" height="31" align="MIDDLE" border="0" src="img425.png" alt="$\theta$" />, this indicates that we are close to convergence. Again, we need to combine it with a bound on the number of iterations to prevent very long runtimes. </li> 
  </ul> 
  <p> We now show that <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means converges by proving that <img width="31" height="32" align="MIDDLE" border="0" src="img1443.png" alt="$\mbox{RSS}$" /> monotonically decreases in each iteration. We will use <i>decrease</i> in the meaning <i>decrease or does not change</i> in this section. First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to <img width="31" height="32" align="MIDDLE" border="0" src="img1443.png" alt="$\mbox{RSS}$" /> decreases. Second, it decreases in the recomputation step because the new centroid is the vector <img width="12" height="32" align="MIDDLE" border="0" src="img433.png" alt="$\vec{v}$" /> for which <img width="37" height="32" align="MIDDLE" border="0" src="img1444.png" alt="$\mbox{RSS}_k$" /> reaches its minimum. <br /> </p>
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray}
\mbox{RSS}_k(\vec{v}) & = & \sum_{\vec{x} \in \omega_k} | \vec{v}-\vec{x}|^2 =  \sum_{\vec{x} \in \omega_k} \sum_{m=1}^M (v_{m}-x_{m})^2\\
\frac{\partial \mbox{RSS}_k(\vec{v})} {\partial v_m}  & = & 
\sum_{\vec{x} \in \omega_k} 2 (v_{m}-x_{m})
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="60" height="33" align="MIDDLE" border="0" src="img1445.png" alt="$\displaystyle \mbox{RSS}_k(\vec{v})$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="248" height="64" align="MIDDLE" border="0" src="img1446.png" alt="$\displaystyle \sum_{\vec{x} \in \omega_k} \vert \vec{v}-\vec{x}\vert^2 = \sum_{\vec{x} \in \omega_k} \sum_{m=1}^M (v_{m}-x_{m})^2$" /></td> 
      <td width="10" align="RIGHT"> (192)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="72" height="56" align="MIDDLE" border="0" src="img1447.png" alt="$\displaystyle \frac{\partial \mbox{RSS}_k(\vec{v})} {\partial v_m}$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="115" height="54" align="MIDDLE" border="0" src="img1448.png" alt="$\displaystyle \sum_{\vec{x} \in \omega_k} 2 (v_{m}-x_{m})$" /></td> 
      <td width="10" align="RIGHT"> (193)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> where 
  <img width="23" height="32" align="MIDDLE" border="0" src="img1449.png" alt="$x_m$" /> and 
  <img width="22" height="32" align="MIDDLE" border="0" src="img1450.png" alt="$v_m$" /> are the 
  <img width="28" height="37" align="MIDDLE" border="0" src="img1451.png" alt="$m^{th}$" /> components of their respective vectors. Setting the partial derivative to zero, we get: 
  <br /> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray}
v_m = \frac{1}{|\omega_k|} \sum_{\vec{x} \in \omega_k} x_{m}
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="131" height="54" align="MIDDLE" border="0" src="img1452.png" alt="$\displaystyle v_m = \frac{1}{\vert\omega_k\vert} \sum_{\vec{x} \in \omega_k} x_{m}$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (194)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> which is the componentwise definition of the centroid. Thus, we minimize 
  <img width="37" height="32" align="MIDDLE" border="0" src="img1444.png" alt="$\mbox{RSS}_k$" /> when the old centroid is replaced with the new centroid. 
  <img width="31" height="32" align="MIDDLE" border="0" src="img1443.png" alt="$\mbox{RSS}$" />, the sum of the 
  <img width="37" height="32" align="MIDDLE" border="0" src="img1444.png" alt="$\mbox{RSS}_k$" />, must then also decrease during recomputation. 
  <p> Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum. Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids. Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost. </p>
  <p> </p>
  <div align="CENTER"> 
   <p><a name="fig:clustfg5"></a><a name="p:clustfg5"></a></p>
   <img width="556" height="229" border="0" src="img1453.png" alt="\begin{figure}
% latex2html id marker 24763
\psset{unit=0.75cm}
\begin{pspicture...
...{d_1,d_2,d_4,d_5\}, \{d_3,d_6\}\}$, the global optimum for $K=2$.
}
\end{figure}" /> 
  </div> 
  <p> While this proves the convergence of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means, there is unfortunately no guarantee that a <i>global minimum</i> in the objective function will be reached. This is a particular problem if a document set contains many <a name="24781"></a> <i>outliers</i> , documents that are far from any other documents and therefore do not fit well into any cluster. Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations. Thus, we end up with a <a name="24783"></a> <i>singleton cluster</i> (a cluster with only one document) even though there is probably a clustering with lower RSS. Figure <a href="#fig:clustfg5">16.7</a> shows an example of a suboptimal clustering resulting from a bad choice of initial seeds. </p>
  <p> Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise <a href="exercises-3.html#ex:emptyclusters">16.7</a> ). </p>
  <p> <a name="p:seedselection"></a> Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering. Since deterministic hierarchical clustering methods are more predictable than <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means, a hierarchical clustering of a small random sample of size <img width="20" height="31" align="MIDDLE" border="0" src="img1454.png" alt="$i K$" /> (e.g., for <img width="39" height="31" align="MIDDLE" border="0" src="img1455.png" alt="$i =5$" /> or <img width="46" height="31" align="MIDDLE" border="0" src="img1456.png" alt="$i =10$" />) often provides good seeds (see the description of the Buckshot algorithm, Chapter <a href="hierarchical-clustering-1.html#ch:hierclust">17</a> , page <a href="implementation-notes-1.html#p:buckshot">17.8</a> ). </p>
  <p> Other initialization methods compute seeds that are not selected from the vectors to be clustered. A robust method that works well for a large variety of document distributions is to select <img width="8" height="31" align="MIDDLE" border="0" src="img8.png" alt="$i$" /> (e.g., <img width="46" height="31" align="MIDDLE" border="0" src="img1456.png" alt="$i =10$" />) random vectors for each cluster and use their centroid as the seed for this cluster. See Section <a href="references-and-further-reading-16.html#sec:flatclustref">16.6</a> for more sophisticated initializations. </p>
  <p> What is the time complexity of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means? Most of the time is spent on computing vector distances. One such operation costs <img width="47" height="33" align="MIDDLE" border="0" src="img1251.png" alt="$\Theta(M)$" />. The reassignment step computes <img width="29" height="32" align="MIDDLE" border="0" src="img1457.png" alt="$KN$" /> distances, so its overall complexity is <img width="72" height="33" align="MIDDLE" border="0" src="img1458.png" alt="$\Theta(KNM)$" />. In the recomputation step, each vector gets added to a centroid once, so the complexity of this step is <img width="61" height="33" align="MIDDLE" border="0" src="img1459.png" alt="$\Theta(NM)$" />. For a fixed number of iterations <img width="11" height="32" align="MIDDLE" border="0" src="img1399.png" alt="$I$" />, the overall complexity is therefore <a name="p:kmeanscomplexity"></a> <img width="79" height="33" align="MIDDLE" border="0" src="img1460.png" alt="$\Theta(IKNM)$" />. Thus, <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is linear in all relevant factors: iterations, number of clusters, number of vectors and dimensionality of the space. This means that <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is more efficient than the hierarchical algorithms in Chapter <a href="hierarchical-clustering-1.html#ch:hierclust">17</a> . We had to fix the number of iterations <img width="11" height="32" align="MIDDLE" border="0" src="img1399.png" alt="$I$" />, which can be tricky in practice. But in most cases, <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means quickly reaches either complete convergence or a clustering that is close to convergence. In the latter case, a few documents would switch membership if further iterations were computed, but this has a small effect on the overall quality of the clustering. </p>
  <p> There is one subtlety in the preceding argument. Even a linear algorithm can be quite slow if one of the arguments of 
   <!-- MATH
 $\Theta(\ldots)$
 --> <img width="49" height="33" align="MIDDLE" border="0" src="img1461.png" alt="$\Theta(\ldots)$" /> is large, and <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> usually is large. High dimensionality is not a problem for computing the distance between two documents. Their vectors are sparse, so that only a small fraction of the theoretically possible <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> componentwise differences need to be computed. Centroids, however, are dense since they pool all terms that occur in any of the documents of their clusters. As a result, distance computations are time consuming in a naive implementation of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means. However, there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities. Truncating centroids to the most significant <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> terms (e.g., <img width="65" height="31" align="MIDDLE" border="0" src="img1462.png" alt="$k=1000$" />) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in Section <a href="references-and-further-reading-16.html#sec:flatclustref">16.6</a> ). </p>
  <p> <a name="p:kmedoid"></a> The same efficiency problem is addressed by <a name="24801"></a> <i>K-medoids</i> , a variant of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means that computes medoids instead of centroids as cluster centers. We define the <a name="24804"></a> <i>medoid</i> of a cluster as the document vector that is closest to the centroid. Since medoids are sparse document vectors, distance computations are fast. </p>
  <p> </p>
  <div align="CENTER"> 
   <p><a name="fig:clustercard"></a><a name="p:clustercard"></a></p>
   <img width="338" height="307" align="BOTTOM" border="0" src="img1465.png" alt="\includegraphics[width=8cm]{kmeansknee.eps}" /> Estimated minimal residual sum of squares as a function of the number of clusters in 
   <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means. In this clustering of 1203 Reuters-RCV1 documents, there are two points where the 
   <!-- MATH
 $\widehat{\mbox{RSS}}_{min}$
 --> 
   <img width="53" height="39" align="MIDDLE" border="0" src="img1463.png" alt="$\widehat{\mbox{RSS}}_{min}$" /> curve flattens: at 4 clusters and at 9 clusters. The documents were selected from the categories China, Germany, Russia and Sports, so the 
   <img width="45" height="32" align="MIDDLE" border="0" src="img1464.png" alt="$K=4$" /> clustering is closest to the Reuters classification. 
  </div> 
  <p> <br /></p>
  <hr /> 
  <!--Table of Child-Links--> 
  <a name="CHILD_LINKS"><strong>Subsections</strong></a> 
  <ul> 
   <li><a name="tex2html4227" href="cluster-cardinality-in-k-means-1.html">Cluster cardinality in K-means</a> </li>
  </ul> 
  <!--End of Table of Child-Links--> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html4225" href="cluster-cardinality-in-k-means-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4219" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4213" href="evaluation-of-clustering-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4221" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4223" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4226" href="cluster-cardinality-in-k-means-1.html">Cluster cardinality in K-means</a> 
  <b> Up:</b> 
  <a name="tex2html4220" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4214" href="evaluation-of-clustering-1.html">Evaluation of clustering</a> &nbsp; 
  <b> <a name="tex2html4222" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4224" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>
 11.1 Introduction to Information System Evaluation  In recent years the evaluation of Information Retrieval Systems and techniques for indexing, sorting, searching and retrieving information have become increasingly important (Saracevic-95).  This growth in interest is due to two major  reasons: the growing number of retrieval systems being used and additional focus on 258                                                                                               Chapter 11  evaluation methods themselves. The Internet is an example of an information space (infospace) whose text content is growing exponentially along with products to find information for value. Information retrieval technologies are the basis behind the search of information on the Internet. In parallel with the commercial interest, the introduction of a large standardized test database and a forum for yearly analysis via TREC has provided a methodology for evaluating the performance of algorithms and systems. There are many reasons to evaluate the effectiveness of an Information Retrieval System (Belkin-93, Callan-93):  To aid in the selection of a system to procure  To monitor and evaluate system effectiveness  To evaluate query generation process for improvements  To provide inputs to cost-benefit analysis of an information system  To determine the effects of changes made to an existing information  system.  From an academic perspective, measurements are focused on the specific effectiveness of a system and usually are applied to determining the effects of changing a system's algorithms or comparing algorithms among systems. From a commercial perspective, measurements are also focused on availability and reliability. In an operational system there is less concern over 55 per cent versus 65 per cent precision than 90 per cent versus 80 per cent availability. For academic purposes, controlled environments can be created that minimize errors in data. In operational systems, there is no control over the users and care must be taken to ensure the data collected are meaningful.  The most important evaluation metrics of information systems will always be biased by human subjectivity. This problem arises from the specific data collected to measure the user resources in locating relevant information. Metrics to accurately measure user resources expended in information retrieval are inherently inaccurate. A factor in most metrics in determining how well a system is working is the relevancy of items. Relevancy of an item, however, is not a binary evaluation, but a continuous function between an item's being exactly what is being looked for and its being totally unrelated. To discuss relevancy, it is necessary to define the context under which the concept is used. From a human judgment standpoint, relevancy can be considered:  t  Subjective                        - depends upon a specific user's judgment  Situational                        - relates to a user's requirements  Cognitive                         - depends on human perception and behavior  Temporal                         - changes over time  Measurable                      - observable at a points in time  The subjective nature of relevance judgments has been documented by Saracevic  and was shown in TREC-experiments (Harman-95, Saracevic-91). In TREC-2 and Information System Evaluation                                                                  259  TREC-3, two or three different users were given the same search statement and the same set of possible hits to judge as relevant or not. In general, there was a unanimous agreement on 70-80 per cent of the items judged by the human. Even in this environment (i.e., where the judges are not the creators of the query and are making every effort to be unbiased) there is still significant subjective disagreement on the relevancy of items. In a dynamic environment, each user has his own understanding of the requirement and the threshold on what is acceptable (see Chapter 1). Based upon his cognitive model of the information space and the problem, the user judges a particular item. Some users consider information they already know to be non-relevant to their information need. For example, a user being presented with an article that the user wrote does not provide "new" relevant information to answer the user's query, although the article may be very relevant to the search statement. Also the judgment of relevance can vary over time. Retrieving information on an "XT" class of PCs is not of significant relevance to personal computers in 1996, but would have been valuable in 1992. Thus, relevance judgment is measurable at a point in time constrained by the particular users and their thresholds on acceptability of information.  Another way of specifying relevance is from information, system and situational views. The information view is subjective in nature and pertains to human judgment of the conceptual relatedness between an item and the search. It involves the user's personal judgment of the relevancy (aboutness) of the item to the user's information need. When reference experts (librarians, researchers, subject specialists, indexers) assist the user, it is assumed they can reasonably predict whether certain information will satisfy the user's needs. Ingwersen categorizes the information view into four types of "aboutness" (Ingwersen-92):  Author Aboutness - determined by the author's language as matched by the system in natural language retrieval  Indexer Aboutness - determined by the indexer's transformation of the  author's natural language into a controlled vocabulary  Request Aboutness - determined by the user's or intermediary's processing of a seafth statement into a query  User Aboutness - determined by the indexer's attempt to represent the  document according to presupposition about what the user will want to knowIn this context, the system view relates to a match between query terms and terms within an item. It can be objectively observed, manipulated and tested without relying on human judgment because it uses metrics associated with the matching of the query to the item (Barry-94, Schamber-90).   The semantic relatedness between queries and items is assumed to be inherited via the index terms that represent the 260  Chapter 11  semantic content of the item in a consistent and accurate fashion.  Other aspects of the system view are presented in Section 11.2.  The situation view pertains to the relationship between information and the user's information problem situation. It assumes that only users can make valid judgments regarding the suitability of information to solve their information need. Lancaster and Warner refer to information and situation views as relevance and pertinence respectively (Lancaster-93). Pertinence can be defined as those items that satisfy the user's information need at the time of retrieval. The TRECevaluation process uses relevance versus pertinence as its criteria for judging items because pertinence is too variable to attempt to measure in meaningful items (i.e., it depends on each situation).   
 11.2 Measures Used in System Evaluations  To define the measures that can be used in evaluating Information Retrieval Systems, it is useful to define the major functions associated with identifying relevant items in an information system (see Figure 11.1). Items arrive  ITEMS ARE INDEXED INTO SYSTEM  QUERY HITS RETURNED  SYSTEM EXECUTES  QUERY AGAINST  INDEX  Search Statement  it File  USER CREATES  SEARCH STATEMENT  USER REVIEWS HITS FROM QUERY  Figure 11.1 Identifying Relevant Items  in the system and are automatically or manually transformed by "indexing" into searchable data structures. The user determines what his information need is and creates a search statement. The system processes the search statement, returning potential hits. The user selects those hits to review and accesses them.  Measurements can be made from two perspectives: user perspective and system perspective. The user perspective was described in Section 11.1. The Author's Aboutness occurs as part of the system executing the query against the Information System Evaluation                                                                   261  index. The Indexer Aboutness and User Aboutness occur when the items are indexed into items are indexed into the system. The Request Aboutness occurs when the user creates the search statement. The ambiguities in the definition of what is relevant occurs when the user is reviewing the hits from the query.  Typically, the system perspective is based upon aggregate functions, whereas the user takes a more personal view. If a user's PC is not connecting to the system, then, from that user's view the system is not operational. From the system operations perspective, one user not having access out of 100 users still results in a 99 per cent availability rate. Another example of how averaging distorts communications between the system and user perspective is the case where there are 150 students taking six courses. Assume there are 5 students in three of the courses and 45 students in the other three courses. From the system perspective there is an average of 25 students per instructor/course. For 10 per cent of the students (15 students) there is a great ratio of 10 students per instructor. But, 90 per cent of the users (students) have a ratio of 45 students to one instructor. Thus most of the users may complain of the poor ratio (45 to one) to a system person who claims it is really good (25 to one). Techniques for collecting measurements can also be objective or subjective. An objective measure is one that is well-defined and based upon numeric values derived from the system operation. A subjective measure can produce a number, but is based upon an individual users judgments.  Measurements with automatic indexing of items arriving at a system are derived from standard performance monitoring associated with any program in a computer (e.g., resources used such as memory and processing cycles) and time to process an item from arrival to availability to a search process. When manual indexing is required, the measures are then associated with the indexing process. The focus of the metrics is on the resources required to perform the indexing function since this is the major system overhead cost. The measure is usually defined in terms of time to index an item. The value is normalized by the exhaustivity and specificity (see Chapter 3) requirements. Ajiother measure in both the automatic and manual indexing process is the completeness and accuracy of the indexes created. These are evaluated by random sampling of indexes by quality assurance personnel.  A more complex area of measurements is associated with the search process. This is associated with a user creating a new search or modifying an existing query. In creating a search, an example of an objective measure is the time required to create the query, measured from when the user enters into a function allowing query input to when the query is complete. Completeness is defined as when the query is executed. Although of value, the possibilities for erroneous data (except in controlled environments) are so great that data of this nature are not collected in this area in operational systems. The erroneous data comes from the user performing other activities in the middle of creating the search such as going to get a cup of coffee.  Response time is a metric frequently collected to determine the efficiency of the search execution. Response time is defined as the time it takes to execute the search.  The ambiguity in response time originates from the possible definitions of 262                                                                                               Chapter 11  the end time. The beginning is always correlated to when the user tells the system to begin searching. The end time is affected by the difference between the user's view and a system view. From a user's perspective, a search could be considered complete when the first result is available for the user to review, especially if the system has new items available whenever a user needs to see the next item. From a system perspective, system resources are being used until the search has determined all hits. To ensure consistency, response time is usually associated with the completion of the search. This is one of the most important measurements in a production system. Determining how well a system is working answers the typical concern of a user: "the system is working slow today."  It is difficult to define objective measures on the process of a user selecting hits for review and reviewing them. The problems associated with search creation apply to this operation. Using time as a metric does not account for reading and cognitive skills of the user along with the user performing other activities during the review process. Data are usually gathered on the search creation and Hit file review process by subjective techniques, such as questionnaires to evaluate system effectiveness.  In addition to efficiency of the search process, the quality of the search results are also measured by precision and recall. Precision is a measure of the accuracy of the search process. It directly evaluates the correlation of the query to the database and indirectly is a measure of the completeness of the indexing algorithm. If the indexing algorithm tends to generalize by having a high threshold on the index term selection process or by using concept indexing, then precision is lower, no matter how accurate the similarity algorithm between query and index. Recall is a measure of the ability of the search to find all of the relevant items that are in the database. The following are the formulas for precision and recall:  Number _ Re trieved _ Re levant  Precision = 贸7:----;------贸----;贸r-----------:贸  Number _ Total_ Re trieved  Number   Re trieved  Re levant  Recall =  Number   Possible   Re levant  where Number JPossihle_Relevant is the number of relevant items in the database, Number_Retrieved_Relevant is the number of relevant items in the Hit file, and Number_TotalJ(etrieved is the total number of items in the Hit File. In controlled environments it is possible to get values for both of these measures and relate them to each other. Two of the values in the formulas, Number_Retrieved_Relevant and NumberJTotal^Retrieved, are always available. Number JPossible-Relevant poses a problem in uncontrolled environments because it suggests that all relevant items in the database are known. This was possible with very small databases in some of the early experiments in information systems. To gain the insights associated with testing a search against a large database makes collection of this data almost Information System Evaluation                                                                  263  impossible. Two approaches have been suggested. The first is to use a sampling technique across the database, performing relevance judgments on the returned items. This would form the basis for an estimate of the total relevant items in the database (Gilbert-79). The other technique is to apply different search strategies to the same database for the same query. An assumption is then made that all relevant items in the database will be found in the aggregate from all of the searches (Sparck Jones-75). This later technique is what is applied in the TREC-experiments. In this controlled environment it is possible to create Precision/Recall graphs by reviewing the Hit file in ranked order and recording the changes in precision and recall as each item is judged.  In an operational system it is unrealistic to calculate recall because there is no reasonable approach to determine Number_Possible_Relevant. It is possible, however, to calculate precision values associated with queries, assuming the user provides relevance judgments. There is a pragmatic modification that is required to the denominator factor of Number_TotalJRetrieved The user can not be forced to review all of the items in the Hit file. Thus, there is a likely possibility that there will be items found by the query that are not retrieved for review. The adjustment to account for this operational scenario is to redefine the denominator to Number_Total__Reviewed versus Nnumber_Total_Retrieved. Under this condition the Precision factor becomes the precision associated with satisfying the user's information need versus the precision of the query. If reviewing three relevant items satisfies the user's objective in the search, additional relevant items in a Hit file do not contribute to the objective of the information system. The other factor that needs to be accounted for is the user not reviewing items in the Hit file because the summary information in the status display is sufficient to judge the item is not likely to be relevant. Under this definition, precision is a more accurate measure of the use of the user's time.  Although precision and recall formed the initial basis for measuring the effectiveness of information systems, they encounter mathematical ambiguities and a lack of parallelism between their properties (Salton-83). In particular, what is the value of recall if there are no relevant items in the database or recall if no items are retrieved (Fairthorne-64, Robertson-69)? In both cases the mathematical formula becomes 0/0. The lack of parallelism comes from the intuitiveness that finding more relevant items should increase retrieval effectiveness measures and decrease with retrieval of non-relevant items. Recall is unaffected when non-relevant items are retrieved. Another measure that is directly related to retrieving non-relevant items can be used in defining how effective an information system is operating. This measure is called Fallout and defined as (Salton-83):  Number _ Retrieved _ Nonrelevant  Fallout = 贸~-贸-------贸贸露贸贸------------------- Number _ Total _ Nonre levant  where Number JTotal_Nonreievant is the total number of non-relevant items in the database. Fallout can be viewed as the inverse of recall and will never encounter the  situation of 0/0 unless all the items in the database are relevant to the search. It can 264                                                                                              Chapter 11  be viewed as the probability that a retrieved item is non-relevant. Recall can be viewed as the probability that a retrieved item is relevant. From a system perspective, the ideal system demonstrates maximum recall and minimum fallout. This combination implicitly has maximum precision. Of the three measures (precision, recall and fallout), fallout is least sensitive to the accuracy of the search process. The large value for the denominator requires significant changes in the number of retrieved items to affect the current value. Examples of precision, fallout and recall values for systems tested in TREC-4 are given in Section 11.3.  There are other measures of search capabilities that have been proposed. A new measure that provides additional insight in comparing systems or algorithms is the "Unique Relevance Recall" (URR) metric. URR is used to compare more two or more algorithms or systems. It measures the number of relevant items that are retrieved by one algorithm that are not retrieved by the others:  Number_ unique  relevant  Unique_Relevance_Recall =-------------------------=------------ Number _ relevant  Numberjunique_relevant is the number of relevant items retrieved that were not retrieved by other algorithms. When many algorithms are being compared, the definition of uniquely found items for a particular system can be modified, allowing a small number of other systems to also find the same item and still be considered unique. This is accomplished by defining a percentage (Pu) of the total number of systems that can find an item and still consider it unique. Number_relevant can take on two different values based upon the objective of the evaluation:  VALUE                                        INTERPRETATION  Total Number Retrieved               the total number of relevant items found by all  Relevant   (TNRR)                       algorithms  Total Unique Relevant                  the total number of unique items found by all  Retrieved (TURR)                       the algorithms  ABCDEFGHI          JKLM  3       4       2       22      1        100     200    22      100      10     500    6       15 Figure 11.2a Number Relevant Items information System Evaluation  265  Algorithm I  Algorithm II  Algorithm III  Figure 11.2b Four Algorithms With Overlap of Relevant Retrieved  Using TNRR as the denominator provides a measure for an algorithm of the percent of the total items that were found that are unique and found by that algorithm. It is a measure of the contribution of uniqueness to the total relevant items that the algorithm provides. Using the second measure, TURR, as the denominator, provides a measure of the percent of total unique items that could be found that are actually found by the algorithm. Figure 11.2a and 11.2b provide an example of the overlap of relevant items assuming there are four different algorithms. Figure 11.2a gives the number of items in each area of the overlap diagram in Figure 11.2b. If a relevant item is found by only one or two techniques as a "unique item," then from the diagram the following values URR values can be produced:  Algorithm I Algorithm II Algorithm III Algorithm IV  -  6 unique items (areas A, C, E)  -  16 unique items (areas B, C, J)  -  29 unique items (areas E, H, L)  -  31 unique items (areas J, L, M)  TURR =  Algorithm  Algorithm I Algorithm II  URRjnrr  URRturr  6/985 = .0061     6/61 =    .098 16/985 = .0162    16/61=   .262 266                                                                                              Chapter  Algorithm III                    29/985 = .0294    29/61 =   .475  AlgorithmlV                    31/985 = .0315    31/61 =.508  The URR value is used in conjunction with Precision, Recall and Fallout to determine the total effectiveness of an algorithm compared to other algorithms. The URRtnrr value indicates what portion of all unique items retrieved by all of the algorithms was retrieved by a specific algorithm. The URRturr value indicates the portion of possible unique items that a particular algorithm found. In the example, Algorithm IV found 50 per cent of all unique items found across all the algorithms. The results indicate that if I wanted to increase my recall by running two algorithms, I would choose algorithm III or IV in addition to the algorithm with the highest recall value. Like Precision, URR can be calculated since it is based upon the results of retrieval versus results based upon the complete database. It assists in determining the utility of using multiple search algorithm to improve overall system performance (see Chapter 7).  Other measures have been proposed for judging the results of searches (Keen-71,Salton-83):  Novelty Ratio: ratio of relevant and not known to the user to total relevant retrieved  Coverage Ratio: ratio of relevant items retrieved to total relevant by the user before the search  Sought Recall: ratio of the total relevant reviewed by the user after the  search to the total relevant the user would have liked to examine  In some systems, programs filter text streams, software categorizes data or intelligent agents alert users if important items are found. In these systems, the Information Retrieval System makes decisions without any human input and their decisions are binary in nature (an item is acted upon or ignored). These systems are called binary classification systems for which effectiveness measurements are created to determine how algorithms are working (Lewis-95). One measure is the utility measure that can be defined as (Cooper-73):  U = a*(Relevant_Retrieved) + p*(Non-Relevant_Not Retrieved) 8*(Non-Re!evant_Retrieved) - y*(Relevant_Not Retrieved)  where a and P are positive weighting factors the user places on retrieving relevant items and not retrieving non-relevant items while 5 and y are factors associated with the negative weight of not retrieving relevant items or retrieving non-relevant items.    This formula can be simplified to account only for retrieved items with p  and y equal to zero (Lewss-96). Another family of effectiveness measures called the E-measure that combines recall and precision into a single score was proposed by Van Rijsbergen (Rijsbergen-79).  
foa-0001	1  Overview  ^r^v4      l     ¶  "Its about nothing."  Finding Out About. Reproduced by permission of The New Yorker *  "What's the final episode  of "Seinfeld' abovtV
foa-0002	1.1 Finding Out About - A Cognitive Activity  We are all forced to make decisions regularly, sometimes on the spur of  the moment. But the rest of the time we have enough warning that it is possible to collect our thoughts and do some research that makes our  * Robert Mankoff, 0 The New Yorker, 26 January 1998. FINDING OUT ABOUT  decision as sound as it can be. This book is a closer look at the process of finding out about (FOA), research activities that allow a decision-maker to draw on others' knowledge. It is written from a technical perspective, in terms of computational tools that speed the FOA activity in the modern era of the distributed networks of knowledge collectively known as the World Wide Web (WWW). It shows you how to build many of the tools that are useful for searching collections of text and other media. The primary argument advanced is that progress requires that we appreciate the cognitive foundation we bring to this task as academics, as language users, and even as adaptive organisms.  As organisms, we have evolved a wide range of strategies for seeking useful information about our environment. We use the term "cognitive" to highlight the use of internal representations that help even the simplest organisms perceive and respond to their world; as the organisms get less simple, their cognitive structures increase in complexity. Whether done by simple or complex organisms, however, the process of finding out about is a very active one - making initial guesses about good paths, using complex sets of features to decide if we seem to be on the right path, and proceeding forward.  As humans, we are especially expert at searching through one of the most complex environments of all: language. Its system of linguistic features is not derived from the natural world, at least not directly. It is a constructed, cultural system that has worked well since (by definition! ) prehistoric times. In part, languages remain useful because they are capable of change when necessary. New features and new objects are noticed, and it becomes necessary for us to express new things about them, to form our reactions to them, and to express these reactions to one another.  Our first experience of language, as children and as a species, was oral - we spoke and listened. As children we learn Sprachspiele (word or language games) [Wittgenstein, 1953] - how to use language to get what we want. A baby saying "Juice!" is using the exclamation as a tool to make adults move; that's what a word means. Such a functional notion of language, in terms of the jobs it accomplishes, will prove central to our conception of what keywords in documents and queries mean as part of the FOA task.  Beyond the oral uses of language, as a species we have also learned the advantages of writing down important facts we might otherwise forget. OVERVIEW  Writing down a list of things to do, which we might forget tomorrow, extends our limited memory. Some of these advantages accrue to even a single individual: We use language personally, to organize our thoughts and to conceive strategies.  Even more important, we use writing to say things to others. Writing down important, memorable facts in a consistent, conventional manner, so that others can understand what we mean and vice versa, further amplifies the linguistic advantage. As a society, we value reading and writing skills because they let us interpret shared symbols and coordinate our actions. In advanced cultures' scholarship, entire curricula can be defined in terms of what Robert McHenry (Editor-in-Chief of Encyclopedia Britannica) calls "KnowingHow to Know"1  It is easiest to think of the organism's or human's search as being for a valuable object, sweet pieces of fruit in the jungle, or (in modern times) a grocer that sells them. But as language has played an increasingly important role in our society, searching for valuable written passages becomes an end unto itself. Especially as members of the academic community, we are likely to go to libraries seeking others' writings as part of our search. Here we find rows upon rows of books, each full of facts the author thought important, and endorsed by a librarian who has selected it. The authors are typically people far from our own time and place, using language similar but not identical to our own.  Of course the library contains many such books on many, many topics. We must Find Out About a topic of special interest, looking only for those things that are relevant to our search. This basic skill is a fundamental part of an academic's job:  ï  We look for references in order to write a term paper.  ï  We read a textbook, looking for help in answering an exercise.  ï  We comb through scientific journals to see if a question has already been answered.  We know that if we find the right reference, the right paper, the right paragraph, our job will be made much easier. Language has become not only the means of our search, but its object as well  wwwjustanother. com/howtoknow FINDING OUT ABOUT  Today we can also search the World Wide Web (WWW) for others' opinions of music, movies, or software. Of course these examples are much less of an "academic exercise"; Finding Out About such information commodities, and doing it consistently and well, is a skill on which the modern information society places high value indeed. But while the infrastructure forming the modern WWW is quite recent, the promise offered by truly connecting all the world's knowledge has been anticipated for some time, for example, by H. G. Wells [Wells, 1938].  Many of the FOA searching techniques we will discuss in this text have been designed to operate on vast collections of apparently "dead" linguistic objects: files full of old email messages, CD-ROMs full of manuals or literature, Web servers fiill of technical reports, and so on. But at their core, each of these collections is evidence of real, vital attempts to communicate. Typically an author (explicitly or implicitly) anticipates the interests of some imagined audience and produces text that is a balance between what the author wants to say and what he or she thinks the audience wants to hear. A textual corpus will contain many such documents, written by many different authors, in many styles and for many different purposes. A person searching through such a corpus comes with his or her own purposes and may well use language in a different way from any of the authors. But each individual linguistic expression - the authors' attempts to write, the searchers' attempts to express their questions and then read the authors' documents - must be appreciated for the word games [Wittgenstein, 1953] that they are. FOA is centrally concerned with meaning: the semantics of the words, sentences, questions, and documents involved. We cannot tell if a document is about a topic unless we understand (at least something of) the semantics of the document and the topic. This is the notion of about-ness most typical within the tradition of library science [Hutchins, 1978].  This means that our attempts to engineer good technical solutions must be informed by, and can contribute to, a broader philosophy of language. For examples it will turn out that FOA's concern with the semantics of entire documents is well complemented by techniques from computational linguistics, which have tended to focus on syntactic analysis of individual sentences. But even more exciting is the fact that the recent OVERVIEW       5  Questioner  Question answerer  Question  Answer  Assessment  FIGURE 1.1 The FOA Conversation Loop.  availability of new types of electronic artifacts - from email messages and WWW corpora to the browsing behaviors of millions of users all trying to FOA - brings an empirical grounding for new theories of language that may well be revolutionary.  At its core, the FOA process of browsing readers can be imagined to involve three phases:  1.  asking a question;  2.  constructing an answer; and  3.  assessing the answer.  This conversational loop is sketched in Figure 1.1.  Step 1. Asking a Question  The first step is initiated by people who (anticipating our interest in building a search engine) we'll call users, and their questions. We don't know a lot about these people, but we do know they are in a particular frame of mind, a special cognitive state; they may be aware^ of a specific  gap in their knowledge (or they be only vaguely puzzled), and they're motivated to fill it. They want to FOA ... some topic.  Supposing for a moment that we were there to ask, the users may not  even be able to characterize the topic, that is, to articulate their knowledge gap. More precisely, they may not be able to fully define characteristics of the "answer" they seek. A paradoxical feature of the FOA problem is  Meta-cognition about  ignorance FINDING OUT ABOUT  that if users knew their question, precisely, they might not even need the search engine we are designing: Forming a clearly posed question is often the hardest part of answering it! In any case, well call this somewhat befuddled but not uncommon cognitive state the users' information need.  While a bit confused about their particular question, the users are not without resources. First, they can typically take their ill-defined, internal cognitive state and turn it into an external expression of their question, in some language. We'll call their expression the query, and the language in which it is constructed the query language.  Step 2. Constructing an Answer  So much for the source of the question; whence the answer? If the question is being asked of a person, we must worry about equally complex characteristics of the answerer's cognitive state:  ï  Can they translate the user's ill-formed question into a better one?  ï  Do they know the answer themselves?  ï  Are they able to verbalize this answer?  ï  Can they give the answer in terms the user will understand?  ï  Can they provide the necessary background knowledge for the user to understand the answer itself?  We will refer to the question-answerer as the search engine, a computer program that algorithmically performs this task. Immediately each  of the concerns (just listed) regarding the human answerer's cognitive state translates into extremely ambitious demands we might make of our computer system.  Throughout most of this book, we will avoid such ambitious issues and instead consider a very restricted form of the FOA problem: We will assume that the search engine has available to it only a set of preexisting, "canned" passages of text and that its response is limited to identifying one or more of these passages and presenting them to the users; see Figure 1.2. We will call each of these passages a document and the entire set of documents the corpus. Especially when the corpus is very large (e.g., assume it contains millions or even billions of documents), selecting a very small set (say 10 to 20) of these as potentially good answers to OVERVIEW  Corpus FIGURE 1.2 Retrieval of Documents in Response to a Query  be retrieved will prove sufficiently difficult (and practically important) that we will focus on it for the first few chapters of this book. In the final chapters however, we will consider how this basic functionality can be extended towards tools for "Searching for an education" (cf. Section 8.3.9).  Step 3. Assessing the Answer  Imagine a special instance of the FOA problem: You are the user, waiting in line to ask a question of a professor. You're confused about a topic that is sure to be on the final exam. When you finally get your chance to ask your question, we'll assume that the professor does nothing but select the three or four preformed pearls of wisdom he or she thinks come closest to your need, delivers these "documents," and sends you on your way. "But wait!" you want to say. "That isn't what I meant" Or, "Let me ask it another way." Or, "That helps, but I still have this problem."  The third and equally important phase of the FOA process "closes the loop" between asker and answerer, whereby the user (asker) provides an assessment of how relevant they find the answer provided. If after your first question and the professor's initial answer you are summarily ushered out of the office, you have a perfect right to be angry because the FOA process has been violated. FOA is a dialog between asker and answerer; it does not end with the search engine's first delivery of an answer. This initial exchange is only the first iteration of an ongoing conversation by which asker and answerer mutually negotiate a satisfactory exchange. In the process, the asker may recognize elements of the answer he or she 8       FINDING OUT ABOUT  FIGURE 1.3 Assessment of the Retrieval  What FOA data can we observe?  seeks and be able to reexpress the information need in terms of threads taken from previous answers.  Because the question-answerer has been restricted to a simple set of documents, the asker's relevance feedback must be similarly constrained; for each of the documents retrieved by the search engine, the asker reacts by saying whether or not the document is relevant. Returning to the student/professor scenario, we can imagine this as the student saying "Thanks, that helps" after those pearls that do and remaining silent or saying, "Huh?" or "What does that have to do with anything?!" or "No, that's not what I meant!" otherwise. More precisely, relevance feedback gives askers the opportunity to provide more information with their reaction to each retrieved document - whether it is relevant (©), irrelevant (©), or neutral (#). This is shown as a Venn diagram-like labeling of the set of retrieved documents in Figure 1.3. We'll worry about just how to solicit and make use of relevance feedback judgments in Chapter 4.t
foa-0003	1.1.1 Working within the IR Tradition  If it seems to you that the last section has sidestepped many of the most difficult issues underlying FOA, you're right! Later chapters will return to redress some of these omissions, but the immediate goal of Chapters 2 to 4 is to "operationalize'3 FOA to resemble a well-studied problem within computer science, typically referred to as information retrieval (IR). IR is a field that has existed since computers were first OVERVIEW       9  used to count words [Belkin and Croft, 1987]. Even earlier, the related discipline of library science had developed many automated techniques for efficiently storing, cataloging, and retrieving physical materials so that browsing patrons could find them; many of these methods can be applied to the digital documents held within computers. IR has also borrowed heavily from the field of linguistics, especially computational linguistics.  The primary journals in the field and most important conferences^ Other places to in IR have continued to publish and meet since the 1960s, but the field has taken on new momentum within the last decade. Computers capable of searching and retrieving from the entire biomedical literature, across an entire nation's judicial system, or from all of the major newspaper and magazine articles, have created new markets among doctors, lawyers, journalists, students, everyone! And of course, the Internet, within just a few years, has generated many, many other examples of textual collections and people interested in searching through them.  The long tradition of IR is therefore the primary perspective from which we will approach FOA. Of course, every tradition brings with it tacit assumptions and preconceived notions that can hinder progress. In some ways, an elementary school student using the Internet to FOA class materials is related to the original problem considered by library science and IR, but in many other ways it couldn't be more different (cf. Section 8.1). In this text, "FOA" will be used to refer to the broadest characterization of the cognitive process and "IR" to this subdiscipline of computer science and its traditional techniques. When we talk of the "search engine," this is not meant to refer to any particular implementation, but to an idealized system most typical of the many different generations and varieties of actual search engines now in use. If you are using this text as part of a course, you may build one simple example of a search engine.  Using Figure 1.4 as a guide, well return to each of the three phases and be a bit more specific about each component of our search engine. Here, finally, the human question-answerer has been replaced by an algorithm, the search engine, that will attempt to accomplish the same purpose. This figure also makes clear that the fundamental operation performed by a search engine is a match between descriptive features mentioned by users in their queries and documents sharing those features. By far the most important kind of features are keywords. 10      FINDING OUT ABOUT  Query  ^    Retrieved documents  Match  Information  retrieval  system  r  Indices  Documents  FIGURE 1.4 Schematic of Search Engine
foa-0004	1.2 Keywords  Keywords are linguistic atoms - typically words, pieces of words, or phrases - used to characterize the subject or content of a document. They are pivotal because they must bridge the gap between the users' characterization of information need (i.e., their queries) and the characterization of the documents' topical focus against which these will be matched. We could therefore begin to describe them from either perspective: how they are used by users, or how they become associated with documents. We will begin with the former.
foa-0005	1.2.1 Elements of the Query Language  If the query comes from a student during office hours or from a patron at a reference librarian's desk, the query language theyll use to frame their question is entirely natural, that most expressive "mother tongue" familiar to both question-asker and -answerer. But for the software search engines we will consider, we must assume a much more constrained "artificial" query language. Like other languages, ours will have both a meaningful vocabulary - the set of important keywords any user is allowed to mention in any queries - and a syntax that allows us to construct more elaborate query structures.
foa-0006	OVERVIEW       11  1.2.2 Topical Scope  The first constraint we can apply to the set of keywords we will allow in our vocabulary is to define a domain of discourse - the subject area within which each and every user of our search engine is assumed to be searching. While we might imagine building a truly encyclopedic reference work, one capable of answering questions about any topic whatsoever, it is much more common to build a search engine with more limited goals, capable of answering questions about some particular subject. We will choose the simpler path (it will prove enough of a challenge!) and focus on a particular topic. To be concrete, throughout this text we will assume that the domain of discourse is ARTIFICIAL INTELLIGENCE (AI). Briefly, AI can be defined as a subdiscipline of computer science, especially concerned with algorithms that mimic inferences which, had they been made by a human, would be considered "intelligent." It typically includes such topics as KNOWLEDGE REPRESENTATION, MACHINE LEARNING, and ROBOTICS.  Thus COMPUTER SCIENCE is a broader term than ARTIFICIAL INTELLIGENCE. This hypernym relationship between the two phrases is something we will return to later (cf. Section 6.3). For example, our task becomes more difficult if we assume that the corpus of documents contains material on the broader topic of COMPUTER SCIENCE, rather than just (!) ARTIFICIAL INTELLIGENCE. Conversely, the topics KNOWLEDGE REPRESENTATION, MACHINE LEARNING, and ROBOTICS are all narrower terms, and our task would, caeteris parihus,* be made easier if we only had to help users FOA one of them.  Constraining the vocabulary so that it is exhaustive enough that any imaginable and relevant topic is expressible within the language, while remaining specific enough that any particular subjects a user is likely to investigate can be distinguished from others, will become a central goal of our design. ROBOTICS, for example, would seem a descriptive keyword because it identifies a relatively small subarea of ARTIFICIAL INTELLIGENCE. COMPUTER SCIENCE would be silly as a keyword (for this corpus), because we are assuming it would apply to every document and hence does nothing to discriminate them - it is too exhaustive. At the  * (Assuming) all other things are equal. 12      FINDING OUT ABOUT  Other extreme, ROBOTIC VACUUM CLEANERS FOR 747 AIRLINERS is almost certainly too specific.  The vocabulary size - the total number of keywords - depends on many factors, including the scope of the domain of discourse. A typical language user has a reading vocabulary of approximately 50,000 words. Web search engines and large test corpora formed from the union of many document types may require vocabularies ten times this size. It is unlikely that such a large lexicon of keywords would be required for restricted corpora, but it is also true that even a narrow field can develop an extensive, specialized jargon or terms of art. In practice, search engines typically have difficulty reducing the number of usable keywords to much below 10,000.
foa-0007	1.2.3 Document Descriptors  We've introduced keywords as features mentioned by users as part of their queries, but the other face of keywords is as descriptive features of documents. That is, we might naturally say that a document is about ROBOTICS. Users mentioning ROBOTICS in their query should expect to get those documents that are about this topic. Keywords must therefore also function as the documents' description language. The same vocabulary of words used in queries must be used to describe the topical content of each and every document. Keywords become our characterization of what each document is about. Indexing is the process of associating one or more keywords with each document.  The vocabulary used can either be controlled or uncontrolled (a.k.a. closed vocabularies or open vocabularies). Suppose we decide to have all the documents in our corpus manually indexed by their authors; this is quite common in many conference proceedings, for example. If we provide a list of potential keywords and tell authors they must restrict their choices to terms on this list, we are using a controlled indexing vocabulary. On the other hand, if we allow the authors to assign any terms they choose, the resulting index has an uncontrolled vocabulary [Svenonius, 1986].  To get a feel for the indexing process, imagine that you are given a piece of text and must come up with a set of keywords that describe what the document is about. Let's make the exercise more concrete. You are the author of a report entitled USIHG A MEUHAL NETWORK FOR OVERVIEW       13  PREDICTION, and you are submitting it to a journal. One of the things this particular journal requires is that the author provide up to six keywords under which this article will be indexed. If you are sending it to the Communications of the ACM, you might pick a set of keywords that identify, to the audience of computer scientists you think read this publication, connections between this new work and prior work in related areas: NONLINEAR REGRESSION; TIME SERIES PREDICTION.  But now imagine that you've decided to submit the exact same paper to Byte magazine, and you must again pick keywords that have meaning to this audience. You might choose: NEURAL NETWORKS; STOCK MARKET ANALYSIS.  What is the context in which these keywords are going to be interpreted? Who's the audience? Who's going to understand what these keywords mean? Anticipating the FOA activity in which these keywords will function, we know that the real issue to be solved is not only to describe this one document, but to distinguish it from the millions of others in the same corpus. How are the keywords chosen going to be used to distinguish your document from the others?  It is often easiest to imagine keywords as independent features of each document. In fact, however, keywords are best viewed as a relation between a document and its prospective readers, sensitive to both characteristics of the users' queries and other documents in the same corpus. In other words, the keywords you pick for Byte should be different from those you pick for Communications of the ACMgt; and for deeper reasons than what we might cynically consider "spin control."
foa-0008	1.3 Query Syntax  Keywords therefore have a special status in IR and as part of the FOA process. Not only must they be exhaustive enough to capture the entire topical scope reflected by the corpora's domain of discourse, but they must also be expressive enough to characterize any information needs the users might have.  Of course we need not restrict our users to only one of these keywords. It seems quite natural for queries to be composed of two or three, perhaps even dozens, of keywords. Recent empirical evidence suggests that many typical queries have only two or three keywords 14      FINDING OUT ABOUT  (cf. Section 8.1), but even this number provides a great combinatorial extension to the basic vocabulary of single keywords. Other applications, for example, using a document itself as a query (i.e., using it as an example: "Give me more like this"), can generate queries with hundreds of keywords. Regardless of size, queries defined only as sets of keywords will be called simple queries. Many Web search engines support only simple queries. Often, however, the search engines also provide more advanced interfaces, including operators in the query language. Perhaps, because you have previously been warped by an exposure to computer science:), you think that sets of keywords might be especially useful if joined by Boolean operators. For example, if we have one set of documents about NEURAL NETWORKS and another set of documents about SPEECH RECOGNITION, we can expect the query: NEURAL NETWORKS AND SPEECH RECOGNITION to correspond to the intersection of these two sets, while NEURAL NETWORKS OR SPEECH RECOGNITION would correspond to their union.  The Boolean NOT operator is a bit more of a problem. If users say they want things that are not about NEURAL NETWORKS, they are in fact referring to the vast majority of the corpus. That is, NOT is more appropriately considered a binary, subtraction operator. To make this distinction explicit we will call it BUT_NOT.  There are other syntactic operators that are often included in a search engine's query language, but discussion of these will be put off until later. Even with these simple Boolean connectives and a keyword vocabulary of reasonable size, users can construct a vast number of potential queries when attempting to express their information need.
foa-0009	1.3.1 Query Sessions  As we consider the specific features of each query, it is important to remember the role these short expressions play in the larger FOA process. Queries are generated as an attempt by users to express their information need. As with any linguistic expression, conveying a thought you have can be difficult, and this is likely to be especially true of the muddled cognitive state of our FOA searcher. Users who are familiar with the special syntactic features of a query language may be able to express their need more easily, but others for whom this unnatural syntax is new or  "Typical" users      difficult will have additional difficulties.1 have changed OVERVIEW       15  1                                  2  FIGURE 1.5 A Query Session  As with many of the idealizing assumptions we are at least temporarily making, it is often simpler to think about only one iteration of the three-step query/retrieve/assess FOA process at a time. In most realistic situations we can expect that single queries will not occur in isolation but as part of an iteration of the FOA process. An initial query begins the dialog; the search engine's response provides clues to the user about directions to pursue next; these are expressed as another query. An abstract view of this sequence is presented in Figure 1.5. Note especially the concatenation of a series of basic FOA three-step iterations. Data are produced by the user, then by the search engine, and then by the user; this constructs a very natural alternation of user-search engine exchanges. Users' assessments can also function as their next query statement. This can be achieved simply if we have some method for automatically constructing a query from relevance feedback. For example, if users click on documents they like, the search engine can, by itself, form a new query that focuses on those keywords that are especially associated with these documents.  There are many such techniques for using relevance feedback from a single query/retrieval, and there are many more things we can learn from the entire query session. The full query session provides more complete evidence about the users' information need than we can gain from any one query. In fact, as will be discussed extensively in Chapter 7, there exist algorithmic means by which the search engine itself might "learn" from such evidence. Learning methods might even be expected to make transitive leaps, from the users' initial expressions of their information needs to the final documents that satisfied them.* (Of course, this tran- Transitivity sitive leap is only warranted if we are certain that users ended the session 16      FINDING OUT ABOUT  satisfied and aren't just quitting in frustration!) For all these reasons, we must try to identify a query session's boundaries, that is, when one focused search session ends and the next session, involving the same user searching on a different topic, begins.
foa-0010	1.4 Documents  When "documents" were first introduced as part of the FOA process, it was as one of the set of potential, predefined answers to users' queries. Here we will ground this abstract view in practical terms that can be readily applied, for example, to the searches that are now common on the Web. Our goal will be to balance this practical description of how search engines work today with the abstract FOA view that goes beyond current practices to other kinds of searches still to come.  A useful working definition is that a document is a passage of free text. It is composed of text, strings of characters from an alphabet. We'll typically make the (English) assumption that uses the Roman alphabet, Arabic numerals, and standard punctuation. Complications like font style (italics, bold) and non-Roman marked alphabets that add characters like a, Q, ij, and ae; and the iconic characters of Asian languages require even more thought.  By "free" text we mean it is in natural language, the sort native readers and writers use easily. Good examples of free text might be a newspaper article, a journal paper, or a dictionary definition. Typically the text will be grammatically well-formed language, in part because this is written language, not oral People are more carefiil when constructing written artifacts that last beyond the moment. Informal texts like email messages, on the other hand, help to point to ways that some texts can retain the spontaneity of oral communication, for better and worse [Ong, 1982].  Finally, we will be interested in passages of such text, of arbitrary size. The newspaper example makes us imagine documents of a few thousand words, but journal articles make us think of samples ten times larger, and email messages make us think of something only a tenth that size. We can even think of an entire book as a single document. All such passages satisfy our basic definition; they might be appropriate answers to a search about some topic. OVERVIEW       17  The length of the documents will prove to be a critical issue in FOA search engine design, especially when some corpus contains documents of widely varying lengths. This is because longer documents can discuss more topics, so they are capable of being about more. Longer documents are more likely to be associated with more keywords, and hence they are more likely to be retrieved (cf. Section 3.4.2).  One possible response is to make a simple but very consequential assumption.  ASSUMPTION I    All documents have equal about-ness.  In other words, if we ask the (a priori) probability of any document in the corpus being considered relevant, we will assume that all are equiprobable. This would lead us to normalize documents' indices in some way to compensate for differing lengths. The normalization procedure is a matter of considerable debate; we will return to consider it in depth later (cf. Section 3.4.2).  For now, we will take a different tack toward the issue of document length, as captured by an alternative pair of assumptions.  ASSUMPTION 2   The smallest unit of text with appreciable about-ness is the paragraph.  ASSUMPTION 3   All manner of longer documents are constructed out of basic paragraph atoms.  The first piece of this argument is that the smallest sample of text that can reasonably be expected to satisfy an FOA request is a paragraph. The claim is that a word, even a sentence, does not by itself provide enough context for any question to be answered or "found out about." But if the paragraph has been well constructed, as defined by conventional rules of composition, it should answer many such questions. And unless the text comes from James Joyce, Proust, or Jorge Luis Borges, we can expect paragraphs to occupy about half an average screen page - nicely viewable chunks.  Assumption 3 alludes to the range of structural relationships by which the atomic paragraphs can typically be strung together to form longer passages. First and foremost is simple sequential flow, the order 18       FINDING OUT ABOUT  in which an author expects the paragraphs to be read. The sequential nature of traditional printed media, from the first papyrus scrolls to modern books and periodicals, has meant that a sequential ordering over paragraphs has been dominant. It may even be that the modern human is especially capable of understanding rhetoric of this form (cf. Section 6.2.3).  In any case, a sequential ordering of paragraphs is just one possible way they might be related. Other common relationships include:  ï  a hierarchical structure composing paragraphs into subsections, sections, and chapters;  ï footnotes, embellishing the primary theme;  ï  bibliographic citations to other, previous publications;  ï  references to other sections of the same document; and  ï pedagogical prerequisite relationships ensuring that conceptual foundations are established prior to subsequent discussion.  Of course each of these relationships has grown up within the tradition of printed publication. Special typographical conventions (boldface, italics, sub- and superscripting, margins, rules) have arisen to represent them and distinguish them from sequential flow.  But new, electronic media now available to readers (and becoming available to authors) need not follow the same strictly linear flow. The new capabilities and problems of traversing text in nonlinear ways - hypertext - have been discussed by some visionaries [Bush, 1945; Nelson, 1987] for decades. This new technology certainly permits us to make some traversals more easily (e.g., jumping to a cited reference with the click of a button rather than a trip to the library), but this same ease may make it more difficult for an author to present a cogent argument.  For now we will not worry about how arguments can be formed with nonlinear hypermedia. Assumptions 2 and 3 simply allow us to infer Assumption 1: If all the documents are paragraphs, we can expect them to have virtually uniform about-ness. These are also simplifying assumptions, however. In an important sense, a scientific paper's abstract is about the same content as the rest of the paper, and a newspaper article's first paragraph attempts to summarize the details of the OVERVIEW       19  following story. These issues of a text's level of treatment will be discussed later.
foa-0011	1.4.1 Structured Aspects of Documents  In addition to their free text, many documents will carry meta-data that gives some facts about the document. We may have publication information, for example, that this document appeared in this journal, in this issue, on this page. We are likely to know the author(s) of the document. Queries will often refer to aspects of both free text and meta-data.  QUERY 1   Vm interested in documents about Fools7 Gold that have been published in childrens magazines in the last five years.  The first portion of this query depends on the same about-ness relation that is at the core of our FOA process. But the last two criteria, concerning publication type and date, seem to be just the sort of query against structured attributes that database systems perform very successfully. In most real-life applications a hybrid of database and IR technologies will be necessary. (We distinguish between these techniques in Section 1.6.) The most interesting examples concern characteristics that do not clearly fall into either IR or database categories. For example, can you define precisely what you mean by a "children's magazine" in terms of unambiguous attributes on which a database would depend? Consider another query.  QUERY 2   What sort of work has K. E. Smith done on metabolic proteins affecting neurogenesis?  Finding an exact match for the string K. E. Smith in the AUTHORS attribute is straightforward. But the conventions in much of medical and biological publication (as well as in some areas of physics) sometimes lead to dozens of authors on papers, from the director of the institute through all of the laboratory assistants. Although K. B. Smith might well fulfill the syntactic requirements of authorship on a particular paper, users searching for "the work of" this person might well have a more narrowly defined semantic relationship in mind.
foa-0012	20       FINDING OUT ABOUT  1.4.2 Corpora  We have focused on individual documents, but of course the FOA problem would not interest us except that we are typically faced with a corpus of millions of such documents, and we are interested in finding only the handful that are of interest. The actual number of documents and their cumulative size will matter a great deal, as some of our IR methods have time or space complexities that make them viable only within certain parameters. To pick a simple example, if you are trying to find a newspaper article (you read it a few days ago) for a friend, exhaustively searching through all the pages is probably quite effective if you know it was in Friday's paper, but not if you need to search through an entire month's recycling pile! Similarly, a standard utility like the Unix grep command can be a practical alternative if the corpus is small and the queries simple.  Card catalogs were the first search engines
foa-0013	1.4.3 Document Proxies  Do you remember the library's original card catalogs, those wooden, beautifully constructed cabinets full of rows and rows of drawers, each full of carefully typed index cards? The card catalog contained proxies abridged representations of documents, acting as their surrogate - for the books it indexed. No one expected the full text of the books to actually be found in the drawers.  Computerized card catalogs are only capable of supporting a similar function. They do allow more extensive indexing and efficient retrieval, from terminals that might be accessed far from the library building. At the heart of this system is a text search engine capable of matching features of a query against book titles J Just like with the original index cards, however, retrieval is limited to some proxy of the indexed work, a bibliographic citation, or perhaps even an abstract. The text of ultimate interest - in a book, magazine, or journal - remains physically quite distinct from the search engine used to find it.  As computer storage capacities and network communication rates have exploded, it has become increasingly common to find retrieval systems capable of presenting the full text of retrieved items. In the modern context, proxies extend beyond the bibliographic citation information and subject headings we associate with card catalogs and include a OVERVIEW       21  document's title, an article's abstract, a judicial opinion's headnote, or a book's table of contents.  The distinction between the search engine retrieving documents and retrieving proxies remains important, however, for at least two reasons. First, the radically changing technical capabilities of libraries (and computers and networks more generally) can create conceptual confusion about just what the search engine is doing. While it has been possible for a decade or more to get the full text of published journal articles through commercial systems such as DIALOG and Lexis/Nexis, free access to these through your public library would have been almost unheard of until quite recently. In fact, most libraries did not even try to index individual articles in their periodical collections. Changing technical capacities, changes in the application of intellectual property laws, changes in the library's role, and resulting changes in the publishing industry are radically altering the traditional balance. Even when all new publications are easily available electronically, the issue of retrospectively capturing previously published books and journals remains unresolved.  Looking far into the future and assuming no technical, economic, or legal barriers to a complete rendering of any document in our corpus, there is still an important reason to consider document proxies. Recall that FOA is a process we are attempting to support and that retrieving sets of documents to show users is a step we expect to repeat many times. Proxies are abridged versions of the documents that are easier for browsing users to quickly scan and react to (i.e., provide relevance feedback) than if they had to read the entire document. If a document's title is accurate (if its abstract is well written, if its bibliographic citation is complete), this proxy may provide enough information for users to decide if it seems relevant J
foa-0014	1.4.4 Genre  A more subtle characteristic of documents that may need to concern  us is their genre - the voice or style in which a document is written.  You would, um, like, be pretty darn surprised to find stuff like this in a textbook, but not if it came to you over the phone. The genre of email seems to be settling somewhere between typical printed media and spoken conversation, with special markings of sarcasm:) and expletives #! ?% common. Newspaper journalists are carefully trained to produce articles  A misleading title, or did the document teach you something? S 22       FINDING OUT ABOUT  consistent with what newspaper readers expect, and their editors are paid to ensure that these stories maintain a consistent voice. Scientific journal articles are written to be understood by peers in the same field, according to standards that pertain to that community [Bayerman, 1988]. An important component of this audience focus is the vocabulary choice an author makes (cf. Section 8.2.1); stylistic variations and document structure may also differ. In a field like psychology, for example, it would be difficult to get a paper accepted in some journals if it is not subdivided into sections like Hypothesis, Methodology, and Subjects. Legal briefs are also written in highly conventionalized forms [Havard Law Review Association, 1995], and legislation is drafted to satisfy political realities [Allen, 1980; Goodrich, 1987; Levi, 1982; Nerhot, 1991].  In part, these variations in genre are difficult to detect because they remain consistent within any single corpus. That is, the typical email message would jump out at you as out of place if it appeared in your newspaper, but probably not if it were on the Letters to the Editor page. These examples highlight how much context about the corpus we bring with us whenever we read a particular document. They also foreshadow problems Web searchers are just beginning to appreciate, as WWW search engines include every document to which they can crawl, intermixing their very different contexts and writing styles. Without the orienting features of the newspaper's masthead, the "Letters to the Editor" rubric, or the purposeful selection of a tool that scans only Usenet news, the browsing users' abilities to understand an arbitrary document is diminished. Individual textual passages have been stripped of much of the context that made them sensible. As more and more of us generate content- in new hypermedia forms as well as traditional publications - that more and more of us retrieve, the range of genres we will experience can only increase, and our methods for FOA must help to represent not just the document but contextual information as well.
foa-0015	1.4.5 Beyond Text  Our definition of "documents" has hewn closely to the printed forms that still dominate the FOA retrievals most people now do. But print media are not the only form of answer we might reasonably seek, and we must ensure that our methods generalize to the other media that are OVERVIEW       23  increasingly part of the Net. Sound, images, movies, maps, and more are all appearing as part of the WWW, and they are typically intermixed with textual material. We need to be able to search all of these.  One reason for casting the central problem of this text as "finding out about" is that many aspects of multimedia retrieval remain the same from this perspective. We still have users, who have information needs. We can still reasonably use the term "document" to include any potential answer to users' queries, but now we expand this term to include whatever media are available. Most centrally, we must still characterize what each document is about in order to match it to these queries, and users can still assess how well the search engine has done.  At the same time, many parts of the FOA problem change as we move away from textual documents to other media. Most important is the increased difficulty of algorithmically extracting clues related to the documents' semantic content from their syntactic features. The primary source of semantic evidence used within text-based IR is the relative frequencies of keywords in document corpora, and a major portion of this text will show that this is a powerful set of clues indeed. We will also discuss the role other syntactic clues (e.g., bibliographic links) associated with texts can play in understanding what they are about. As we move to other media, the important question becomes what consistent features these new media have that we can also process to reliably infer semantic content. For example, what can we know about an image from the distribution of its pixel values? Do all SUNSETS share a brightness profile (dark below a horizontal line, symmetrically bright above it) that is reliable enough that this clue can be exploited to identify just these scenes?^ If so, can this mode of analysis be generalized sufficiently to allow Signature of retrieval of images based on more typical descriptors such as CHILDREN human FEEDING ANIMALS?  Even if we imagine that certain obvious, superficial aspects of some images may be extracted, our hopes must not blind us to the rich vocabulary that many images use every day. Consider a query like FIDELITY AS A POLITICAL ISSUE and consider Figure 1.6. Would any reasonable person claim that they could provide an exhaustive list of all the things these pictures "say"? Did you include the set of Hillary's jaw? The angle of Bill's gaze? The attitudes about divorce prevalent when the Doles' picture was taken and now? The tacit commentary by the editors of 24      FINDING OUT ABOUT  c::^-' - ':?*%£        ^ ..        ^    .-^  \ ..""     V  FIGURE 1.6 Finding Out About POLITICAL FIDELITY* Reproduced by permission of The New York Times  The New York Times produced by the juxtaposition of these two photos? Note also that this picture (and its selection for use in this text!) occurred MONICA the       years before anyone had even heard of Monica Lewinsky!^ meme                        Figure 1.7 gives a second example. This is a photograph of a locking  display case, containing a concert performance schedule. Pasted over the glass of the case is a sign, saying: "IGNORE THIS CALENDAR: these dates are 3 years old." But the photo also reveals a number of more subtle clues - that the key to the case has been lost (for three years!), that some frustrated teacher finally got tired of dealing with confused parents, that none of the school's administrators can think of a more imaginative solution.  These examples may seem far-fetched. But those of you old enough to remember the Cold War may also remember that there was an entire job category known as "Kremlinologist": someone expert at divining various power shifts among the Politburo based on evidence such as where various participants were placed within group photos! The conventional wisdom is that "a picture is worth a thousand words" and although some images may not require much explanation, others speak volumes. As we move from still images to movies, entirely new channels  * The New York Times, 15 Sept. 1996, Week in Review, p. 1. OVERVIEW       25  FIGURE 1.7 Obsolete Concert Schedule  for meaning - conveyed with the camera's attentional focus, soundtrack, etc. - are available to a skilled director. Music itself has an equally rich but distinct vocabulary. The ability to easily record and transmit digital spoken documents (speech) makes this form of audio especially worthy of analysis [Sparck Jones et al., 1996].  As with text, music, film, and motion pictures all predate their representations on computers. The convenience and availability of all these electronic media make it more possible and even more important to analyze them.  Once again, text is an excellent place to begin. Semiotics is one label for the subfield of linguistics concerned with words as symbols, as conveyors of meaning. Words in a language represent a particularly coherent system of symbol use, but so do the symbols used by photo journalists, painters, and movie directors. The meaning of these symbols changes with time; recall the pictures of the Clintons and Doles, their interpretation at the time of publication, and their interpretation now. What these pictures mean is different if we ask about the original context of 1996 and its meaning now. And again, complex, shifting meanings are 26      FINDING OUT ABOUT  typical not only of images but of documents as well: Watson and Crick's publication of theDNA code in Nature in 1953 [Watson and Crick, 1953] was important even then, but what that paper means now could not have been anticipated.  Yet the prospects for associating contentful descriptors with images and even richer media are not quite as bleak as they might seem. In many important cases (e.g., the archives of news photos maintained by magazines and newspapers), images are accompanied by captions, and video streams with transcripts. This additional manually constructed textual data means that techniques for inferring semantic content directly from images can piggyback on top of text-based IR techniques. In conjunction with the machine learning techniques we will discuss (cf. Chapter 7), statistically reliable associations found in captioned image and video corpora can be extrapolated to situations where we have images without captions and video without transcripts.  In the interim, we will return to the narrower, text-only notion of a document with which we began and consider FOA solutions for this simpler (!) case.
foa-0016	1.5 Indexing  Indexing is the process by which a vocabulary of keywords is assigned to all documents of a corpus. Mathematically, an index is a relation mapping each document to the set of keywords that it is about:  Index : doc\ a-^ {ku)j}  The inverse mapping captures, for each keyword, the documents it describes:  T    ,     _ 1         f |       -|   describes   ,  Index     : {kwj}   ó? docj  This assignment can be done manually or automatically. Manual indexing means that people, skilled as natural language users and perhaps with expertise in the domain of discourse, have read each document (at least cursorily) and selected appropriate keywords for it. Automatic indexing refers to algorithmic procedures for accomplishing this same result. Because the index relation is the fundamental connection between the users' expressions of information need and the documents that can OVERVIEW       27  satisfy them, this simply stated goal - "Build the Index relation" - is at the core of the IR problem and FOA generally.
foa-0017	1.5.1 Automatically Selecting Keywords  We begin by considering the document at its most mechanical level, as a string of characters. Our first candidates for keywords will be tokens, things broken by white space. That is, each token in the document could be considered one of its keywords.  How good is this simple solution? Suppose users ask for documents about CARS and the document we are currently indexing has the string CAR. It seems reasonable to assume that users are interested in this document, despite the fact that the query happens to contain the plural form CARS while the document contains the singular CAR. For many queries we might like to consider occurrences of the words CAR and CARS, or even RETRIEVAL and RETRIEVE, as roughly interchangeable with one another; the suffixes do not affect meaning dramatically. And of course our problem doesn't end with plurals; we could make similar arguments concerning past-tense -ED endings and -ING participles.  This simple solution also depends too much on where spaces occur. Consider the German noun GESCHWIIsroiGKEITSBESCRAJSIiaiMG, corresponding to the English phrase SPEED LIMIT. In many ways, the fact that English happens to put a white space between the words while German does not is not semantically critical to the meaning of these descriptors or the documents in which they might occur. Such morphological features - used to mark relatively superficial, surfacestructure features (such as tense or singular versus plural) - can be considered less important to the meaning. And differences between German and English are trivial when they are compared to Asian texts, where the relationship between characters and words is radically different.  What about hyphenation? Use of the word DATABASE, the phrase DATA BASE, and the hyphenated phrase DATA-BASE is highly variable, depending on author preference and current practice at the time and place of publication. Yet we would hope that all occurrences of any of these tokens would be treated as references to approximately the same semantic category. Similarly, we hope that the end-of-line hyphenation 28       FINDING OUT ABOUT  (breaking long words at syllable boundaries) would not create two keywords when we would expect only one. But simply adding "-" to the set of white space characters defining tokens would make CLINTON-DOLE and A-Z keywords, too!  Hyphenation is concerned with the situation in which a potential keyword is broken up by punctuation; what about those situations where a space also breaks up a semantic unit? SPEED LIMIT seems semantically cohesive, but what algorithm could distinguish it from other bigrams (consecutive pairs of words) that happen to occur sequentially? The problem only becomes that much more complicated if we attempt to consider longer noun phrases like APPLES AND ORANGES or BACK PROPAGATION NEURAL NETWORK, let alone more complicated syntactic compounds such as verb phrases, clauses, or sentences. Identifying phrases is an important and active area of research from the perspectives of both IR and computational linguistics.  Summarizing, we will take a token to be our default keyword because this is straightforward. More sophisticated solutions will handle hyphenation, multiword phrases, subtoken stems, and so on (cf. Section 2.3.1).
foa-0018	1.5.2 Computer-Assisted Indexing  The field of library science has studied the manual process of constructing effective indices for a very long time. This standard becomes a useful comparison against which our best automatic techniques can be compared, but it also demonstrates how difficult comparison will be. There are data, for example, that suggest that the capacity of one person (e.g., theindexer) to anticipatethe words used by another person (e.g.,asecond indexer or the query of a subsequent user) is severely limited [Furnas et al, 1987]; we are all quite idiosyncratic in this regard. The lack of interindexer consistency among humans must make us humble in our expectations for automated techniques.  But manual and automatic indexing need not be viewed as competing alternatives. In economic terms, if we had sufficient resources, we could hire enough highly trained catalogers to carefully read every document In a corpus and index each of them. If we couldn't afford this very expensive option, we would have to be satisfied with the best index our automatic system could construct. But if we have enough resources OVERVIEW      29  to hire one or two human indexers, what tools might we give them that would make the most effective use of their time?  We seek methods that leveragethe editorial resource, in the sense that this manual effort does not grow as the corpus does. How might editors and librarians guide an automatic indexing process? What information should this computation provide that would allow intelligent human readers the assurance of a high-quality indexing function? Chapter 7 will discuss ways that editors can train machine learning systems, and a number of analyses that are of interest to editors will be mentioned, especially in Chapter 6.
foa-0019	1.6 FOA versus Database Retrieval  Within the field of computer science, the sub fields of databases and IR are often closely aligned. Databases have well-developed theoretic underpinnings [Abiteboul et al., 1995] that have generated efficient algorithms [McFadden and Hoffer, 1994] and become the foundation for one of the most successful elements of the computer industry.  Both databases and search engines attempt to characterize a particular class of queries by which many users are expected to attempt to get information from computers. Historically, database systems and theory have been perceived as central to the discipline of computer science, probably more so than the IR techniques that are the core technologies for FOA. Things may be changing, however.  The general public's discovery of the Internet and subsequent interest in search engines like Alta Vista, InfoSeek, and Yahoo! suggest that many users find value in the lists of Web pages returned in response to searches. These search engines are clearly doing an important job for many people. It is also a quantitatively different job from organizing their address book (or record collection or baseball statistics) databases. How are IR and database technologies to be distinguished?  To make the distinctions more concrete, let's imagine a particular information need and think about how both a database and a search engine might attempt to satisfy it. An example query might be as follows.  QUERY 3   What is the best SCSI disk drive to buy? 30      FINDING OUT ABOUT   TABLE 1.1 Hypothetical Database       Size  Price Speed  Model number Manufacturer Vendor (GB) Interface ($) (msec)  123 Seag JR 2.4 SCSI 162 12  123 Seag Fry 2.4 SCSI 159 12  456 Metrop AB 2.5 IDE 0 12.5  789 Seag JR 1.5 EIDE 121 10.5  In the case of databases, strong assumptions must first be made about structure among attributes of individual records. Good database design demands that the fundamental elements of data, their format, and logical relations among them be carefully analyzed and anticipated in a logical data model long before any data are actually collected and maintained within a physical implementation. These assumptions allow specification of a syntax for the query language, strategies for optimizing the query's use of computational resources, and efficient storage of the data on physical devices.  Now let's assume that a logical data model has been constructed and that a large catalog of information from various hard drive manufacturers and vendors has been collated. We will also make the larger and problematic assumption that the users can translate the natural language of Query 3 into the somewhat baroque syntax of a query language such as SQL. The result of the database search might look something like  NLPfor              Table 1.1.1"  databases                  Creating an example relation like this and populating it with a few  instances is simple, but performing the necessary data modeling, collating the data from all of the manufacturers and vendors, and keeping it all up to date are much more daunting tasks. If the database catalog is out of date or missing data from important vendors, users might leave the database badly informed.  Now let's imagine using a search engine on the same query. When run against a UseNet news search engine like DejaNews, this query results in the retrieval shown in Figure 1.8 with the most highly ranked posting shown in Figure 1.9.  Users of this search engine will read about many issues related to hard disks, some of which may be relevant to their particular situation. OVERVIEW      31  Matches 1-20 of 726 for search: best SCSI disfc  Date      Scr              Subject  Re: IDE or SCSI?________  Re: switching" from IDE t Re: Sun SCSI and Linux Re: switching from IDE t Replace SCSI Disk on R3. FS: 486-lQQHhz multimedi  SCSI problems___________  Re: Which is Best: More Best Scsi CDR for $3Q0-$ Re: IDE increases Interr FS:Iticropolis 3243WAV dr  CDR Server---Audio CD  comp.sys.appleZ Freg#3/1 Re: V95 vs. KacOS Micros miro configuration utili TFOR SALE1 Dual PowerPC Re: IBH OEH 0664-CSH and error i kernel32.dll alt SUN SPARC 20 FOR SALE. Re: Help* Replacing pri  1. 97/08/29 (P62  2. 97/08/04 053  3. 97/08/28 051  4. 97/08/03 050  5. 97/08/02 050  6. 97/08/21 049  7. 97/08/11 049  8. 97/08/03 049  9. 97/08/01 049  10. 97/08/16 048  11. 97/08/16 048  12. 97/08/09 048  13. 97/09/01 047  14. 97/08/31 047  15. 97/08/27 047  16. 97/08/05 047  17. 97/08/04 047  18. 97/08/01 047  19. 97/08/12 046  20. 97/07/26 046  Newsgroup  comp. os. ms-wiiwiows. comp. null, unixvare. comp. os. linux-iardw comp. uoaix. uniiware. comp.unix. sco.mlsc misc. for sale, comput comp. os. linni.jb.ardw comp. os. linuz.liardv alt. cd-rom microsoft.p-ablic.iFi rec. video.marlcetpla ttk. for sale comp. SY3.apple2 comp. sys. mac. advoca rec. video, desktop comp. sys. be. misc comp.peripb.s. scsi no.pc  comp. sys. sun. liardwa comp. os. ms-wiiidovs.  Author  hiioaQur arthurg tallpau Larry E ericl@n tiioe(ßi ///ax 2 o r c Ä "Scott "Nolan John Cc Hattgor natbarug df ieldE Edv Gas "Frak  FIGURE 1.8 Results of SCSI Search of UseNet  Subject:         Re: IDE or SCSI?  From:            helpful@urban.or.jp (me @ my humble abode)  Date:             1997/08/29  Message-Id:    lt;3406f93a.50446926@imrp.gol.comgt;  Newsgroups: comp.os.ms-wmdows.nt.setiip.hardware,comp.windows.nt.misc  [More Headers]  On Thu, 28 Aug 1997 23:10:03 GMT, Michael Query lt;query@dpi.qld.gov.augt; wrote:  gt;My question is, should I get another 2 Gb SCSI disk for putting the gt;OS (NT 4.0 WS), software, etc on, or should I get an IDE disk for this?  Having played around with different con^Sgsfor a while, I'd say go SCSI. I'd do that even if I had to get a second SCSI controller.  (you'll "hear1' a lot of people arguing that IDE is good enough, but if you are  after overall improved performance SCSI is best.)  my 2Y.  FIGURE 1.9 A Relevant Posting  For example, does the "best" qualifier in Query 3 mean lowest cost, maximum capacity, minimum access time, or something else? Can users choose between IDE and SCSI, or are they restricted to SCSI? Depending on what kind of users they are, some of the information retrieved may be 32       FINDING OUT ABOUT  TABLE 1.2 IR versus Database Retrieval   IR Database  System provides Pointer to data Data item  User's query General Specific  Retrieval method Probabilistic Deterministic  Success criteria Utility (Correctness)    Efficiency,    User-friendliness,...  immediately applicable to the purchase being considered, while other parts of it are better considered collateral knowledge (D. E. Rose, personal communication) that simply leaves users better informed.  A very different set of assumptions from those we made about the database system are necessary to imagine the search engine working. For example, who wrote these postings? Are they a credible source of good information; what is their authority? Well-trained database users should ask equally skeptical questions about the data retrieved, but rarely are authority, data integrity, and the like considered part of database analysis.  But the key assumption for our IR users is that they can "listen in" on this previous "conversation" and interpret the text that has been left behind as containing potential answers to the current question. The search engine is charged with retrieving textual passages that are likely to answer the users' questions. Once presented with these retrievals, FOA users have more humble expectations and are willing to do more interpretive work. Because FOA searches are often even less concrete than Query 3 and are issued by users simply trying to learn about a topic, semantic issues central to the interpretation of a textual passage and its context, validity, and so on are at the heart of the FOA enterprise.  Van Rijsbergen, p. 2, table 1.1 has summarized these issues along a number of dimensions by which IR and database systems can be distinguished, and several of these are duplicated in Table 1.2. Database systems are almost always assumed to provide data items directly. Search engines provide a level of indirection, a pointer to textual passages that contain many facts, hopefully including some of interest. The information need of the users is quite vague when compared to that of database OVERVIEW       33  users. The search engine users are searching for information about a topic they don't completely understand. Typical database users have a fairly specific question, like Query 3, in mind. It might even be that the database is missing some data; for example, the special null value 0 in Table 1.1 shows that the price of the third disk drive is not known. Even in this case, however, the database system "knows that it doesn't know" this information. FOA queries are rarely brought to such a sharp point; ambiguity is intrinsic to the users5 expectations.  Because the queries are so general, an FOA retrieval must be described in probabilistic terms. If a particular hard disk's price is part of our database, we are certain, with probability = 1.0, of its value. Never would a database system reply with "This hard disk might cost about $300." As discussed in depth in Section 5.5, a search engine can use sophisticated methods for reasoning probabilistically, and available evidence might even allow it to be quite confident that retrieved items will be perceived as relevant. But never will we be entirely certain that a document is what users want; we can only have high confidence that it may be.  Finally, one of the problems in evaluating search engines is just what success criteria are to be used. We typically assume that information we get back from a database system is correct. (Try to find an ad for a database system that boasts, "Our system retrieves only right answers"!) One database system claims to be more efficient, cheaper, easier to integrate into existing code, and more user-friendly than others.  This list of ways that search engines might be distinguished from databases is far from exhaustive; Blair has proposed a more extensive analysis [Blair, 1984]. More recently, as search engine technology and WWW-inspired applications have both burgeoned, hybrids of databases and search engines have blurred the historical differences further. Some bases of database/search engine interaction are mentioned in Chapter 6.  Chapter 4 discusses the evaluation of search engines in great detail, but typically the bottom line is: Does the system help you? If you are writing a research paper, did this search engine help you find material that was useful in your research? If you are a lawyer preparing a case and you want to find every relevant judicial opinion, does the search engine offer an advantage over an equivalent amount of time combing 34      FINDING OUT ABOUT  through books in a law library? Such squishy, qualitative judgments are notoriously difficult to measure, and especially to measure consistently across broad populations of users. The next section provides a quick preview of several precise measurements that have proven useful to the IR community but would not be found persuasive within the database community.
foa-0020	1.7 How Well Are We Doing?  Suppose you and I each build an FOA search tool; how might we decide which does the better job? How might a potential customer decide on their relative values? If we use a new search engine that seems to work much better, how can we determine which of its many features are critical to this success? If we are to make a science of FOA, or even if we only wish to build consistent, reliable tools, it is vital that we establish a methodology by which the performance of search engines can be rigorously evaluated.  Just as your evaluation of a human question-answerer (professor, reference librarian, etc.) might well depend on subjective factors (how well you "communicate") and factors that go beyond the performance of the search engine (does any available document contain a satisfying answer?), evaluation of search engines is notoriously difficult. The field of IR has made great progress, however, by adopting a methodology for search engine evaluation that has allowed objective assessment of a task that is closely related to FOA. Here we will sketch this simplified notion of the FOA task.  The first step is to focus on a particular query. With respect to this query, we identify the set of documents Rel that are determined to be Omniscient         relevant to it."'" Then a good search engine is one that can retrieve all  relevance            an(j onjy ^ documents in jRel. Figure 1.10 shows both Rel and Retr, the  set of documents actually retrieved in response to the query, in terms of a Venn diagram. Clearly, the number of documents that were designated both relevant and retrieved, Retr D Rel, will be a key measure of success.  But we must compare the size of the set \Retr n Rel | to something, and several standards of comparison are possible. For example, if we are very concerned that the search engine retrieve every relevant document, OVERVIEW       35  High-recall retrieval  ^ - "*"  Corpus  FIGURE 1.10 Comparison of Retrieved versus Relevant Documents  then it is appropriate to compare the intersection to the number of documents marked as relevant, | Rel |. This measure of search engine performance is known as recall:  \Retr n Rel \  Recall =  However, we might instead be worried about how much of what the users see is relevant, so an equally reasonable standard of comparison is what number of the documents retrieved, | Retr |, are in fact relevant. This measure is known as precision:  \Retr fl Rel\ Precision =  \Retr  (1.2)  Note that even in this simple measure of search engine performance,  we have identified two legitimate criteria. In real applications, our users will often vary as to whether high precision or high recall is more important. For example, a lawyer looking for every prior ruling (i.e., judicial opinions, retrievable as separate documents) that is on point for his  or her case will be more interested in high-recall behavior. The typical undergraduate, on the other hand, who is quickly searching the Web for a term paper due the next day, knows all too well that there may be many, many relevant documents somewhere out there. But the student cares much more that the first screen of hits be full of relevant leads. 36      FINDING OUT ABOUT  Examples of high-recall and high-precision retrievals are also shown in Figure 1.10.  To be useful, this same analysis must be extended to consider the order in which documents are retrieved, and it must consider performance across a broad range of typical queries rather than just one. These and other issues of evaluation are taken up in Chapter 4.
foa-0021	1.8 Summary  This chapter has covered enormous ground and attempted to summarize topics that will be discussed in the rest of this text. Major points include:  ï  We constantly and naturally Find Out About (FOA) many, many things. Computer search engines need to support this activity, just as naturally.  ï  Language is central to our FOA activities. Our understanding of prior work in linguistics and the philosophy of language will inform our search engine development, and the increasing use of search engines will provide empirical evidence reflecting back to these same disciplines.  ï  IR is the field of computer science that traditionally deals with retrieving free-text documents in response to queries. This is done by indexing all the documents in a corpus with keyword descriptors. There are a number of techniques for automatically recommending keywords, but it also involves a great deal of art.  ï  Users' interests must be shaped into queries constructed from these same keywords. Retrieval is accomplished by matching the query against the documents' descriptions and returning a list of those that appear closest.  ï  A central component of the FOA process is the users' relevance feedback, assessing how closely the retrieved documents match what they had "in mind."  ï  Search engines accomplish a function related to database systems, but their natural language foundations create fundamental differences as well. OVERVIEW       37  ï In order to know how to shop for a good search engine, as well as to allow the science of FOA to move forward, it is important to develop an evaluation methodology by which we can fairly compare alternatives.  In this overview we've made some simplifying assumptions and raised more questions than we've answered, but that is the goal! By now, I hope you have been convinced that there are many facets to the problem of FOA, ranging from a good characterization of what users seek, to what the documents mean, to methods for inferring semantic clues about each document, to the problem of evaluating whether our search engines are performing as we intend. The rest of this book will consider each of these facets - and others - in greater detail. But like all truly great problems, issues surrounding FOA will remain long after this text is dust.
foa-0023	2.1 Building Useful Tools  The promise offered by Chapter 1 is that many real-world problems can be viewed as instances of the FOA problem. The proof is to be found in concrete code - a relatively small technology base that will prove useful in a wide array of applicatons. In this chapter we will present a suite of software tools that together build a search engine for a wide variety of situations. Source code is provided so that these tools can be easily modified for applications of your own* We will work through two different examples of IR systems, in order to demonstrate how slight variations of the same basic code can handle both.  Compared to the broad generalities of Chapter 1 , the technical details of this chapter will sound a very different tone. Describing a complex algorithm requires the specification of many, sometimes tedious, details. To make the software executable on machines that are likely to be available to you, the details are provided for several operating environments. But the processor speeds, internal memory, and hard disk sizes available on computers are changing dramatically each year, so many of the assumptions on which these routines are based will require constant reevaluation.  We will develop the software tools in three phases. The first phase will convert an arbitrary pile of textual objects into a well-defined corpus 40       FINDING OUT ABOUT  of documents, each containing a string of terms to be indexed. The second phase involves building efficient data structures to invert the Index relation so that, rather than seeing all the words contained in a particular document, we can find all documents containing particular keywords. All of these efforts are in anticipation of the third and final phase, which matches queries against indices to retrieve those that are most similar. These three major phases are central to building any search engine.  This chapter will be most concerned with the first two phases, which together extract lexical features. Our goal will be the extraction of a set of features worthy of subsequent analysis. As in any cognitive science, the specification of an appropriate level of analysis - whether it is the resolution and depth of an image, the subphonemes of continuous speech, the speech acts of language, or something else - the specification of this atomic feature set is the first important step.  This will involve a great deal of work, much of it unpleasant except to those who enjoy designing efficient algorithms and data structures (some of us actually do enjoy this!:). The promise is that we will, as a consequence of good software design, develop useful tools that allow us to spend the rest of our time exploring interesting features of language.
foa-0024	2.2 Interdocument Parsing  The first step is to break the corpus - an arbitrary "pile of text" - into individually retrievable documents. This demands that we be specific about the format of the corpus and that we decide how it is to be divided into individual documents. For all operating systems we will consider, this problem can be defined more precisely in terms of paths, directories, files, and positions within files. For any application in which the corpus can be described by the path to its root, these tools will translate directories, files, and documents-within-files into a homogeneous corpus. Of course, there are some situations (e.g., when documents are maintained within a database) that cannot be captured in these terms, but these primitives do allow a wide range of corpora to be specified.  Our model will assume that many documents may be contained within a single file and that each document occupies a contiguous region within the file. EXTRACTING LEXICAL FEATURES       41  Issues concerning structure within a single document are closely related to assumptions we may or may not be able to make about the length of the documents in question. Our assumptions about how long a typical document is will recur throughout this book. It is obvious, for example, that different document browsers are necessary if we need to browse through an entire book rather than look at a single paragraph. Less obvious is that the fundamental weighting algorithms used by our indexing techniques will depend very sensitively on the number of tokens contained in each document.  In this textbook we will focus primarily on two particular test corpora, AI theses (AIT) and email; these are discussed in more detail in Section 2.4. Each of these has natural notions of the individual document: In the case of the AIT it is the thesis's abstract, and for email it is the entire message. In both cases, more refined notions of document (the individual paragraphs within the abstract or within the email message) are possible.  With these assumptions, we can define our corpus simply with two files: one specifying full path information for each file, and a second specifying where within these files each message resides. A large portion of the task of navigating a directory full of files and visiting each of them can be accomplished using the dirent.^ This utility allows the recursive descent through all directories from a specified root, visiting every file contained therein.  In many cases, the files we will be indexing have a great deal of syntactic structural information above and beyond the meaningful text itself. For example, our email will often contain a great deal of mail header information, as (loosely.) specified in RFC822J Many text-formatting languages, for example, TjX, XML, and HTML, now produce documents with a well-defined syntax. If, for example, the documents are written in HTML, we don't want to index pseudo-words like lt;H1gt;. In many of these situations, filters exist that can extract just the meaningful text from surrounding header or format information; DeTeX1 is an example of a useful filter for removing EJTeX andT^X markup. Use of such utilities spares us the task of parsing this elaborate structure, but it also means that more elaborate solutions for maintaining the difference between the document's index and the document's presentation must be addressed.  What is dirent?  What is RFC8S2?  1 www. cs.purdue.edu/trinkle/detex/lndex.html 42       FINDING OUT ABOUT  Advisor Committee University Department  Proxy text  This is a sample of some text.  It might be from an email message, a dissertation, or anything else.  We simply assume it is a string of characters about a paragraph in length.  FIGURE 2.1 Parsing Email and AIT to Common Specifications  The basic data elements to be parsed from our two examples, email and AIT, are shown in Figure 2.1.
foa-0025	23 Imtradocumeiit Parsing  Having now focused our attention on a particular file and on the beginning and ending locations within that file associated with a particular document, we can consider this file segment simply a stream of characters.  Reading each and every character of each and every document, deciding whether it is part of a meaningful token, and deciding whether these tokens are worth indexing will be the most computationally intensive aspect of the indexing chore; this is our "inner loop " For that reason, we will devote some real attention to making this lexical analysis as efficient as possible.  Several general criteria will shape our design. First, because we are assuming that our textual corpus is very large, we will do our best to avoid duplicating this primary text. That is, we will attempt to deal with the text in situ and not make a second copy for use by the indexing and retrieval system. Thus, we will be creating a system of pointers into locations within the corpora directories and files. EXTRACTING LEXICAL FEATURES       43  FIGURE 2.2 Finite State Machine  A wide range of alternative designs are possible even at this early stage, and so we desire as much flexibility as possible in the specification of the lexical analyzer. A lexical analyzer generator, such as the lex tool in Unix, allows the specification of very complicated lexical analyzers for very elaborate languages. The fundamental representation used by all such algorithms is a finite state machine, like that shown in Figure 2.2. This simple representation breaks the set of possible characters coming from a text stream into classes (drawn as circular states), with transitions from one state to the next on the occurrence of particular characters. By careful construction of the sets of characters (e.g., white space characters corresponding to state 0 in Figure 2.2), arbitrary text sequences can be handled very efficiently.  For our two example corpora and many other situations, the stream of characters, a straightforward analysis in terms of a simple finite state machine, will suffice. We will depend on a utility written by Christopher Fox [Fox, 1992]. This utility simultaneously achieves two critical goals. First, the lexical analyzer tokenizes the stream of characters into a sequence of wordlike elements. At first blush this seems straightforward: A token is anything separated by white space* where the standard definition of white space is used. But what about hyphens? Should the hyphenated phrase DATA-BASE be treated as two separate tokens or as a single one? Should a file name* like WIMDOWS.EXE be treated as a single token? Which host, directory, and file elements in a full URL like 44      FINDING OUT ABOUT  www.cs.ucsd.edu/-rik are to be kept intact as individual tokens? More elaborate elements such as these can quickly demand the sophistication of a tool like lex.  The presence of digits among the alphabetic characters presents another problem. Are numbers to be allowed as tokens? Perhaps we only want to allow "special" numbers (e.g., 1776, 1984, 2001, 3.14159). Perhaps we want to use rules similar to those for programming language identifiers and require that a token begin with an alphabetic character, which may then be followed by numbers or letters.  We must also worry about the case of the characters at this earliest lexical analysis stage. Are we to treat capitalization as significant in distinguishing tokens from one another? An enormous reduction in vocabulary size is possible if we fold case so as to treat upper- and lowercase characters interchangeably. But of course then we have also precluded the possibility of many proper name analyses that may be useful for identifying singular people, places, or events (see Chapter 6). In some cases the semantics of the documents make decisions about case automatic. For example, if the documents are program source files, the language in question may or may not treat differences in case as significant.
foa-0026	2.3.1 Stemming and Other Morphological Processing  From the perspective of linguistics, many of the early design issues we address are considered morphological transformations of language,  i.e., an analysis of what we can infer about language based on structural features. As discussed briefly in Chapter 1, the arbitrary way in which white space may or may not separate tokens whose meanings are interdependant (e.g., recall the German word GESCHWDTOIGKEITS B1SCHRANEUETG and English phrase SPEED LIMIT example) will make us interested in phrasal units of indexing as well. In many Asian texts, the relationship between characters and words is quite radically altered. The Kanji alphabet and Unicode standards help to define the problem but bring biases of their own [Fujii and Croft, 1993],  For now we will focus on one of the most common morphological tricks, stemming. Stemming is a direct attempt to remove certain surface markings from words to reveal root form. Beyond deciding which characters are to be combined into tokens, Chapter 1 discussed how important it can be to use a token's root form as an index term: We can EXTRACTING LEXICAL FEATURES       45  hope that our retrieval is robust even when the query contains the plural form CARS while the document contains the singular CAR. Linguists would say that the number feature (whether a noun is singular or plural) is morphologically marked. Linguists also distinguish between inflectional morphology like plurals, and derivational morphology, which can change a word's syntactic category (e.g., changing the noun PRODUCT to the verb PRODUCTIZE) and meaning more radically.  In stemming, suffixes are dropped. Even in the simple case of plural endings, it isn't as simple as removing ss. Consider:  WOMAN/WOMEN  LEAF/LEAVES  FERRY/PERRIES  ALUMlSniS/ALUMOT  DATUM/DATA  Conversely, we cannot assume that every time there is an ending s we can remove it; stemming CRISIS and CHESS to CRISI and CHES would damage their meaning.  The most common approach to this problem [Fox, 1992] is to identify more elaborate patterns over character sequences that reliably pare tokens down to their root forms. A broad range of such patterns can be defined in terms of a context-sensitive transformation grammar.  For example:  Rule 2.1 t*)SSES-+ /ISS  Rule2.2 (.* [AEIOU].*)ED-gt; /I  Rule2.3 (.* [AEIOU].*)Y-+ /U  Rule 2.1 says that strings ending in -SSES should be transformed by taking the stem (i.e., characters prior to these four) and adding only the two characters SS. Rule 2.2 says that stems containing a vowel and ending in -ED should be transformed to leave only the stem; Rule 2.3 says that stems containing a vowel and ending in -Y should be transformed to the stem with an I replacing the Y.*  * These rules are pseudo-code only, using grep-like syntax for regular expressions. Further, they are meant as illustrative examples. See Fox's chapter for a complete exposition of the actual rules used in Porter's stemmer f Fox, 1992]. 46       FINDING OUT ABOUT  A complete algorithm for stemming involves the specificaton of many such rules and a regime for handling conflicts when multiple rules match the same token. An early and influential algorithm due to Lovins [Lovins, 1968] specified 260 suffix patterns and used an iterative longest match heuristic. This means that first preference is given to the pattern (left-hand side of the grammar rule) that matches the most characters in a target token (because this prefers more specific matches over shorter, more generally applicable ones); then rules are iteratively reapplied until no other rules match.  The Porter stemmer [Porter, 1980] (included as part of the FOA software) is a simplified version of Lovin's technique that uses a reduced set of about 60 rules and organizes them into sets, with conflicts within one subset of rules resolved before going on to the next. In fact, if only the first set of rules in Porter's stemmer (focusing exclusively on plurals and the most straightforward suffixes like -ED and -ING) is used, the result has been called weak stemming [Walker, 1989]. A key advantage of all such rule-based grammatical representations of the stemming process (and of efficient implementations of them, such as that provided by Fox) is that modifications to the rules and to ordering among the rules can be accomplished by changing the grammar rather than by endless ad hoc hacking (ad hacking?:) in response to particular character sequences.  The use of any stemmer obviously reduces the size of the keyword vocabulary and consequently results in a compression of the index files. Such compression can vary from 10 to 50 percent, depending on the total size of the keyword vocabulary and how aggressive (e.g., how many suffix rules are used) the stemmer is.  The primary effect of stemming, however, is that two keywords that were once treated independently are considered interchangeable. Stemming is therefore an example of a recall-increasing operation because it will cause a keyword used in the query to match more documents.  The fundamental problem with any stemming technique, of course, is that the morphological features being stripped away may obscure differences in the words' meanings. For example, the token GRAVITY has two word senses5 one describing an attractive force between any two masses and the other having to do with a serious mood. But once the word GRAVITATION" has been stemmed, we have lost the information that might constrain us to the first interpretation. Krovetz EXTRACTING LEXICAL FEATURES       47  [Krovetz, 1993] considers several more sophisticated approaches to keyword morphology, including augmenting Porter's stemmer with a dictionary that is checked after each phase of stemming rules has been applied.
foa-0027	2.3.2 Noise Words  From the earliest days of IR (e.g., Luhn's seminal work [Luhn, 1957]), two related facts have been obvious: First, a relatively small number of words account for a very significant fraction of all text's bulk. Words like IT, AMD, and TO can be found in virtually every sentence. Second, these noise words make very poor index terms. Users are unlikely to ask for documents about TO, and it is hard to imagine a document about BE.^ Due then to both their frequency and their lack of indexing consequence, we will build the capability of ignoring noise words into our lexical analyzer.  As will be discussed extensively in Chapter 3, noise words are often imagined to be the most frequently occurring words in a corpus. One problem with defining noise words in this way is that it requires a frequency analysis of the corpus prior to lexical analysis. It is possible to use frequency analyses from other corpora, assuming that the distribution of noise words is relatively constant across corpora, but such an extrapolation is not always warranted. Worse, the most frequently used words often include those that might make very good keywords. Fox notes that the words TIME, WAR, HOME, LIFE, WATER, and WORLD are among the 200 most frequently used words in general English literature [Fox, 1992, p. 113].  Instead, we will define noise words extensionally, in terms of a finite list or negative dictionary. The list we use, STOF.WRD, was derived by Fox from an analysis of the Brown corpus [Fox, 1990].  The relationship between these noise words and those words most critical to syntactic analysis of natural language sentences is striking. Note that the same tokens that are thrown away as noise because they have no meaning are precisely those function words that are most important to the syntactic analysis of well-formed sentences. This is the first, but not the last, suggestion of a fundamental complementarity between FOA's concern with semantics and computational linguistics' concern with syntax.  But sometimes we care about noise words!
foa-0028	48       FINDING OUT ABOUT  2.3.3 Summary  We have described the lexical analyzer in terms of the job it must do processing every document in the corpus, because this task confronts us first. But because our central task will be to match these documents against subsequent users' queries, it is critical that the identical lexical analysis be performed on the queries. This creates several implementation constraints (e.g., that the same code libraries are available to the indexer and to the query processing interface), but these are minor. If the query language is designed to support any special operators (e.g., Boolean combinators, proximity operators), the query's lexical analyzer may accept a superset of the tokens accepted by the document's analyzer. In any case, it is imperative if queries and documents are to be matched correctly that the same lexical analysis be applied to both streams. Using an identical code library is the easiest way to ensure this.  It may seem nonsensical to worry so much about processing each character efficiently, when we assume that some other previous process has already identified each interdocument break - doesn't such processing require the same computational effort, and, if so, doesn't this make our current efficiency worries moot?  Perhaps. A conclusive answer depends on many architecture and operating system specifics. There are two reasons we have made such assumptions. The first is that the practicalities of delivering the FOA corpora and code currently make this convenient. But the more serious reason is that the most theoretically and intellectually interesting questions involve analysis of operations downstream from the first stages of interdocument parsing: how to identify tokens, how to count them, etc. If these latter operations are made especially efficient, it means we can afford to do more experimentation, more playfully. For a text, that is the primary concern.
foa-0029	2.4 Example Corpora  In these experiments, and the rest that follow, we will consistently use two example corpora.  The first of these we will call the "Artificial Intelligence Thesis" (AIT) corpus. This is approximately 5000 Ph.D. and Master's dissertations and abstracts. Virtually every dissertation published within the last 30 years EXTRACTING LEXICAL FEATURES       49  Year  FIGURE 2.3 AIT Year Distribution  has been microfilmed by University Microfilms, Inc. (UMI)J* The corpus is a fairly exhaustive set of theses classified as AI by UMI from the years 1987 to 1997. A histogram of the theses distribution by year is shown in Figure 2.3.  We will focus on a handful of characteristics of each thesis:  ï  Thesis number  ï  Title  ï  Author  ï  Year  ï  University  ï  Advisor  ï  Language  ï  Abstract  ï  Degree  For now, we will lump these attributes into two categories: textual fields and structured attributes. Structured attributes are ones for which we can reason more formally, using database and artificial intelligence techniques. For now, we will concentrate on only the textual fields. The abstract will be the primary textual element associated with each thesis, while its title (also a textual field) will be used as its proxy - a synopsis of  More about AIT origins 50       FINDING OUT ABOUT  the thesis that conveys much of its meaning in a highly abbreviated form. Proxies will prove very important surrogates for the documents (for example, when users are presented with hitlists of retrieved documents). The second corpus we will study could not be provided on the CD because you must provide it yourself; it is all of your email. Email is now a fundamental form of transient, nearly immediate communication for many, but the resulting permanent record of these conversations can also be treated as a static, long-lived type of "literature" [Belew and Rentzepis, 1990]. We will assume that with disk storage as cheaply available as it How do I index    is today, you at some point began to collect email.^ Typically, some of my email?           ^jg w{\\ \ye email others have sent you, but you may have also kept a  copy of all of your "outgoing" email. Many email clients support onthe-fly segregation of email into separate folders. Minimally, this means that our procedures for indexing email must be capable of traversing elaborate directory structures. Later, we will also consider the use of this user-generated structure as a source for learning; cf. Section 7.4.  The directory in which you have filed an email message is one feature we may associate with each message; whether it is an incoming or outgoing message is another. But of course email also has many structured attributes associated with it, in its header. These include:  ï  From: ïTo:  ï  Cc:  ï  Subject:  ï  Date:  In general, we will put off all consideration of structured attributes  associated with documents until later. For now, simply note the many  parallels between our two example corpora: Both have well-defined authors, well-defined time-stamps, and excellent and obvious candidates for proxy text.
foa-0030	2.5 Implementation  The range of potential implementations of the basic techniques discussed in this chapter and subsequent ones is quite remarkable. Each depends EXTRACTING LEXICAL FEATURES       51  on features of the specific application, available hardware, and so on, such as:  ï  using a massively parallel supercomputer of the mid-1980s to provide current news to financial analysts [Stanfill and Kahle, 1986a];  ï  searching for file names as part of the MacOS Finder on a single personal computer and then extending to support file content searching as part of the MacOS Sherlock2 utility;  ï  SMART3 is a classic software suite designed to support experimentation into basic IR techniques (see Section 3.4.3 for more details);  ï  providing a generic utility for Managing Gigabytes4 (MG), for example, building an index for a CD-ROM or DVD; and  ï  making all of the pages on a WWW server searchable via Web Server Search Tools5 or Information filtering tools.6  Design decisions depend on features such as corpus size, available memory, and query response time. Two implementations have been developed to accompany this textbook, an earlier one in C and a more recent one in Java; see the FOA Web site7 for details.
foa-0031	2.5.1 Basic Algorithm  We now assume that:  ï  prior technology has successfully broken our stream of characters, our large corpus, into a set of documents;  ï  within each document we have identified individual tokens; and  ï  noise word tokens have been identified.  Then the basic flow of what we will call the postdoc function operates as follows (see Algorithm 2.1).  2  www.apple.com/slierlock/  3  ftp://ftp.os.cornell.eciu/pub/smart/  4  www.mcis.iiiiit.e4u.au/nig/welcome.iitml :lt; www.searGtitools.eom/tools/tooIs.litm.l  * www.glue.umcL edu/dlrg/filter/software,html " http://www.cse.urad.edu/-rik/FOA/ 52       FINDING OUT ABOUT  Algorithm 2.1 Basic Algorithm  for every doc in corpus  while (token = getNonNoiseToken) if(StemP)  token = stem (token) Save Posting(token,doc) in Tree  for every token in Tree  Accumulate ndoc(token), totfreq(token) Sortp G Postings(token)  descending docfreq(p) order write tokengt;ndoc,totfreq,Postings  For every document in the corpus we will iterate through a loop until weVe exhausted every token in that document. So let's call getNonNoiseToken a routine that repeatedly builds tokens from the document's stream, does whatever character assessments are required, checks it against a negative dictionary, and returns a token. If stemming is to be applied, well stem the word at this point. Then we will save a posting for that token's occurrence in that document. A posting is simply a correspondence between a particular word and a particular document, representing the occurrence of that word in that document.* That is, we have a document in front of us and it contains a set of tokens. We are now going to build a representation for each token that tells all of the documents in which ones it occurs. For each keyword we will maintain the token itself as the key used for subsequent access and the head of a linked list of all postings, each containing the document number and the number of occurrences of the keyword in that document. A sketch of these data structures is shown in Figure 2.4.  After going through every document in the corpus in this fashion, we have a large collection of postings. Here we recommend splay trees as an appropriate data structure for these keywords and their postings. In the C implementation shown in Algorithm 2.2, the installTermO  Implementation    function inserts a new posting into the Terms tree J details  * Well discuss either data we might also keep with the posting later; c£. Section 2.5.2. EXTRACTING LEXICAL FEATURES       53  KW_INV token totdoc  head      "aardvarck" 20 65       totfreq I c POSTING locno       freq next     ^ª         o  FIGURE 2.4 Basic Postings Data Structures  Algorithm 2.2 postdoc.c Details  void postDoc (int docno, FILE *docf, long int bpos, long int epos, char *proxy){  GetTermString(proxyPos, Noise, MaxTermSize,newterm); GetTerm(docf, Noise, MaxTermSize,newterm); InstallTerm(newterm, docno, Terms);  During the processing of each document, it will prove important to know how many keywords are extracted from it. This will be known as the documents length, denoted lengths this quantity is important when normalizing documents of different lengths. One way to implement this computation is to maintain a small separate file doclend.d containing only this one number for each document.  When the set of documents has been exhausted, we need to write out this inverted representation to a file for subsequent processing. For every token in the splay tree (typically the traversal will be in lexicographic order), we will organize all its postings. First, we count the number of occurrences of the keyword across all the documents in the corpus; we will call this variable totfreqk. A second, less obvious statistic we will maintain is how many documents contain this keyword; this variable will be called docfreqi. If there is exactly one occurrence of a 54      FINDING OUT ABOUT  KW token totdoc  wgt head  "aardvarck" 20 65 0.634   totfreq  FPOST  freq   dochd next   o o  FIGURE 2.5 Refined Postings Data Structures  keyword in each document, then these two numbers will be the same. But typically there are multiple occurrences of the same keyword in a single document and totfreq gt; docfreqk. Both variables will be important to us in determining appropriate weights for the Index relation (cf. Chapter 3).  After going through all of the documents and accumulating for each these two statistics, we must sort the postings in decreasing frequency order. The reason for this won't be apparent until we discuss the matching algorithms (cf. Section 3.5), but it turns out to be important that documents that use a keyword most often are at the beginning of the list.  Once the documents' postings have been sorted into descending order of frequency, it is likely that several of the documents in this list will have the same frequency, and we can exploit this fact to compress their representation. Figure 2.5 shows the POSTING list broken into a list of FPOST sublists, one for unique frequency count.
foa-0032	2.5.2 Fine Points  Changing Indices for Dynamic Corpora  One reason to keep raw frequency counts in the inverted keyword file used by our experimental implementation is that this provides maximum flexibility as we consider various keyword weighting schemes. But there is another reason these raw statistics are useful in real applications. EXTRACTING LEXICAL FEATURES  55  It is often important to be able to update a corpus's index as documents are added to or deleted from it. Retention of raw keyword frequency information allows these statistics to be updated as our corpus changes. Adding a new document simply requires that it be analyzed (as outlined earlier), simply incrementing existing counters for each keyword. ^ Similarly, deletion of documents from an index exploits the full text of the document itself to identify all keywords it contains. For each keyword then, posting counts are simply decremented.*  Implementation details  Posting Resolution  Typically we need only keep track of which document contains the posting. But an important element of many query languages is proximity operators, which allow users to specify how close two keywords must be (adjacent words, within the same sentence, within the same paragraph, within a fc-word window of one another, etc.). To support such queries, we may also be concerned with recording higher resolution posting information than which document it is in. For example, many systems retain the exact character position of the (beginning of the) keyword. Figure 2.6 shows the elaborate data structure used by the STAIRS IR system.^ In addition to very high-resolution postings, this representation supports other query attributes (e.g., security)."^  Emphasizing Proxy Text  The fact that keyword tokens occur in both the proxy text and the main text of the document gives us the opportunity to treat them differently. For example, we can emphasize the importance of words used in the proxy over those occurring in the raw text. This would be sensible if we believed that those occurrences in, for example, the subject of a message or the title of a dissertation, are better characterizations of a document than words picked from the text of the abstract or the text of the email message. In our code, this emphasis will be controlled by an integer variable EmphProxy, which notes occurrences of keywords in the proxy by doubling (EmphProxy = 2) or tripling (EmphProxy = 3) the keyword counters for proxy text.  More about STAIRS  Proximity searching with low-resolution posting  information  * The optimized fpost data structure makes this update awkward as well. 56      FINDING OUT ABOUT  Dictionary  Number of  documents to  which term is  assigned  FIGURE 2.6 STAIRS Posting Information  Reproduced with permission from Salton and McGill [Salton and McGill, 1983,  figure 8.5, p. 244]  Document Number  Because we have made the first stage of our processing flexible with  respect to how a corpus extends across multiple files in general, two  numbers will uniquely identify each of your documents: its file number and the document number within that file. For that reason, and because each posting must retain a unique identifier for each document, it becomes important to construct a single number that folds them together. Maintaining a single integer, instead of two integers, therefore becomes a worthwhile space-saver.  One simple way to accomplish this is to multiply the document's file number by some number larger than the maximum number of documents within any file, and then add its document number. Just EXTRACTING LEXICAL FEATURES       57  Subject:             Re: IDE or SCSI?  From:               helpful@urban.or.jp (me @ my humble abode)  Date:                1997/08/29  Message-Id:        lt;3406f93a.50446926@nnrp.gol.oomgt;  Newsgroups:      comp.os.ms-windows.nt.setup.hardware,comp. windows, nt.misc  [More Headers]  On Thu, 28 Aug 1997 23:10:03 GMT, Michael Query lt;query@dpi.qld.gov.augt; wrote:  gt;My question is, should I get another 2 Gb SCSI disk for putting the gt;0S (NT 4.0 WS), software, etc on, or should I get an IDE disk for this?  Having played around with different conngs for a while, I'd say go SCSI. I'd do that even if I had to get a second SCSI controller.  (You'll "hear" a lot of people arguing that IDE is good enough, but if you are after overall improved performance SCSI is best.)  my 2Y.  FIGURE 2.7 Quoted Lines in an Email Message  how large a number this must be and whether your machine/compiler efficiently supports integers this large (or whether you are better off keeping the two numbers separate) will vary considerably. For this reason it makes good sense to isolate these issues in a separate routine.  Dependencies on Document Type  The process of indexing has been idealized as having a first stage, where we worry about what kind of document it is (e.g., whether it's a thesis or an email message), and then assuming that subsequent processing is completely independent of document type. Like all software designs, this idealization breaks down in the face of real data.  Consider email messages. One common element of these documents is quoted text from another email message. Often this is marked by a gt; prefix, as shown in Figure 2.7. The role of interdocument citations like this is considered in depth in Section 6.1, but for now, a reasonable design decision is that all text should be indexed only once. This is especially appropriate if we have both the original email message and the quoted version of it; we might want to elide (ignore) quoted lines.  Other software designs are possible, but the easiest way to implement this is to check for quoted lines within the postdoc routine; if the first character of a line is a greater-than symbol don't do any of the subsequent processing. Don't check it against noise words, don't stem, don't index, 58      FINDING OUT ABOUT  and don't install it in the term tree. Unfortunately, this creates precisely the kind of email-specific processing that should be avoided by wellengineered software.
foa-0033	2.5.3 Software Libraries  As much as possible, we will depend on standard libraries for some of our basic utilities. In particular, it is recommended that you use:  ï  gethash - This is the standard hashtable routine, part of most Unix distributions.  ï  stopper - Chris Fox's lexical analyzer, also incorporating stopword removal. This code accompanies Fox's chapter, "Lexical Analysis and Stoplists" [Fox, 1992].  ï  splay - Splay trees provided many of the benefits of balanced binary trees without many of the hassles of perfectly balanced (e.g., AVL) trees [Sleator and Tarjan, 1985]. This implementation was written by David Brower [Brower, 1994, chapter 10].  ï  stem-This is the "Porter" stemmer, a pattern-matching affixremoval stemmer [Porter, 1980].
foa-0034	3___________  Weighting and Matching against Indices  The Bible Code as Ouija Board: The fascination with the subliminal, the camouflaged, and the encrypted is ancient. Getting a computer to munch away at long strings of letters from the Old Testament is not that different from killing animals and interpreting the entrails, or pouring out tea and reading the leaves. It does add the modern impersonal touch - a computer found it, not a person, so it must be "really there." But computers find what people tell them to find. As the programmers like to say, "prophesy in, prophesy out." [Menaud, 1996]
foa-0035	3.1 Microscopic Semantics and the Statistics of Communication  In the last chapter, we described the FOA process linguistically, in terms of words that occur in documents, morphological features of these words,  structures organizing the sentences of documents, etc. We now want  to treat all of these words - which have meaning to their authors and  to us reading them - as a meaningless stream of data: word after word after word. (Imagine it coming from some SETI radio telescope, eavesdropping on the communication of some other planet!) We will now seek patterns and trends common to this data, using the same sorts of statistical tricks that physicists typically use on their data streams. What  60 WEIGHTING AND MATCHING AGAINST INDICES       61   TABLE 3.1 English Letter Frequencies  Letter         Frequency Letter Frequency  E .120 F .024  T .085 M .024  A .077 W .022  I .076 Y .022  N .067 P .020  O .067 B .017  s .067 G .017  R .059 V .012  H .050 K .007  D .042 Q .005  L .042 J .004  U .037 X .004  C .032 z .002  can we learn from looking at the statistics of our data stream, treating  text as meaningless and attempting ; to infer a new notion of meaning  from those statistics?  But now let's narrow our focus, all the way down to the bits and characters used to represent the corpus as, for example, a file on a physical device, like a hard disk. Imagine that you are an archaeologist trying to study some civilization that left this evidence behind. How might you interpret this modern Rosetta Stone?  Let's ignore those issues relating to basic ASCII encoding. That is, suppose we have special knowledge of a character set. Then the frequency of these characters' occurrences would already give us a great deal of information. Anyone who has studied simple cipher techniques (or played Scrabble) knows that a table of most frequently used letters (cf. Table 3.1 [Welsh, 1988]) can be used to break simple codes.  In this chapter we will move another level above characters. We will consider morphological transformations we can perform on character sequences that help us to identify root words. We will briefly mention phrases by which multiple words can be joined into simple phrasal units.  At each level we will ask very similar questions: What is our unit of analysis; i.e., what are we counting? Then, what does the distribution of frequency occurrences across this level of features tell us about the pattern of their use? What can we tell about the meaning of these features, based on such statistics [Francis and Kucera, 1982]? 62       FINDING OUT ABOUT  In fact, many influential thinkers have looked at such patterns among symbols. Going back to some of the most ancient writings suggests that statistical analyses of the original Hebrew characters and their positions within the two-dimensional array of the page reveals new "codes" [Witztum et aL, 1994; Drosnin, 1997].  Donald Knuth, one of computer science's most renowned theoreticians, has analyzed an apparently random verse (Chapter 3, verse 16) from 59 of the Bible's books and used these verses as the basis of stratified sampling of the approximately 30,000 Biblical verses [Knuth, 1990]. He found, for example, that the 3:16 verses were particularly rich in occurrences of YHWH, the ancient Hebrew name for God. Personally, Knuth considered this analysis the source for "historical and spiritual insights," as part of a Bible study class he led. But speaking scientifically, how can we find meaning in text, and how are such attempts to be distinguished from the kinds of "Ouija board" numerology criticized by Menaud in the quotation opening this chapter?
foa-0036	3.2 Remember Zipf  Looking at our corpus as a very long string of characters, something that even a monkey could generate, provides a useful baseline against which we can evaluate larger constructs.  Associate with each word w its frequency F{w)gt; the number of times it occurs anywhere in the corpus. Now imagine that we've sorted the vocabulary according to frequency so that the most frequently occurring word will have rank r = 1, the next most frequently used word will have r = 2, and so on.  George Kingsley Zipf (1902-50) has become famous for noticing that the distribution we find true of our corpus is in fact very reliably true of any large sample of natural language we might consider. Zipf {Zipf, 1949] observed that the words' rank-frequency distribution can be fit very closely by the relation:  F(r) = ó,    a´l,     C´0.1                    (3.1)  This empirical rule is now known as Zipf's law.  But why should this pattern of word usage, something we can reasonably expect to vary with author or type of publication, be so WEIGHTING AND MATCHING AGAINST INDICES       63  100,000 y     Log(Freq)  10,000 - 1000  - 100  - 10 - Log(Rank)  H  10                         100                       1000  FIGURE 3.1 Zipfian Distribution of AIT Words  10,000  universal? What's more, the notion of "word" used in this formula has varied radically: in tabulations of word frequencies by Yule and Thorndike, words were stemmed to their root form; Yule counted only nouns [Yule, 1924; Thorndike, 1937]. Dewey [Dewey, 1929] and Thorndike collected statistics from multiple sources; others were collected from a single work (for example, James Joyce's Ulysses). The frequency distribution for a small subset of (nonnoise words in) our AIT corpus is shown in Figure 3.1. Note the nearly linear, negatively sloped relation when frequency is plotted as a function of rank, and both are plotted on log scales.
foa-0037	3.2.1 Looking for Meaning in All the Wrong Places (At the Character Level)  The ubiquity of data obeying Zipf *s law has made it a lightning rod, attracting a number of "explanations." These explanations come from 64      FINDING OUT ABOUT  an extremely impressive set of original thinkers, in widely ranging disciplines:  ï  Noam Chomsky, the most influential linguist of the past 30 years, together with George Miller, the mathematical psychologist famous for such insights as the "7 ± 2 chunks" of memory limitation [Miller, 1957];  ï  Herbert Simon, the Nobel Prize-winning economist and one of the fathers of artificial intelligence [Simon, 1955]; and  ï  Benoit Mandelbrot, the mathematician and physicist most famous for his work on fractals [Mandelbrot, 1953].  Herbert Simon, a keen observer of much cognitive activity, suggests that the ubiquity of Zipf 's law across heterogeneous collections should make us somewhat suspicious of its ability to address the "fine structure"  of linguistics:  No one supposes that there is any connection between horse kicks suffered by soldiers in the German army and blood clots on a microscope slide other than that the same urn scheme provides a satisfactory abstract model of both phenomena. It is in the same direction that we shall look for an explanation of the observed close similarities among the five classes of distributions___[Simon, 1955]  (With "urn " Simon is referring to mathematical models, e.g., related to Poisson processes. See Section 3.3.2 for more on the "five classes" of  Simon's models.)  We therefore begin this section by reviewing a number of early attempts to explain the phenomena underlying Zipf *s law; its mathematical derivation is reserved for Chapter 5.
foa-0038	3.2.2 Zipfs Own Explanation  To explain his empirical observations, Zipf himself proposed a theoretical model that described the ultimate purpose of communication between authors and readers.  Zipfs theory was extraordinarily broad, addressing not only (!) patterns in text but also patterns across all human activities. According WEIGHTING AND MATCHING AGAINST INDICES       65  to Zipf 's fundamental Principle of Least Effort all activities can be viewed as interactions between jobs needing to be done and tools developed to accomplish the jobs. In a mature society in which a variety of jobs and tools have existed for some time, a "reciprocal economy" forms. That is, there is a set of tools appropriate for doing certain jobs, and there is a set of jobs requiring certain tools. The Principle of Least Effort asserts that a person attempting to apply a tool to a job does so in order to minimize the probable effort in using that tool for that particular job.  In applying this principle to texts, Zipf makes an important correspondence - words work as tools, accomplishing jobs we need done. To simplify the situation greatly, imagine that the job an author is attempting to accomplish is simply to "point" to some referent, something in the world."'' Authors would find it most convenient to simply use one Pointing word all the time for all the jobs they are trying to accomplish. It makes their task much easier; picking the right word is effortless. The author has a pressure toward unification of the vocabulary.  From the reader's point of view, it would be least ambiguous if a unique term were used for every possible function, every possible interpretation, every meaning. Readers therefore have a pressure toward diversification of the vocabulary. This leads to the vocabulary balance we observe in Zipf's rule. Zipf hypothesized that interplay between the forces of diversification and unification results in the use of existing words, which does not extend the vocabulary in most situations, together with the inclusion of new words in those novel situations that demand them. The trick is to find an economy of language that best satisfies both writer and reader. Note, however, that the maintenance of the balance requires that authors receive feedback from their readers, confirming that they are both "pointing" to the same referent.  Blair has extended Zipf's analysis, considering ZipPs tool/job setting as it's applied to our FOA task [Blair, 1990; Blair, 1992]. He argues that one of the primary reasons FOA systems fail is that the vocabulary balance is upset. The system of descriptors indexing the authors' works (for example, the library or the Web), standing between the authors who are writing the books and the searchers attempting to find them, breaks the feedback channel that keeps the shared vocabulary in balance when author and reader are in direct contact.
foa-0039	66      FINDING OUT ABOUT  3.2.3 Benoit Mandelbrot's Explanation  The early days of cybernetics were heady, and Zipf was not alone in seeking a grand, unifying theory to explain the phenomena of communication on computational grounds like those proving so successful in physics. Benoit Mandelbrot was equally ambitious.  Mandelbrot's background as a physicist is clear when he considers the message decoder as a physical piece of apparatus, "... cutting a continuous incoming string of signs into groups, and recoding each group separately" [Mandelbrot, 1953]. This "differentiator" complements an "integrator," which reconstitutes new messages from individual words. Within this model communication can be considered "fully analogous to the perfect gas of thermodynamics." Minimizing the cost of transmission corresponds to minimization of free energy in thermodynamics.  Mandelbrot was also interested in how the critical parameter a varied from one vocabulary to another. Extending the physical analogy of thermodynamic energy, the informational temperature or temperature of discourse is proportional to I/or, which Mandelbrot argues provides a much better measure of the richness of a vocabulary than simply counting the number of words it contains.  The value I/a can also be used to relate our analysis of Zipf s law to Mandelbrot's fractals. If the letters of our alphabet are imagined to be digits of numbers base n + 1 and a leading decimal point is placed before each word, then each word corresponds to a number between 0 and 1.  The construction amounts in effect to cutting out of [0,1 ] all the numbers that include the digit 0 otherwise than at the end. One finds that the remainder is a Cantor dust, the fractal dimension  of which is precisely I/or. [Mandelbrot, 1982, p. 346]  Mandelbrot proposed a more general form of Zipf s law:  (3'2)  which has proved important in analysis of the relationship between word frequencies and their rank (cf. Section 5.1).  Mandelbrot also suggested this as a potential model of cognition:  Whatever the detailed structure of the brain it recodes information many times. The public representation through phonemes WEIGHTING AND MATCHING AGAINST INDICES       67  is immediately replaced by a private one through a string of nerve impulses.... This recorded message presumably uses fewer signs than the incoming one; therefore when a given message reaches a higher level it will have been reduced to a choice between a few possibilities only without the extreme redundancy of the sounds. The last stages are "idea" stages, where not only the public representation has been lost, but also the public elements of information. [Mandelbrot, 1953, pp. 488-9]  Mandelbrot makes other provocative suggestions, for example, that schizophrenics provide the best test of his theory because these individuals impose the fewest "semantic constraints" on the random process (generating language) of interest?!  Although he was unsuccessful at his more ambitious goal of wedding a physical model of communcation to models of semantics such as Saussure's, Mandelbrot was probably the first to characterize the real truth underlying Zipfian distributions. This derivation is put off until Chapter 5. We conclude here with one more historical perspective, due to Herbert Simon, and a couple of more modern rediscoveries of Zipfian phenomena.
foa-0040	3.2.4 Herbert Simon's Explanation  Simon considered a very different model, focused on the author's activity of constructing a text. Simon's model is based on two assumptions. First, that new words - neologisms - introduced by the author and not previously used are introduced at some constant probability. Second, that the probability of a word having occurred exactly i times is proportional to the total number of occurrences of all words that have appeared exactly i times. These assumptions allow Simon to use the basic mathematics of "pure birth processes" to account for word frequency rankings.*  Simon was interested in models for the "associative" processes underlying authors' cognition, "... sampling earlier segments of the word sequence" [Simon, 1955, p. 434] as they compose. He acknowledged  * In the same paper, "On a class of skew distributions" [Simon, 1955], Simon gives similar accounts for the numbers of papers published by scientists, city populations, and distributions of biological taxa. 68      FINDING OUT ABOUT  that authors also use processes of "imitation: sampling segments of word sequences from other works of other authors, and, of course, from sequences he has heard." Simon imagined this model as potentially applying to three different distributions:  ï  the distribution of word frequencies in the whole historical sequence of words that constitute a language;  ï  the distribution of word frequencies in a continuous piece of prose; and  ï  the distribution of word frequencies in a sample of prose assembled from compositive sources.  He seems most engaged by the second of these alternatives, considering the activity of a particular author. (He uses James Joyce's Ulysses as an example.)  Obviously this word-based model provides a very different explanation of Zipf s law from Mandelbrot's and Miller's character-based ones. Simon was familiar with such models but argued [Simon, 1955, p. 435] that his own "averaging rather than [Miller's] maximizing assumptions" are more desirable. But as Miller notes, "The assumption of maximization can be replaced by the assumption of random spacing" [Miller, 1957, p. 313]. Worse, in terms of the empirical bottom line, Simon's equation does not fit available data as well.
foa-0041	3.2.5 More Recent Zipfian Sightings  The debate concerning these models dates back almost 40 years, but Zipfian distributions and attempts to explain them continue to arise. For example, many have been struck by language-like properties exhibited by  the long sequences of genetic codes found in the DNA of all living species. That is, a simple "alphabet" of four nucleic acid base-pairs (BPs) (A, C,  G, T in DNA) grouped into three-letter codons that mean one of twenty  possible "words" corresponding to amino acids has led many to wonder what we might learn by viewing the genome as a linguistic object [Sereno,  1991].  Mantegna et al. [Mantegna et ai, 1994] was led to consider the "word" frequency distributions of such words in the DNA "corpus." They considered differences in the distributions across coding regions of the WEIGHTING AND MATCHING AGAINST INDICES       69  genome as well as noncoding regions that are never expressed. Their first result is that this sequence data does indeed contain "linguistic features," especially in the noncoding regions. By analyzing various genetic corpora (e.g., approximately a million BPs taken from 14 mammalian sequences), they found that, in contrast to what we might expect of completely random sequences, the rank-frequency distribution of six-BP words could be well fit by a (log-log linear) Zipf exponent equal to ó0.28. They conclude:  These results are consistent with the possible existence of one (or more than one) structured biological languages present in noncoding DNA sequences. [Mantegna et al., 1994, italic not in original]  Subsequent analysis, however, makes it quite clear that any such interpretations are ill-founded [Bonhoeffer et al., 1996]. Deviations from fully random sequence behavior can be attributed to two simple characteristics of biological sequence data. First, define H(n) to be the entropy of the distribution of n-length nucleotide sequences. Then the redundancy R(n) of length n words is:  R(n) = 1-^1                             (3.3)  In  so the nonrandom R{ 1) reflects a simple increase with the variance of the four base-pairs, a well-known biological fact. Further, very short range correlations between nucleic acids (easy to imagine given the basic threeletter genetic code) means that in DNA the most common words are simply combinations of the most probable letters, especially in regions of short repeats. There are still interesting questions (e.g., why coding and noncoding regions differ in their nucleic acid frequencies), but none that suggest any large-scale language-like properties within the DNA sequence.  A final recent example of how Zipf-like distributions arise is offered by analyses of WWW surfing behaviors [Huberman et al., 1998] and makes this same point (but cf. Section 8.1 for more recent, apparently contradictory data generated from massive Alta Vista logs). Consider each page click by a browsing user to be a "character" and the amount of time spent by the same user on a host to be the length of a 41 word." Then (surprise!), empirical data capturing the rank-frequency 70      FINDING OUT ABOUT  1000 i  *    100 o  10 1                          10                       100                        1000  log(Clicks)  FIGURE 3.2 Rank/Frequency Distribution of Click-Paths From [Huberman, 1998]. Reprinted with permission of Science  distribution of each WWW surfing "ride" again show a (log-log linear) Zipfian relationship with slope equal to ó1.5, as shown in Figure 3.2.  Huberman et al. also propose a model explaining this empirical data. Assume that the "value" (what we might think of as perceived relevance) V{L) of each page in a browsing sequence of length L goes up or down according to identical, independently distributed (iid) Gaussian random variables 6^:  V(L) = V(L- 1) + ÄL                          (3.4)  Using economic reasoning, Huberman et al. then hypothesize:  ... an individual will continue to surf until the expected cost of continuing is perceived to be larger than the discounted expected  value of the information to be found in the future___Even if  the value of the current page is negative, it may be worthwhile  to proceed, because a collection of high value pages may still be found. If the value is sufficiently negative, however, then it is no longer worth the risk to continue. [Huberman et al., 1998]  If users' browsing behaviors follow a random walk governed by these considerations, Huberman et al. show that the passage times to this cutoff threshold are given by the inverse Gaussian distribution:  (3.5) WEIGHTING AND MATCHING AGAINST INDICES       71  where \i is the mean of the random walk length variable Ly fi3/k is its variance, and A is a scaling parameter.
foa-0042	3.2.6 Summary  I have provided this historical background because these eminent scientists' stories remain compelling. The plausibility of the proposed theories, coupled with our retrospective knowledge of their incorrectness, also provides a sobering background as we attempt to infer semantic properties from the statistics arising in FOA. As we shall discuss in Chapter 5 (cf. Section 5.1), the real basis of ZipPs law can be traced to much simpler mechanisms, relating only to patterns of characters rather than any underlying semantics or purposes. Benoit Mandelbrot, George Miller, and Noam Chomsky have shown that the underlying phenomena relating a word's frequency to its rank order is obeyed as much by random text generated by monkeys at typewriters, for example - as by samples of text (the Bible, James Joyce's Ulysses, etc.) we seem to find more literate.  The fact that the simple, four-character sequence ZIPF should bring together such a rich combination of mathematical and semantic issues is ironic, to say the least. There is obviously a great deal we can predict about our language by assuming nothing more than we would about monkeys at keyboards. At the same time, the fact that we can change the meaning of a simple sequence of characters, for example, the title of this section REMEMBER ZIPF, dramatically by adding a single additional character to form either REMEMBER ZIPF! or REMEMBER ZIPF? should make it clear how much more there still is to say.
foa-0044	3.3.1 Lexical Consequences, Internal/External Perspectives  The plot in Figure 3.2 is based on word-frequency statistics like those  shown in Table 3.2. Note that on the log-log plot in Figure 3.2, frequency  is a nearly linear inverse function of rank.  One way to make the various lexical decisions considered in the last chapter is to consider the effects of various decisions in terms of statistics such as these. Table 3.3 shows the statistics for stemmed, nonnoise 72       FINDING OUT ABOUT  TABLE 3.2 AIT Keywords Frequency/Rank Distribution.  Rank Frequency Keyword Rank Frequency Keyword  1 25,438 system 20 4836 algorithm  2 24,745 univers 100 1646 dissert  3 12,107 base 200 971 util  4 11,938 network 300 767 Zurich  5 11,930 model 400 624 genet  6 10,303 de 500 474 event  7 8568 knowledg 600 363 definit  8 8320 neural 700 289 underli  9 7465 process 800 234 explicit  10 7293 design 900 196 teach  11 6758 control 1000 171 lisp  12 6308 intellig 1500 89 advis  13 6308 develop 2000 51 compound  14 6243 use 2500 33 praisal  15 6074 learn 3000 24 af  16 5837 applic 4000 13 meshe  17 5617 expert 5000 8 hermeneut  18 5558 approach 6000 4 html  19 5464 comput 6660 3 replai  word tokens (shown in monospaced font, e.g., SYSTEM), together with noise words (shown in italics, e.g., the). As expected, the noise words occur very frequently. But it is interesting to contrast those very frequent words defined a priori in the negative dictionary with those that occur especially frequently in this particular corpus. In many ways these are excellent candidates for external keywords: characterizations of this corpus's content, from the "external" perspective of general language use. That is, these are exactly the words (cf. NEURAL NETWORK, BASE, LEARN, WORLD, KNOWLEDGE) that could suggest to a WWW browser that the AIT corpus might be worth visiting. Once "inside" the topical domain of AI, however, these same words (cf. SYSTEM, MODEL, PROCESS, DESIGN) become as ineffective as other noise words, as internal keywords, discriminating the contents of one AIT dissertation from the next.  Table 33 also shows statistics both with and without stemming. For example, the token SYSTEM itself appeared only 8632 times; variations like SYSTEMS and SYSTEMATIC must account for the other 12,856. This simple example also demonstrates how issues of phrase recognition WEIGHTING AND MATCHING AGAINST INDICES       73   TABLE 3.3 Consequences of Lexical Decisions on Word Frequencies    Unstemrned   Unstemmed  Token Frequency Frequency Token Frequency Frequency  the 78,428  that 9820   of 50,026  are 9792   and 33,834  LEABN 9293   a 31,347  WORLD 8103   to 28,666  la? 7678   in 21,512  an 7593   SYSTEM 21,488 8,632 KNOWLEDG 7410 5,496  is 18,781  HETOBAL 7220 3,912  MODEL 14,772 4,796 with 7197   for 14,640  as 6964   de? 11,923  on 6920   NETWORK           10,306 3,965 by 6886   this 10,095  PROCESS 6569 2,900  BASE 9838  DESIGN 6362 3,308  (cf. NEURAL NETWORK) and other messy issues (e.g., the presence of French noise words in some of the dissertation abstracts but not in our English negative dictionary) can arise in even the simplest, "cleanest" corpora.
foa-0045	3.3.2 Word Occurrence as a Poisson Process  When the words contained in a corpus are ranked and shown to be distributed according to a Zipfian distribution, an obvious but important observation can be made: The most frequently occurring words are not really about anything. Words like NOT, OF, THE, OR, TO, BUT, and BE obviously play an important functional role, as part of the syntactic structure of sentences, but it is hard to imagine users asking for documents about OF or about BUT. Define function words to be those that have only (!) a syntactic function, for example, OF, THE, BUT, and distinguish them from content words, which are descriptive in the sense that we're interested in them for the indexing task. This is one of the first - but most certainly not the last - examples FOA makes using a priori determinations of a word's semantic utility based on its statistical properties. For example, we might hope that function words occur randomly throughout arbitrary text, while content words do not. One ubiquitous 74      FINDING OUT ABOUT  model of randomness is as a Poisson process, used in the past to model things like:  ï  raisins' distribution across slices of bread; or  ï  misprints' distribution across printed pages; or  ï  the distribution of people's birthdays across days of the year.  In the case of our documents, we'll start with a slightly simpler Bernoulli model wherein we imagine an author making binary decisions, picking a keyword k with probability p^ Then in a document of length L the probability that a keyword was selected exactly n times in document d is:  Pr(/w = n) = (L) (pk)n(l - pk)l~n                  (3.6)  In other words, we'd expect it to occur an average of pk - L times in a document of length I.  As I ógt; oo and p ó+ 0 (and the mean value X = p ï L ó* 1), the Poisson distribution:  Pr(/W = n) = ói-L                           (3.7)  converges to this same distribution. We will generally be interested in a large set of parameters A.fcgt; each corresponding to a particular keyword  L If we imagine a Bernoulli-like experiment, where individual function  words are placed with low probability and observed across the many "experiments" of words occurring in documents, we can expect that a particular word k will occur n times in a randomly selected document according to a Poisson distribution. (Because documents are of different lengths, we must also take care to normalize them all to the same number of experiments.)  As an example of how a Poisson model might be applied to good use, work pioneered by Bookstein and Swanson in the mid-1970s proposed that function words are distributed according to a relatively constant Poisson distribution, while content words are not [Bookstein and Swanson, 1974; Bookstein and Kraft, 1977, Croft and Harper, 1979]. That is, when a keyword is found in a document, it is for one of two possible reasons: Either it just happens (randomly) to be there, or it really WEIGHTING AND MATCHING AGAINST INDICES       75  means something. Robertson and Walker [Robertson and Walker, 1994] distinguish the latter elite occurrences of a keyword:  We hypothesize that occurrences of a term in a document have a random or stochastic element, which nevertheless reflects a real but hidden distinction between those ... "elite" documents which are about the concept represented by the term and those which are not. We may draw an inference about eliteness from the term frequency, but this inference will of course be probabilistic. Furthermore, relevance (to a query which may of course contain many concepts) is related to eliteness rather than directly to term frequency, which is assumed to depend only on eliteness. [Robertson and Walker, 1994, p. 233, underline not in original]           ?1 s 2  In addition to discriminating function from content words, the Poisson model has been used to measure the degree to which a content word is effective as a keyword for a document [Robertson and Walker, 1994]. If we assume that a potential keyword effectively describes some documents in a corpus but occurs at the level of chance throughout the rest of the corpus, the distribution of this keyword across the corpus can be described as the mixture of a Poisson process with some other distribution.  The so-called two-Poisson model models both distributions (i.e., one over the Rel documents that could accurately be characterized as about this keyword and a second over the rest of the Rel documents, which are not) as Poisson, but with distinct means Xlw and X2wy with the superscripts 1 and 2 referring to the Rel and Rel distributions, respectively. One advantage of assuming that both distributions are Poisson and that we only need to discriminate between two classes (relevant versus nonrelevant) is that a single-parameter pTe\ = Pr(Relevance) controls the probability that the word w is relevant:  about w | k occurrences of w  (3.8)  This probability can then be used as part of a decision theoretic model related to the costs of indexing too many or too few documents with a keyword w (cf. Section 5.5.6). 76      FINDING OUT ABOUT
foa-0046	33.3 Resolving Power  Zipf observed that the frequency of words' occurrence varies dramatically, andPoisson models explore deviations of these occurrence patterns from purely random processes. We now make the first important move toward a theory of why some words occur more frequently and how such statistics can be exploited when building an index automatically. Luhn, as far back as 1957, said clearly:  It is hereby proposed that the frequency of word occurrence in an article furnishes a useful measurement of word significance. [Luhn,1957]  That is, if a word occurs frequently, more frequently than we would expect it to occur within a corpus, then it is reflecting emphasis on the part of the author about that topic. But the raw frequency of occurrence in a document is only one of two critical statistics recommending good keywords.  Consider a document taken from our AIT corpus, and imagine using the keyword ARTIFICIAL INTELLIGENCE with it. By construction, virtually every document in the AIT is about ARTIFICIAL INTELLIGENCE!? Assigning the keyword ARTIFICIAL INTELLIGENCE to every document in AIT would be a mistake, not because this document isn't about ARTIFICIAL INTELLIGENCE, but because this term cannot help us discriminate one subset of our corpus as relevant to any query. If we change our search task to looking not only in our AIT corpus but through a much larger collection (for example, all computer industry newsletters), then associating ARTIFICIAL INTELLIGENCE with those articles in our AIT subcorpus becomes a good idea. This term helps to distinguish AI documents from others.  The second critical characteristic of good Indices now becomes clear: A good index term not only characterizes a document absolutely, as a feature of a document in isolation, but also allows us to discriminate it relative to other documents in the corpus. Hence keywords are not strictly properties of any single document, but they reflect a relationship between an individual document and the collection from which it might be selected.  These two countervailing considerations suggest that the best keywords will not be the most ubiquitous, frequently occurring terms, nor WEIGHTING AND MATCHING AGAINST INDICES       77  Upper cutott  Lower cutoff  Zipf s first law  Rank order of words  Too common  Significant  Too rare  FIGURE 3.3 Resolving Power  those that occur only once or twice, but rather those occurring a moderate number of times. Using Zipf's rank ordering of words as a baseline, Luhn hypothesized a modal function of a word's rank he called resolving power, centered exactly at the middle of this rank ordering. If resolving power is defined as a word's ability to discriminate content, Luhn assumed that this quantity is maximal at the middle and falls off at either very high or very low frequency extremes, as shown in Figure 3.3.* The next step is then to establish maximal and minimal occurrence thresholds defining useful, midfrequency index terms. Unfortunately, Luhn's view does not provide theoretical grounds for selecting these bounds, so we are reduced to the engineering task of tuning them for optimal performance.  We'll begin with the maximal-frequency threshold, which is used to exclude words that occur too frequently. For any particular corpus, it is interesting to contrast this set of most-common words with the negative dictionary of noise words, defined in Section 2.3.2. While there is often great overlap, the negative dictionary list has proven itself to be useful across many different corpora, while the most frequent tokens in a particular corpus may be quite specific to it.  Establishing the low-frequency threshold is less intuitive. Assuming that our index is to be of limited size, including a certain keyword means we must exclude some other. This suggests that a word that occurs in  * After [van Rijsbergen, p. 16, figure 2.1 ]. 78      FINDING OUT ABOUT  Specificity                                Exhaustivity  ¶i  Discritninability                     Representation of        _______________       of  few      "'¶ª"¶¶¶´¶.......¶¶¶...............................................I    many  doc/jkw                             kw/doc  leads to  High precision                              High recall  FIGURE 3.4 Specificity/Exhaustivity Trade-Offs  exactly one document can't possibly be used to help discriminate that document from others regularly. For example, imagine a word - suppose it is DERIVATIVE - that occurs exactly once, in a single document. If we took out that word DERIVATIVE and put in any other word, for example, FOOBAR, in terms of the word frequency co-occurrence statistics that are the basis of all our indexing techniques, the relationship between that document and all the other documents in the collection will remain unchanged. In terms of overlap between what the word DERIVATIVE means, in the FOA sense of what this and other documents are about, a single word occurrence has no meaning!  The most useful words will be those that are not used so often as to be roughly common to all of the documents, and not so rarely as to be (nearly) unique to any one (or a small set of) document. We seek those keywords whose combinatorial properties, when used in concert with one another as part of queries, help to compare and contrast topical areas of interest against one another.
foa-0047	3.3.4 Language Distribution  We next move beyond characteristics of single keywords to an analysis of the distribution of the entire set of index terms. Any index, whether constructed manually or automatically based on word frequency patterns, is defined by a tension between exhaustivity on the one hand and specificity on the other. An index is exhaustive if it includes many topics. It is specific if users can precisely identify their information needs.  Unfortunately, these two intuitively reasonable desiderata are in some sense at odds with one another, as suggested by Figure 3.4. The best explanation of this trade-off is in terms of precision and recall WEIGHTING AND MATCHING AGAINST INDICES       79  (cf. Section 4.3.4): High recall is easiest when an index is exhaustive but is not very specific; high precision is best accomplished when the index is not very exhaustive but is highly specific. If we assume that the same index must serve many users, each with varying expectations regarding the precision and recall of their retrieval, the best index will be at some balance point between these goals.  If we index a document with many keywords, it will be retrieved more often; hence we can expect higher recall, but precision may suffer. Van Rijsbergen has talked about this extreme as a "document" orientation, or representation bias [van Rijsbergen, pp. 24, 29 ]. A documentoriented approach to index-building focuses the system builder's attention on a careful representation of each document, based on an analysis of what it is about.  However, an index's fundamental purpose is to reconcile a corpus's many document descriptions with the many anticipated users' queries. We could equally well analyze the problem from a query-oriented perspective - How well do the query terms discriminate one document from another?  From the users' perspective, we'd like to have these queries match meaningfully onto the vocabulary of our index. From the perspective of the corpus, we'd like to be able to discriminate one document from another. These are very different perspectives on an index, and they reflect a fundamental vocabulary mismatch [FurnasetaL, 1987] between the way users describe their interests and the way documents have been described.  If an indexing vocabulary is specific, then a user should expect that just the right keyword in a magic bullet query will elicit all and only relevant documents. The average number of documents assigned to specific keywords should be low. In an exhaustive indexing, the many aspects of a document will each be reflected by expressive keywords; on average many keywords will be assigned to a document:  Ikw\  Exhaustivity  oc   { ó- } \docj  The important observation is that these two averages must be taken across different distributions. We already know from Zipf s law that the 80      FINDING OUT ABOUT  Documents  Keyword distribution  ndex  NDoc  Document corpus  FIGURE 3.5 Indexing Graph  number of occurrences varies dramatically from one keyword to another. Once we make an assumption about how keywords occur within separate documents, we can derive the distribution of keywords across documents. However, the distribution of keywords assigned to documents can be expected to be much more uniform; documents are about a nearly uniform or constant number of topics. Figure 3.5 represents the index as a graph, where edges connect keyword nodes on the left with document nodes on the right. The Index graph is a bipartite graph, with its nodes divided into two subsets (keywords and documents) and nodes in one set having connections only with those in the other. If we assume that the total number of edges must remain constant, we can assume that the total area under both distributions is the same. The quantity capturing the exhaustivity/specificity trade-off is therefore the ratio of Vocab to corpus size NDoc  Although this analysis is crude, it does highlight two important features of every index. First, in most applications NDoc is fixed and Vocab is a matter of discretion, a free variable that can be tuned to increase or decrease specificity and exhaustivity. Second, certainly in WEIGHTING AND MATCHING AGAINST INDICES       81  most modern applications (i.e., with the huge disk volumes now common), NDoc ^gt; Vocab. This is one of the most important ways in which experimental collections (including AIT) differ from real corpora. A useful indexing vocabulary can be expected to be of a relatively constant size, Vocab ´ 103 to 105gt; while corpora sizes are likely to vary dramatically, NDoczt 104tol09.  Along similar lines, it is always useful to think about what this means in the context of the WWW, where the notion of a closed corpus disappears. The WWW is an organic, constantly growing set of documents; our vocabulary for describing it is more constrained.  Several other basic features of an index are shown in Figure 3.5. The flipped histogram along the left side is meant to reflect the Zipfian distribution of keywords, with the most frequent keywords beginning at the top. Recall that this distribution captures the total number of word occurrences, regardless of how these occurrences are distributed across interdocument boundaries. A second distribution is also sketched, suggesting how the number of documents versus word occurrences might be distributed; we can expect these two quantities to be at least loosely correlated. The distinction between intra- and interdocument word frequencies is a topic we'll return to in Section 3.3.7.
foa-0048	3.3.5 Weighting the Index Relation  The simplest notion of an index is binary- either a keyword is associated with a document or it is not. But it is natural to imagine degrees of about-ness. We will capture this strength of association with a single real number, a weight, capturing the strength of the relationship between keyword and document. This weight can be used in two different ways. The first is to reduce the number of links to only the most significant relationships, those with the highest weights. In this respect a weighted indexing system is a more general formulation than a binary formulation; we can always go to a binary relation from the weighted one. This might make weights useful even if our retrieval method is Boolean (as it often was in early information retrieval (IR) systems). But a second and today more common reason for using a weighted indexing relation is that the retrieval method can exploit these weights directly. 82      FINDING OUT ABOUT  One way to describe what this number means is probabilistic: We seek a measure of a document's relevance, conditionalized on the belief that a keyword is relevant:  Wkd c* ?r(^ relevant | k relevant)                   (3.10)  Note that this is a directed relation; we may or may not believe that the symmetric relation  Wdk ∞^ Pr(^ relevant | d relevant)                   (3.11)  should be the same. Unless otherwise specified, when we refer to a weight w we will intend it to mean Wkd In order to compute statistical estimates for such probabilities we define several important quantities:  fkd = number of occurrences of keyword k in document d fk = total number of occurrences of keyword k across entire corpus Djfc = number of documents containing keyword k                  (3.12)  We will make two demands on the weight reflecting the degree to which a document is about a particular keyword or topic. The first one goes back to Luhn's central observation [Luhn, 1961]: Repetition is an indication of emphasis. If an author uses a word frequently, it is because he or she thinks it's important. Define fkd to be the number of occurrences of keyword k in a document d.  Our second concern is that a keyword be a useful discriminator within the context of the corpus. Capturing this notion of corpus-context statistically proves much more difficult; for now, we simply give it the name discrim^.  Because we care about both, we will devise our weight to be the product of the two factors, corresponding to their conjunction:  u*kd oc fkd ' discrimk                                (3.13)  We will now consider several different index weighting schemes that have been suggested over the years. These all share the same reliance on f^ as a measure of keyword importance within the document and the same product form as Equation 3.13. What they do not share WEIGHTING AND MATCHING AGAINST INDICES       83  is how best to quantify the discrimination power discrintk  of the keyword.
foa-0049	3.3.6 Informative Signals versus Noise Words  We begin with a weighting algorithm derived from information theory. Information theory has proven itself to be an extraordinarily useful model of many different situations in which some message must be communicated across a noisy channel and our goal is to devise an encoding for messages that is most robust in the face of this noise.  In our case, we must imagine that the "messages" describe the content of documents in our corpus. On this account, the amount of information we get about this content from a word is inversely proportional to its probability of occurrence. In other words, the least informative word in our corpus is the one that occurs approximately uniformly across the corpus. For example, the word THE occurs at about the same frequency across every document in the collection; its probability of occurrence in any one document is almost uniform. We gain the least information about the document's contents from observing it.1"                                                                                                                              What is  Salton and McGill [Salton and McGIll, 1983], following Dennis    "information"? [Dennis, 1967], use Shannon's classic binary logarithm to measure the amount of information conveyed by each word's occurrence in bits and noise to be the absence of information:  pk = Pr(keyword k occurs)                         (3.14)  Infok= -log pk                                           (3.15)  Noisek = -log(l/p*)                                  (3.16)  Note that our evidence about the probability of a keyword occurring comes from statistics of how frequently it occurs. We must compare how frequently a keyword occurs in a particular document, relative to how frequently it occurs throughout the entire collection. We can calculate the expected noise associated with a keyword across the corpus, and from this we can infer its remaining signal. Signal then becomes another measure we can use to weight the frequency of occurrence of the keyword 84      FINDING OUT ABOUT  fid  Informative word  Noise word  liliiliii  FIGURE 3.6 Hypothetical Word Distributions  document:  (Noisek) = (pjfclog(l/pjt)gt; =  Signalk = log fk - Noisek = fkd * Signalk  A  Po%fd        (3.17)  (3.18) (3.19)  Two hypothetical distributions, for a noise word and a useful index term, are shown in Figure 3.6. A noise word is equally likely to occur anywhere; its distribution is nearly uniform. On the other hand, if all of the occurrences of a keyword are localized in a few documents (conveniently clustered together in the cartoon of Figure 3.6) and mostly zero everywhere else, this is an informative word. You've learned something about the document's content when you see it.  3.3.7 Inverse Document Frequency  Up to this point, we've been concerned only with the total number of times a word occurs across the entire corpus. Karen Sparck Jones has observed that, from a discrimination point of view, what we'd really like to know is the number of documents containing a keyword. This thinking underlies the inverse document frequency (IDF) weighting:  The basis for IDF weighting is the observation that people tend to express their information needs using rather broadly defined, frequently occurring terms, whereas it is the more specific* i.e., WEIGHTING AND MATCHING AGAINST INDICES       85  low-frequency terms that are likely to be of particular importance in identifying relevant material. This is because the number of documents relevant to a query is generally small, and thus any frequently occurring terms must necessarily occur in many irrelevant documents; infrequently occurring terms have a greater probability of occurring in relevant documents - and should thus be considered as being of greater potential when searching a database. [Sparck Jones and Willett, 1997, p. 307]  Rather than looking at the raw occurrence frequencies, we will aggregate occurrences within any document and consider only the number of documents in which a keyword occurs. IDF proposes, again using a "statistical interpretation of term specificity" [Sparck Jones, 1972], that the value of a keyword varies inversely with the log of the number of documents in which it occurs:  (  ugt;kd = fkd * (log ^P + l)                     (3.20)  where Dk is as defined in Equation 3.12.  The formula in Equation 3.20 is still not fully specified in that the count Dk must be normalized with respect to a constant Norm. We could normalize with respect to the total number of documents in the corpus [Sparck Jones, 1972; Croft and Harper, 1979]; another possibility is to normalize against the maximum document frequency (i.e., the most documents any keyword appears in) [Sparck Jones 1979a; Sparck Jones, 1979b]:  XT            iNDoc          or                           /aoi\  Norm = \               _                               (3.21)  [argmaxkDk  Today the most common form of IDF weighting is that used by Robertson and Sparck Jones [Robertson and Sparck Jones, 1976], which normalizes with respect to the number of documents not containing a keyword (NDoc ó Dk) and adds a constant of 0.5 to both numerator and denominator to moderate extreme values:  /      (NDoc -Djt) + 03\  = fkd * ^log ó^^5-----j         (3.22)
foa-0050	86      FINDING OUT ABOUT  Virtual spaces  Sparse vector spaces  3.4 Vector Space  One of life's most satisfying pleasures is going to a good library and browsing in an area of interest. After negotiating the library's organization and finding which floor and shelves are associated with the call numbers of your topic, you are physically surrounded by books and books, all of interest to you. Some are reassuring old friends, already known to you; others are new books by familiar authors, and (best of all!) some are brand-new titles by unknowns.  This system works because human catalogers have proven themselves able to reliably and consistently identify the (primary!) topic of a book according to conventional systems of subject headings like the Library of Congress Subject Headings or the Dewey Decimal system.  Our goal is to abstract away from this very friendly notion of physical space in the library to a similar but generalized notion of semantic space in which documents about the same topic remain close together. But rather than allowing ourselves to be restricted by the physical realities of three-dimensional space and the fact that books can only be shelved in a single place in a library, we will consider abstract spaces of thousands of dimensions.^  We can make concrete progress toward these lofty goals beginning with the Index matrix relating each document in a corpus to all of its keywords. A very natural and influential interpretation of this matrix (due to Gerry Salton [Salton et al., 1975; Salton and McGill, 1983]) is to imagine each and every keyword of the vocabulary as a separate dimension of a vector space. In other words, the dimensionality of the vector space is the size of our vocabulary. Each document can be represented as a vector within such a space. Figure 3.7 shows a very simplified (binary) Index matrix, and a cartoon of its corresponding vector representation.  Estimates of the vocabulary size of a native speaker of a language approach 50,000 words; if you are articulate, your speaking and reading vocabularies might be 100,000 or more words. Assuming that we have a modest 106 document corpus, this matrix is something like 106 x 105. That's a big matrix, even by modern supercomputing standards."^  In addition to the vectors representing all documents, another vector corresponds to a query. Because documents and queries exist within a common vector space, we naturally characterize how we'd like our U2    :   I  docl    / 1 0   1  doc2  doc3  ...   o\  0 1   1 0 0   1  WEIGHTING AND MATCHING AGAINST INDICES       87  kw3  d3  docn   \ 1  1   0      ...    0  Q    .100    ...  kwl  FIGURE 3.7 Vector Space  retrieval system to work - just as we go to a physical location in the library to be near books about a topic, we seek those documents that are close to our query vector. This is a useful characterization of what we'd like our retrieval system to accomplish, but it is still far from a specification of an algorithm for accomplishing it. For example, it seems to require that the query vector be compared against each and every document, something we hope to avoid. ^  An even more important issue to be resolved before the vector space model can be useful is being specific about just what it means for a document and query to be close to one another. As will be discussed in Section 5.2.2, there are many plausible measures of proximity within a vector space. For the time being, we will assume the use of the inner product of query and document vectors as our metric:  Sim(q, d) = q ï d  (3.23)  People have difficulty imagining spaces with more than the three physical dimensions of experience, so it is no wonder that abstract  spaces of 105 dimensions are difficult to conceptualize. Sketches like Figure 3.7 do the best they can to convey ideas in the three dimensions we appreciate, but it is critically important that we not let intuitions based on such small-dimensional experiences bias our understanding of the large-dimensional spaces actually being represented and searched.  Implementation hack 88      FINDING OUT ABOUT  Theres a quicker way to compute average  similarity
foa-0051	3.4.1 Keyword Discrimination  We can immediately use this vector space for something useful, as the source of yet another approach to the question of appropriate keyword weightings. Recall that in Figure 3.7 our initial assumption was that each and every keyword was to be used as a dimension of the vector space. Now we ask: What would happen if we removed one of these keywords? The first step is to extend the measure Sim(q, d) of documentquery similarity to measure interdocument similarities Sim{di, dj) as well. Then, for an arbitrary measure of document-document similarity (e.g., the inner product measure mentioned earlier), we consider all pairs of documents and then the average similarity across all             t  Sim{di, dj) = Similarity among documents D* = Centroid; average document  1  NDOC2 *r•  Y]Sim(dhdj)  (3.24) (3.25)  (3.26)  Recall that our goal is to devise a representation of documents that makes it easy for queries to discriminate among them. Because each keyword corresponds to a dimension, removing one results in a compression of the space into K ó 1 dimensions, and we can expect that the representation of each document will change at least slightly. For example, removing a dimension along which the documents varied significantly means that vectors that were far apart in the iC-dimensional space are now much closer together.  This observation can be used to ask how useful each potential keyword is. If it is discriminating, removing it will result in a significant compression of the documents' vectors; if removing it changes very little, the keyword is less helpful Using the average similarity as our measure of how close together the documents are, and asking this question for each and every keyword* we arrive at yet another measure of keyword discrimination:  = Sim when terntk removed  Disci; h Sinii ~~ Sim w'u = fkd * Disc*  (3.27) (3.28) (3.29)
foa-0052	WEIGHTING AND MATCHING AGAINST INDICES       89  3.4.2 Vector Length Normalization  One good example involves the length of document and query vectors. So far, we have placed no constraint on the number of keywords associated with a document. This means that long documents, which, caeteris paribus, can be expected to give rise to more keyword indices, can be expected to match (more precisely, have nonzero inner product with) more queries and be retrieved more often. Somehow (as discussed in Section 1.4) this doesn't seem fair: The author of a very short document who worked hard to compress the meaning into a pithy few paragraphs is less likely to have his or her document retrieved, relative to a wordy writer who says everything six times in six different ways!  These possibilities have been captured by Robertson and Walker in a pair of hypotheses regarding a document's scope versus its verbosity:  Some documents may simply cover more material than others ... (the "Scope hypothesis"). An opposite view would have long documents like short documents but longer: in other words, a long document covers a similar scope to a short document, but simply uses more words (the "Verbosity hypothesis"). [Robertson and Walker, 1994, p. 235]  Once we have decided that about-ness is conserved across documents, all documents' vectors will have constant length. If we make the same assumption about the query vector, then all of the vectors will lie on the surface of a sphere, as shown in Figure 3.8. Without loss of generality, we will assume that the radius of the sphere is unity.  Making Weights Sensitive to Document Length  Unfortunately, this very simple normalization is often inadequate, as can be shown in terms of the inverse document frequency (IDF) weights discussed in Section 3.3.7. IDF weighting highlights the distinction between inter- and intradocument keyword occurrences. Because its primary focus is on discrimination among documents, Intradocument occurrences of the same keyword become insignificant. This makes IDF very sensitive to the definition of how document boundaries are defined (cf. Section 2.2), as suggested by Figure 3.9. 90      FINDING OUT ABOUT  kw3  kwl  FIGURE 3.8 Length Normalization of Vector Space  Egt;oc10G  Ñ     __    __     parallel __    _  ó    ó    ó    ó    ó    ó DOC2  parallel ó  .__  -*ó   ó  ó  Doqo  T  -Docjooo  FIGURE 3.9 Sensitivity of IDF to "Document" Size  The IDF weight that results from encapsulating more text within the same "document" is, in a sense* the converse of normalizing the number of keywords assigned to every document. In either case, the advantage of using the paragraph as our canonical document (cf. Section 1.4), and/or relying on all documents in the corpus to be of nearly uniform size (as in the AIT dissertation abstracts) is apparent. WEIGHTING AND MATCHING AGAINST INDICES       91  Probablity      Probability    /\ i of retrieval      of relevance ~T i  Pivot  03  B Pivoted normalization Old normalization/  i 77 1 /         a / slope = tan(aj Pivot  Document length                                                    Old normalization factor  FIGURE 3.10 Pivot-Based Document Length Normalization. From [Singhal et al., 1996]. Reproduced with permission of the Association of Computing Machinery.  The OKAPI retrieval system of Robertson et al. [Robertson and Walker, 1994] has proven itself successful (in retrieval competitions like TREC; cf. Section 4.3.3) by combining IDF weightings with corpus-specific sensitivities to the lengths of the documents retrieved. They propose that the average length of all documents in a corpus, digital libraryen, provides a "natural" reference point against which other documents' lengths can be compared.  Define Len(d) to be the number of keywords associated with the document. OKAPI then normalizes the first component of our weighting formula, keyword frequency, by a term that is sensitive to each document's deviation from this corpuswide average:  Wkd =  fki  (kgt;Len(d)/Dlen)+ fkd  log  (NDoc - Djb) + 0.5 Dk + 0.5  (3.30)  Robertson and Walker report that k w 1.0 ó 2.0 | Q J seems to work best, where | Q | is the number of query terms.  Singhal et al. [Singhal et alo 1996] approach the problem of length normalization by doing a post hoc analysis of the distributions of retrieved versus relevant documents (in the TREC corpus) as a function of their length. A sketch of typical curves is shown in Figure 3.10.A. The 92      FINDING OUT ABOUT  fact that these two distributions cross suggests a corpus-specific length normalization pivot value, p, below which match scores are reduced and above which they are increased. The amount of this linear increase or decrease, shown as the length normalization slope m of the length normalization function in Figure 3.10.B, is the second corpus-specific parameter of Singhal et al.'s model. Returning to the "generic" form of the weighting function originally given in Equation 3.13, the pivot-based length normalization is:  wkd =----------------¶--------------discrimk            (3.31)  (1 ó m) ï p + m- norm  where norm is whatever other normalization factor (e.g., cosine) is already in use; several possible values are given in the next section.  Both OKAPI and pivot-based document length normalizations rely on the specification of additional corpus-specific parameters (k\ and p, m, respectively). Although the addition of yet more "knobs to twiddle" is generally to be avoided in a retrieval system, recent experience with machine learning techniques suggests the possibility of training such parameters to best match each corpus. This approach is sometimes called a regression technique and is discussed more fully in Chapter 7.
foa-0053	3.4.3 Summary: SMART Weighting Specification  Although the variety of potential keyword weighting schemes (signal/noise, IDF, keyword discrimination, etc.) may seem large, there is  in fact a systematicity to this variation.  SMART is an extremely influential and widely used software system for investigating IR techniques [Salton, 1971; Buckley, 1985]. One secret of its success is that SMART provides a simple parameter-driven mechanism for easily changing from one form of index weighting to another.  In SMART, the weight is decomposed into three factors:  = freqkd * discrim,                          ^  norm WEIGHTING AND MATCHING AGAINST INDICES       93  Each of these components can then be specified independently:  freclkd =  discrimk =  {0,1} binary  fkd maxNorm  max(fkd) k  i,i    fkd augmented  2      2max(fkd) k  In {fkd) + 1 log  NDoc   log  inverse  1        ND0CV                  A  log------- )         squared  Dk  )  NDoc- Dk         .,.,.,. log-------ó-------    probabilistic  1 At  Dk  frequency  norm =  r 2_\ wi      sum  2  fourth  max(iy;)    max  I   vector  (3.33)  (3.34)  (3.35)
foa-0054	3.5 Matching Queries against Documents  In Chapter 2 we first identified documents, then lexical features to be associated with each. Then we built an inverted keyword list to make going from keywords to documents about those keywords as easy as possible. Now we'll become specific about how we measure the similarity between a document and a query. 94      FINDING OUT ABOUT  The discussion of matching queries and documents is simplified if we adopt the vector space perspective of Section 3.4 and imagine both the query q and all documents d to be vectors in a space of dimensionality equal to NKwy the keyword vocabulary size.  In this space, the answer to the question of which documents are the best match for a query seems straightforward - those documents that are most similar, relative to some particular metric Sim measuring distance between points in the space. Students of algebra and abstract vector spaces know that many different choices are possible; see Section 5.2.2.
foa-0055	3.5.1 Measures of Association  Given a vocabulary of lexical indexing features, our central concern will be how to moderate the raw frequency counts of these features in documents (and, to a lesser extent, queries) based on distributional characteristics of that feature across the corpus. We will be led to use real-valued weights to capture these relations, but we begin with cruder methods: simply counting shared keywords.  The most direct way to say that a query and a document are similar is to measure the intersection of their respective feature sets. Let Kq be the set of keyword features used in a query and Kd be the analogous set found in a document. The coordination level is exactly how many terms in the query overlap with terms in the document:  CoordLevel(q} d) = | (Kq n Kd) |                   (3.36)  If we have many, many features and our query and documents are highly variable, then the presence of any significant overlap may be enough to  identify the set of documents of interest. On the other hand, if there is a great deal of overlap between our query and many of the documents,  then simply counting how big this intersection is will look like a gross measure. This fine line between one or two documents matching a query and an avalanche of thousands occurs regularly as part of FOA,  One part of the problem is normalization - the coordination level of intersecting features shared by both query and document seems a good measure of their similarity, but compared to what? It's easy to show that WEIGHTING AND MATCHING AGAINST INDICES       95  this normalization matters. Consider the case where CoordLevel{q, d) = 1, and imagine first that Kq = Kd = l also; i.e., a single-word query and a one-word document. In this case the two match on the one feature they each have. Intuitively, this is a perfect match; you couldn't do any better. But now imagine that the query includes 10 keywords, the document contains 1000 words, but still CoordLevel(q, d) = 1. The same intuition suggests that this should be judged a much poorer match, but our measure does not reveal this. Unless we normalize by something that reflects how many other features we might have matched, we can't have a useful measure of association.  One natural normalizer is to take the average of the number of terms in the query and in the document and compare the size of the intersection to it. This gives us the Dice coefficient:  |(icqn Kd)\  SimDice(q, d) = 2"?      J\                            (3.37)  Kq\ + \Kd\  The average number of features may or may not be appropriate, again depending on what a typical query and typical document are. Often a document has many, many keywords associated with it, and queries have only one or two. This average is not a very good characterization of either. Another perspective on similarity says that missing features are as significant as shared ones. The simple matching coefficient gives equal weight to those features included in both query and document and those excluded from both and normalizes by the total number of keyword features NKw:  - \(K*n **)!+ \((NKw ~ Klt;Jn (NKw ~ K^)\  NKw\  (3.38)
foa-0056	3.5.2 Cosine Similarity  We will focus here on the cosine measure of similarity. Not only does this respect the useful mathematical properties mentioned at the beginning of this section, but it is consistent with a geometric interpretation of the 96      FINDING OUT ABOUT  vector space that many find insightful: Sim(q, d) = cos(qZd)  q d  wkd  yjkd ¶  =====                (3.39)  ked             V keq
foa-0057	3.6 Calculating TF-IDF Weighting  Following the careful empirical investigation of Salton and Buckley [Salton and Buckley, 1988d; Salton and Buckley, 1990] and many others since [Harman, 1992a], we will concentrate on the TF-IDF weighting, which multiplies the raw term frequency (TF) of a term in a document  by the term's inverse document frequency (IDF) weight:  idfk = log (^^j                      (3.40)  = fkd  where fkd is the frequency with which keyword fc occurs in document d, NDoc Is the total number of documents in the corpus, and D is the number of documents containing keyword k.  Due to the wide variety observed in users' query patterns, methods for weighting queries vary more, primarily depending on the length of the query. We will consider two weighting methods, especially designed for ushorf and "long" queries. Short queries (of as few as one or two terms; cf. [Silverstein et al, 1999]) seem typical of those issued by Web search engine users. For these, we can assume multiple occurrences of the same keyword will be rare, and we ignore length normalization. This WEIGHTING AND MATCHING AGAINST INDICES       97  leaves us with simply the term's inverse frequency weight:  tyk                                 (3-42)  Long queries are often generated indirectly, as the result of relevance feedback from the users in response to prior retrievals. The long query corresponds to a particular document that the users like; searching for others like a known target is called query-by-example. By symmetry, it makes sense to use the same weighting of terms in this query-cum-document as we used for documents (Equation 3.40):  Wkq^Qn    =  fkq idfy                                    (3.43)  Notice that once the lengths of the q and d vectors in the denominator of Equation 3.39 are known, the computation of Sim requires a simple sum of products over all terms shared by query and document. Because both these lengths can be computed independently, it makes sense to compute them first.^                                                                              Implementation:  In fact, the document length Len(d) can be computed before any    Storing  ,        ,                                                                             document  query activity takes place:                                                                       lenglhs  (3.44)  With these definitions in place, we can begin to design an algorithm for computing similarity.
foa-0058	3.7 Computing Partial Match Scores  With length normalization to the side, we can concentrate on the main calculation of matching, summing the weight products of terms shared  by query q and document d:  Wkd ' wkq                                  (3.45)  kÄ(qnd)  This mathematical characterization hides a number of messy details associated with actually computing it. We will need to make efficient use of two data structures in particular. The first is the inverted index (recall 98      FINDING OUT ABOUT  Figure 3.5). The critical feature of this organization is that we can, for each term in the query, find the set of all document postings associated with it. In particular, the freq statistic maintained with each posting allows us to compute the weight wu we need for each. Even with these efficiencies, however, the need to consider every document posting for every query term can create a serious processing demand, and one that must be satisfied immediately - the users are waiting!  Because the elements of the sum can be reordered arbitrarily, we will consider a partial ranking (a.k.a. filtering, or pruning) algorithm that attempts to include the dominant elements of the sum but ignore small, inconsequential ones [Salton, 1989; Buckley and Lewit, 1985; Harman andCandela, 1990].  The fact that Wkq and Wkd va*T considerably suggests that, by ordering the inclusion of products into the sum, we may be able to truncate this process when they become smaller than we care to consider.  We therefore begin by sorting the terms in decreasing order of query weights Wkq- Considering terms in this order means we can expect to accumulate the match score beginning with its largest terms. Then the fact that our list of postings was similarly (and not coincidentally) ordered by decreasing frequency means that:  (V; gt; i) Wkdi gt; Wkd}                              (3.46)  Once these weights diminish below some threshold rfl^, we can stop going down the postings list. (In fact, it may be that the weight associated with the very first posting is too small and we can ignore all weights associated with this term.)  The second important data structure is an accumulator queue in which each document's match score is maintained. Because each query term may add an additional term to the match score for a particular document, these accumulators will keep a running total for each document. For moderate corpus sizes, it may not be unreasonable to allocate an accumulator for each document, but this can demand too much memory for very large corpora. Define NAccum to be the number of accumulators we are willing to allocate. Then one obvious way to restrict this set is to only allocate an accumulator when a document's score becomes significant, again in comparison to some threshold xtmeTt. Because we will be processing query terms in decreasing Wkq order and heuristically WEIGHTING AND MATCHING AGAINST INDICES       99  value the space associated with new accumulators more than the slightly longer time to run down posting lists a bit further, we can assume that  ^¶insert *gt; ^add%  Picking appropriate values for these two thresholds is something of a black art, but Persin [Persin, 1994] reports one especially careful experiment in which both are made proportional to the most highly matched document's accumulator A* (i.e., A* is the maximum match score in any document's accumulator):  Tinsert  == ?}insert   *   A                                    (3.47)  *add = riadd ' A*                                     (3.48)  Persin's experiments suggest that values ofr}insert = 0.07, rjadd = 0.001 give retrieval effectiveness near that of full matches (i.e., considering all query-document term products) while minimizing NAccum?                   Partial  These two thresholds divide the range of possible query-document    matching isn't term products into three conditions:                                                       Kr .       .  ugt;kq gt; insert      Always add; create new accumulator A4        wor] s   etter if necessary  lt; ^insert   Add only if accumulator Ad already exists £ iad      Ignore; move on to the next query term  We want to remain flexible with respect to both long and short queries, so we will assume that the query weights Wkq are precomputed and passed to our ranking procedure.* Using our definition for Wkd and focusing first on the TimeTt threshold:  X\nsen ( fkd ' Wk ) ' Wkq  gt; insert  fkd   gt;  ^  idf  r  Jkd  Vmsert  tdfk wkq  * However, it Is generally more efficient to retain "raw" frequency counts in the postings (as integers) rather than length-normalized weights wkd (a$ floats). This means that length normalization of documents is performed after the partial ranking match has been completed. 100      FINDING OUT ABOUT  Algorithm 3.1 Partial Ranking  prank(qry[],A) {  // qry = vector of lt; keyword, weight gt; pairs  // A = queue of lt; docid, score gt; accumulators  // initially empty and returned with most highly ranked  Sort(qry,  descending WgtCmp);  for (q G query ) {  ?insert = iV insert ' A*)/{q.Wgt ï ldfq)\  ?add = (rjadd ' A*)/(q.wgt - idfq); for (fpe fpost(q)){ if (fp.freq lt;= xadd)  break;  newscore = fp-freq ï idfq ï q.wgt; for (dp e dpost(/ª){  fhd = hashfind(docTbl, docno); if(fndORdwgtgt; rimert) { if(fnd)  fhd.score + = newscore; else{  hashadd(docTbl, docno); if(length(A) gt; NAccum)  pop(A); insertQ(newscore, docno. A);  }  A* = max(A*,ftid.score,newscore);  }  } } //eo-posting loops }//eo-qryloop  This finally becomes an operational question we can apply with respect to each posting's frequency /jy. Note that this threshold must be updated every time we move to a new term of the query. Of course, the computation of t^ proceeds similarly. WEIGHTING AND MATCHING AGAINST INDICES       101  All the basic features of a partial ranking algorithm are now in place, and a pseudo-code sketch is shown in Algorithm 3.1. It also includes a few minor complications. First, a hashtable is required to find accumulators associated with a particular docno. Second, the set of accumulators Ad is described as a queue, but it must be slightly trickier than most: It must maintain them in order so that only the top NAccum are maintained, and it must support lengthO queries and a popO function when it is full. Nondeterministic skip lists [Pugh, 1990] are recommended for this purpose [Cutting and Pedersen, 1997].                                                    ?59 69 7, 8S 9
foa-0059	3.8 Summary  We've covered enormous ground in this chapter. We began by wondering just what it might mean to try to understand communicative acts, like the publication of a document or the posing of a question or query. We looked in some detail at one of the most fundamental characteristics of texts, Zipf's law, and found it to be, in fact, quite meaningless! But the next level of features, tokens like those produced by the machinery of Chapter 2, supported a rich analysis of the index, as a balance between the vocabularies of the documents' authors and searchers' subsequent queries. The vector space provides a concrete model of potential keyword vocabularies and what it might mean to match within this space. Finally, we considered an efficient implementation of nearly complete matching. In the next chapter we will consider the problem of evaluating how well all these techniques work in practice.  However, there are gaps in this story that are so obvious we don't even need to measure them. Some of the gaps involve implementation issues that can be critical, especially when faced with very large corpora [Harper and Walker, 1992]. Parallel implementation techniques, for example, those pioneered on "massively parallel'* SIMD (single instruction/multiple data) Connection Machines, become important in this respect [Salton and Buckley, 1988c; Stanfill and Kahle, 1986a; Stone, 1987]. In the modern age of multiple search engines each indexing (only partially overlapping versions [Lawrence and Giles, 1998]) of the WWW techniques for fusing multiple hitlists into a single list for the same user suggests another level of parallelism in the FOA task [Voorhees et al., 1995]. 102      FINDING OUT ABOUT  However, linguists, in particular, must have more serious, implementation-independent concerns. Imagine that you are someone who has studied the range of human languages and who appreciates both their wide variety and equally remarkable commonalities. You would be appalled at the violence we have done to the syntactic structure of language. For linguists, Finding Out About documents by counting their words must seem like trying to understand Beijing by listening to a single microphone poised high over the city: You can pick up on a few basic trends (like when most people are awake), but most of the texture is missing!  DOG BITES MAN and MAM BITES DOG clearly mean two different things. Word order obviously conveys meaning beyond that brought by the three words. And the problem doesn't end with word order. Look how different the meaning of these phrases are:  ï NEUTRALIZATION" OF THE PRESENT  ï REPRESENTING NEUTRONS  ï REPRESENTATIONS, NOT NEUTRONS  despite the fact that all of them (conceivably) reduce to the same set of indexable tokens! Note especially how critical the same "noise" words thrown away on statistical grounds (in Chapter 2) are when analyzing a sentence's syntactic structure.  The attempt to understand the phenomena of meaning by looking for patterns in word frequency statistics alone is reminiscent of the tea leaves and entrails of this chapter's opening quote. Still, the success of many WWW search engines that use very little beyond this kind of gross analysis suggests that there is much more information in the statistics than traditional, syntactically focused linguists might have believed.
foa-0060	4________  Assessing the Retrieval  We've come a long way since Chapter 1, where we first sketched the full range of activities we might consider FOA. As Chapter 2 considered the various ways of breaking text into indexable features, and Chapter 3 explained the various ways of weighting combinations of these features to identify the best matches to a query, I hope you have been aware of how many alternatives have been mentioned! That is, rarely has there been a single method that can be proven to be better than all others. IR has traditionally been driven by empirical demonstrations, and the range of commercial competitors now trying to provide the "best" search of the WWW makes it likely this performance orientation will continue. But whether we are search engineers, scientists objectively assessing one particular technique, or consumers of WWW search engine technology interested in buying and using the best, a solid basis of performance assessment is criticalSeveral perspectives on assessment are possible. In the first chapter FOA was viewed as a personal activity, adopting the users1 points of view. Section 4.1 will continue in this theme, considering how users assess the results of their retrievals and how they can express their opinions using relevance feedback (relevance feedback). Oddy is credited with first identifying this important stream of data, naturally provided by users as a part of their FOA browsing [Oddy, 1977; Belkin et al, 1982].  105 106       FINDING OUT ABOUT  But in this book we are also concerned with FOA from the IR system builder's point of view. Ideally, we would like to construct a search engine that robustly finds the "right" documents for each query and for each user. The second section of this chapter discusses performance measures of statistical properties that are reliable across large numbers of users and their highly variable queries. The key to these measures is having some insight into which documents should have been retrieved, typically because some idealized omniscient expert has determined (within a specially constructed experimental situation) that certain documents "should" have been retrieved. Alternatively the relevance feedback of many users can be combined to form a consensual opinion of relevance, as described in Section 4.4.  A concrete notion of relevance would seem a fundamental precondition for understanding either an individual's relevance feedback or how this can be used to assess a search engine. But in this respect, information retrieval generally, and relevance feedback in particular, is like many other academic areas of study (including artificial intelligence and genetics) in that the lack of a fully satisfactory definition of the core concept (information, intelligence, genes, and so on) has not entirely stopped progress. That is, a great deal can be done by operationalizing relevance feedback to be simply those relevance assessment behaviors produced as part of an FOA dialog. This operational simplification will hold us until fundamental issues of language and communication are again addressed in Section 8.2.1.
foa-0062	4.1.1 Cognitive Assumptions  "Garbage in, garbage out" is one of the first insights every software developer learns, and FOA is no exception. The primary source of data considered by traditional IR methods, and the focus of Chapters 2 and 3 of this text, are the documents of the corpus, particularly the keywords they contain. (Chapter 6 will consider the use of other document attributes.) A fundamental feature of the broader FOA view is that browsing users provide an equally important source of data concerning  Portions previously published with John Hatton [Belew and Hatton, 1996]. ASSESSING THE RETRIEVAL       107  what keywords mean and what documents are about. It is therefore appropriate to begin by characterizing who these users we will be watching are.  We begin with one important cognitive assumption we must make about our users: How thorough an FOA search do they wish to perform? Is this an important search to which the users are willing to dedicate a great deal of time and attention, or will a quick, cursory answer suffice? For example, Chapter 1 mentioned how much less thorough the typical undergraduate (doing some quick research before submitting a term paper) is than the Ph.D. candidate (who wants to ensure his or her proposed dissertation topic is new). The typical WWW searcher seems satisfied with only a few useful leads, but the professional searcher (a lawyer looking for any case that might help, a doctor looking for any science that might heal a patient) will search diligently if there is even a small chance of finding another relevant document. This kind of variability can be observed not only across different classes of users but even  across the same user at different times.f                                                   In for a fact,  stay for a lesson  Prototypic Retrievals  From the perspective of cognitive psychology, the task facing users who are asked to produce relevance feedback (relevance feedback) can best be described as one of object recognition, in the tradition of Rosch and others [Rosch and Mervis, 1975; Rosch, 1977]. The object to be recognized is an internally represented prototypic document satisfying the users' information need. In this case, the prototype corresponds to the model the subjects maintain of ideally relevant documents. As users consider an actual retrieved document's relevance, they evaluate how well it matches the prototype. Barry and others have suggested the many and varied features over which prototypes can be defined [Barry, 1994]. Only a small number of these may be revealed by any one of the users' queries, of course.  Here we will simply assume that users are capable of grading the quality of this match. Users might be asked to score the quality of relevance match according to a five-point scale like that shown in Figure 4.1. Users can qualify the middle Relevant response either by weakening it (Possibly Relevant) or strengthening it (Critically Relevant). Such distinctions are often made in experimental settings (e.g., in the use of the STAIRS 108      FINDING OUT ABOUT  Not                Possibly    Relevant   Critically  i  -e (No response)  FIGURE 4.1 Relevance Scale  retrieval system by lawyers [Blair and Maron, 1985]) and relate to the different purposes for FOA that different users may have. To make these distinctions concrete, we might imagine "Critically Relevant" to apply only to those documents that must be read even for an undergrad term paper, while "Possibly Relevant" would be much more broadly applied to those a Ph.D. student needs as part of his or her literature review.  For now, however, we will simplify the types of relevance feedback to allow users to reply with only a single grade of "Relevant" (©) or "Not Relevant" (0). These two assessments require overt action on the part of subjects; "No response" (#) is the default relevance feedback assessment for documents not receiving any other responses. Again, this frees users from the much more cognitively demanding task of exhaustively assessing every retrieved document Those documents that "jump out" at users as particularly good or especially bad - examples of the prototype they seek provide the most informative relevance feedback. Figure 4.1 also introduces a color-code convention in the electronic version: © will be used to indicate positive relevance feedback and 0 to indicate negative relevance feedback.  relevance feedback is Nonmetric  As we move from a cognitive understanding of the users' tasks to statistical analyses of their behaviors, we must understand one important feature of the relevance feedback data stream: relevance feedback is nonmetric data. That is, while users find it easy and natural to critique retrieved documents with ©, ©, and #, they would find it much more difficult to reliably assign numeric quantities reflecting something like the relative applicability of each retrieval.  Think for a moment just why this is hard, by imagining your reactions to a typical retrieval Is the first document to be rated 10 or 6743? If you rate the first document as 10, the second as 6ª and the third as 2, then you must ensure that the third document is exactly as much less relevant ASSESSING THE RETRIEVAL       109  FIGURE 4.2 relevance feedback Labeling of the Retr Set  than the second as the second is from the first. Trying to keep all Rel(di) assessments consistent in the metric sense, for many retrieved documents or any other set, makes people crazy.  This is not only a property of relevance assessments. A large literature on psychological assessment [Kruskal, 1977a; Kruskal, 1977b; Shepard et al, 1972] has demonstrated that human subjects can quite easily and reliably sort objects into "piles," and that they like one pile better than another. Yet the same people find it more difficult to quantify how much they like each object, let alone make these quantitative assessments consistent with one another. Reliable estimates of both cognitive qualities would be necessary if we are to have a true preference metric*  Rather than assuming that users can provide a separate score for each retrieved document, we will treat this as an ordered nonmetric scale of increasing preference:  #lt; @  (4.1)  Each of these assumptions about what "relevance" is and how it can be measured is a matter of considerable debate [Wilson, 1973; Froehlich, 1994] and is likely to be the topic of much future work. It is also interesting to note that in our later attempts at a comprehensive model of how and why humans use language, "relevance" again plays a central role (cf. Section 8.2.2).
foa-0063	4.2 Extending the Dialog with relevance feedback  Figure 4.2 focuses on a single instance of relevance feedback, shown as a labeling  over the set of Retr documents. But beyond any one reaction to a single  * The fact that relevance feedback Is fundamentally nonmetric will have real consequence, for example, In the learning mechanisms that use it; cf. Chapter 7. 110      FINDING OUT ABOUT  A VQuery  Qo                         Qi                                    Q2  IRS  Ret!                         Ret2                                      Retf  FIGURE 4.3 Query Session, Linked by relevance feedback  retrieved set product, a central premise of the FOA process is that users' reactions to just-retrieved documents provide the pivotal linkage between assessments, to form the FOA search dialog. This is perhaps more clear in Figure 4.3, where relevance feedback is used to link a series of reactions into a query session.  Attempts to support this searching process, and then attempts to rigorously evaluate how well software systems support browsing users as they FOA, is one of the most vexing issues within IR evaluation [Daniels et al., 1985; Saracevic et al., 1988; Larson, 1996; Oddy et al., 1992; O'Day and Jeffries, 1993; Cawsey et al., 1992; Russel et al., 1993; Koenemann and Belkin, 1996]. Part of the problem is the misconception that if a search engine works perfectly and the user issues the perfect magic bullet query, out will spill all and only relevant documents! Such simplistic definitions of optimality come naturally to computer scientists; library scientists, who are used to the naturalistic behaviors of real patrons in their libraries, know that a much more extended and nebulous form of support is required.  Marcia Bates's famous berrypicking metaphor [Bates, 1986; Bates, 1989] is useful here:  [A] query is not satisfied by a single final retrieved set, but by a series of selections of individiual references and bits of information at each stage of the ever-modifying search. A bitat-a-time retrieval of this sort is here called berrypicking. This term is used by analogy to picking huckleberries or blueberries in the forest. The berries are scattered on the bushes; they do not come in bunches. One must pick them one at a time. One could ASSESSING THE RETRIEVAL       111  do berrypicking of information without the search need itself changing (evolving) but... [we] consider searches that combine both of these characteristics. [Bates, 1989, p. 410]  In addition to highlighting the same iterative browsing behavior central to FOAs characterization of the dialog, the "evolving" character of the information need in Bates's metaphor is also important. Imagine that you are in the forest on an idyllic day with only one purpose: to fill your bucket with the best blueberries you can find. Early in the day, with your whole afternoon in front of you, you are likely to be very choosy. At this juncture, you could bump into a bush full of blueberries that were not as ripe or as large as you imagine must exist somewhere else in the forest, and not drop a single one into your basket. But late in the afternoon, if you have had poor luck and little to show for your efforts, you could come across an even worse bush and grab every single berry, even the shriveled ones!  Applying this metaphor to FOA is provocative in many respects. For example, it suggests that maintaining an explicit representation of the retrieved document "basket" might be a useful addition to any search engine interface. It predicts a time course to the distribution of users' relevance feedback assessments. For now, we simply observe that it seems quite likely that an assessment of one document's relevance will depend greatly on the "basket" of other documents we have already seen. The general idea of thinking of an "evolutionary ecology of Information foraging" [Pirolli and Card, 1997] has become less metaphoric and more concrete as information search agents (like the InfoSpiders described in Section 7.6) explore the "environment" of the WWW.
foa-0064	4.2.1 Using relevance feedback for Query Refinement  While Figure 4.2 showed the retrieved set of documents as a simple set, it is interesting to impose the relevance feedback labeling on the retrieved documents viewed in vector space. Figure 4.4 shows a query vector and a number of retrieved documents, together with a plausible distribution of relevance feedback over them. That is, we can expect that there Is some localized region in vector space where © relevant documents are most likely to occur. If we believe that these positively labeled retrieved documents are in fact clustered, it becomes appropriate to consider a hypothetical centroid 112      FINDING OUT ABOUT  FIGURE 4.4 relevance feedback Labels in Vector Space  FIGURE 4.5 Various Ways of Being Irrelevant  (average) document d+gt; which is at the center of all those documents that users have identified as relevant.  It is less reasonable* however, to imagine that the negatively labeled 9 documents are similarly clustered. Documents that were inappropriately retrieved failed to be relevant for one reason or another; there may be several such reasons. These are shown as discriminating planes in Figure 4.5. ASSESSING THE RETRIEVAL       113  -bcT  FIGURE 4.6 Using relevance feedback to Refine the Query  The vector space view also lets us easily portray two very different uses to which relevance feedback information might be applied. Most typically, relevance feedback is used to refine the user's query. Figure 4.6 represents this refinement in terms of two changes we can make to the initial query vector. The first is to "take a step toward" the centroid of the positive relevance feedback cluster. The size of this stept reflects how confident we are that the positive cluster centroid is a better characterization of the user's interests than the original query.  There is one important difference between the query and even a slight perturbation of it toward a cluster's centroid: While the original query vector is often very sparse and results from just those few words used in the user's original query, any movement toward the centroid will include (linear combinations of) all those keywords used in any of the positively labeled documents. The additional difficulty in implementing this more densely filled feature vector becomes a serious obstacle in many system implementations. The fact that relevance feedback-refined queries involve many more nonzero keyword entries also means that query weighting and matching techniques may be sensitive to this difference.  Seminal work on the use of relevance feedback was done by Saltongt; especially with students Rocchio, Brauen, and Ide in the late 1960s and early 1970s [Rocchio, 1966; Brauen, 1969; Salton, 1972]. More recent students have extended the theory of query refinement and related it to topics in machine learning [Buckley et aL, 1994; Buckley and Salton, 1995; Allan, 1996].  Neural-net style learning 114       FINDING OUT ABOUT  Some of these experiments explored a second modification to the query vector. In addition to moving toward the d"^ center of ©, it is also plausible to move away from the irrelevant retrieved documents 0. As noted earlier, however, it is much less likely that these irrelevant documents are as conveniently clustered. As Salton [Salton and McGill, 1983] reported:  ... retrieval operation is most effective when the relevant documents as well as the non-relevant documents are tightly clustered and the difference between the two groups is as large as  possible___The relevance feedback operation is less favorable in the more  realistic case where the set of non-relevant documents covers a wider area of the space, [p. 145]  One possible strategy is to take a single element d~~ of the irrelevant retrieved documents (for example, the most highly ranked irrelevant retrieval) and define the direction of movement with respect to it alone.  As we have discussed in connection with Figure 4.2, relevance feedback helps to link individual queries into a browsing sequence. And so, although we have focused here on the simplest form of query refinement, with respect to the users' initial queries, relevance feedbackcn be given again and again. An initial query vector is moved toward the centroid of documents identified as relevant (and perhaps away from an especially bad one); this modified query instigates a new retrieval, which is again refined. In practice, it appears that such adjustments result in diminishing returns after only a few iterations of query refinement [Salton and McGill, 1983; Stanfill and Kahle, 1986b].  However, Section 7.3 will discuss a type of FOA in which a document corpus is constantly changing and the user's interest in a topic is longlived. In this case, we can imagine the query as a filter against which a constant stream (e.g., of newly published Web documents) is applied relevance feedback has also been used in this setting, to make ongoing changes to the query/filter that continue to improve its effectiveness [Allan, 1996].  Using relevance feedback for query refinement produces results that are immediately satisfying to the users. First, it automatically generates a new query with which they can continue their browsing process. Second, the statistical analysis of positively labeled retrieved documents can provide other forms of useful information to the users as well. For example, rather than ASSESSING THE RETRIEVAL       115  FIGURE 4.7 Document Modifications due to relevance feedback  simply retrieving a new set of documents, new keywords that are not in the users' original queries but are present in positively labeled documents at statistically significantly levels can be suggested to the users as new vocabulary. Conversely, words that were in the original query but are negatively correlated with d+ (and/or positively correlated with d~~) can also be identified.
foa-0065	4.2.2 Using relevance feedback to Adapt Documents' Indices  An alternative use of relevance feedback is to make changes to the documents rather than to the query. Previously, the argument was that it is sensible to make the query look more like those documents the users liked; now the argument is the converse: Documents found relevant to a query should be described more like the query used to identify them. Changes made to document vectors according to this heuristic are known as document modifications and are shown in Figure 4.7.  Note that unlike query modification, adaptive document modifications made in response to ReWhkre not expected to be of (immediate) use to the users who provide it. Instead, the hope is that these changed document representations will be available later, to others who might be searching. As Salton [Salton and McGill, 1983] described the goal: 116      FINDING OUT ABOUT  Following a large number of such interactions documents which are wanted by the users will have been moved slowly into the active portion of the document space - that part in which large numbers of users' queries are concentrated, while items which are normally rejected will be located on the periphery of the space, [p. 145]  This provocative proposal, allowing a search engine to learn from its users, is considered in much greater detail in Chapter 7.
foa-0066	4.2.3 Summary  We have been discussing relevance feedback from the individual user's point of view. WeVe focused on how this information might be collected and how it might be used, both in the short term to modify users' retrievals and in a longer-term way to change the documents' indices. Now we want to consider a third use for relevance feedback information. When we have more than one system to use for retrieval and would like to evaluate which is doing the better job, users' assessments of retrieved documents' relevance can be used as "grades." If one system can consistently, across a range of typical queries, more frequently retrieve documents that the users mark as relevant and fewer that they mark as irrelevant, then that system is doing a better job.
foa-0067	43 Aggregated Assessment: Search Engine Performance  The last section considered the assessment problem from the perspective of a single individual, the browsing user. Now we would like to generalize on this individual performance to attempt to obtain statistically significant observations.
foa-0068	43.1 Underlying Assumptions  As with all scientific models, our attempts to evaluate the performance of a search engine rests on a number of assumptions. Many of these involve the user and simplifying assumptions about how users assess the  relevance of documents. ASSESSING THE RETRIEVAL       117  1.  Real FOA versus Laboratory Retrieval. From the FOA perspective, users retrieve documents as part of an extended search process. They do this often, and because they need the information for something important to them. If we are to collect useful statistics about FOA, we must either capture large numbers of such users "in the act" (i.e., in the process of a real, information-seeking activity) or attempt to create an artificial laboratory setting. The former is much more desirable but makes strong requirements concerning a desirable corpus, a population of users, and access to their retrieval data. So, typically, we must work in a lab. The first big assumption, then, is that our lab setting is similar to real life; i.e., "guinea pig" users will have reactions that mirror real ones.*  2.  Intersubject Reliability. Even if we assume we have a typical user and that this user is engaged in an activity that at least mirrors the natural FOA process, we have to believe that this user will assess relevance the same as everyone else! But clearly, the educational background of each user, the amount of time he or she has to devote to the FOA process relative to the rest of the task, and similar factors will make one user's reaction differ from another's. For example, there is some evidence that stylistic variations also impact perceived relevance [Karlgren, 1996]. The consensual relevance statistic (cf. Section 4.3.2) is one mechanism for aggregating across multiple users.  This becomes a concern with intersubject reliability. If we intend to make changes to document representations based on one user's relevance feedback opinions, we would like to believe that there is at least some consistency between this user's opinion and those of others. This is a critical area for further research, but some encouraging, preliminary results are available. For example, users of a multilingual retrieval system that presents some documents in their first language ("mother tongue") and others in foreign languages they read less well, seem to be able to provide consistent relevance feedback data even for documents in their second, weaker language [Sheridan and Ballerini, 1996].  3.  Independence of Interdocument Relevance Assessments. Finally, notice that the atomic element of data collection for relevance assessments is typically a (query;, documentj) pair: documentj is relevant to query^ Implicitly, this assumes that the relevance of a document can be assessed independently of assessments of other documents. Again, this is a very questionable assumption.  uWith Web search engines don't we have access to enormous numbers of users searching the same corpus?" [SG] 118      FINDING OUT ABOUT  Recall also that often the proxy on which the user's relevance assessment depends is distinct from the document itself. The user sees only the proxy, a small sample of the document in question, for example, its title, first paragraph, or bibliographic citation. While we must typically take user reaction to the proxy as an opinion about the whole document, this inference depends critically on how informative the proxy is. Cryptic titles and very condensed citation formats can make these judgments suspect. And of course the user's ultimate assessment of whether a document is relevant, after having read it, remains a paradox.
foa-0069	43.2 Consensual Relevance  In most search engine evaluation, the assumption has been that a single expert can be trusted to provide reliable relevance assessments. Whether any one, "omniscient" individual is capable of providing reliable data about the appropriate set of documents to be retrieved remains a foundational issue within IR. For example, a number of papers in a recent special issue ofthe Journal ofthe American Society for Information Systems devoted to relevance advocated a move toward a more "user-centered, situational" view of relevance [Froehlich, 1994].  Our attention to the opinions of individual users suggests the possibility of combining evidence from multiple human judges. Rather than having relevance be a Boolean determination made by a single expert, we will consider "relevance" to be a consensual, central tendency ofthe searching users' opinions. The relevance assessments of individual users and the resulting central tendency of relevance is suggested by Figure 4.8. Two features of this definition are significant. First, consensual relevance posits a "consumers'" perspective on what will count as IR system success. A document's relevance to a query is not going to be determined by an expert in the topical area, but by the users who are doing the searching. If they find it relevant, it's relevant, whether or not some domain expert thinks the document "should" have been retrieved.  Second, consensual relevance becomes a statistical, aggregate property of multiple users' reactions rather than a discrete feature elicited from any one individual. By making relevance a statistical measure, our confidence in the relevance of a document (with respect to a query) ASSESSING THE RETRIEVAL       119  Consensually relevant  ivant  respect to User,.  FIGURE 4.8 Consensual Relevance  increases as more relevance assessment data are collected. This reliance on statistical stability creates a strong link between IR and machine learning (cf. Chapter 7). Allen's investigation into idiosyncratic cognitive styles of browsing users [Allen, 1992] and Wilbur's assessment of the reliability of relevance feedback across users [Wilbur, 1998] provide a more textured view of how multiple relevance assessments can be compared and combined.  It seems, however, that our move from omniscient to consensual relevance has only made the problem of evaluation that much more difficult. Test corpora must be large enough to provide robust tests for retrieval methods, and multiple queries are necessary to evaluate the overall performance of a search engine. Getting even a single person's opinion about the relevance of a document to a particular query is hard, and we are now interested in getting many! However, software like RAVE (cf. Section 4.4) allows an IR experimenter to effectively collect large numbers of relevance assessments for an arbitrary document corpus.
foa-0070	4.33 Traditional Evaluation Methodologies  Before surveying all of the ways in which evaluation mighthe performed, it is worthwhile to sketch how it has typically been done in the past [Cleverdon and Mills, 1963]. In the beginning, computers were slow and had very limited disk space and even more limited memories; initial test corpora needed to be small, too. One benefit of these small corpora was that it allowed at least the possibility of having a set of test queries compared exhaustively against every document in the corpus. 120      FINDING OUT ABOUT  The source of these test queries, and the assessment of their relevance, varied in early experiments. For example, in the Cranfield experiments [Lancaster, 1968], 1400 documents in metallurgy were searched according to 221 queries generated by some of the documents' authors. In Salton's experiments with the ADI corpus, 82 papers presented at a 1963 American Documentation Institute meeting were searched against 35 queries and evaluated by students and "staff experts" associated with Salton's lab [Salton and Lesk, 1968]. Lancaster's construction of the MEdigital libraryARS collection was similar [Lancaster, 1969].  As computers have increased in capacity, reasonable evaluation has required much larger corpora. The Text Retrieval Conference (TREC), begun in 1992 and still held annually, has set a new standard for search engine evaluation [Harman, 1995]. The TREC methodology is notable in several respects. First, it avoids exhaustive assessment of all documents by using the pooling method, a proposal for the construction of "ideal" test collections that predates TREC by decades [Sparck Jones and van Rijsbergen, 1976]. The basic idea is to use each search engine independently and then "pool" their results to form a set of those documents that have at least this recommendation of potential relevance. All search engines retrieve ranked lists of k potentially relevant documents, and the union of these retrieved sets is presented to human judges for relevance assessment.  In the case of TREC, k = 100 and the human assessors were retired security analysts, like those that work at the National Security Agency (NSA) watching the world's communications. Because only documents retrieved by one of the systems being tested are evaluated there remains the possibility that relevant documents remain undiscovered, and we might worry that our evaluations will change as new systems retrieve new documents and these are evaluated. Recent analysis seems to suggest that, at least in the case of the TREC corpus, evaluations are in fact quite stable [Voorhees, 1998].  An important consequence of this methodological convenience is that unassessed documents are assumed to be irrelevant This creates an unfortunate dependence on the retrieval methods used to nominate documents, which we can expect to be most pronounced when the methods are similar to one another. For example, if the alternative retrieved sets are the result of manipulating single parameters of the same basic ASSESSING THE RETRIEVAL       121  retrieval procedure, the resulting assessments may have overlap with, and hence be useless for comparison of, methods producing significantly different retrieval sets. For the TREC collection, this problem was handled by drawing the top 200 documents from a wide range of 25 methods, which had little overlap [Harman, 1995]. Vogt [Vogt and Cottrell, 1998] has explored how similarities and differences between retrieval methods can be similarly exploited as part of combined, hybrid retrieval systems (cf. Section 7.5.4).  It is also possible to sample a small subset of a corpus, submit the entire sample to review by the human expert, and extrapolate from the number of relevant documents found to an expected number across the entire corpus. One famous example of this methodology is Blair and Maron's assessment of IBM's STAIRS retrieval system [Blair and Maron, 1985] of the early 1980s. This evaluation studied the real-world use of STAIRS by a legal firm as part of a litigation support task: 40,000 memos, design documents, etc. were to be searched with respect to 51 different queries. The lawyers themselves then agreed to evaluate the documents5 relevance. As they reported:  To find the unretrieved relevant documents we developed sample frames consisting of subsets of unretrieved databases that we  believed to be rich in relevant documents___Random samples  were taken from these subsets, and the samples were examined by the lawyers in a blind evaluation; the lawyers were not aware they were evaluating sample sets rather than retrieved sets they had personally generated. The total number of relevant documents that existed in these subsets could then be estimated. We sampled from subsets of the database rather than the entire database because, for most queries, the percentage of relevant documents in the database was less than 2%, making it almost impossible to have both manageable sample sizes and a high level of confidence in the resulting Recall estimates. Of course, no extrapolation to the entire database could be made from these Recall calculations. Nonetheless, the estimation of the number of relevant unretrieved documents in the subsets did give us a maximum value for Recall for each request. [Blair and Maron, 1985, pp. 291-293] 122      FINDING OUT ABOUT  High-recall retrieval ^ ï*ï"*"'  Retrieved  /        /High-precision  retrieval ** "^^ó       - v  i \  Corpus  FIGURE 4.9 Relevant versus Retrieved Sets  Killing the messenger  This is a difficult methodology, but it allows some of the best estimates of Recall available. And the news was not good: On average, retrievals captured only 20 percent of relevant documents^  In short, methodologies for valid search engine evaluations require much more sophistication and care than is generally appreciated. Careful experimental design [Tague-Sutcliffe, 1992], statistical analysis [Hull, 1993], and presentation [Keen, 1992] are all critical.
foa-0071	4.3.4 Basic Measures  Figure 4.9 shows the relationship between relevant (Rel) and retrieved (Retr) sets as a Venn diagram, against the backdrop of the universe 17 of the rest of the documents of the corpus. Obviously, our focus should be on those documents that are in the intersection of Rel and Retr and on making this intersection as large as possible. Informally, we will be most happy with a Rel set when it best overlaps with the Retr set, and therefore we seek evaluation measures that reflect this. The basic relations between the sizes of these sets can also be captured in the contingency table of Table 4.1.  We know we want the intersection of the Rel and Retr sets to be large, but large relative to what?! As mentioned in Chapter 1, if we are most focused on the Rel set and use it as our standard of comparison, ASSESSING THE RETRIEVAL       123  TABLE 4.1 Contingency Table  Not Relevant        Relevant  Retrieved           Ret A Rel     Ret lt;-ª Rel      NRef  Not retrieved     Wt *-* Rel     'Ret lt;-gt; llel     NNRe^ NRelc           NNReld        NDolt;f  a NRet is the number of retrieved documents. b NNRet is the number of documents not retrieved. c NRel is the number of relevant documents. d NNRel is the number of irrelevant documents. e NDoc is the total number of documents.  we'd like to know what fraction of these we've retrieved. This ratio is called recall:    Anticipating the probabilistic analysis of Section 5.5, we can think of Recall as (an estimate of) the conditional probability that a document will be retrieved, given that it is relevant: Pr{Ret | Rel).  Conversely, if we instead focus on the Ret set, we are most interested in what fraction of these are relevant; this ratio is called precision:  ^     . .          \RetD Rel\                            fA   N  Precision = -ó;-----:ó-                           (4.3)  \Ret\  Similarly, this is the probability that a document will be relevant, given that it is retrieved: Pr(Rel \Ret). A closely related but less common measure is called fallout, where we (perversely!) focus on the irrelevant documents and the fraction of them retrieved:  'Ret n Rel I Fallout = -----==-----l-                               (4.4)  \Ret  The close relationship between these three measures can be defined  precisely, if the generality G of the query (cf. Section 4.3.7) is known:  Recall ï G                                ,     ,  Precision =---------------------------------------               (4.5)  Recall- G + Fallout- (I - G) 124      FINDING OUT ABOUT  This is Pr(Ret\Rel). These two measures, Recall and Precision, have remained the bedrock of search engine evaluation since they were first introduced by Kent in 1955 [Kent et al, 1955; Saracevic, 1975]. By far the most common measures of search engine performance are just the pair of measures, Precision and Recall.  Ideally, of course, we'd like a system that has both high precision and high recall: only relevant documents and all of them. But real-world, practical systems must select documents based on features that are only statistically useful indicators of relevance; we can never be sure. In this case efforts made to improve recall must retrieve more documents, and it is likely that precision will suffer as a consequence. The best we can hope for is some balance.  In some applications it is nevertheless desirable to evaluate IR system performance according to a single measure rather than the twoSingle                 dimensional Recall/Precision criteria.^ We will return to this topic in dimensions for      Section 4.3.8. simple minds
foa-0072	4.3.5 Ordering the Retr Set  Do not worry about large numbers of results: the best ones come first! (www.AltaVista.com, 1998)  The next step is to move beyond thinking of Retr as simply a set. We will suppose that retrieved documents are returned in some order by the search engine, reflecting its assessment of how well each document matches the query. Following current Web vernacular, we will call this ordering of the Retr set a hitlist and a retrieved document's position its hitKst rank Rank(di). This is a positive integer assigned to each document in the Retr set, in descending order of similarity with respect to the matching function Match{q, d):  Match(q, d)eU Rank(d) e M^  Rankidi) lt; Rank(dj) lt; Matchiq, dª) gt; Match(q, dj)      (4.6)  Sparck Jones | Sparck Jones, 1972] and others have historically referred to a document's rank in Retr as its "coordination level" (cf. Eq. 3.36). Strictly speaking, coordination level refers to the number of ASSESSING THE RETRIEVAL       125  keywords shared by document and query. In Boolean retrieval systems, sensitive only to the presence or absence of keywords, ranking by coordination level may be the only available measure on document/query similarity.  For long queries, hitlist rank and coordination level are likely to be similar, because it is unlikely that different documents will match exactly the same number of words from the query. But for short queries, it is likely that coordination level will only partially order the Retr set. This is why van Rijsbergen, p. 161, speaking of the Boolean systems typical at that time, said, "Unfortunately, the ranking generated by a matching function is rarely a simple ordering, but more commonly a weak ordering." Most modern search engines, however, exploit keyword weightings and can provide much more refined measures, thereby providing a total ordering of the hitlist.  According to the Probability Ranking Principle (cf. Section 5.5.1), a retrieval system is performing optimally if it retrieves documents in order of decreasing probability of relevance. For now we simply assume that there is a total ordering imposed over Retr. We will use the hitlist ranking to effectively define a series of retrievals. Setting a very high threshold on this ordering would mean retrieving a very small set, while setting a lower threshold will retrieve a much larger one.  Now consider a particular query q and the set Relq of relevant documents associated with it. Assuming that Retr is totally ordered makes it possible for us to define the fundamental analytic tool for search engine performance: the Recall/Precision curve (Re/Pre curve). The basic procedure is to consider each retrieved document in hitlist rank order and to ask for the precision and recall of the retrieval of all documents up to and including this one.  Consider the first of the two hypothetical retrievals shown in Table 4.2.  With respect to this query, we will assume there are exactly five relevant documents out of a total of 25 in the corpus. The very first one retrieved is deemed relevant; if we stopped retrieval at this point, our recall would be 0.2 (because we would have retrieved one of five relevant documents), and our precision is perfect (the one retrieved document is relevant). Our good luck continues as we consider the next document, which is also relevant; this generates a second Re/Pre data point of (0.4,1.0). We are not so lucky with the third document retrieved; precision drops to 0.67 and recall remains at 0.4. Proceeding down the 126      FINDING OUT ABOUT    TABLE 4.2 Two Hypothetical Retrievals      Query 1    Query 2    Relevant? NRel Recall Precision Relevant? NRel Recall Precision  1 1 1 0.20 1.00 0 0 0.00 0.00  2 1 2 0.40 1.00 0 0 0.00 0.00  3 0 2 0.40 0.67 0 0 0.00 0.00  4 1 3 0.60 0.75 1 1 0.50 0.25  5 0 3 0.60 0.60 0 1 0.50 0.20  6 0 3 0.60 0.50 0 1 0.50 0.17  7 0 3 0.60 0.43 0 1 0.50 0.14  8 0 3 0.60 0.38 0 1 0.50 0.13  9 0 3 0.60 0.33 0 1 0.50 0.11  10 0 3 0.60 0.30 0 1 0.50 0.10  11 0 3 0.60 0.27 0 1 0.50 0.09  12 0 3 0.60 0.25 0 1 0.50 0.08  13 0 3 0.60 0.23 0 1 0.50 0.08  14 0 3 0.60 0.21 0 1 0.50 0.07  15 1 4 0.80 0.27 1 2 1.00 0.13  16 0 4 0.80 0.25 0 2 1.00 0.13  17 0 4 0.80 0.24 0 2 1.00 0.12  18 0 4 0.80 0.22 0 2 1.00 0.11  19 0 4 0.80 0.21 0 2 1.00 0.11  20 0 4 0.80 0.20 0 2 LOO 0.10  21 0 4 0.80 0.19 0 2 LOO 0.10  22 0 4 0.80 0.18 0 2 LOO 0.09  23 0 4 0.80 0.17 0 2 LOO 0.09  24 0 4 0.80 0.17 0 2 LOO 0.08  25 1 5 1.00 0.20 0 2 LOO 0.08  retrieval in rank order, and plotting each point in this fashion gives the Re/Pre curve shown in Figure 4.10.  At this point we can already make several observations. Asymptotically, we know that the final recall must go to one; once we have  retrieved every document we've also retrieved every relevant document. The precision will be the ratio of the number of relevant documents to the total corpus size. Ordinarily, unless we are interested in very general  queries or very small sets of documents, this ratio will be very close to zero.  The other end of the curve, however, turns out to be much less stable. We would hope that a retrieval system's very first candidate for retrieval, the document with hitlist rank = 1, will be relevant, but it may not be. ASSESSING THE RETRIEVAL       127  1.00 0.90 ¶0.80 ¶0.70 -0.60 -0.50 -0.40 - 0.30 - ï 0.20 - ¶ 0.10 - 0.00  0.00  0.10  0.20  0.30  0.40  0.50 Recall  0.60  0.70  0.80  0.90  1.00  FIGURE 4.10 Recall/Precision Curve  LOO ¶0.90 ¶0.80 -0.70 -0.60 - 0.50 - 0.40 ¶0.30 -0.20 -0.10 - 0.00  0.00        0.10        0.20        0.30        0.40        0.50        0.60        0.70        0.80  Recall  FIGURE 4.11 Instability of Beginning of Re/Pre Curve  0.90  LOO  Figure 4.11 shows a second pair of hypothetical data points (dashed line), corresponding to the case that a single irrelevant document is ranked  higher than the relevant ones. This relatively small change in assessment creates a fairly dramatic effect on the curve, with real consequence once 128      FINDING OUT ABOUT  0.00  0.00  0.10         0.20         0.30         0.40         0.50         0.60          0.70         0.80  Recall FIGURE 4.12 Best/Worst Retrieval Envelope  0.90  LOO  we need to juxtapose multiple queries' curves (see Section 43.7). Such instability is an inevitable consequence of the definitions ofPrecision and Recall: If the first retrieved document happens to be relevant, its Re/Pre coordinates will be less than 1, and ~^j greater than 1; otherwise it will belt;0gt;Ggt;.  Figure 4.12 puts this particular retrieval in the context of the best and worst retrievals we might imagine. The best possible retrieval (hashed) would be to retrieve the five relevant documents first, and then all other documents. This would produce the upper, square Re/Pre curve. Alternatively, the worst possible retrieval (shaded) would retrieve all but the relevant documents before returning these; this produces the lower line.
foa-0073	4.3.6 Normalized Recall and Precision  The best/worst "envelope" surrounding an actual Re/Pre curve is related to a similar comparison known as normalized recall [Rocchio, 1966]. Imagine plotting the fraction of relevant documents retrieved as a function of the fraction of the total number of documents retrieved. Such a function is plotted in Figure 4.13. Comparing the area between the actual retrieval and the worst case (shaded) to the total area between the ASSESSING THE RETRIEVAL       129  1.0  NRel  Recall  NRel                rj                                               NDoc          Rank  FIGURE 4.13 Normalized Recall  best (hashed) and worst cases (that above the region) is very much like the best/worst-case envelope of Figure 4.12.  We can derive expressions for this area. Let r j be the hitlist rank of the fth relevant document. Then the area under the curve corresponding to any actual query is (if we define ro = 0):  NRel  Actual =  NRel  (4.7)  In the best case r; Best =  i:  NRel+l  NRel              NRel^'             2  when NRel ógt; oo. In the worst case ri = NDoc ó NRel + i:  ((NDoc- NRel+ i) - (NDoc- NRel+ i - 1)) ï i  (4.8)  Worst =  NRel  = NDoc- NRel  (4.9)
foa-0074	4.3.7 Multiple Retrievals across Varying Queries  It should come as no surprise, given the wide range of activities in which FOA is a crucial component, that there is enormous variability among the kinds of queries produced by users. The next step of our construction  is therefore to go beyond a single query to consider the performance of a system across a set of queries. 130      FINDING OUT ABOUT  0.00  0.00         0.10         0.20         0.30         0.40         0.50         0.60         0.70         0.80  Recall  FIGURE 4.14 Multiple Queries, Fixed Recall Levels  0.90  1.00  One obvious dimension to this variability concerns the "breadth" of the query: How general is it? If the Rel set for a query is known, this can be quantified by generality, comparing the size of Rel to the total number of documents in the corpus:  Generality  =  NDoc  (4.10)  There are many other ways in which queries can vary, and the fact that different retrieval techniques seem to be much more effective on some types of queries than others makes this a critical issue for further research. For now, however, we will treat all queries interchangeably but consider average performance across a set of them.  Figure 4.14 juxtaposes two Re/Pre curves corresponding to two queries. Query 1 is as before, and Query 2 is a more specific query, as evidenced by its lower asymptote. Even with these two queries, we can see that, in general, there is no guarantee that we will have Re/Pre data points at any particular recall level This necessitates interpolation of data points at desired recall levels. The typical interpolation is done at prespecified recall levels, for example, 0, 0.25, 0.5, 0.75, and 1.0. As van Rijsbergen, p. 152 discusses, a number of interpolation techniques ASSESSING THE RETRIEVAL       131  1.0' i-      / Logical, stem  0.8  Numeric, stem  |  0.6    u    £  0.4    0.2     1               1                \               1   0 0.2    0.4    0.6    0.8 1.0   Recall   FIGURE 4.15 11-Point Average Re/Pre Curves. From [Salton and Lesk, 1968]. Reproduced with permission of Prentice-Hall  are available, each with its own biases. Because each new relevant document added to our retrieved set will produce an increase in precision (causing the saw-tooth pattern observed in the graph), simply using the next available data point above a desired recall level will produce an overestimate, while using the prior data point will produce an underestimate.  With preestablished recall levels, we can now juxtapose an arbitrary number of queries and average over them at these levels. For 30 years the most typical presentation of results within the IR community was the 11point average curves, like those shown in Figure 4.15 [Salton and Lesk, 1971; Salton and Lesk, 1968]. (These data happen to show performance on the ADI corpus of Boolean versus weighted retrieval methods; they include only the last 10 data points.)  It is not uncommon to see research data reduced even further. If queries are averaged 2X fixed recall levels and then all of these recall levels are averaged together, we can produce a single number that measures retrieval system performance. Note the serious bias this last averaging across recall levels produces, however. It says that we are as interested in how well the system did at the 90 percent recall level as at 10 percent!? Virtually all users care more about the first screenful of hits they retrieve than the last.  This motivates another way to use the same basic recall/precision data. Rather than measuring at fixed recall levels, statistics are collected at the 10-, 25-, and 50-document retrieval levels. Precision within the first 10 or 15 documents is arguably a much closer measure of standard browser effectiveness than any other single number. 132       FINDING OUT ABOUT  All such atempts to reduce the Re/Pre plot to a single number are bound to introduce artifacts of their own. In most cases the full Re/Pre curve picture is certainly worth a thousand words. Plotting the entire curve is straightforward and immediately interpretable, and it lets viewers draw more of their own conclusions.  We must guard against taking our intuitions based on this tiny example (with only 25 documents in the entire corpus) too seriously when considering results from standard corpora and queries. For example, our first query had fully 20 percent of the corpus as relevant; even our second query had 8 percent. In a corpus of a million documents, this would mean 80,000 of them were relevant!? Much more typical are queries with a tiny fraction, perhaps 0.001 percent, relevant. This will mean that the precision asymptote is very nearly zero. Also, we are likely to have many more relevant documents, resulting in a much smoother curve.
foa-0075	4.3.8 One-Parameter Criteria  This section began with recall and precision, the two most typical measures of search engine performance. From that beginning, richer, more elaborate characterizations of how well the system is performing have been considered. But even having the two measures of recall and precision, it is not a simple matter to decide whether one system is better or worse than another. What are we to think of a system that has good recall but poor precision, relative to another with the opposite feature?  For example, if we wish to optimize a search engine with respect to one or more design parameters (e.g., the exact form of the query/ document matching function, cf. Section 5.3.1), effective optimization becomes much more difficult in multicriterial cases. Such thinking has generated composite measures based on the basic components of recall and precision.  For example, Jardine and van Rijsbergen [ Jardine and van Rijsbergen, 1971; van Rijsbergen, 1973] originally proposed the F-measure for this purpose:  p   ~~ (@2 + l)* Precision-Recall  P "       ^Precision + Recall                            *    j  van Rijsbergen, p. 174 * has since defined the closely related effectiveness  * Van Rijsbergen's original paper used the function symbol E rather than the F we use here. The substitution is made to maintain the E ó "effectiveness" mnemonic. ASSESSING THE RETRIEVAL       133  of measure £, which uses a to smoothly vary the emphasis given to precision versus recall:  /     a            1-cA""  Ea = 1 -           ..     + -rój:                        (4.12)  \Prectswn      Recall/  The transform a = -ott^ converts easily between the two formulations, with E = 1 ó F. Van Rijsbergen, p. 174 also presents an argument that a perfectly even-handed balance of precision against recall at a = 0.5 is most appropriate. Setting a = 0.5, f5 = 1 also has the pleasing consequence that the F, statistic corresponds to the harmonic mean of Precision and Recall  As discussed at some length in Section 7.4, it is possible to view retrieval as a type of classification task: Given a set of features for each document (e.g., the keywords it contains), classifiy it as either Rel or Rel with respect to some query. Lewis and Gale [Lewis and Gale, 1994] used the Fp measure in the context of text classification tasks, and they recommend a focus on the same j8 = 1.0 balance. Classification accuracy measures how often the classification is correct. If we associate the choice to retrieve a document with classifying it as Rel, we can use the variables defined in the contingency table of (Table 4.1):  I Retr H Rel I + I Retr D Rel\  Accuracy- -----------------ó--------------------             (4.13)  NDoc  Sliding Ratio  The fact that the Retr set is ordered makes it useful to compare two rank orderings directly. If the "correct," idealized ranking is known (for example, one corresponding to perfectly decreasing probability of relevance) , then an actual search engine's hitlist ranking can be compared against this standard. More typically, the rankings of two retrieval systems are compared to one another.  Given two rankings, we will prefer the one that ranks relevant documents ahead of irrelevant ones. If our relevance assessments are binary, with each document simply marked as relevant or irrelevant,* the normalized recall measure considered in Section 4.3.6 (or the  * As always, these assessments of relevance are with respect to some particular query. 134      FINDING OUT ABOUT  expected search length measure to be described in Section 4.3.10) is the best we can do in distinguishing the two rankings.  But if we assume instead that it is possible to impose a more refined measure Rel(di) than simply Rel/Rel (e.g., recall the richer preference scale of Figure 4.1), more sophisticated measures are possible. In this case, we prefer a ranking that ranks d\ ahead of dj just in case Rel(d{) gt; Rel{ dj). One way to quantify this preference is to sum the Rel{ d\) for the NRet most highly ranked documents:  NRet  (4.14)  The ratio of this measure, computed for each of the two systems' rankings, is called the sliding ratio score [Pollack, 1968]:  Rank\{dt) lt;NRet  E       Rd(dt)  Rank2(dl) lt;NRet  E       Wdi)  f=i  As NRet increases, this ratio comes closer to unity:  Ranki(di) lt;NRet  E        ReKdi) lim     ------ó---------------= 1                  (4.16)  NRet-+NDoc Ranked,) lt;NRet  i=l  and so it is most useful for distinguishing between two rankings when only a small NRet is considered.  Point Alienation  The sliding ratio measure provides a more discriminating measure but depends entirely on the availability of metric Rel(dj) measures for retrieved documents. As discussed in Section 4.1.1, it is much easier to derive nonmetric assessments directly from relevance feedback data given naturally as part of users' browsing:  e -lt;#-lt;©                                  (4.17)  In an effort to exploit the nonmetric preferences often provided by human subjects5 Guttman [Guttman, 1978] defined a measure known ASSESSING THE RETRIEVAL       135  as point alienation. Bartell has pioneered a variation of it for use with document rankings rated by relevance feedback [Bartell et al., 1994a]. The basic idea is deceptively simple: Compare the difference in rank between two differentially preferred documents to the absolute difference of these ranks:  y.   Rank(d)~Rank(d') ~ f^d, \Rank{d) - Rank(df)\                   l '    j  If d is really preferred over d! - (d y d!) - (e.g., if some user has marked d as Rel but said nothing about df), we can hope that Rank(d) lt; Rank(df),* and so the numerator (Rank(d)~ Rank(df))wHl be negative; if, on the other hand, the two documents are incorrectly ordered by the ranking, the numerator will be positive. Comparing this arithmetic difference to its absolute value, and then summing over the rankings for all pairs of documents (d, df) that are differentially preferred {dy dr) gives Equation 4.18.
foa-0076	4.3.9 Test Corpora  By test corpora we refer to collections of documents that have associated with them a series of queries for which relevance assesments are available. One of the earliest such test sets was a collection of 1400 research papers on aerodynamics developed by C. Cleverdon in the mid-1960s, known as the Cranfield corpus [Cleverdon and Mills, 1963]. For most of the 1980s, a set of corpora known as CACM, CISI, INSPEC, MED, and NPL (sometimes referred to as the Cornell corpora) were developed, maintained, and distributed by Gerald Salton and his students at Cornell; it became the de facto standard for testing within the IR community. For some time, the most influential test corpora have been the TREC corpora l associated with the Text Retrieval Evaluation Conference meetings [Harman, 1995].  Table 4.3 gives a sample of statistics for a number of the most widely used corpora. One obvious trend is the increasing size of these collections over time. The Reuters corpus2 classification labels are invaluable for  ' Note that my notation deviates from BarteiTs somewhat. In particular, we assume here that Rank() increases from most- to least-highly ranked document, so that the first element of the hitlist has Rank = 1. potomac. ncsl. nlst. go v/ tree/  www.research.att.com/ lewis/reutersS 1578.html 136      FINDING OUT ABOUT  TABLE 4.3 Common Test Corpora  Collection       NDocs      NQrys     Size (MB)     Term/Doc     Q-DRelAss  ADI 82 35     AIT 2109 14 2 400 gt; 10,000  CACM 3204 64 2 24.5   CISI 1460 112 2 46.5   Cranfield 1400 225 2 53.1   LISA 5872 35 3    Medline 1033 30 1    NPL 11,429 93 3    OSHMED 34,8566 106 400 250 16,140  Reuters 21,578 672 28 131   TREC 740,000 200 2000 89-3543 £3 100,000  training classifiers (cf. Section 7.4). With our AIT corpus, the OSHMED [Hersh, 1994] is one of the few to provide multiple relevance assessments of the same (q, d) pair.  Figure 4.16 shows a sample query from the TREC experiments. For this query, and hundreds like it, considerable manual effort has gone into assessing whether documents in the TREC corpus should be considered "relevant." Note the way the "basic" query (Line 2) has been embellished with general and specific topical orientation (Lines 1 and 3), important terms and abbreviations have been explicated, etc. This is much more  1  gt;         Science and Technology  2 gt;         ADDS treatments  3 gt;         Document will mention a specific AIDS or ARC treatment.  4 gt;         To be r, a document must Include a reference to at least one specific potential Acquired  Immune Deficiency Syndrome (AIDS or AIDS Related Complex treatment.  5 gt;         1. Acquired Immune Deficiency Syndrome (AIDS, AIDS Related Complex (ARC  6 gt;        2. treatment, drug, pharmaceutical  7 gt;         3. test, trials, study  8 gt;         4. AJZT, TEA  9 gt;        5. Genentech, Burroughs-Wellcome  10 gt;       ARC - AIDS Related Complex  11  gt;      .  A set of symptoms similar to AIDS.  12  gt;       A2T - Azidothyinidine, a drug for the treatment of Acquired Immune Deficiency  Syndrome, its related pneumonia, and for severe AIDS Related Complex,  13 gt;       TPA - Tissue Plasminogen Activator - a blood clot-dissolving drug.  14 gt;       treatment - any drug or procedure used to reduce the debilitating effects of AIDS or  ARC.  FIGURE 4.16 TREC Query ASSESSING THE RETRIEVAL       137  information than most users typically provide, but it also allows much more refined assessment of systems' performance.  As the testing procedures of the TREC participants have developed over the years, multiple "tracks" have formed, corresponding to typical search engine usage patterns. The task on which we have focused throughout this section is termed ad hoc retrieval, in the sense that a constant corpus is repeatedly searched with respect to a series of ad hoc queries. This is distinguished from the routing task, which assumes a relatively constant standing set of queries (for example, corresponding to the interests of various employees of the same corporation). Then, an ongoing stream of documents is compared, with relevant documents routed to appropriate recipients.  More recently, a special type of routing termed filtering has also been considered. In the filtering task, the standing query is allowed to adapt to the stream of relevance feedback generated by the users as they receive and evaluate routed documents (cf. Section 7.3).
foa-0077	4.3.10 Other Measures  The performance measures already listed are by far the most common ways in which search engines are evaluated in the literature. Several others, however, have been important in the past and may again prove useful in some situations.  Expected Search Length  The ordered list of relevance assessments described in Section 4.3.5 also recommends another, holistic evaluation of the entire retrieval's behavior; this method is known as expected search length (ESL). ESL considers the length of a "path" as users walk down the ordered hitlist, measuring how many irrelevant documents were seen on this path before each relevant document; "expected" refers to the average length of each path ending in a relevant document. Cooper initially proposed this model to measure the work a search engine saves, in comparison to searching the entire collection at random [Cooper, 1968].  Given that a search engine retrieves documents in hitlist order, ESL also requires a criterion by which the users' wandering paths are stopped. Van Rijsbergen, p. 160 discusses a number of predicates that 138       FINDING OUT ABOUT  might be used to terminate the search: some fixed number of relevant documents, some fraction of all relevant documents, etc. Because the generality of queries can vary considerably, it is desirable to terminate the ESL after some fixed fraction E of relevant documents has been retrieved.  For this same reason, it makes sense to normalize ESL with respect to the number of relevant documents we might expect to retrieve if we were retrieving at random. If we use NRet for the number of retrieved documents (i.e., those satisfying the predicate mentioned earlier), we can estimate the expected random search length RandSL as:  NRet- (NDoc- Rel)  óknó        (4-"gt;  Then the expected search length reduction factor  RandSL ~ ESL  ESLRF- -----óó-----                        (4.20)  RandSL  captures the amount a real search method improves over the random case.  Operating Characteristic Curves  Swets [Swets, 1963] enumerated a number of abstract desiderata (quoted by van Rijsbergen, p. 155) that we might wish for any assessment measure. According to these, IR's standard Re/Pre plot leaves much to be desired, in particular because this two-dimensional assessment makes direct comparison impossible. Swets therefore recommended an analysis from the perspective of signal detection, based on several key assumptions:  ASSUMPTION 1 There is a "relevant" signal we wish to distinguish from background noise. We can consider the worst case to be comparison against an "irrelevant" signal with both signals imposed over the data collection. We can imagine that this signal is generated by the presence or absence of some keywords. ASSESSING THE RETRIEVAL       139  FIGURE 4.17 Distinguishing between Overlapping Distributions  ASSUMPTION 2   These two signals are to be discriminated according to only a single dimension.  ASSUMPTION 3   These signals are both distributed normally across the corpus.  In this idealized case, we get a picture similar to Figure 4.17. Then, because our corpus has been ordered by the ranking, the goal becomes to select a value RankT that best separates these two modal distributions.  Using a simple retrieval rule that retrieves a document if its value is above the threshold RankTgt; wherever we place this threshold we are bound to make two types of errors. There will be some Rel documents that fall below our threshold (shaded in Figure 4.17) and some irrelevant documents that fall above it (cross-hatched). Following signal detection theory we can call the first set "FALSEó " errors and the second "FALSE+" errors. (These are often called Type 1 and Type 2 errors, respectively.) Note that the ratio of the right tail of the Rel curve (the area not cross-hatched in in Figure 4.17) to the total area under the Rel curve corresponds exactly to the Recall measure defined earlier (Equation 4.2), while the ratio of the right tail of the NRel curve (cross-hatched in Figure 4.17) to the total area under the NRel curve corresponds exactly to Fallout (Equation 4.4). 140       FINDING OUT ABOUT  0.8  0.6  0.4  0.2  Fraction Rel gt; %  Fraction NRel gt; x  0.2                 0.4                 0.6                 0.8  FIGURE 4.18 Operating Characteristic Curve  The parametric curve defined by the percentage of Rel versus NRel documents retrieved as r is varied is called the operating characteristic curve. Obviously, if these two distributions are identical, this curve will be exactly a diagonal line, from (0,0) to (1,1). If the mean value of the Rel distribution is greater than that of the NRel the operating characteristic curve is moved closer to the upper-left corner, as shown in Figure 4.18.  While Swets (and subsequently others [Robertson, 1969;Bookstein and Kraft, 1977]) then considered fairly elaborate tests to discriminate the relative performance of retrieval systems with respect to such curves, it is fair to say that the 1979 assessment of van Rijsbergen, p. 154 still stands:  ... although the Swets model is theoretically attractive and links IR measurements to ready-made and well-developed statistical theory, it has not found general acceptance amongst workers in the field. ASSESSING THE RETRIEVAL       141  Optimal selection of Rankr depends on specification of the costs (losses) of making FALSE+ or FALSEó errors. For example, if you are an overworked and underpaid law clerk and you read an irrelevant document (FALSE+), youVe wasted precision attention, but that's all; if you miss a reference you should have found (FALSEó) the cost might be huge. But if you're a partying undergraduate with one more term paper between you and summer vacation, your assessments might be quite different. Section 5.5.6 gives an example of how explicit models of these various costs can be incorporated within a Bayesian decision-making framework.
foa-0078	4.4 RAVE: A Relevance Assessment VEhicle  Section 4.3.2 argued that the opinions of many users concerning the relevance of a document to a query provides a more robust characterization than any single expert. It seems, however, that our move from omniscient to consensual relevance has only made the problem of evaluation that much more difficult: Test corpora must be large enough to provide robust tests for retrieval methods, and multiple queries are necessary to evaluate the overall performance of an IR system. Getting even a single person's opinion about the relevance of a document to a particular query is hard, and we are now interested in getting many!  This section describes RAVE, a relevance assessment vehicle that demonstrates that it is possible to operationally define relevance in the manner we suggest. RAVE is a suite of software routines that allow an IR experimenter to effectively collect large numbers of relevance assessments for an arbitrary document corpus. It has been used with a number of different classes of students to collect the relevance assessments used for evaluation with respect to the AIT corpus; your teacher may have you participate in a similar experiment. It can also be used to collect assessments for other document corpora and query sets.
foa-0079	4.4.1 RAVeUnion  It would be most useful if, for every query, the relevance of every document could be assessed. However, the collection of this many assessments, 142       FINDING OUT ABOUT  More elaborate ways of  merging ranked lists  for a corpus large enough to provide a real retrieval test, quickly becomes much too expensive. But if the evaluation goal is relaxed to being the relative comparison of one retrieval system to one or more alternative systems, assessments can be constrained to only those documents retrieved by one of the systems.  We therefore follow the pooling procedure used by many other evaluators, viz., using the proposed retrieval methods themselves as procedures for identifying which documents are worth assessing.  The first step in constructing a RAVE experiment is to combine the ranked retrieval lists of the two or more retrieval methods, creating a single list of documents ordered according to how interested we are in having them assessed by a subject.  RAVeTJnion produces the most straightforward "zipper" merge of the lists, beginning with the most highly ranked and alternating one.^ The output of RAVeUnion is a file of (query, document) pairs along with a field that indicates if the pair was uniquely suggested by only one of the methods. This last information can be used to compare the average relevance scores of documents suggested by one method alone to those retrieved by more than one.
foa-0080	4.4.2 RAVePlan  A second challenge is to find the desired density or redundancy of sample points. That is, for each document that we believe may be relevant to a query, how many subjects should evaluate it? The answer will vary depending on such factors as the number of participants, their expertise, their motivation to produce quality judgments, how long each will spend rating documents, etc. A higher density means that fewer documents will be evaluated, but also that the intersubject cumulative assessment is likely to be more statistically stable. This can be especially important when relevance feedback is to be used to train the system.  The trade-off between the most important of these factors is captured in the following formula:  x =  NR STQ  (4.21) ASSESSING THE RETRIEVAL       143  where  x = number of documents to be evaluated for each query N = number of subjects  R = expected subject efficiency (votes/user/time) 5 = desired density (votes/document) T = time spent by subjects Q = number of queries to be evaluated  Note that this formula ignores the overlap between queries that occurs when users see a document that may be relevant to two or more of the queries in their list. Care must be taken, therefore, to minimize expected overlap between the topical areas of the queries. We have also found that the assessment densities constructed using this formula are unfortunately uneven. The main source of these is variability in JR, the rate at which subjects are able to produce relevance assessments.  EAVePlan takes as input a list of Q query specifications, a list of N subject logins, the desired density S, and the number of documents R* T that should be allocated to each subject. The query specifications indicate which queries can go in which fields and which queries should not be shown together. This allows us to limit possible interactions between queries about similar topics.^                                                                  A better  RAVePlan outputs two files. The plan file, which is an input to the RAVePlan RAVE interactive application, lists each subject ID along with the queries and document numbers that have been selected for that subject. The assignments file is a list of document-query pairs, which tells us which query we expect the document to be relevant to. RAVeCompile uses this file after data collection is complete to generate true and false-positive measures.
foa-0081	4.4.3 Interactive RAVE  The interactive portion of RAVE is an HTML document, as shown in  Figure 4.19. The top of RAVE's window displays the three queries against  which the subject is to judge each document. Two queries are short sentences or phrases, in this case REASONIMG ABOUT UNCERTAINTY 144      FINDING OUT ABOUT  *;j    Back      Forward    Reload       Home Search       Guide       images       Print     Security       Slop m  TITLE: DESIGN OF A The need for a reliable method of measuring the depth, of ±  RECOGNITION SYSTEM FOR anesthesia has existed since the introduction of s  MONITORING THE DEPTH OF anesthesia. Hemodynamic variables are most commonly used   ANESTHESIA, BASED ON THE as a measure of anesthetic depth by anesthesiologists in   AUTOREGRESSIYE MODELING the operating room. It has besn found that the   AND NEURAL NETWORK hemodynamic variables by themselves may not be providing   ANALYSIS OF THE EEG enough information to predict the depth, of anesthesia.   SIGNALS Since the system most affected by anesthetic agents is   AUTHOR: SHARMA, the central nervous system, electroencephalograms (EEG),   ASHUTOSH a record of brain activity, can be used to monitor the   YEAR: 1993 anesthetic depth. This thesis establishes the   INSTITUTION: RENSSELAER feasibility of using a computer-based EEG recognition   POLYTECHNIC INSTITUTE; system to monitor anesthetic depth during halothane   0185 anesthesia. The spectral information contained in the    EEG signals "vas represented using a tenth, order    autoregxessive (AR) model. A four layer perception    feedforward type of network Tsras used in designing the    recognition system. The system im trained and its    coStofcA ftnm aSmal experiments, lie input features to                                           | :ï!'.';   |, the recognition system wiie based on thje AJR parameterst    and tfee:ba$$t of f$ª system w depth of anestheslat.    various levels of IjatotJiane; wWk itself i?st?controled I'jjii   !   KJtSlliil^ ^'f^yri^ftn * i^frffi^w ftT^ffitirtf%ffiifo ^i^YYin^liilfey!, DeptiXOf                                                                 ! Si      Query 1 seasoning about uncertainty Query 2:legal applications  Query 1: $gt; Not Q Possibly Q Relevant Q Critically        Query 2; % Not Q Possibly Q Relevant Q Critically   Ne´ Document j 0 Quit after this o ne                          Query 3 (belov): # Not Q Possibly Q Relevant Q Critically       TITLE: A PLANNING MODEL A planning approach characterized by its problem analysis and   WITH PROBLEM ANALYSIS goal-oriented hierarchical operator representation is presented in this i  AND OPERATOR HIERARCHY dissertation. The planning process is divided into two phases In the first   AUTHOR: CHANG, phase, the system analyses the problem from a global viewpoint. This is   KAI-HSIUNG done by inferring from the heuristic rules to order the original subgoals   YEAR: 1986 and to post constraints. With this global analysis, fruitless search space   INSTITUTION: UNIVERSITY OF can be pruned so that the effort is directed in the right direction during   CINCINNATI (0045) the plan generation. In the second phase, the refinement process selects    operators from the goal-oriented operator representations to achieve each    subgoal. Each operator representation b named by its goal and contains    local knovMge of hov to refine the operator under various conditions.   bti,:.:.;...........;,:,;........,ï,:::,:....................... ,......,,Ñ,........,,.......,..........,........;!,............*,............ÑÑ............*..........................,......................................Ñ..................................ª.....................................,.............................   FIGURE 4.19 RAVE Interface  and LEGAL APPLICATIONS, and the third Is a scrolling pane containing the text of the long, J?e/FfcA:-defined document (in this example, the  thesis A PLANNING MODEL WITH PROBLEM ANALYSIS...). While the subject is asked to judge the documents shown to him or her as ASSESSING THE RETRIEVAL       145  being about the two short queries, the task associated with the relevance feedbackdocument query is to find "documents like this."  Beneath each query the RAVE window contains four radio buttons labeled "Not" (relevant), "Possibly" (relevant), "Relevant," and "Critically" (relevant). Because we asked our subjects to spend two hours each but could not assume their participation would be continuous, there is a "Quit" button that allows the subject to suspend the session; when the subject launches RAVE again, the session is resumed where he or she left off. Finally, the "Next" button is pressed after the subject has read and recorded his or her relevance assessments for a document.
foa-0082	4.4.4 RAVeCompile  RAVeCompile processes the votes recorded by interactive RAVE, creating a relevance assessment file and some statistics about each query and subject. The first step is to map the four-valued relevance assessments into the Boolean relevant/nonrelevant discriminations typically used by standard IR evaluation techniques. RAVeCompile lets the experimenter configure a simple predicate of the following form:  5 = a ï NPoss + b ¶ NRel + c ï NCrit NVote = NPoss + NRel + NCrit  RelP = (NVote gt; Quorum) A (5 * NVote gt; MinAvg) where  s = weighted aggregate score across relevance levels a = weight assigned to votes of "possibly relevant" b = weight assigned to votes of "relevant" c = weight assigned to votes of "critically relevant" NVote = total number of active votes collected for (q, d) pair Quorum = minimum number of votes required for (q, d) to be  considered relevant  MinAvg = minimum weighted sum required for (q, d) to be considered relevant  In one set of experiments [Belew and Hatton, 1996], the data used two predicates constructed in this fashion. These are: 146      FINDING OUT ABOUT  ï  permissive: if (two or more POSSIBLE votes) or (at least one RELEVANT vote)  ï  stringent: if (two or more RELEVANT votes) or (at least one CRITICAL vote)
foa-0083	4.5 Summary  In this chapter we began by making some assumptions about users of a search engine in order to figure out how well the system is satisfying users' information needs. We focused on two separate notions of assessment: first, assessing the relevance of documents retrieved by the system in response to a particular query, and second, assessing the search engine's overall utility through aggregating relevance judgments provided by many users performing many queries.  Section 4.1 discussed both metric and nonmetric relevance feedback and the difficulties in getting users to provide relevance judgments for documents in the retrieved set. We saw, however, that relevance feedback could be used to either suggest query refinements to the users or to modify the underlying document representations to improve future system performance.  The concept of consensual relevance introduced in Section 4.2 addresses an issue raised in Chapter 1 , where we asked what success criteria can be used in evaluating a search engine. Consensual relevance tells us that relevant documents are those documents that many users find to be useful. We can ask how useful a particular search engine is, or compare one search engine with another, by posing the question: How useful (relevant) do users find the documents retrieved in response to queries?  To answer that question we quantified several measures of system performance. The generality of a query is a measure of what fraction of documents in the corpus is relevant to the query. Fallout measures the fraction of irrelevant documents found in the retrieved set of a given query. The key notions of recall, the fraction of relevant documents in the retrieved set, and precision, the fraction of retrieved documents that are relevant, allow us to make direct comparisons between two search engines' performances on any query. Other methods of comparison ASSESSING THE RETRIEVAL       147  include sliding ratio, point alienation, expected search length, and oper afiner rhatarteristir rnrvps.  g         p  ating characteristic curves.
foa-0084	5_________  Mathematical Foundations  No inquiry can go very far without a rigorous notion of where it's been. Mathematics continues to provide our most reliable representation for the construction of new hypotheses and their testing. This chapter pulls together a wide range of mathematical issues arising in the FOA context.
foa-0085	5.1 Derivation of Zipf's Law for Random Texts  As before, we begin by defining a word to be any sequence of characters separated by spaces. Let us therefore consider an alphabet of M characters, interspersed with a specially designated space character 0. We will consider an especially simple model (similar to that used by many others [Li, 1992; Miller, 1957; Hill, 1970; Hill, 1974]) in which a random monkey generates words by hitting all keys - space and letters - with equal probability p:  p = pT{%) = Pr(A) ==...= Pr(Z) = ó^ó           (5.1)  M+ 1  We can use lexicographic trees to conveniently organize words of length fc, say, by the order in which the k characters occur prior to the terminating space, as shown in Figure 5.1 This shows a set of M + 1 trees, each rooted in the words' starting character. Leaf nodes at level k  149 150       FINDING OUT ABOUT  o  Pr("A")  FIGURE 5.1 Lexicographic Tree Underlying Zipfian Distribution  Double counting  spaces?!  are all labeled with the probability of the sequence of k ó 1 characters prior to the space occurring at level k.  One immediate observation is that Njt, the number of words W{ of length k or less, is:  Nk = Number(wi I i lt; k) = Y" Nf = M(1~M }           (5gt;2)  In an infinitely long sequence of characters generated according to Equation 5.1, we will expect to find a "word" Wk terminating at level k (i.e., a string of k unbroken nonspace characters bracketed by two spaces) with probability defined in terms of the independent character probabilities p:  L                         ,5.3)  We can compute c, the constant of proportionality, by including all the Mk words of length k and summing these probabilities over all possible  words (including unrealistic, infinitely long ones!):^"  ,                       (M+l)2  Mlcp        lgt;c  h    ï¶ï pk =  Next consider the rank of these words. Because the probability of a  word's occurrence is an exponentially decreasing function of its length, MATHEMATICAL FOUNDATIONS       151  we know that the M highest ranked words are the one-character words; next come the M2 two-letter words; and so on. Using Equation 5.2 we therefore know how the rank rgt; of all words wk terminating on level k must be bounded above and below:  Nfc-i lt; rklt;Nk  where f  denotes a compromise "average" rank for all the Mk equiprobable words.*  Note that Equations 5.4 and 5.5 define the words' probability and rank, respectively, in terms of the common metric k. As Li [Li, 1992] notes, Zipf's law is fundamentally about this transformation, from an exponential distribution onto a rank variable.  Solving both equations for k:  k=  ln(M+ 1)  i__ f 2(M ó l)f)t    i , _       V    M+l     +  ~            lnM  we can now set them equal and derive an expression for a word's probability in terms of its rank:  'z(M-\)fk M+ 1  ln(M+l)              lnM  1   /2(M-l)ffc  This has the functional form required by Mandelbrot's generalized Zipf's law (cf. Equation 3.2):  _        C Pk ~ (ft + B)´ where  1   /   M+l   V            M+l           ln(Af+l)  MV2(M     1)7   '         2(M     1)'                         P;  1   /   M+l   V MV2(M- 1)7   '  2(M- 1)'              lnM  * The model can be extended by replacing this simple average with distributional information, for example, incorporating realistic character frequency information. 152       FINDING OUT ABOUT  1.25  1.15  1.05  80                            100  0.95  FIGURE 5.2 a as Function of M, Number of Distinct Characters
foa-0086	5.1.1 Discussion  Obviously these formulas all depend on the alphabet size selected, and this will certainly not be feed across the systems considered. For example, it is not unusual for an IR system to "fold case," i.e., to treat upperand lowercase letters interchangeably, but many also preserve this case information. The capitalization of proper names will sometimes provide critical clues for appropriate index terms. Similarly, our choice of which characters we use to break the stream into wordlike tokens has consequence.  Fortunately for the robustness of ZipPs law, the alphabets typically considered in these analyses are generally large enough that differences between only uppercase or upper- and lowercase alphabets are inconsequential. Figure 5,2, showing how quickly a becomes nearly unity as the size of the character set grows, also makes it clear why Zipf's simpler hyperbolic form is an adequate approximation.  It is also interesting to note a potential connection between ZipPs law and information theory. Mandelbrot [Mandelbrot, 1953; Mandelbrot, 1982] initially attempted to derive ZipPs law as the solution minimizing the average cost per unit of information conveyed by a text. George Miller seems to have found this effort amusing: MATHEMATICAL FOUNDATIONS       153  ... the random placement of spaces which leads to ZipFs rule is actually the optimal solution. Our monkeys are doing the best possible job of encoding information word-by-word, subject to the constraints we impose on them. If we were as smart as the monkeys we, too, would generate all possible sequences of letters and so make better use of our alphabet. [Miller, 1957]
foa-0087	5.2 Dimensionality Reduction  As Section 3.5 has already suggested, our interest in matching queries and documents within a vector space can benefit greatly from other kinds of matching that have arisen in other kinds of vector spaces [Schutze, 1993]. We review several other basic features of the mathematical topic of linear algebra before applying them to the problem of FOA.  It is worth remembering that similarity information can come from many sources. For example, we will later (cf. Section 6.1) have much to say about how citation structure can be represented as a graph, with each document represented by a node in the graph and a directed edge going from d\ to dj, when dj appears in the bibliography of d\. Note that the documents' citation information is entirely independent of the words they contain and can be the basis for another characterization of the topical similarity between documents:  Two documents are about the same topic to the extent that they share the same documents in their bibliographies.  For now, such bibliometric data need only seem like a plausible, new way to analyze document content. Chapter 6 will discuss other features that we might also exploit.  Our goal in casting similarity matching of queries with documents in general mathematical terms is to make the resulting solutions sufficiently broad to handle any kind of features, keywords, or bibliographic citations.
foa-0088	5.2.1 A Simple Example  Imagine that we've collected data on the HEIGHT and WEIGHT of everyone in a classroom of N students. If these are plotted, the result would be 154       FINDING OUT ABOUT  Beyond the puny three dimensions of  human existence  Size  Weight FIGURE 5.3 Weight and Height Data Reduction  something like Figure 5.3. Notice the correlation around an axis we might call something like SIZE. Students vary most along this dimension; it captures most of the information about their distribution. It is possible to capture a major source of variation across the HEIGHT/WEIGHT sample because, just as with our keywords, the two quantities are correlated.  In this section we analyze similar statistical correlations among the keywords and documents contained in the much larger vector space model first mentioned in Section 3.4. Recall that in the vector space model, the Index relation placing D = NDoc vectors corresponding to the corpus documents within the space 5ft v, where V =NKw (for vocabulary size), is defined by its keyword vocabulary.  Here we describe this in the terms of linear algebra,* where / = Index is a D x V element matrix, t  Attempts to reduce this large dimensional space into something smaller are called dimensionality reduction. There are two reasons we might be interested in reducing dimensions. The first is probably more obvious: It's a very unwieldy representation of documents' content. Individual documents will have many zeros, corresponding to the many words in the corpus V not present in an individual document; the vector space matrix is very sparse. Dimensionality reduction is a search for a representation that is denser, more compressed.  Another reason might be to exploit what has become known as latent semantic relationships among these keywords. When we make each term in our vocabulary a dimension, we are effectively assuming they are orthogonal to one another; we expect their effects to be independent.  * In this language, single-letter identifiers are simplest, but that would make the Index relation /. Unfortunately, the letter / already plays a useful role in linear algebra, as the identity matrix; hence, / = Index. For similar reasons, within this section, we will use V = NKwmd D=NDoc MATHEMATICAL FOUNDATIONS       155  But many features of FOA suggest that index terms are highly dependent, highly correlated with one another. If that's the case, we can exploit that correlation by capturing only those axes of maximal variation and throwing away the rest.
foa-0089	5.2.2 Formal Notions of Similarity  Two features of the FOA problem can help us to focus on what is known as the Minkowski metric [Luenberger, 1969; Jain and Dubes, 1988]. First, the result of our calculations will be a real-valued weight associating a keyword with a document or query, and we can assume that this is a continuous quantity. Further, we can make the somewhat more questionable assumption that these weights make "natural" use of zero, and so index weights also fall on what is known as a "ratio" scale. With these two assumptions, the Minkowski metrics are defined as:  /NKw                     \ lll  Sim(q, d) = I ]T \wqk - wdk\L 1                     (5.9)  where L gt; 1. The most common version is the 1=2 norm, and we will use it here. The L = 1 ("Manhattan distance") and L = oo ("sup" norm, where E^ is replaced with max) are also seen often.  A metric is a scalar function over pairs of points in the vector space. Minkowski metrics satisfy three critical properties:  Sim(x, y) gt; 0                                              (5.10)  Sim(x, y) = Sim(yy x)                                  (5.11)  Sim(x, x) = ||x ||                                          (5.12)  gt; arg max Simix, y)                     (5.13)  The measure Sim(x, x) of a vector with itself is what we typically think of as the length of the vector, or more precisely, its norm* \\x\\.  Two other important features of metric spaces follow from these axioms:  1.  Sim(x, y) lt; || x|! ï \\y\\          (Cauchy-Schwarz inequality)  2.  || x + y || lt; ji x (|+ [I y ||            (triangle inequality)
foa-0090	156       FINDING OUT ABOUT  5.2.3 Singular Value Decomposition  Just as students' HEIGHT and WEIGHT are correlated along the dimension SIZE, we can guess that (at least some small sets of) keywords are correlated and that the vector space representation of a document corpus (cf. Section 3.4) might be simplified in a similar way. The goal is to "reduce the dimensionality" of the documents' representation in the same way we reduced that of our students' sizes.  But although we can draw a simple picture revealing the correlational structure between two dimensions, the picture is much more complicated when we try to conceive of the full V ´ 105 space of index terms. For the D vectors we seek a smaller k lt; V-dimensional solution. We still have as many documents as we had before, but we're going to use a different set of descriptors.  One of the most effective ways to characterize the correlational structure among large sets of objects is via eigenfactor analysis. The technique we consider is called singular value decomposition. This factors any rectangular matrix into three terms:  / = ULAT                                  (5.14)  where U and A are each orthonorinal and 1 is a diagonal matrix "conMathematical       necting" them.t  details                      As before (cf. Equation 3.39), using inner product to measure simi larity, we can define X:  X = ]f                                    (5.15)  Because / is a D x V matrix, X is a D x D symmetric matrix capturing all (^) interdocument similarities. Then U is the system of eigenvectors of X, and L has the square roots of the eignenvalues, y/TT^ along its diagonal. By convention, we order these in decreasing order:  ' gt;  y/h;               (5.16)  Large eigenvalues correspond to dominant correlations and so, just as looking for the SIZE dimension that captured the main interaction between HEIGHT and WEIGHT, we will rely on the first k dimensions to MATHEMATICAL FOUNDATIONS       157  Terms  FIGURE 5.4 SVD Decomposition  capture the dominant modes of interaction in /:  Jk=UkLkATk                                 (5.17)  This operation is shown schematically in Figure 5.4.1"  As always, whenever we throw something away (viz., the small eigenvectors), the result must be an approximation. That is, there will be a difference between our reduced-dimension representation Jk and the original /.  One easy way to measure this discrepancy is by referring to the D2 interdocument similarities latent in the X matrix and considering how different they are in the approximate matrix Xk  Xk = Jkj£                                   (5.18)  using the Li norm || ï ||2 to measure deviation. Then:  Err= || X-Xkh                          (5.19)  In fact, approximating X with its reduced fc-rank SVD decomposition turns out to be optimal, in the sense that it results in minimal Err over all other rank-fc alternatives [Bartell et aL, 1992].
foa-0091	5.2.4 How Many Dimensions k to Reduce To?  A central question remains: How many dimensions k should be used? To date, the only answers are empirical. Early experiments suggest that  using too few dimensions dramatically degrades performance.^ The original MED corpus used in early LSI experiments [Deerwester et aL, 1990; Dumais, 1991] happens to be distinguished by a particularly topically focused vocabulary. While a few hundred dimensions may suffice to discriminate this small set of documents, it might be necessary to use more dimensions when describing a broader domain of discourse.  Dimensionality reduction using neural networks  Earlier attempts to reduce dimensionality 158       FINDING OUT ABOUT  I  1           10         100       1000     10,000  Number of dimensions in LSA (log)  FIGURE 5.5 Retrieval Performance as Function of SVD Dimension (fc).  From [Landauer and Dumais, 1997]. Reproduced with permission of the Association of Computing Machinery  Empirically, however, reduction to around 500 dimensions seems to provide significant improvements to even very large corpora. For example, Figure 5.5 shows one experiment [Landauer and Dumais, 1997], where/: is varied over four orders of magnitude. Some attempts to select k theoretically, borrowing a rank-plus-shift method from signal processing, have also been made [Zha and Zhang, 1998]. The most satisfying answer may come from probabilistic models that relate the raw frequency statistics to an underlying distribution [Papadimitriou et al., 1998; Hoffman, 1999].
foa-0092	5.2.5 Other Uses of Vector Space  The same interdocument similarity information captured in the X = }JT matrix can be used for other purposes, too. For example, Section 7.5.1 will discuss one approach to the problem of classifying documents known as nearest neighbor.  The Index captures patterns of keyword usage across a corpus of documents. The preceding sections have held the corpus constant and used this data to analyze transformations of the keyword dimensions, but the converse is also possible. For example, Section 6.3 will discuss the representation of interkeyword relationships known as thesauri. One simple baseline for keywords is their pairwise similarities, as captured by /:  Y = JTJ                                      (5.20) MATHEMATICAL FOUNDATIONS       159  This produces a. V xV symmetric, square matrix capturing all (2) interkeyword similarities, exactly analogous to the interdocument similarities of Equation 5.15.  Littman has also considered an interesting application of LSI to the problem of searching across multilingual corpora [Littman et al., 1998].
foa-0093	5.2.6 Computational Considerations  All these techniques depend, of course, on being able to compute the SVD for the Index relation. Given the enormous size of this matrix ó typically 104 keywords times 105 to 109 documents - this is a nontrivial concern. However, the fact that these matrices are sparse (i.e., any one document vector has a relatively small fraction of keywords associated with it) means that special techniques can be used. In particular, Berry has developed methods based on Lanczos and subspace iteration methods in his SVDPack1 implementation [Berry, 1992; Berry et al., 1994; Letsche, 1996].+  Once the document corpus has finally been compressed, another serious recurring computation must be done: Every time a user issues a query, it must be transformed from the original space of "raw" keywords into the reduced /c-dimensional space. This means we must keep the matrix Uk available to transform each query into the reduced representation.  Another computational issue has to do with the effect of new documents being added to the collection, or older ones being removed. This is one particular type* of drift we might expect in FOA; Section 7.3.2 discusses several others. The addition or deletion of any one document will affect the SVD statistics of the entire corpus very slightly, especially if it is a very large corpus.  SVD is patentable?!  Temporal drift
foa-0094	5.2.7 "Latent Semantic" Claims  Within the IR community, SVD was first applied to the Index matrix by Deerwester et al. [Deerwester et al., 1990; Dumais, 1991] and was called latent semantic indexing (LSI). The "latent semantic" claim derives from  the authors* belief that the reduced dimension representation of documents in fact reveals semantic correlations among index terms. Further, they argue that evidence collected across entire corpora transcend individually "fallible" document instances. That is, while one document's  www. netlib. org. s vdpack 160      FINDING OUT ABOUT  author might use the word CAR and another the synonym AUTO, the correlation of both of these with other terms like HIGHWAY, GASOLINE, and DRIVING will result in an abstracted document feature/dimension on which queries using either keyword, CAR or AUTO, will project equivalently. "Synonymous" retrieval has been accomplished!  Landauer and Dumais have recently extended this algebraic manipulation of the Index relation into an ambitious model of human memory [Landauer and Dumais, 1997]. Much of psychology is concerned with the problem of how children, those most powerful learning agents, are able to learn so much from such a "poverty of the stimulus." That is, by many forms of analysis the stimuli driving learning do not by themselves contain sufficient information to induce the elaborate conceptual structures children demonstrate.  Applying these ideas to textual corpora, Landauer and Dumais "trained" an LSI model with presentation of paragraph after paragraph drawn from more than 30,000 encyclopedia articles. Using retrieval on a standardized synonym test as their performance measure, the emerging eigenvector representation (compressed to 300 dimensions) showed a rate of improvement comparable to that of schoolchildren! Because this performance required "indirect inference" like that supported by LSI eigenvectors and beyond what could be accomplished on the basis of simple word cooccurrence alone, Landauer and Dumais suggested that  A new                LSI provides an important model of human memory. 1"  argument for  [Sjome domains of knowledge contain vast numbers of weak correlations that... can greatly amplify learning by a process of  inference___[A] substantial portion of the information needed  ... can be inferred from the contextual statistics of usage alone. [Landauer and Dumais, 1997]  At the very least, LSI demonstrates how traditional associative memory models [James, 1893; Hebb, 1949; Baddeley, 1976; Anderson and Kline,  1979] can be extended to exploit higher-order correlations.  The earlier work trying to connect a small number of factor-analytic,  "semantically" meaningful dimensions [Koll, 1979; Borko and Bernick, 1963] is also interesting in this respect. Jones and Furnas have also investigated how well cosine/inner product reflects human similarity judgments [Jones and Furnas, 1987]. In any case, a cognitive interpretation of these issues promises to remain an active area of investigation. MATHEMATICAL FOUNDATIONS       161  We now consider how relevance feedback assessments might also be used to provide document-similarity information, and how they can be used to reduce the dimensionality of documents' representations.
foa-0096	5.3.1 Multidimensional Scaling  In many kinds of retrieval interfaces, it is easy and natural for a user to indicate a preference for one retrieved document over another. The relevance feedback discussed in Section 4.1 is our standard characterization of this information:^  e -lt; #-lt;  (5.21)  The literature on multidimensional scaling (MDS) was developed to deal with assessments by human subjects of the similarity or difference among multiple stimulus patterns [Borg and Lingoes, 1987]. A robust and important result from this analysis is that people can much more easily and consistently provide nonmetric assessments of the similarity than if they are forced to specify metric quantities. Rather than an experimenter arbitrarily imposing dimensions they believe important,  We can ask the subjects to globally judge, without criteria provided by the experimenter, the overall similarities of different facial expressions. The proximities are then mapped into [MDS] distances xgt; and the configuration examined for systematic characteristics of the distribution of points. [Borg and Lingoes, 1987, p. 72, emphasis added]  The "facial expression'' example is a reference to Woodworth's experiments in 1938 [Woodworth, 1938]. Imagine a hypothesis that facial expressions can be characterized in terms of two dimensions of variability. We could test this by showing human subjects pairs of pictures and asking them to judge how similar or dissimilar the two facial expressions are, in terms of "difference in emotional expression or content." Then imagine they are given some proximity p scale-from 1, 2,... k -along which they are to rank the picture pairs; each pair of pictures would therefore have a proximity score pij. If these pictures are to be characterized in a two-dimensional plane, we can also associate with each pair of pictures a distance d{ j according to their two-dimensional  IRhas historically ignored preferences 162      FINDING OUT ABOUT  Proximity can capture similarity or dissimilarity/distance  coordinates^  ij = proximity (evaluation)  jj = distance (in two-dimensional plane)  The MDS analysis described by Borg and Lingoes (they actually prefer the term "similarity structure analysis," or SSA) is a key contribution. It is an algorithm for iteratively moving vectors corresponding to the objects of evaluation (pictures of facial expressions) within an arbitrary dimensional space so as to minimize as much as possible the stress they experience, relative to their pairwise proximities. We think that the pictures have been well placed if those with similar proximities are close together as measured by this distance.  Within any such space, we can replace the pairwise distances dij with di associated with each point (picture) measuring the distance of this point from the origin of the space. Proximities pi are defined in terms of the projections of these points onto the space's principal component vectors.  To quantify this notion of correspondence between humans' proximity assessments and their embedding in arbitrary spaces, Guttman has defined a measure of monotonic correspondence between two variables:  fl2 =  (5.22)  EJ:\pi-Pj\-\di-dj\  '   j  This measure captures the extent to which the placement of the items within the two-dimensional space is consistent with the proximities. This formula really is as simple as it looks: It is approximately a correlation of proximities with distances, "gated" by the comparison of differences in the numerator and absolute values of differences in the denominator. The value of /X2 is always greater than simple linear correlation, and the two quantities are exactly equal when a linear relationship holds between proximity and distance.  In the FOA retrieval situation, the obvious measure of interest is the matching function Match(q, d) (cf. Section 4.3.4) score with respect to a (henceforth implicit) query q:  J2   Match(di) - Match(dj)  d. vdl. /  =    d,  |Match(di)  j)\ MATHEMATICAL FOUNDATIONS       163  The / measure provides a criterion for the retrieval function Match(-). In experimental situations the only preferences available are that Rel gt;- ReU but in natural retrieval situations, users' richer relevance feedback preference data can be used.  A particularly interesting use of this criterion is as part of error correction learning (cf. Section 7.3). If we assume that the ranking function R has certain free variables ©, that we again have a training set of documents T, and that the / criterion is differentiate with respect to 0, a gradient search procedure can be used to adjust © toward an optimal retrieval:  91 (MatchÆ)       v^ 3 / (MatchÆ) 9MatchÆ (d)  9©             £^T 9MatchÆ (d)        9©  For example, Bartell et al. [Bartell et al., 1994b; Bartell et al., 1998] consider the problem of picking a document-query similarity measure R parameterized by © to vary across a broad range of alternatives:  Matche(d) = SimÆ(d, q) = -----** 'd                   (5.25)  0 = (0i, 62)                                   (5.26)  This characterization of similarity score includes standard inner product (02 = 0, 0i can be anything), cosine (6\ = 2, 02 = 0.5), and pseudocosine (0i = 1, 02 = 1) as special cases.  Through empirical testing using the CISI experimental corpus (cf. Section 4.3.9) and its binary relevance assessments for preference relations (Rel gt;- Rel), Bartell et al. found optimal values of 0\ = 2.5, 02 = 0.3, a curious and nonstandard type of similarity measure indeed, but functionally quite similar to standard cosine. In fact, changing the matching function to this "optimal" value improves performance (as measured by average precision) almost not at all.
foa-0097	5.3.2 Information in RelFhk  Section 5.2.7 raised several important cognitive questions arising from  attempts to study "semantic" interpretation of LSI/SVD dimensions. The connection to the psychologically important analysis of MDS adds even  more. If we interpret relevance feedback information as constraints about which documents a user likes, how much can we reduce dimensionality without 164       FINDING OUT ABOUT  violating the relevance feedback constraints they are giving us? That is, how much can we reduce the representational space before we're changing somebody's order? Second, how many relevance feedback statements do we need to accurately determine a good compression? How many people have to tell us things before we have enough information to form this reduced representation? These questions also connect to ones related to learning these representations (cf. Chapter 7) and to our mechanisms, as part of the interface or as part of a special experimental system like RAVE, for efficiently collecting large volumes ofrelevance feedback (cf. Section 4.4). Most of these questions remain unanswered, but a beginning is simply counting the number of preference constraints provided by each relevance feedback labeling of a hitlist by a user.  First note that the total number of preference statements required to completely determine n elements grows very rapidly as ( 2 )gt; corresponding to the fact that preference order information is defined over pairs of pairs of evaluated documents. Against this backdrop, each relevance feedback labeling produces:  NPlus = |0|                                                  (5.27)  NDCare = |#|                                                    (5.28)  NMinus- |e|                                                  (5.29)  NRetr = NPlus + NDCare + NMinus            (5.30)  NPref = NPlus ï (NDCare + NMinus)  + (NDCare ï NMinus)                       (5.31)  When this information is used as part of error correction learning, another useful quantity is the number of documents over which the relevance feedback preference relation and the ranked list disagree:  Disagree = {(diy d/)|(df- ~lt; dj) A (Rank(di) lt; Rank(dj))}    (532)  Because this relatively small number of data points will always be dwarfed by the number of total preferences across the entire corpus, the goal becomes to constrain the application of relevance feedback data to those subsets  of the corpus possibly appropriate to a particular query. And because we intend to learn from the browsing users, we can afford to be patient:  The documents are only written once but browsing users will continue to read them and provide relevance feedback for some time.
foa-0098	MATHEMATICAL FOUNDATIONS       165  53.3 Connections between MDS and LSI  Bartell et al. [Bartell et al., 1994a] have shown that any time the basis of MDS is S, a positive semidefinite matrix of d observations of t variables (cf. Equation 5.15), the LSI decomposition together with inner product similarities provides an optimal MDS scaling. That is, under reasonable conditions MDS and LSI arrive at identical results.  This correspondence is more than a mathematical curiosity. MDS allows generalization of the scaling to any other source of documentdocument similarity information! There are many possible sources of information regarding ways to analyze the similarity of a number of documents; bibliometric information is one practical alternative already mentioned. MDS is potentially important because it allows us to go beyond the set of static features of the corpus, to consider the unending stream of relevance feedback generated by browsing users. Chapter 4 discussed users' relevance feedback assessments in detail. Recall that one of the critical features of relevance feedback information is that it is nonmetric. MDS allows us to define optimal scalings directly from nonmetric relevance feedback data.
foa-0099	5.4   Clustering"1"                                                                           Van Rijsbergen's  cr^iT-i^^i             tt          i^-                                                                    long shadow
foa-0100	5.4.1 The Cluster Hypothesis  We have been talking about a measure of association between query and document, but as discussed in Section 5.2.5, we can use X = JJT (cf. Equation 5.15) to capture similarities or differences among the documents themselves. Van Rijsbergen's cluster hypothesis suggests how useful this data might be:  Closely associated documents tend to be relevant to the same request, [van Rijsbergen, p. 45]  The cluster hypothesis suggests that if we first compute a measure of distance X among all the documents, we could cluster those documents that seem to be close to one another or seem to be about the same topic  and retrieve them all in response to a query that matched any of them. If  a query happens to match one document based on an overlap of features,  then we will claim that other documents that are similar to it should also be retrieved. 166      FINDING OUT ABOUT  As van Rijsbergen, p. 38 notes, the fact that most measures of association are monotonic with respect to one another makes it sufficient that clustering methods only respect the same rank ordering of their results; once again the nonmetric quality of critical FOA quantities is striking.  Divisive/ partitional  clustering
foa-0101	5.4.2 Clustering Algorithms  Because of its widespread application, clustering is one of the most wellstudied problems within statistics and computer science [Jain and Dubes, 1988; Griffiths et al., 1986]. It can be stated in generic terms, in terms of an arbitrary similarity measure between items to be clustered. Especially in iterative applications of clustering, the relative frequency of various clustering techniques matters. The art, therefore, in applying a clustering method in a particular application depends greatly on the particular features of items to be clustered and the number of partitions within which these items are to be clustered. W. Willett has provided an excellent survey of applications of clustering applied to various aspects of the FOA task [Willett, 1988]. More recently, Zamir has considered clusterings of document "snippets" (roughly the same as the paragraph units proposed in Section 2.3) rather than on complete documents [Zamir and Etzioni, 1998].  The most typical clustering method for application within the FOA context is single-link hierarchical clustering. This is considered an agglomerative technique, because it begins with each data point considered to be in its own cluster and then iteratively asks which two clusters should be combined.^  The most typical method for performing this task builds a minimum spanning tree (MST), iteratively merging the two documents that are closest together and then including the merged nodes. The result is a tree from the finally merged root to each of the documents as a leaf. Alternatively, the complete set of interdocument similarity measures can be progressively "filtered" by a gradually increasing threshold, which is used to define whether two documents are considered connected by an edge. We can then ask for certain properties of the emerging graph, for example, stopping when all components become connected. To keep things simple and focused on robust assumptions [van Rijsbergen, p. 59], O(n2) algorithms are assumed.  One early motivation for clustering algorithms was efficient disk access: If documents were preclustered appropriately, it would be likely MATHEMATICAL FOUNDATIONS       167  that a "page" of disk memory containing one document matching a query might also contain other elements of the same cluster. That is, an efficient physical disk allocation algorithm might try to ensure that clustered documents were written to the same block.  This example suggests how MST representations of the documents might also support efficient query/document matching algorithms. In brief, the MST can be interpreted as a "decision tree," with a query beginning at the root and comparing itself to a candidate representation (e.g., centroid) of each cluster.  The cluster hypothesis suggests that if a document that is similar to the query is relevant, then other documents similar to it are likely to also be relevant. By clustering neighboring documents, nearest neighbor documents should also be retrieved. But as Cutting et al. note, for a partitioning cluster to be useful in this application k must be very near JV [Cutting et al., 1992]. We do not want to retrieve more than a linear constant of other documents when we retrieve one.  ScatterGather is an intereseting example of how partitional clustering can be applied in a provocative way [Cutting et al., 1992; Hearst and Pedersen, 1992]. Beginning with a clustering of the entire corpus, initial queries are posed by selecting some of the clusters. Documents identified as members of these clusters form a new subcorpus; clustering is applied across these, and so on. Evaluating the utility of such a novel browsing pattern is difficult, but the ability of the ScatterGather interface (as well as a similar effort applied to the MacOS interface called Piles [Rose et al, 1993] to improve retrieval effectiveness is quite clear. A problematic feature of the ScatterGather procedure is its sensitivity to the selection of initial "buckshot" random seeds. But Cutting doesn't necessarily see this as a bug:  Indeed, the lack of determinism might be interpreted as a feature, since the user then had the option of discarding an unrevealing partition in favor of a fresh re-clustering. [Cutting et al., 1992, p. 324]
foa-0102	5.5 Probabilistic Retrieval  It should be clear by now that there are few logical grounds on which we could irrefutably prove that any document is relevant to any query. Our best hope is to retrieve relevant documents with high probability; i.e., we 168      FINDING OUT ABOUT  can only know about uncertainly. Beginning in the early 1970s, the most seminal thinkers within IR have attempted to connect fundamental concepts like "relevance" from within IR to probability theory (see [Cooper, 1971; Cooper, 1973; Bookstein and Swanson, 1974; Cooper and Maron, 1978; Cooper, 1983] and other references throughout this section). As typical search engines moved past simpler Boolean retrieval to ranked retrievals [Cooper, 1988], and more recently as representational languages like Bayesian networks have become widespread within artificial intelligence (cf. Section 5.5.7), a probabilistic foundation for FOA has become an increasingly central concern. The derivation of the basic components of the Bayesian decision models presented here follows van Rijsbergen, p. 115 quite closely; the lecture notes on probabilistic IR2 developed by Nobert Fuhr provide a similar treatment of this background.  Who stated the PRP?  The PRP hides  another assumption
foa-0103	5.5.1 Probability Ranking Principle  We begin with an important assumption called the Probability Ranking Principle (PRP):t  If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance... the overall effectiveness of the system to its user will be the best that is obtainable, [van Rijsbergen, p. 113]  This assumption reduces the problem of building an optimal IR system to one of ordering documents in order of decreasing probability of relevance. Defining our retrieval task in these terms is optimal in the sense that it minimizes the amount of expected error in retrieval performance (cf. Section 5.5.6) J  There are at least two possible interpretations of precisely what a probability of relevance, Pr(Rel), means, in terms of an underlying event space [Maron, 1977; Maron, 1982a; Maron and Kuhns, I960], The first is to imagine (again considering a particular query) that the "experiment" of showing the document to a user is repeated across multiple users. Alternatively, we can imagine that the same query/document relevance  ^ MATHEMATICAL FOUNDATIONS   169  question is put repeatedly to the same user, who sometimes replies that it is relevant and sometimes that it isn't. However we interpret Pr(Rel), we will focus on one particular query and compute Pr(Rel) conditionalized by any and all features we might associate with the document d.  Consistent with previous notation (cf. Sections 4.3.4 and 5.3.1), we will define Rank(d) to be a positive integer assigned to each document in the Retrset, in descending order of similarity with respect to a (henceforth implicit) query q, and using the matching function Match(q, d):  Match(q, d) G 5ft Rank(d) G Af+ Rank(di) lt; Rank(dj) 4=4gt; Match(q, df) gt; Match{q, d;-)     (5.33)  According to the PRP, we can hope that our Match(q, d) function accurately reflects the probability of relevance:  Match{qy d) oc Pr(Relq\di)                        (5.34)  (Henceforth, we again restrict our attention to a single query and drop the subscript q.) In fact we only need to require that it reliably ranks documents (again with respect to an implicit query q):  Rank(di) lt; Rank{dj) lt;=^ Pr{Rel\d{) gt; Pr{Rel\dj)         (5.35)  Finally, to emphasize the representation of the document into its constituent features, as well as the sensitivity of our notions of relevance on this representation (cf. [van Rijsbergen, 1992]), the remainder of this section will change notation slightly. Let x be a vector of features x\ describing the document. The PRP is then restated:  Matchiq, d) oc Pr(Rel \ x)                         (536)
foa-0104	5.5.2 Bayesian Inversion  If we had worked hard on a particular test corpus of documents to identify (always with respect to some particular query) which documents were  Rel and which were not, it would be possible to carefully study which features xt were reliably found in relevant documents and which were not. Collecting such statistics for each feature would then allow us to estimate:  Pr(x\Rel) 170       FINDING  OUT ABOUT  the probability of any particular set of features x, given that we know it is ReL (Just which statistics we collect, and how, will be discussed in more detail in Section 7.4 as part of a more general classification task.) The retrieval question requires that we ask the converse: the probability that a document with features x should be considered relevant. This inversion is accomplished via the familiar Bayes rule:  (5.37,
foa-0105	5.5.3 Odds Calculation  In addition to evidence the features provide to show that a document is Rational world      Rel, Pr(Rel |x), they may also provide evidence that it is not: Pr(Rel | x)J An odds calculation balances both probabilities as a ratio:  (5.38)  Pr(Rel\x) The Bayes rule can also be applied to this ratio:  Pr(Rel\x)      Pr(Rel)    Pr(x\Rel)  Pr(Rel\x)      Pr(Rel)    Pr(x\Rel)  Pr(ic\Rpl)  (5.39)  Odds(Rel | x) =  Pr(x\Rel)  The first term will be small; the odds of picking a relevant versus irrelevant document independent of any features of the document are not good. Still, Odds(Rel) can be expected to be a characteristic of an entire corpus or the generality of the current query but insensitive to any analysis we  might perform on a particular document.  In order to calculate the second term, we need a more refined model  of how documents are "constructed" from their features.
foa-0106	5.5.4 Binary Independence Model  Perhaps the simplest model proceeds by imagining binary, independent features; it is conventionally called (surprise!) the Binary Independence  Model (BIMJ [RobertsonandSparck Jones, 1976; van Rijsbergen, 1977]. First, the binary assumption is that all the features xt are binary. This is not a very restrictive assumption and is used only to simplify the derivation. MATHEMATICAL FOUNDATIONS       171  The much bigger assumption is that the documents' features occur independently of one another. We have discussed the problems with such an assumption before. Van Rijsbergen, p. 120 quotes J. H. Williams's expression of the paradox:  The assumption of independence of words in a document is usually made as a matter of mathematical convenience. Without the assumption, many of the subsequent mathematical relations could not be expressed. With it, many of the conclusions should be accepted with extreme caution. [Williams, 1965, emphasis in original]  The key advantage it allows is that the probability of a feature vector x becomes simply the product of the marginal probabilities of the individual features:  Pr(x\Rel) = f{Pr(x,- \Rel)                      (5.40)  i  Very convenient - and very unrealistic.^"                                                   Maybe this  Applying this decomposition to our odds calculation gives:                assumption  isn't so bad?  Odds(Rel | x) = Odds(Rel) ï fT Pr{xi^  It will be convenient to introduce the variables pi and qi to capture the probabilities that feature xx is present, given that a document is or is not relevant:  xi = l\Rel)                             (5.42)  qi = Pr(xi = l\Rd)                             (5.43)  The complementary probabilities concerning documents in which the feature is absent can also be defined easily:  1 - pi = Pr{Xi = 0 \Rel)                         (5.44)  1 - ca = Pr(xi = 0 \Rel)                         (5.45)  These definitions break the product into two portions, the first having to do with those features that are present in a particular document 172       FINDING OUT ABOUT  Rel Rel  Q                   D  FIGURE 5.6 Random Variables Underlying Binary Independence Model  and the second with those that are not:  Odds(Rel\x) = Odds(Rel)   Yl ~    Yl ^~~^          (5*46)  Recall that both queries and documents live within the same vector space defined over the features x\. The two products of Equation 5.46 (defined in terms of presence or absence of a feature in a document) can be further broken into four subcases, depending on whether the features occur in the query. We next make another "background" assumption concerning all the features x\ that are not in both the query and the document of current interest; we assume that the probability of these features being present in relevant and irrelevant documents is equal: pi = qi. In other words, for those terms we don't care about (because they don't affect this query/document comparison), we are happy to think that their occurrence is independent of their relevance.  Consider the sets D and Q shown in Figure 5.6 defined in terms of those features Xj present and absent in the document and query, respectively.* Regrouping the two products of Equation 5.46 into four products created by the two sets D and Q, the ^ terms cancel except in the intersection of the query and document (where the feature Xj is present in both) and in Q\ Dgt; the set difference of Q less D:  Odds(Rel\x) = Odds(Rel)'     Yl    ~ '    Yl    ^"^      (5*47)  xteDf\Q 4*     XteQ\D l ~   * Apologies for the unfortunate overuse of the same letter 'q' for denoting both the set Q of features contained in the query and the probability qt of the presence of a feature in Irrelevant documents, but there is no intended, direct connection between these two quantities. MATHEMATICAL FOUNDATIONS       173  In the retrieval situation we will exploit the sparseness that makes it much more efficient to keep track of where a feature does occur (x,- = 1) than all the places it does not (x\ = 0). Since the second product is defined over all the features of q except those in d, if we are careful to "premultiply" each feature in their intersection by a reciprocal, we can then safely multiply everything in the query by the same ratio:  Odds(Rel\x) = Odds(Rel)- U  ]ólL-    T\  The next section will show the utility of separating the last term, which depends on features of the document in question, from the first two, which do not, as part of an online retrieval calculation.  But first, it is worthwhile considering how we might attempt to estimate some of the required statistics [Robertson and Sparck Jones, 1976]. Fuhr [Fuhr, 1992], for example, considers the retrospective case when we have relevance feedback from a user who has evaluated each of the top N documents in an initial retrieval and has found R of these to be relevant (as well as evaluating all the N ó R remaining and found them to be irrelevant!). If a particular feature x\ is present in n of the retrieved documents with r of these relevant, then this bit of relevance feedback provides reasonable estimates for pi and q\\  P, = J                                          (5.49)
foa-0107	5.5.5 Linear Discriminators  The retrieval problem is made simpler by noting that all we really need is a simple discrimination threshold, above which the evidence for Rel is sufficient that our retrieval system elects to show a user the document as part of a hitlist. We reflect this by making the Ran A: function proportional  to the odds of relevance given the document:  Rank(x)o:      FT     ^n " *!  Assuming the features x,- are stochastically independent is very helpful, but manipulating the product of terms is still awkward. For example, 174      FINDING OUT ABOUT  we might reasonably want to regress (train) the feature variables against known Rel documents to compute weights for their relative contributions [Cooper et al., 1992; Gey, 1994]. While nonlinear regression algorithms (e.g., neural networks) do exist, regression with respect to a linear combination of features is most straightforward.  Logarithmic functions perform just this transformation (i.e., transforming products to sums) and are also guaranteed to be monotonic. Monotonicity guarantees that if a lt; b then log(a) lt; log(b)gt; and the ranking scores we use to order our hitlist will not change the relative order of any two documents on our hitlist. The two steps of  ï  first comparing two probabilities in an Odds ratio and then  ï  taking logarithms to form linear combinations rather than products  arise so frequently that the two operators are often composed and conWeighting           sidered a single LogOdds calculation.^ evidence                   por our discrimination purposes, then, LogOdds will suffice:  LogOdds(Rel\x) = LogOdds(Rel) + J^ loB  f "----~ )  LogOdds(Rel) reflects the prior probability that we are likely to find a relevant document in our corpus, independent of any features of the query/document. This could vary, for example, between very general and very specific queries, but again, it should not alter our ranking of documents pertaining to a particular query. Focusing exclusively on those factors that will be useful in discriminating one document from another, with respect to a particular query, Equation 5.52 simplifies considerably, and we come up with a Rank measure:  Rank(x) =     ]P    a + k                         (5.53)  xt GQCiD  where the weight a associated with each feature  c = log fJlzM                     lSM)  q(l     q) MATHEMATICAL FOUNDATIONS       175  is called the relevance weight (also known as retrieval status value (RSV)) of this feature, and k is the query-invariant constant:  k = LogOdds(Rel) + ]T LllL                 (5.55)  1      q  corresponding to the log of the first two document-insensitive terms of Equation 5.52.
foa-0108	5.5.6 Cost Analysis  The discussion of Section 4.3.4 suggests that, as with most real-world decisions, there is no perfect way to select relevant documents. Even if we can accomplish the PRP and order all the documents, there remains a retrieval threshold to set. If we set the threshold too high, we will not show users some documents they might wish to see, and if we set it too low we will show them too many.  But we can capture this tension by associating two costs (a.k.a. losses) with each of two possible sources of error. The first cost Crn is incurred when we retrieve an irrelevant document, the second Cnr when we don't retrieve a relevant document. To make these costs concrete, we might imagine that there is a limited resource (hitlist screen real estate, user search time), and the first cost is proportional to using up this precious resource. Similarly, the second cost might be proportional to the cost of losing a malpractice suit, when a legal case on point wasn't found but should have been!  In terms of the LogOdds ranking function of Equation 5.54, the trade-off between these two costs can be realized by another term added to the constant k of Equation 5.55:  /          . 1     Crn ó Cnr  k ' = k + log ó----------ó                        (5.56)  We can easily imagine adding a knob to a browser reflecting the trade-off between these two costs [Russel et al, 1993].
foa-0109	5.5.7 Bayesian Networks  As will be discussed in great detail in Chapter 6, there are many types of information on which we might wish to base our retrieval. Most of this 176       FINDING OUT ABOUT  FIGURE 5.7 Interaction between Parental Influences  section has concerned probabilistic models for the crucial Index relation between keywords and documents, but we may wish to model other information probabilistically as well.  Bayesian networks (also known as inference or belief networks) are a modeling language within which many probabilistic relationships can be expressed as part of a common representation and used as part of a unified inference procedure [Pearl, 1988]. A Bayesian network is a graph in which nodes correspond to propositions and links correspond to conditional probabilistic dependencies between these propositions. A directed link from node p to node q is used to model the fact that p causes qgt; although other semantics (e.g., logical implication) are sometimes also used.  When representing interactions among n propositions, we must generally consider the possible dependency of each proposition on every other. To do this completely requires an exponential number of statistics, which is impractical for most situations and certainly if we attempt to model interactions between all the documents in our corpora and their keywords. Within Bayesian networks, this full set of statistics - the joint probability distribution - is replaced by a sparse representation only among those variables directly influencing one another. Interactions among indirectly related variables are then computed by propagating inference through a graph of these direct connections.  The key integration of probabilistic information across interacting variables is accomplished by specifying how each child node depends on the set of its parents' values. A table of conditional dependency probabilities specifies, for each possible value of each parent node, the probability of each of the child variable's value; see Figure 5.7. With these condiditional relationships specified for each node* querying a Bayesian network corresponds to placing prior probabilities on some elements of the network and then asking for the probability at other nodes.  One of the most comprehensive applications of Bayesian network representations to the propositions associated with FOA is due to Croft MATHEMATICAL FOUNDATIONS       177  FIGURE 5.8 Bayesian Network Representation for FOA  with Turtle and other students [Turtle, 1990; Turtle and Croft, 1990; Turtle and Croft, 1991; Turtle and Croft, 1992; Callan et al., 1995] and is shown in Figure 5.8. This graph shows four types of nodes: documents, keywords, queries, and a single information need node.^  This graph is rooted on the document nodes. To use this representation, a single document is "instantiated," meaning that we posit that only this one document is retrieved and we ask for the probability that the information need is satisfied. Given these estimates for each document, the hitlist is formed according to the Probability Ranking Principle. Estimates for the keywords' dependencies on documents rely on the same weighting techniques discussed in Section 3.3.  Fuhr and Buckley have extended this formalism to include an additional level, which they call relevance description /(ï) [Fuhr and Buckley, 1991]. This is an arbitrary function over terms and documents X(k, d). Using this descriptive layer, rather than computing  Pr{Rel\ky d)  the probability of relevance given a particular keyword and a particular document, they propose to evaluate  Pr{Rel\Xihd))  where this is the "probability that a document will be judged relevant to an arbitrary query, given that one of the document's index terms* which  Multiple representations of the same document 178      FINDING OUT ABOUT  FIGURE 5.9 Concept-Matching Version of Bayesian Network  also occurs in the query, has the relevance description x" Fuhr argues that by separating the description function from the keyword and the document, a wider range of descriptions can be considered, and there is no longer a need to associate a probability of relevance with individual keywords or documents.  An alternative formulation proposed by Ribeiro and Muntz, following work by Wong and Yao, imagines the keyword vocabulary describing a universe of discourse [Ribeiro, 1995; Ribeiro and Muntz, 1996; Wong and Yao, 1995], as shown in Figure 5.9. Treating each keyword as a binary variable, any of the 2V possible subsets corresponds to a concept; any query and every document can be described as a concept within this space. The goal of retrieval becomes one of conceptual matching of a query against that of the documents. Either Pr (d|q) or Pr(q|d) can be used to reflect the strength of the concept matching relationship. In fact, these two quantities can generally be made equivalent with proper normalization [Wong and Yao, 1995].  Note that the Bayesian network generated from this perspective inverts the causal links from those proposed by Turtle and Croft! Riberiro and Muntz argue that their formulation is superior, because it treats queries and documents symmetrically (as we would expect for a concept matching retrieval). Further, the fact that conventional cosine-similarity matching requires normalization of the document vector over all index terms creates a dependence on terms not contained in the query. Any such dependence violates a fundamental condition for Bayesian network graphs [Pearl, 1988].  A good example of modeling within the Bayesian network formalism is to show how multiple query formulations, for example, Boolean and MATHEMATICAL FOUNDATIONS       179  CD  (information AND retrieval) OR (-satellite)  information retrieval  B  information retrieval  FIGURE 5.10 Two Examples of Query Modeling  weighted vector space query strategies, can be modeled interchangeably. Figure 5.10 shows in detail two possible ways in which query nodes might be connected to particular keywords. The jfirst shows a Boolean combination of keywords, and the second shows a network capturing dependencies between a phrase keyword and its constituent elements. Interaction among multiple queries all designed to satisfy the same information need, as discussed in terms of a query session in Section 4.2, can also be modeled.
foa-0110	6__________  Inference beyond  the Index  The Index, that critical mapping between documents and descriptive keywords, has dominated our approach to FOA in all the preceding chapters. But there is of course a larger context of available information: FOA can be accomplished by showing a user relations among keywords, by acquainting him or her with important authors, by pointing to important journals where relevant documents are often published, and so on. Retrieval of all these information resources, especially when structured in meaningful interfaces, can tell a user much more than a simple list of relevant documents.  This chapter is concerned with exploiting a variety of other clues we might have about documents (and keywords, authors, etc.), above and beyond the statistical, word-frequency information that has been at the heart of the Index relation. In all cases, these techniques identify some new source of data, represent it efficiently, and then perform some kind of inference over the representation.  AI is a subdiscipline of computer science that is centrally concerned with questions of knowledge representation and inference over those representations, especially when these algorithms arguably lead to "intelligent" behaviors. (In many ways the best characterization of the AI domain is the extensional one provided by the AIT corpus of Ph.D. dissertations.) We could expect, therefore, that there would be a great deal of cross-fertilization between AI methods and IR methods, both having  182 INFERENCE BEYOND THE INDEX      183  grown up within computer science during the same period. But for complicated reasons, until recently there has been very little interaction.^         History: AI xor  By and large, AI has defined its notions of inference in logical terms, IR? originally based on automatic theorem-proving results. Chapter 5 discussed IR's probabilistic foundations, and one immediate axis of difference between AI and IR is the distinction between primarily logical and primarily probabilistic modes of inference. Nevertheless, some in IR perceived early on the advantages offered by AFs knowledge representations [Smith, 1981] and expert systems techniques [Fox and France, 1987; McCune et al, 1985; Fidel, 1986].  Today, the fields of AI and IR align much more closely. For example, both machine learning and natural language processing have always been considered central issues within AI. The next chapter will discuss at length machine learning techniques as they have been applied to document corpora. Section 8.2 can only sketch another large intersection, corpus-based linguistics, where natural language issues and IR techniques also merge.  The advantages of applying AI knowledge representation techniques become especially obvious when additional structured attributes are associated with documents, keywords, and authors. Early on, Kochen [Kochen, 1975] considered a broad range of these forms of information as shown in Figure 6.1. Even more inclusive lists have since been proposed [Katzer et al., 1982; Hanson, 1988].  This shows the primary Index relation in the larger context of other information we might have available. What all of these additional forms of information have in common is their ability to shed new light on the semantic questions of what the documents are about. Information on the publication details of documents, for example, the journal date or page numbers of documents, can help provide a context within which individual documents can be better understood. Much of this data-about-data document is now referred to as meta-data. This additional modeling of document structure, in languages like XML and codified in standards like the Dublin Coreyl is one of the most important ways in which database and IR technologies now interact. This constructively blurs many of the database/search engine differences mentioned earlier (cf. Section 1.6). Techniques for performing fact extraction - building database relations 184      FINDING OUT ABOUT  Broader/narrower term  Publication  FIGURE 6,1 Other Information Available for FOA (after [Kochen, 1975])  from analysis of textual WWW pages [Craven et al, 1998] - suggest a broad range of new ways that structured attributes may enter into the retrieval task.  Section 6.1 discusses one of the most important ways in which documents can be understood independent of their keywords. In science, in the common law tradition, more recently in email newsgroups, and now with HTML hyperlinks, the ability to link one document to another can provide vital information about how the arguments of one document relate to those contained in another.  Section 6.3 will discuss some of the special representation techniques that have been used to organize keywords in the vocabulary. It is also INFERENCE BEYOND THE INDEX      185  possible to reason about authors of documents. Section 6.4.1 discusses Ph.D. "genealogies" in which dissertation authors are related to one another by shared advisors. Coauthorship and membership in the same research institution have also been proposed as ways to provide context on a particular author's words. In some cases, characterizations of expertise of the authors, independent of the documents themselves, are available.  The chapter concludes with several suggestions of how these varied information sources can become integrated as part of next-generation FOA tools. Section 6.5 considers several "modes of inference" by which new conclusions about keywords and documents can be reached from elementary facts. Section 6.6 suggests a few of the new interface techniques that become available as richer data streams are provided by and presented back to a user. Sections 6.7 and 6.8 look at two domains of discourse in particular - the law and science surrounding molecular genetics - as examples of how such techniques can be marshaled toward particular FOA purposes. After considering all these ways that the methods of AI can be used to help with FOA, Section 6.9 concludes by speculating about how the problem of intelligence itself might be changed as we take seriously the prospect of basing it on textual representations.
foa-0111	6.1 Citation: Interdocument Links  The bibliography at the end of a scientific publication, links from one World Wide Web page to another, references in a legal brief to prior judicial opinions, and conversational threads connecting postings to one another within a common UseNet group may seem completely unrelated. In each case, however, the author of one document has found it useful to cite another document. Perhaps it is because the author wishes to extend a prior scientific or legal argument, or perhaps it is to attack it. It may be to pull together disjointed Web pages into a single "home page" theme. Or the citation may be designed to quiet a bunch of "newby" newsgroup discussion participants by alerting them to a FAQ (frequently asked question) answer. In all cases, a new piece of text is being woven into the larger fabric of other texts, uniting one author's contribution into the legacy of many others. The value citations can offer in supporting the FOA activity has been recognized by many [Pao and 186       FINDING OUT ABOUT  FIGURE 6.2 Basic Structure of Citations  Worthen, 1989; Salton and Bergmark, 1979] and leads to methods that allow users to capture and organize their own bibliographic materials [Belew and Holland, 1988]. As more and more scientific publishing moves to open electronic repositories, efforts such as the Open Citation Project2 are leading the way toward new standards for the exchange of this important information.  At its core, a citation is a pointer from a document to a document. We typically think of a citation pointing from one document to another document of the same type: Scientific papers cite other journal articles, email messages refer to prior messages, HTML pages point to one another. But in today's quickly changing scene, it is not uncommon to find heterogeneous forms of citation, from one document type to another, as shown in Figure 6.2.  For many publications, citations are collected at the very end of a document, in its bibliography. Often the real locus of a citation, however, is someplace earlier in the document, and many compositional styles insert an expicit bibliographic citation there. We will be interested in the citation resolution of both ends of the pointer: How accurately do we know the location of the citation in the citing paper, and how precisely is its pointer into the cited paper? Does it point to a particular paragraph, page, section, or the entire document?  The application of very similar citation mechanisms has been exploited in different ways in different contexts. Table 6.1 summarizes a number of dimensions across several contexts. Here we consider citations as exemplified in two particular classes of documents, generated by science and by law, which have supported social activities for a very  2 jourrials.ecs.soton.ac.iik/x3cites/ INFERENCE BEYOND THE INDEX      187   TABLE 6.1 Comparing Citations across Document Types    Science Law Email/News Web  Purpose/ ^Standing on the Stare decisis Many! ??  programmatics shoulders of the giant      that came before     Time scale 0 (years) 0(10 years) 0 (day) 0 (day-years)  Mark-up  SheparcTs Moderation FAQ Topical home      pages!  Inference Impact Impact Public/private ??   Co-citation similarity  communication    Graph theoretic      (Cliche = college,...)  Audience    'Hardness' of science!?     long time. Section 6.1.5 will report on new analyses of citation patterns observed on the WWW.
foa-0112	6.1.1 Bibliometric Analysis of Science  Most extensive analysis of citation has been in science. Long before Newton [Newton, 1776] appreciated "standing on the shoulders of the giants that came before," scientists realized that they need one another to advance. In some cases the reference is to arguments on which a new author builds; in other cases there is disagreement about hypotheses, data, or other facts.  The field of bibliometrics has found a great deal of interesting structure in graphs created by bibliographic citation links. That is, imagine each document in a corpus is represented by a node in a graph, and a directed edge is drawn from document dj to djy just in case dj refers to d{ in its bibliography.  Figure 6.3 shows this citation structure when the references are ordered by a natural, temporal feature. In any subject area, papers can be indexed chronologically, and a dot is placed at location (/, j) just in case document d\ cites document dj. Because citations can only run backward in time, this graph is upper triangular. "*"  As with many fields, this one begins with a small number of highly cross-linked papers in the upper-left corner. Strong horizontal and vertical stripes can also be seen against a more uncorrelated background.  What field's literature is this? 188       FINDING OUT ABOUT  Citing paper  0            20  Classic  40  100        120         140        160         180  Research front  width ´ hardness (field)  Review article  200  FIGURE 6.3 Temporal Structure in Citations From [Price, 1986]. Reproduced with permission of Columbia University Press  Horizontal lines correspond to citations of classic papers: chestnuts that everyone includes in their bibliography. Vertical stripes are papers that  have much more extensive bibliographies and that stretch much farther back in time than typical; these are often referred to as review articles.  Note how these semantic determinations can be derived from patterns in the syntactic facts of citation. Other inferences are also possible. Perhaps the most common use of citation graphs is impact analysis.  In terms of the bibliographic graph, a document's importance, its effect on a field, is proportional to its in-degree: the number of citation links  pointing to a document node. Price provides motivation for this measure:  Flagrant violations there may be, but on the whole there is, whether we like it or not, a reasonably good correlation between INFERENCE BEYOND THE INDEX      189  the eminence of a scientist and his productivity of papers. It takes persistence and perseverance to be a good scientist, and these are frequently reflected in a sustained production of scholarly writing. [Price, 1986, pp. 35-36]  This suggests a simple heuristic, widely used by university deans who must quickly evaluate faculty members who are up for promotion: Important authors are those with higher impact than their peers! The Institute for Scientific Information (ISI) has made an entire industry of collating bibliographic citations and inverting them. Its Web of Science3 product now makes hypertext navigation of this valuable information straightforward. Similar arguments can be extended to identify important academic departments, universities, even countries. This mode of analysis, used to evaluate individuals (and, by extension, departments, colleges, universities, even countries) consistently makes news when data and politics cross paths [May, 1997].  Finally, as mentioned in Section 5.2.5, cocitation can be used as a basis for interdocument similarity: Two documents are similar to the extent that their bibliographies overlap. Bar-Hillel has been credited with the first suggestion of using cocitation as a similarity metric between documents [Bar-Hillel, 1957; Swanson, 1988]; Henry Small, Eugene Garfield, and others have provided some of the first empirical support for this hypothesis [Small, 1973; Garfield, 1979; Garfield, 1982; Garfield, 1986].  So-called invisible colleges [Merton, 1973], connecting cliques of self-referential colleagues who are relatively independent of the rest of science, also have been identified. Beyond fully isolated cliques, higherorder structure over sets of documents can also be analyzed. We can imagine that the documents of one discipline have much higher connectivity among themselves than they do with papers in other disciplines. A new paper, whose bibliography cites papers coming from more than one discipline, can therefore be imagined to be a new cutting-edge synthesis.  Bibliometrics has also made clear many dangers in using citation data. What we might call the norm of scholarship, the average number of citations in a document, seems to be about 10 to 20 [Price, 1986, p. 161]. Some scientific disciplines rely on much longer bibliographies  3 www.webofscienoe.oom/ 190       FINDING OUT ABOUT  than others; within a discipline, idiosyncratic author variations in bibliography length are also common.
foa-0113	6.1.2 Time Scale  Obviously bibliographic references can only go back in time, but just how far back in time a citing author reaches also tells us something. For typical journal publications, the typical time scale of references is on the order of years. A large fraction of this time has been, to date, sensitive to the production schedules of scientific journals. As the time between publication of an author's words and a reader's browsing has shrunk (e.g., in fields like physics, where the Los Alamos preprints Server4 has become a dominant mechanism of dissemination), much of the time difference is now due exclusively to delays associated with peer review, revision, etc. This delay between the time an author is finished with a document and when it reaches its public obviously provides a lower bound for citation lag, the time between a document's publication date and its reference by another paper's bibliography.  But independent of production schedules, there are wide varieties in how far back citations typically reach. Price sees a deep connection between the social processes underlying various scientific disciplines. First, he distinguishes between two gross types of citation, normal aging and the immediacy effect: "a special hyperactivity of the rather recent literature" [Price, 1986, p. 164]. He also defined Price's index to be the fraction of documents published that are cited within five years.  Price then uses the width of the interval to distinguish between "hard" and "soft" sciences, even "nonsciences," all based on the width of the research front within which most citations are typically made. He sees his Price's index as "corresponding very well with what we intuit as hard science, soft science and non-science as we descend the scale" (p. 168). Using biological metaphors, with different disciplines compared to different kinds of organisms:  ... pathological cases apart, it would seem that the [Price] index provides a good diagnostic for the extent to which a subject  4 xxx.lanl.gov/ciiipJlg/ INFERENCE BEYOND THE INDEX      191  is attempting, so to speak, to grow from the skin rather than from the body. With a low index one has a humanistic type of metabolism which the scholar has to digest all that has gone before, let it mature gently in the cellar of his wisdom, and then distill forth new words of wisdom about the same sorts of questions. In hard science the positiveness of the knowledge and its short term permanence enable one to move through the packed down past while still a student and then to emerge at the research front where interaction with one's peers is as important as the store-house of conventional wisdom. The thinner the skin of science the more orderly and crystalline the growth and the more rapid the process, (pp. 177-8)  Price also infers prescriptions from these statistics for editors of journals who are in a controlling position to influence a field:  I regard the value of this work as being not only diagnostic, but also prescriptive, so let us look in closing at what suggestions it makes for the technology of scientific information. At the personal level of the scientific writer and the editor and publisher of journals, it tells that one might well be as careful about references as we are about titles, authorships, and proper presentation of  data___For a research paper it should be exceptional to allow  an author to eschew interaction with colleagues by citing too few references, and if he cited too many, perhaps he is writing a review paper rather than a research contribution. Similarly, if you want to make the field firm and tight and hard and crystalline you have to play with your peers and keep on the ball by citing their recent work. (p. 178)
foa-0114	6.13 Legal Citation  The use of citation in legal documents is interesting for a number of reasons [Rose, 1994, section 5.4]. First, the common law tradition adjudicating legal behavior is based on arguments of stare decisis: Stand  by previous decision.* The ability to reference prior judicial opinions    U.S. courts are provides the core of many forms of documents, including the judicial    latitudmarian! 192       FINDING OUT ABOUT  opinions themselves, briefs, even legislation. It is no wonder, then, that legal prose has developed an extensive system of conventions for representing how judicial opinions relate to one another.  As with scientific papers, the fact of citation - reference by one judge  to the opinion of another - is never in doubt, and so some analyses like  impact analysis transfer quite directly from scientific corpora. (The fact  that the publishing of our courts' opinions is dependent on commercial  interests means that precedent inflation [Brenner, 1992] can occur,  Precedent            however. ^ See Section 6.7.) But when referring to prior cases, the  inflation              relevance of prior decisions often depends on the judge's interpretation  of the relation holding between the two opinions.  In fact, an entire industry exists within legal publishing to do nothing but elaborate the syntactic fact of reference to a prior ruling with an interpretation of the purpose for which the citation is made. This process is performed especially well by Shepard/Lexis. So critical are the arguments captured by these citations that the process of checking a prior ruling that a lawyer wishes to reference, to be sure that it hasn't been overruled or otherwise rendered obsolete, is known as Shepardizing a case.  Table 6.2 shows the entire range of Shepard citation labels. These are broken into two categories; the first deals with the history of a case, the second with its treatment.  To make sense of this distinction, a brief digression into the purpose of legal citation is necessary. Cases, as they proceed from lower to higher courts, have basically a binary outcome: They are won or lost. There is never ambiguity as to whether a higher court agrees with or overrules the opinion of a lower court. These unambiguous statements are captured as the history of a case.  But in a common law tradition, a much larger number of citations refer to cases and decisions in those cases by other judges. The relationship between these cases and the one before the author-judge is less clear. But as Figure 6.4 makes explicit, the two cases (citing and cited) have at least two important dimensions along which they may be similar or dissimilar. First, the set of facts associated with one case maybe very close to those in the other, or they may be very different. Second, the rules of law that are to be applied may be consistent between one judge and the other, or they may be contrary. Figure 6.4 shows these two dimensions and orders Shepard's treatment labels roughly along dimensions of this two-dimensional similarity space. INFERENCE BEYOND THE INDEX      193  TABLE 6.2 Shepard Citation Labels From [Rose, 1994]. Reproduced with permission of Lawrence Erlbaum  Code  Description  A Affirmed  CC Connected case  CF Certiorari filed  DE Denied  DM Dismissed  GR Granted  IN US cert denied   US cert dismissed   US reh denied   US reh dismissed  M Modified  MI Mandate issued  NP Not published  PD Petition denied  PG Petition granted  R Reversed  S Superseded  sc Same case  V Vacated  c Criticized  D Distinguished  E Explained  EX Examiner s decision  F Followed  H Harmonized  J Dissenting  L Limited  O Overruled  P Parallel  Comments  HISTORY OF CASE  Same case affirmed on appeal  Different case from cited case but arising out of same subject matter or intimately connected therewith  Appeal from same case dismissed  Certiorari denied by U.S. Supreme Court Certiorari dismissed by U.S. Supreme Court Rehearing denied by U.S. Supreme Court Rehearing dismissed by U.S. Supreme Court Same case modified on appeal  Same case reversed on appeal Substitution for former opinion  Same case vacated  TREATMENT OF CASE  Soundness of decision or reasoning in cited case criticized  Case at bar different either in law or fact from case cited  Statement of import of decision in cited case  Not merely a restatement of the facts  Cited as controlling  Apparent inconsistency explained and shown not to exist  Citation in dissenting opinion  Refusal to extend decision of cited case beyond precise issues  involved  Ruling in cited case expressly overruled Citing case substantially alike ("on all fours") with cited case in its  law or facts Soundness of decision or reasoning in cited case questioned  Questioned
foa-0115	6.1.4 Citations and Arguments  Expository documents are arguments: attempts to convince an audience that some set of propositions are true. Depending on the type of 194       FINDING OUT ABOUT  Rule/Issue  Followed  ^                              ./Harmonized  f         ^          /Explained      ^  Facts  Questioned Criticized ^Overruled  FIGURE 6.4 Rose's Two-Dimensional Analysis of Shepard Treatment Codes  document and the type of audience, the argument may be more or less formally structured.  Perhaps the most structured kinds of documents are mathematical papers. Mathematical arguments typically depend on, indeed are defined by, theorem proving, which connects new propositions to previously established results. Even in the case of mathematical papers, however, it is interesting to observe the crucial role natural language text continues to play in providing a context for formal theoretical results. Readers can be persuaded that assumptions are reasonable, that mathematical definitions capture intuitive relationships, etc. Citations in a mathematical paper typically point to papers containing theorems that were used as part of the current proof.  In a scientific paper, the form of the argument often has to do with the discipline. There is often data to present, methodology to describe, and so on. Many of the citations are typically gathered in a section relating the current piece of work to prior literature. This prior literature may be referenced for at least two, very different, purposes. Most typically the prior work is used much as theorems in mathematical papers: to establish a result from which the current work proceeds. The current author points to the work of a prior author as providing validation for a proposition they both believe to be true. Another potential reason for citation is the opposite. In this case, the current author is interested in debating a position put forward by the cited work.  This polarity, reflecting whether the cited work and its conclusions are positively or negatively correlated with those in the citing work, will be termed the polarity of the citation. Here legal documents provide a good example for what might be possible in scientific writing. Every INFERENCE BEYOND THE INDEX      195  law student learns proper legal syntax for codifying references as they learn to write legal briefs, the structured memos provided to judges that are often incorporated into the ultimate opinion [Harvard Law Review Association, 1995]. One feature of legal briefs is the special syntax used to refer to statutory law (e.g., 17 U.S.C 101 refers to a section of the U.S. Code) or case law (e.g., West Pub. Co. v. Mead Data Cent., Inc., 616 RSupp 1571) in a conventional fashion. A more interesting aspect of legal brief style is the explicit syntactic marking of the relation of the cited case to the argument being made in the brief: Reference to a supporting legal precedent is marked by cf., while potentially antagonistic arguments are marked with but cf.!  Note the importance of the syntactic localization of both the source and destination of a citation pointer to the two documents' semantic purposes. Localization helps to anchor the author's purpose in using the citation to the document's larger argument. Most papers make many points and attempt to establish a number of propositions. The ability to point to particular conclusions within the cited paper is therefore important. The citing paper's argument structure is simultaneously being extended, and so reference to a prior argument at a particular location in the current argument can be more persuasive than if all cites aren't localized.    ?35 4,  One reason citation has been less exploited in FOA applications than it might otherwise be is due to the expense of obtaining this data. In the context of the WWW, however, it turns out that the indices built by Web crawlers can be quite easily extended to capture information on cited pages, which can be easily inverted to maintain information on citing pages as well (cf. Section 8.1).
foa-0116	6.1.5 Analyzing WWW Adj acency  To a computer scientist, the WWW looks much like the directed graphs (digraph) we have studied in data structure classes for decades. It's big, it's dynamic, we have special questions about it, etc., but many of the same analyses we would apply to any digraph are good starting points for the WWW. A useful first step is to define the adjacency matrix A connecting all the documents dj of the WWW:  Ajj = 1    if 4 cites dj                              (6.1)  Ai} = 0     otherwise                                (6.2) 196      FINDING OUT ABOUT  For example, the Google search engine5 imagines the WWW graph as the basis of a Markov process, where the probability of jumping from one page is uniform across all of its anchor/citations.  If e is defined to be this (small) probability, the stationary distribution of this Markov process provides some insight into how likely a browsing user would be to find him or herself on a particular page. NEC's CiteSeer6 is another example of how useful citation information can be as part of a tool for searching computer science literature.  A more extensive analysis [Chakrabarti et al., 1998b; Kleinberg, 1998] has analyzed the WWW, looking especially for methods that extract authoritative pages from the vast numbers of other pages the WWW also contains. The first component of this method corresponds approximately to the notion of impact discussed in Section 6.1.1.  ... the fundamental difficulty lies in what could be called the Abundance Problem: The number of pages that could reasonably be returned as "relevant" is far too large for a human user to digest. Thus, to provide effective methods for automated search under these constraints, one does not necessarily need stronger versions of classical information retrieval notions such as relevance; rather one needs a method of providing a user, from a large set of relevant pages, a small collection of the most "authoritative" or "definitive" ones___  Unfortunately, "authority" is perhaps an even more nebulous concept than "relevance," again highly subject to human judgment___  We claim that an environment such as the WWW is explicitly annotated with precisely the type of human judgment that we need in order to formulate a notion of authority. Specifically, the creation of a link in the WWW represents a concrete indication of the following type of judgment: the creator of page p, by including a link to page q, has in some measure conferred authority on q. [Chakrabarti et al., 1998b, p. 2]  Second, they define hub documents to be ones that are particularly exhaustive in their reference to other pages. This is roughly analogous to review papers also mentioned in Section 6.1.1. To a first approximation,  * www.google.com/ h clteseer.na.nec.com/cs INFERENCE BEYOND THE INDEX      197  authoritative pages are those with high in-degree while hubs are those with high out-degree. But Kleinberg imposes an important additional constraint: A community of hubs and authority pages must be mutually self-referential. The thinking underlying Kleinberg's method is provocative:  Authoritative pages relevant to the initial query should not only have large in-degree; since they are all authorities on a common topic, there should also be considerable overlap in the sets of pages that point to them. Thus, in addition to highly authoritative pages, we expect to find what could be called hub pages: these are pages that have links to multiple relevant authoritative pages. It is these hub pages that "pull together" authorities on a common topic, and allow us to throw out unrelated pages of large in-degree. Hubs and authorities exhibit what could be called a mutually reinforcing relationship: a good hub is a page that points to many good authorities; a good authority is a page that is pointed to by many good hubs. [Kleinberg, 1998, p. 4]  Two quantities, x and y, are associated with each document, corresponding to how good an authority or hub, respectively, the document is, based on the adjacency matrix A:  Xi = Authority(ij)                                (6.3)  y{ = Hubness(df)                                 (6.4)  x= {xi)                                           (6.5)  Y = (Yi)                                              (6.6)  x and y values are iteratively updated by premultiplication with the adjacency matrix or its transpose:  xt+i = A y                             (67)  jt+l = Axf                                      (6.8)  It is also important to renormalize these vectors to unit length after each update.  Under reasonable assumptions* this update procedure is guaranteed to converge on values with x* being the principal eigenvector of ATA 198      FINDING OUT ABOUT  FIGURE 6.5 Citation-Expanded Hitlist  and y* the principal eigenvector of A AT:  x* =codATA)                                   (6.9)  y* = coi(AAT)                                (6.10)  Using this notation, the similarity of documents dx and dj can be conveniently measured in terms of cocitation as the (z, j) entry of A A T*  While we can conceive of applying these techniques to the graph corresponding to the entire WWW, the computational time and space required still makes such an analysis intractable. Kleinberg et al. typically recommend applying this adjacency analysis to a subset of pages pulled together by a query against some search engine. In their experiments, they augment this initial hitlist with documents that either point to or are pointed to from documents in the hitlist itself, as shown in Figure 6.5.  Note that the values for authority and hub on which this analysis converges correspond to the first, primary eigenvector. Using these values to identify high hub and anchor nodes gives rise to the first, primary community of documents in the graph. Another interesting application of adjacency analysis is to consider communities other than the one corresponding to the first, largest eigenvalue. A particularly striking application of this analysis concerns bimodal queries: Consider results arising from a query ABORTION, shown in Table 6.3. After first identifying  * Kleinberg also notes that the complementary measure ATA corresponds to the earlier, bibllornetric construct of bibliographic coupling [Kessler, 1963]. INFERENCE BEYOND THE INDEX      199  TABLE 6.3 Authority Nodes Elicited by ABORTION Query, Associated with Second Eigenvector  x2                                                URL                                                                    Title  .321     wwiw.caral.org/abortion.html                                               Abortion and Reproductive  Rights Internet Resources  .219     www.plannedparenthood.org/                                              Welcome to Planned Parenthood  .195     www.gynpages.com/                                                            Abortion Clinics Online  .172     www.oneworld.org/ippf/                                                      IPPF Home Page  .162     www.prochoice.org/naf/                                                       The National Abortion Federation  .161     www. lm. com/Imann/femirnst/abortion. html  -.197     www.awinc.com/partners/bc/commpass/lifenet/                Life WEB  lifenet.htm  - .169     www.worldvillage.com/wv/square/chapel/xwalk/html/      Healing after Abortion  peter.htm  - ,164     www.nebula.net/maeve/hfelink.html  - .150     members.aol.com/pladvocate/  -. 144     www.clark.net/pub/jeffd/factbot.html                                 The Right Side of the Web  - .144     www.catholic.net/H3rperUews/get/abortion.html  a community of pages extensively citing both pro-choice and pro-life documents, the second eigenvector 002 clearly separates pages associated with pro-choice organizations (with relatively high positive values) and pro-life (with negative values).*  Another important feature of this analysis is that it depends on only first-order adjacency information. That is, while it is always easy to find all of the documents pointed to by a target document simply by inspecting the document for its anchors, the in-neighborhood of a document can be identified through direct inspection of other documents. This means, for example, that search engine crawlers that look at every single document can, as part of their normal search, simultaneously collect this adjacency data.
foa-0117	6.2 Hypertext, Imtradociraient Links  Since the very first papyrus scrolls were used to capture written language, it has become natural to conceive of text as a single, linear, and continuous  Of course, the fact that one happens to be positive values and the other negative is completely circumstantial! 200      FINDING OUT ABOUT  thread, authored and then read as a single stream. But as books became longer, tables of contents were prepended, indices were appended, and the opportunities for traversing the text in fundamentally nonlinear ways became more common. We are becoming interested in other kinds of documents, many of which bring their own special structures and writing conventions, for example, the abstract paragraph, introductions, and conclusions of longer papers and the "methods" sections in scientific papers. In news reporting, spiral exposition is often used; a news item is summarized in the first paragraph, then treated in more detail in the paragraphs that fit on page 1 or "above the fold" of the newspaper, and in still more detail in the body of the article.  The attempt to analyze, support, and create such nonlinear hypertext relations among documents began long before the WWW made hypertext links commonplace; Vannevar Bush's "As We May Think"7 article (published in 1945!) [Bush, 1945] and Ted Nelson's revolutionary Xanadus project [Nelson, 1987] are often mentioned as seminal works. Mice and graphical interfaces made clicking on one textual passage, to jump to another, second nature. Hypertext conferences focusing on these new issues began in the mid-1980s and have taken on new energy as the HTTP protocols and HTML authoring languages made it easy to support many kinds of intra- and interdocument relations [Conklin, 1987; Conklin and Begman, 1988; Agosti et al., 1992; Agosti and Marchetti, 1992; Bruza and Weide, 1992; Egan et al., 1991]. In the process, many types of linkages between documents have been proposed. The following sections mention some of the most common and useful, sometimes using this FOA text (self-referentially!) as examples.
foa-0118	6.2.1 Footnotes, Hyperfootnotes, and cf.  Footnote text embellishes a primary text. It provide a more detailed treatment of terms or concepts used in the primary text. A lexical token, typically smaller in size,* creates a correspondence between the primary text and the annotation on it.  Perhaps the most direct claim of hypertext authors is that the standard linear presentation of text does violence to the more networked way  * This footnote doesn't do much to amplify, but It does create a small example:).  7 www.theatlantic.com/unbound/flashLbks /oomputer/bushf. htm  8 jefferson.village.virginia.edu/elab/lifiOl 55.html INFERENCE BEYOND THE INDEX      201  in which we naturally conceive of the concepts being discussed. At the same time, the conventional sequential flow through text mandated by printed media does a great deal to support a rhetorical argument. For this reason, this FOA text was written first as a traditional book, assuming a basically linear flow. On this linear spine, two hypertext extensions have been added (cf. Section 8.2.1).  For example, the primary purpose of the last paragraph was to move (the primary, linear course of the textbook) from a discussion of the syntactic conventions of footnoting to a consideration of communication's ultimate purposes. Because these issues are covered in more depth in Section 8.2.1, a parenthetical cf. reference to this other section was made. The cf. relation is best viewed as an opportunity (offered by the author to a reader) to compare the two passages.^                                    cf. for confer  In this FOA text I have also chosen to distinguish between standard footnotes and hyperfootnotes, which are used to capture more extended digressions. The last paragraph allows readers with access to the CD-ROM version of FOA who are interested in the Latin etymology of the cf. token to poke into that. Hyperfootnotes include a caption, providing a clue to what additional information lies on the other end of the hyperjump; standard footnotes are referenced by a simple asterisk.
foa-0119	6.2.2 Hierarchic Containment  One of the most common features of all document types is that as larger and larger portions are aggregated, an explicit hierarchic structure is used to organize the text. For example, this textbook is broken into chapters, which are broken into sections and subsections, and so on. This is basically a containment relationship, with shorter passages aggregated to form larger ones, but with short textual rubrics providing useful summaries of the themes of the smaller units. Typical atomic units of text can be a few sentences [Salton et al, 1993; Salton et al.5 1994], but typically they are paragraphs [Hearst and Plaunt, 1993],  A provocative picture of how the containment relation can be exploited is shown from Salton et aUs analysis of encyclopedia text in Figure 6.6. This shows the encyclopedia text ordered sequentially around a ring. In the top figure, individual pages of the encyclopedia are treated as separate documents, and in the bottom figure larger sections are aggregated. In both cases, links between documents are created when their similarity 202       FINDING OUT ABOUT  Award to M. Gandhi  India (12017)  lt;J\J^       Mohandas  ∞^-n£    Gandhi  v ∞ -    (9620)  X%  ^  Struggle for Indian  independence  (1919-1945)  Independent India (1945-1947)  Theme 4 Mohandas Gandhi  Indira  p3-7 Gandhi (9619)  Nehru 5( 16579)  Indira Gandhi Prime Minister  Absorption of Kashmir (1957)  (Links from 030 to 0.63 shown)  Theme 3 Indira Gandhi  £,9619.p3  Theme 1 Nehru  Theme 2  Sikhs and Punjab  (Links below 035 ignored)  16579: Nehru, Jawaharlal 12017: India  9620: Gandhi, Mohandas Karamchand  9619: Gandhi, Indira Priyadarshini  FIGURE 6.6 Containment and Document References  From [Salton et al, 1994]. Reproduced with permission of American Association for the Advancement of Science INFERENCE BEYOND THE INDEX      203  20  60  30            40            50  Sentence gap number  FIGURE 6.7 Correlation of Passages From [Hearst and Plaunt, 1993]. Reproduced with permission  70  80  (as measured according to their vector space representations; cf Section 3.4) exceeds a threshold. As Chapter 3 explained in detail, a vector space representation of document content is very sensitive to length. Salton investigated the use of inclusion to aggregate text into larger units (in the bottom of Figure 6.6) and various "global" versus "local" weighting schemes to manipulate the effects of length normalization. Note also how co-reference to similar topical themes can be seen at different scales within the containment hierarchy.  When considering constituent passages of the same document* it becomes possible to ask how much the topic of the prose changes as it goes from one passage to the next. Hearst [Hearst and Plaunt, 1993] analyzed how similarity (usingTF-IDFweightingand cosine similarity) varies across passages.9 The result is the wave of Figure 6.7. Also shown in this figure are vertical bars where a human judge has determined that a topical shift has occurred.  * www.siixis.berkeley.edu/heaj^/tb-exaniple.htEQl 204      FINDING OUT ABOUT  FR8S13-0157 AP; Groups See! SJMN:WOME1 AP: Older Athllt; FR: Committee FR: October Ad FR8S120-0046 FR: Chronic Dis AP: Survey Says FR: Consolidate SJMN:RESEAI  FIGURE 6.8 Visualizing Topical Distributions From [Hearst, 1999]. Reproduced with permission of Addison-Wesley  Having isolated individual passages as part of retrieval, it becomes important to show the user this additional level of analysis as part of the retrieval of the "documents " Figure 6.8 shows topical tiling, another visualization technique Hearst developed to highlight shifts in topical focus from one passage to the next. By increasing the resolution of document analysis, users can see which passages match particular keywords or combinations of keywords in their query.  Containment relations among documents may also be useful in supporting queries of widely varying generality (cf. Section 4.3.4). If a user issues a very broad query, it may mean he or she seeks documents with an equally broad overview or level of treatment. One reasonable hypothesis would be that broader queries should correspond to the retrieval of large sections and narrower queries to smaller sections, but only if the assumption that documents in general obey something like a uniform level of treatment. Such an assumption would not be unreasonable for the encyclopedia text considered by Salton et ai, because we expect (and encyclopedia editors attempt to ensure) that there is some document about every topic. INFERENCE BEYOND THE INDEX      205  Contensious boundaries Emergent  Exhaustive tiling A priori  Legal                                      Encyclopedia  Jsa? Document  FIGURE 6.9 Topical Document Distributions  Figure 6.9 contrasts the encyclopedia's top-down coherent topical tiling with another (hypothetical) document distribution, generated by bottom-up organic case law. People don't go to court unless they disagree, and so what judges must write about are contentious legal issues defining the borders of legal disputes. As courts work out a legal issue (for example, the intellectual property status of software or whether genes can be patented), we can expect many opinions to cover very similar topical ground.
foa-0120	6.2.3 Argument Relations  A central purpose of many documents is to persuade. Especially interesting, then, are relations among documents that reveal the arguments relating among the documents [Sitter and Maier, 1992]. The law provides several interesting examples, as shown in Figure 6.10 (taken from Rose, 1994, Figure 7.4). Rose enumerates four relations often found in legal documents, from simple reference to more elaborate logical relations such as exception.  While less explicit than within legal documents, educational materials also have implicit or explicit prerequisite relationships assumed by a text's author. Figure 6.11 shows dependencies among this book's chapters. In interdisciplinary papers, it is common to have several introductory sections, providing background for experts in one field who may not have the requisite background in another. When taken across many texts, such a prerequisite structure imposes a lattice, which can be exploited to help a reader/browsing user find just those components of the document that are most important to them. 206      FINDING OUT ABOUT  Link Type Example  contains, contained-in refers-to, referred-to-by subject-to, governs excepts, excepted-by despite, ignored-by (Chapter 1 contains ß101) "The works specified in ßß102 and 103..." "Subject to ßß107 through 118..." "Except as provided in ß810 of this title..." "Notwithstanding the provisions of ß106..."  Sec. 104: Subject matter of copyright: National origin (a) Unpublished works. ó The works specified by section 102 and 103, while unpublished, are subject to protection...  Copyright Act of 1976 Chapter 1  includes ---- refers-to  Sec. 101 Defs.  Sec. 102    '  Subj. Matter          .                 ,%  (in general)   (compilations)*^^  Sec. 104  Subj. Matter  (national origin)  FIGURE 6.10 Intradocument Relations From [Rose, 1994]. Reproduced with permission of Lawrence Erlbaum  Taking language seriously  Searching  for an education  Weighting and matching  indices V      Assesssn9  the ^            retrieval  Linear algebra probability,* statistics  Dala structures FIGURE 6 II FOAOverview INFERENCE BEYOND THE INDEX      207
foa-0121	6.2.4 Intra- versus Interdocument Relations  It is interesting to see how similar many conventions for relating textual passages within a single document are to those used to traditionally refer across documents. In part this reflects circumstantial features of a document's production - whether it was printed on newsprint, archival paper, in a journal or book, or a WWW page - that will disappear as a common technology underlies them all. At the same time, integrating new technologies into the publication process makes it clear how certain features will remain true only within a single document.  This can be described in terms of a "membrane" that functionally defines what we will continue to consider the document. The defining membrane in this case has to do with a notion of authorship. The words expressed are the creation/opinion of a single author or multiple authors, each of whom has signed his or her name. For example, when one paragraph of a legal document* refers to another passage within the same document:  Except as provided in the case of certain unauthorized signatures (ß8-205), lack of genuineness of a certificated security or an initial transaction statement is a complete defense___  we can be assured that the document's authors were thinking about both passages and commit to a particular relation between them. Interdocument citations, on the other hand, reflect a new opinion about some preexisting document.  The Talmud, arguably one of the world's first legal documents, is most commonly published as a single volume. But extensive typographical conventions (as shown in Figure 6.12) are used to isolate individual voices and relations among commentaries; a more WWW-conventional approximation is shown in Figure 6.13, taken from an HTML Version of the Talmud.10 In the common law tradition, the connection between implicit and explicit meanings becomes similarly fused. The "primary" text, secondary commentary on this, commentary on the commentary,  * ß8-202.3 of part 2 of the Uniform Commercial Code. Here the "authors" are literally  hundreds of lawyers, judges, and scholars. i∞ www. acs.ucalgary. ca/ elsegal/TalmudPage. html 208       FINDING OUT ABOUT  ?.,5"£'s'gt;-£* e faª ii'D6 jwjinrt ppn wfgt; pn i´raw . cpip KOHI    wª ^pw ^ ww eft ji nª´fgt; fa ?$t on . spit tessi  ' ' *     cowrr pSw3 p ouwi fa' ni; 7rfª pec tSt                 : psUT rt rrcsª fc´ c-^n 'w3 . '1? ,ppic r? cfti : ctfv:w  tee-waKD : set's p'p'ir; pi gt;r p? p o s*7irgt; jwrwaj'a ppn gt;;ft    cbictc .np3 piiw p6i :??"£? ?5"bª .^'ª3 paiw  J-^-'V^p ij:^P -^ ^ P7 rvc~D PiD 'P jreic v? CN1    ^ ?gt;ª ^ rssusiapw %as i'f1 oippjwpnc js^r rn  6qwgt;       ift dew Tfn pariRB ufe rª                                                       -m mm .pa-yw pes' fªta ï fo^ ï ahn  pi DJ?  KTSi   '"D  OVr CJ3      plD£2 Tl?  }QJ*TinQ^ Nip1 nVi  D*aT£l      W0R3 6^  ?jpr O´1Bª K  -, bn n tax na "i0 (jj(:s´lt;p) i'sirr  '3 fc4 ispan wen pcrpc     'ni1n- ^ Iw N^ PJ^: n CNJ     ^ pw  wp fiw ï p iwb ,ªfc  jnrii psD3  to op tjes (    nn2  i.pt ten i5c -75 cjt)    nrrn   ,^ n2r,              Ñ, y             ¶´= is c-'S ^ ;lt;;,-: ii-33  w*) j'r' jpfftjftlj TÆ2 HN XU'13 irxf n3TCT ""jE^ "J2r;    : wsijteopri "soi 5c n?3ratercel irx f?nx CJIibi Jts? nx D"ns mis1    rrsn" c*-c´  'crsisj .ire i-w-j] f                 ^                    h     p Taw .ma xnp    ∞-^ 'T"1 -"-' l^~^ wsc -^    x^ Pixn x4^ ^^  i^v  ITT1 ^i  VB3ftC' TO 'i'frs? ^ TO     121IX Tax fttp  .T,"l CK1 fl"l C512 "TZgt;1     op TTW y*~ rtt VZS T'J-^1     :rr''T6  n; -tgt; r^ P"D *p ^ ^PT1     Itsp ns: oj catre mcx *x tt **; paiy     nj -jj-t .ô;r^ nzr ï. -´;: y ´i ^-^  is V55 Dr feu p? ~rfS (sw     11 ªcp TQ"ª CTB*!2 ""Xr x^x XTH wTi2 "Q     "^rrrrr'-'rs';^ rpirr^'i ^-ir-^  wt37 ?c^ TO1 ^ arro     ^ T3r T2K TO2 Krx ^ snn -H32     f^ª .kfs -32 nr:: raw ';:! ô;^  rª´ª Mf:r au p^ ,7s            Ñ´                                             ^p irv '-' rs1?.- 'as -osi  s s-t-'  ^j             ^p      j              p                  na cx-s cc '-'¶n iw3? 3gt;ih f-TC7 ^ err jrp 'i'5h ;-ix? wrr 'ª-- S  T´^ ?TC"3TfcT rip -rsc ic enr ft Pmfa ?r? ttc ^ : (*' """ 'sS r´2' ** ^! :""3 "'-: T^gt; "; 1Jr' ^r"^ª  n jw psm to oi?a;i prim Dm ursn ^m33 te r;'*n iS 5*fn r!´^i3^ "15c frio ;ctgt; ï´ ^p"3/ i3WD "-£ -´h?Mr tlt;^"1 t^ t´5 """^ P ir? ^ CD? P1"^^ *^ ^^ ""K ^^ i(I' ^sl ´^ ^'33 w: '""S ftft "^ ^C3 ï,"* ri*: Tic^a ?f J 'op* o'plgt; 'ni ^7 i!gt;s '7O' pi 13 JTO fc' fti '¶ª;; fir1 nic pyr rr 3'ªn ï""r :h;:r fnrc; iªn', 3T htt "JSTrt )3^* f?ªª' fe S ?     ^*    ^ rt    *T             ^         -ja* '~rc ps-rr r,ln rrfc pfn iSw V-a jwrc nax pjjr c^cs'7 -^ ^  r no- rtir pi C'p"r r'% ratj pes ?cb nfe iª imf; tire; ii'sfc fmvo uª? cªr c;r nfo i? ;rc ï:; ^ '3"m ri "=; trs -s,,^ 4^ **ï iªª f1*7 i^r" '-*- '"'"! '/sh *ac 3icn f^ rb vamn nir 7=5 pifir 's* nte ter rf -ras a^frossr J" ;^J ws'-^h^ot u d PP two r´#Bsnere^t ps-Ttp Wn fch o^rs crirf- f-;irgt; io^ rr´ ;:3"7n ^ 'ir fij- sre rsrci f-T hr5 "^""^ ª¶´- ^ '^ ^t£n^t£ f^3n*"f7:D'ilt;rS *iri '!5lt; fr*!gt;":""-gt;*r:"'"tgt;r ^-"l-T-T^ w^"s r£7 *31 -": H-cr^r c^nrc"'EC ^ ps^n ´-r ,T'J^  "Tit*'  W^KSP    ^* l*(SSC^ r'VJ 0ffii   1-U* *113 ï*i*ri(ô *,*£** "*^*jSS""   S^£^ ^JJ JTT*^*X* *3 P**^ M^K       ^*'1   31*S   ô*w      *r"C */"!    *f       *-´-jª´t    *^ r*ô*"   Til1*"^ ô  *  "*ô    S1* ^ô l1^11  *"'W'"3 *m* g^T  : tc ^'tepjJisi wjt o cc Nffi *9:^ *^ ^y*K**- me **c*gt;r jep ^k ^jtx w^ tn ?*t?" )t?* i ^ *oi -^r t  FIGURE 6.12 Typography Used to Isolate Taimudic Voices INFERENCE BEYOND THE INDEX      209  Back     forward    Reload      Horn*     Search      Oofefc      knag´*      Print     Security      Stop  navigate ixK^matt  ['Sfm^Teit: Megilfeh 24a   .,,   w lt;  Megiliali 4:5-6  y ª1jo 2´a tits rc?£´*£i.ire raaJUag ´f Our  2jLS3ª´jafe,C*ª *******  myjrWa from t  Sample Text: ^fei-^i 24a Talmud (Gemara )    Sample Text: Megkbk Tosafot  }´´t to t Jut opiaieka. 6J4t  Sample Text: Me^JIIsh Rabbenu Haaian^'    ^                                    .  HilfeliotTefillali 15:1/4 U -I:  Sample Text JfegOteli 24a ^ . ' OiaiJaayyun.53   "   ^   ^ ,    ^  ft ]m any J*  Sample Text Meg^HMlt .24a' 'Arukh ,   ^ ; J .  OraltHayyim 128:34      *  FIGURE 6.13 HTML Version of the Talmud  and so on become fused into a single unified work written by multiple  authors who may have never met.^                                                          What the  Talmud says about Web sites
foa-0122	6.2.5 Beyond Unary About (k) Predicates  Our discussion of FOA has generally assumed that we are attempting to find the topic or topics that a document is about. In the language of logic, we can talk about this as applying a unary predicate About(k). That is, the determination that a document is about keyword k depends 210       FINDING OUT ABOUT  on only one argument, a single keyword. But no piece of writing deals with only a single topic, at least for more than a moment. Good writing connects topics. It discusses a relation among those topics.  Within the paragraph-size textual passages that have been our focus, this relational aspect of language has not been too troubling. But it should make us especially interested in the transitions in topical focus as we move from one paragraph to the next within the same document. Passage-level analysis like this was mentioned earlier (cf. Section 6.2.2).  As we move to larger documents, an additional source of information is citations from one to another. Assuming that we have characterized the topical area of each document independently, an analysis of the citations, placed by one author relating another document to the author's topic, is an explicit indication of the relationships between these topics.  All three levels of analysis ó within-paragraph relations among topics, paragraph-to-paragraph topical progression, and interdocument relations reflected by citations - are some of the clues we can exploit when suggesting potential browsing directions for users trying to FOA.
foa-0123	63 Keyword Structures  Most of what we have said about the Index relation (e.g., as part of the vector space model) assumes that keywords are simply a set of features.  But beyond simply providing access to the retrieval of documents, the fact that keywords are meaning-fill objects in their own right means that  we can analyze relationships among keywords directly.  Thesauri are structured representations of relations among keywords. Common relations represented in thesauri include:  ï  broader term/narrower term (BT/NT); these capture hierarchic relations, generally between a kind of semantics* sometimes a partwhole relation.  ï  related term (RT), capturing synonym or quasi-synonym relationships.  ï  use for (UF), capturing a preferred, conventional, or authoritative term over possible alternatives. INFERENCE BEYOND THE INDEX      211  One of the most extensive examples of such a representation is the MeSH (Medical Subject Headings) thesaurus, part of the National Library of Medicine's extensive PubMed system. Figure 6.14 shows the term LYMPHOMA within the MeSH thesaurus.* Example hierarchic BT/NT relations are shown as indentation. Because this thesaurus allows a single keyword to fit in multiple places in the hierarchy (e.g., treating LYMPHOMA as a kind of NEOPLASM as well as a kind of IMMHN"OLOGIC DISEASE), this browser shows the term as part of three separate paths; note that the children (narrower terms) of LYMPHOMA are repeated in each location.
foa-0124	63.1 Automatic Thesaurus Construction  Before considering WordNet, another elaborate thesaurus, it is useful to relate these manually constructed representations with automatic, statistically derived analogs. Such comparisons have been part of IR research since its beginnings [Joyce and Needham, 1958; Dennis, 1964; Soergel, 1974]. Section 5.2.5 has discussed how the same information used to cluster documents can be used with keywords. The semantics of relations based strictly on cooccurrence frequencies are not obvious [van Rijsbergen, 1977] but seem to provide evidence for the RT (related term, or synonymy relation) discussed earlier.  Using this information to construct hierarchic relations among keywords corresponds to (hierarchic) clustering techniques. Thesaurusspecific techniques generally exploit a heuristic that high-frequency keywords correspond to broad, general terms while low-frequency keywords correspond to narrow, specific ones [Srinivasan, 1992]. This heuristic can be used to organize keywords into levels of a taxonomy, with the hierarchic parent/child relation formed between those keywords with similar document distributions. relevance feedback can also be used to provide thesaurus structure[Guntzer et al, 1989]. Whether constructed manually or automatically, thesuarus structures support many new forms of navigation [Rada and Bicknell, 1989; McMath et al., 1989].  Ellipses are used to elide some parts of this display: other types of LYMPHOMA, MOH-HODGKIM and the redundant children of LYMPHOMA expanded In the second two tree locations. 212      FINDING OUT ABOUT  Netscape: MeSH Browser i  ;   Back      Forward    Reload       Home      Search      Guide      Images      Print     Security  islt;;££^jlfc^  Eater another MsSH term to frrowse  A general term for various aeo$kstic diseases of tta lymjloii tissue.  All MeSH Categorks Disease; COttgorv  Neoplasms fry Histologis Type  Il  Hot´!Te t´rm 'Lym/pliom*.' q$*ws in more tkoa one pkce in tlte I^SH tr%e. , loction 3  MeSH Tree Location 2:  Diseases Category  Hi     I L  ^liatiic Diseases  lymiplnaitig Diseases  B^in Disease  MiSH Tme LocsJtioii 3:  Diseasts Category  MmiiQlogig Diseases  lifeiitCTt Disoiiers  FIGURE 6.14 Example of the Term LYMPHOMA within the MeSH Keyword Thesaurus Reproduced with permission of National Library of Medicine
foa-0125	INFERENCE BEYOND THE INDEX      213  6.3.2 Corpus-Based Linguistics and WordNet  Linguistics has traditionally focused on the phenomena of spoken language, and since Chomsky [Chomsky, 1965; Chomsky, 1972] it has further focused on syntactic rules describing the generation and understanding of individual sentences. But as more large samples of written text have become available, corpus-based linguistics has become an increasingly active area of research. D. D. Lewis and E. D. Liddy have collected a useful bibliography and resource list on NLPfor IR,11 and R. Futrelle and X. Zhang have collected large-scale persistent object systems for Corpus Linguistics and Information Retrieval12 Stuart Shieber maintains the The Computation and Language Archive13 as part of the LANL reprint server.  The sophistication of syntactic analysis of computational linguistics provides a striking contrast to IR's typical bag-of-words approach, which aggressively ignores any ordering effects. Conversely, IR's central concerns with semantic issues of meaning and the ultimate pragmatics of using language to find relevant documents go beyond the myopic concern with isolated sentences that is typical of linguistics. The range of potential interactions between these perspectives is only beginning to be explored, but it includes the introduction of parsing techniques with IR retrieval systems [Smeaton, 1992; Strzalkowski, 1994], as^well as using statistical methods to identify phrases that are a first step from a simple bag-of-words to syntactically well-formed sentences [Lewis, 1992; Krovetz, 1993; Church and Hanks, 1989; Steier, 1994; Steier and Belew, 1994a]. Another important direction of interaction is the use of IR methods across multilingual corpora, for example, arising from the integration of the European Community [Hull and Grefenstette, 1996; Sheridan and Ballerini, 1996].  From a syntactic perspective, the only way to get issues of real meaning into language is via the lexicon: a dictionary of all words and their meanings. Our present concern, interkeyword structures, becomes an issue of lexical semantics [Cruse, 1986], and it is no surprise that linguists have also developed representational systems for interword relationships. An influential and widely used example of a keyword  11 ftp: //ciir-ffcp.cs.ijinass.eciu/pub/papers/lewis/nlir"bib93.ps.Z  12 atgl .wxistl.edu/digital library94/paper/futa^Ue.htanl  n zxxianl.gov/cmp-lg/ 214      FINDING OUT ABOUT  TABLE 6.4 WordNet VocabularyDistribution by Lexical Category  Category      Forms       Meanings (SynSets)  Nouns             57,000               48,800  Adjectives        19,500                10,000  Verbs              21,000                 8,400  Total               95,600                70,100  A wide-ranging     thesaurus is the WordNet14 system developed by George Miller^ and  psychologist         colleagues [Fellbaum, 1998].  One obvious distinction of WordNet is simply the size of its vocabulary: It contains almost 100,000 distinct word forms, divided into lexical categories as shown in Table 6.4. Central to the lexical approach to semantics is distinguishing between lexical items and the "concepts" they are meant to invoke:  "Word form" will be used here to refer to the physical utterance or inscription and "word meaning" to refer to the lexicalized concept that a form can be used to express. Then the starting point for lexical semantics can be said to be the mapping between forms and meanings. [Fellbaum, 1998, p. 4]  The relations connecting words in WordNet are similar - but not identical - to those used within thesauri. The first and most important relation is synonymy. This has a special role in WordNet, pulling multiple word forms together into a synonym set, which, by definition, all have the same meaning:  According to one definition (usually attributed to Leibniz) two expressions are synonymous if the substitution of one for the other never changes the truth value of a sentence in which the substitution is made___A weakened version of this definition  would make synonymy relative to a context: two expressions are synonymous in a linguistic context C if the substitution of one for the other in C does not alter the truth value. [Fellbaum, 1998,  p. 6]  14 www.cogBcl.priiiceton.edu/wn/ INFERENCE BEYOND THE INDEX      215  TABLE 6.5 WordNet's Top-Level Noun Categories  act, action, activity            natural object  animal, fauna                   natural phenomenon  artifact                             person, human being  attribute, property            plant, flora  body, corpus                     possession  cognition, knowledge        process  communication                quantity, amount  event, happening              relation  feeling, emotion               shape  food                                state, condition  group, collection               substance  location, place                  time motive  The BT/NT relation in standard thesauri is refined in WordNet into two types of relations, hypernymy and meronymy. The former relation plays a dominant role, allowing inheritance of various properties of parent words by their children:  Much attention has been devoted to hyponymy/hypernymy (variously called subordination/superordination, subset/superset, or the ISA relation).... A hyponym inherits all the features of the more generic concept and adds at least one feature that distinguishes it from its superordinate and from any other hyponyms of that superordinate. This convention provides the central organizing principle for the nouns in WordNet. [Fellbaum, 1998, p. 8]  This hypernymy relation connects virtually all the words into a forest of  trees rooted on a very restricted set of "unique beginners." In the case of nouns, the top-level categories are those shown in Table 6.5; those for verbs are Table 6.6.  The final category of stative verbs is used to capture the distinction  between the majority of active verbs and those (e.g., SUFFICE, BELONG, RESEMBLE) reflecting state characteristics.  WordNet also represents roughly the opposite of the synonym relation with the antonymy relation. Defining this logically proves more 216       FINDING OUT ABOUT  TABLE 6.6 WordNet's Top-Level Verb Categories  bodily care and functions creation  change emotion  cognition motion  communication perception  competition possession  consumption social interaction  contact weather  stative   difficult, and Miller is forced to simply equate it with human subjects' typical responses:  Antonymy is a lexical relation between word forms, not a semantic relation between word meanings___The strongest psy cholinguistic indication that two words are antonyms is that each is given on a word association test as the most common response to the other. For example, if people are asked for the first word they think of (other than the probe word itself) when they hear VICTORY, most will respond DEFEAT; when they hear DEFEAT most will respond VICTORY. [Fellbaum, 1998, pp. 7, 24]  The use of the antonymy relation in WordNet is particularly interesting when applied to adjectives:  The semantic organization of descriptive adjectives is entirely different from that of nouns. Nothing like the hyponymic relation that generates nominal hierarchies is available for adjectives___The semantic organization of adjectives is more naturally thought of as an abstract hyperspace of N dimensions rather than as a hierarchical tree. [Fellbaum, 1998, p. 27]  First, WordNet distinguishes the bulk of adjectives, which are called descriptive adjectives (such as BIG, INTERESTING, POSSIBLE), from relational adjectives (PRESIDENTIAL, NUCLEAR) and INFERENCE BEYOND THE INDEX      217  Similarity Antonymy  FIGURE 6.15 Bipolar Organization of Adjectives in WordNet From [Fellbaum, 1998]. Reproduced with permission of MIT Press  reference-modifying adjectives (FORMER,  ALLEGED). They then find:  All descriptive adjectives have antonyms; those lacking direct antonyms have indirect antonyms, i.e., are synonyms of adjectives that have direct antonyms. [Fellbaum, 1998, p. 28]  An example of the resulting dumbbell-shaped bipolar organization is shown in Figure 6.15.  Voorhees was one of the first to explore how WordNet data can be harnessed as part of a search engine [Voorhees, 1993].
foa-0126	6.3.3 Taxonomies  As their central role in WordNet suggests, the hierarchic BT/NT, or hypernymy, relations are especially important. Because of the constant analytic pressure of the Western intellectual tradition, topics continue  to be refined into smaller topics, which the next generation of scholars  immediately refines further. This has led to a wide range of classification taxonomies associated with various social groups of scholars and scientists. 218      FINDING OUT ABOUT  Library of Congress  LoC numbers subject headings  Assoc. Computing Machinery  David Waltz (Scientific Data Link)  Extension  ofC.JR. taxonomy  FIGURE 6.16 Nesting Taxonomies from Various Sources  Figure 6.16 shows several different sources from which indexing information might be obtained. The single most important source of subject indexing is the Library of Congress (LoC). Its indexing system is the basis of most large libraries because it covers all disciplines. For exactly the same reason, however, the indexers at the LoC have too much to do, and the resulting indices are admittedly crude. Partly as a response to the lack of adequate indexing structures, various professional groups have developed their own taxonomies to help organize the information within their particular technical specialty. For example, the Association of Computing Machinery has developed the ACM Computing Reviews Classification15 for use by its Computing Reviews publication [ACM, 1986]. This taxonomy is much more specific, and therefore more widely used by computer specialists. However, it lacks many of the advantages of the LoC system. In general, the keywords are assigned by the authors rather than by trained librarians. The system is rarely, if ever, integrated into the operations of libraries. And although the indexing structure is  5 is6-www. infbrmatik .unl-dortmund. de/CRclass. html INFERENCE BEYOND THE INDEX      219  much more refined than that of the LoC, it is still too crude for most research currently going on in any subspecialty. This has caused some practitioners in various subspecialties to develop their own extensions. For example, David Waltz was commissioned by Scientific Data-Link to extend the ACM's Computing Reviews taxonomy for the sub-specialty of artificial intelligence [Waltz, 1985]. Waltz's extension is extremely refined and helpful to AI practitioners. At the same time, it is even more ad hoc, its "sponsoring institution" has less impact, and consequently it is even less well accepted within libraries.  All three of these indexing systems are examples of top-down knowledge structures. That is, they are developed by various social institutions as prescriptive languages used to represent the consensus opinion as to how information is to be organized. Such "consensual" knowledge structures are critical if individuals are to share information. Each indexing system represents a compromise between increased scope and diminished resolution. Increased scope brings along with it broader acceptance and adherence. These advantages occur at the expense of acceptance by users actively involved in technical specialties.  The central role of hierarchic BT/NT relations in organizing keyword vocabularies should make us especially concerned with a precise semantics for this relationship. Most would agree that if B is a broader term than A, then A "is a" B. But as knowledge engineers within AI have known for a long time, the ubiquitous IS_A relation admits a number of interpretations, which can support different types of inference [Brachman, 1979; Brachman, 1983]. In general, the BT/NT relation seems to correspond most closely to the "a kind of" implication relating predicates A andB:  (Vx)B(x)-gt; A(x)                              (6.11)  Earlier generations of Internet users participated in the construction of the extensive UseNet hierarchy16 of discussion boards, on topics from ALT.SEX.FETISH.ROBOTS to COMP.SYS.MAC.OOP.TCL; see  Section 7.5.5 for an example of the use of this hierarchy in text classification tasks. These days the most widely known taxonomies are  Sfe ftp://rtfm.irsJt.edu/pub/usenet-by-liierarcby/ 220       FINDING OUT ABOUT  WWW directories such as Yahoo!,17 whose employees have constructed a hierarchy, primarily of places to spend money. One of the most exciting recent developments is the development of collaborative classification efforts such as the Open Directory Project (DMOZ)yls which involves large communities of experts, each working to make sense within his or her own area of expertise.
foa-0127	6.4 Social Relations among Authors  Another potential source of information about documents that we can use to augment statistial keywords is "cultural" information, capturing some features of the social relationships among authors in a field. Most of our discussion of documents has treated them as if they were completely dead artifacts. But documents are written by people^ authors who write from a particular perspective. When their writing is in science or the law, or any other tradition within which they participate, we can make reasonable guesses about some aspects of what it is they are trying to say. An author's education, in particular, offers clues as to how we can interpret their words. Work and writing in many fields requires extensive graduate education. Students are soon moved from common "core" curricula to more advanced material. Kuhn and others have analyzed the central role textbooks play as part of the social process of codifying a discipline [Kuhn, 1970]. As students move beyond common textbooks to specialized training, their approach to the problem often resembles that of their teachers (at least as long as they are around the teacher). By knowing something about the author's education, and especially about his or her dissertation advisor, we may have a basis for interpreting the writing. The importance of dissertations as an academic resource was recognized as early as 1940 by University Microfilms Inc. (UMI), as copies of virtually every dissertation published by many universities were microfilmed. UMI (now the Information and Learning division of Bell  Howell) makes its Dissertation Abstracts19 corpus available for WWW searching.  17 www.yahoo.com  I!t dmoz.org/about.htanl  !V wwwiib.umi.com/dissertations
foa-0128	INFERENCE BEYOND THE INDEX      221  6.4.1 AI Genealogy  The AI Genealogy project (AIG) is an attempt to collect information relating authors in artificial intelligence to one another through shared advisors. In analogy to genealogical family trees, we can treat the advisor as a parent to the advisee. Students of students become grandchildren, and so on. A subset of this data is shown in Figure 6.17.  As an example of how this additional information about authors can be exploited to understand more about the words used in documents, Steier has analyzed word-pair cooccurrence data as a means of finding potential index phrases [Steier and Belew, 1994b]. Take, for example, the phrase CASE-BASED REASONING. This phrase has a high degree of phrasal information content (using mutual information statistics to identify statistically dependent word pairs) when these statistics are collected across the entire AIT document corpus.  But a statistically significant different distribution is observed within dissertations coming from a particular set of universities: Yale, Georgia Tech, and the University of Massachusetts. Within these particular university contexts, the constituent concepts of CASE-BASED and REASONING are examined in detail and independently. Not only do these words occur together as CASE-BASED REASONING, but they often occur separately (e.g., CASE-BASED PLANNING, CASE-BASED ARGUMENT, CASE-BASED PROBLEM, SCHEMA-BASED REASONING, ANALOGICAL REASONING, COMMONSENSE REASONING). Within the limited context of these subcorpora, the keywords occur more independently and hence are considered less phraselike. As this phrase is "exported" into the general AI vocabulary, the semantic nuances are left behind, and the dominant use of the constituent words is as part of the phrase.  What is it that makes language use so different at Yale, Georgia Tech, and the University of Massachusetts? Perhaps it is merely coincidence, but our hypothesis is that it is the common lineage traced to Roger Schank! Work across these geographically distant research institutions is pulled together by an intellectual tradition captured by the AIG data.  Part of what is interesting about the AIT corpus is its demonstration within a single corpus of many of the representations discussed in this chapter. The AI genealogy captures some of the intellectual linkage due 222       FINDING OUT ABOUT  McCulloch, W. S. ó Minsky, Marvin  ó   Winston, Patrick H. 1973 Waltz, David L.  1980 Finin, Timothy W.  1988   Kass, Robert John  1989   Klein, David A. 1987 Pollack, Jordan B.  1993   Angeline, Peter  1994   Eolen, John  Mey, Jacob  1969  Schank, Roger C. ó- Kolodner, Janet L. 1989 Shinn, Hong Shik 1989  Turner, Roy Marvin  1991   Hinrichs, Thomas Ryland  1992   Redmond, Michael Albert  ó   Dejong, Gerald Francis  1987   Segre, Alberto Maria  1988   Shavlik, Jude William  1991   Towell, Geoffrey Gilmer  1988   Mooney, Raymond Joseph  1992   Ng, Hwee Tou  1989   Rajamoney, Shankar Anand  ó   Lehnert, Wendy G.  1983 Dyer, Michael G. 1987 Mueller, Eric  1987   Zernik, Uri  1988   Pazzani, Michael John  1988   Gasser, Michael  1989   Dolan, Charles Patrick 1989 Alvarado, Sergio Jose 1989 Dolan, Charles  1991   Lee, Geunbae  1991   Reeves, John Fairbanks  1991   Nenov, Valeriy Iliev  1991   Quilici, Alexander Eric  1993   Turner, Scott R.  1990   Williams, Robert Stuart 1976 Meehan, James R.  1978 Wilensky, Robert  1985   Jacobs, Paul  1986   Norvlg, Peter  1986   Arens, Yigal  1987   Cain, David Ngi 1992 Wu, Dekai  1978 CarboneE, Jaime G.  1988   Minton, Steven  1989   Lehman, JiU Fain  1991   Perlin, Mark W.  1991   Hauptmann, Alexander Georg  1992   Veloso, Manuela M. 1980 Lebowitz, Michael  1987   Hovy, Eduard Hendrik  1988   Hunter, Lawrence E.  1989   Ram, Ashwin  1989   Dehn, Natalie Jane  1990   Leake, David Browder  1992   Domeshek, Eric Andrew  1993   Edelson, Daniel Choy  THE SEMANTIC INTERPRETATION... ACQUIRING A MODEL OF THE... SEE CO-ADV SHORTLIFFE, TED ON CONNECTIONST MODELS OF... EVOLUTIONARY ALGORITHMS AND.. COMPUTATION IN RECURRENT...  A CONCEPTUAL DEPENDENCY...  A UNIFIED APPROACH TO... A SCHEMA-BASED MODEL OF... PROBLEM-SOLVING IN OPEN... LEARNING BY OBSERVING AND...  EXPLANATION-BASED LEARNING... GENERALIZING THE STRUCTURE... SYMBOLIC KNOWLEDGE AND... A GENERAL EXPLANATION-BASED... A GENERAL ABDUCTIVE SYSTEM... EXPLANATION-BASED THEORY...  IN-DEPTH UNDERSTANDING: A... DAYDREAMING AND COMPUTATION:. STRATEGIES IN LANGUAGE... LEARNING CAUSAL... SEE CO-ADV HATCH, EVELYN TENSOR MANIPULATION... UNDERSTANDING EDITORIAL... THE USE AND ACQUISITION OF... DISTRIBUTED SEMANTIC... COMPUTATIONAL MORALITY: A... PERCEPTUALLY GROUNDED... THE CORRECTION MACHINE: A... MINSTREL: A COMPUTER MODEL... LEARNING PLAN SCHEMAS FROM... THE METANOVEL: WRITING... UNDERSTANDING GOAL-BASED... A KNOWLEDGE-BASED APPROACH... A UNIFIED THEORY OF... CLUSTER: AN APPORACH TO... INTELLIGENT AGENTS AS A... AUTOMATIC INFERENCE: A... SUBJECTIVE UNDERSTANDING:... LEARNING EFFECTIVE SEARCH... ADAPTIVE PARSING:... AUTOMATING THE CONSTRUCTION... MEANING FROM STRUCTURE IN... LEARNING BY ANALOGICAL... GENERALIZATION AND MEMORY IN... GENERATING NATURAL LANUGAGE... GAINING EXPERTISE THROUGH... QUESTION-DRIVEN... COMPUTER STORY-WRITING: THE... EVALUATING EXPLANATIONS DO THE RIGHT THING: A... LEARNING FROM STORIES:...  FIGURE 6.17 A Sample of the AI Genealogical Record
foa-0129	INFERENCE BEYOND THE INDEX      223  to the Ph.D. advisor/advisee relationship. David Waltz's taxonomy of AI provides an excellent initial thesaurus over keywords [Waltz, 1985.]  6.4.2 An Empirical Foundation for a Philosophy of Science  One advantage of studying a focused corpus like the AIT is that we have an especially good chance of understanding some of these social relations. A history of AI often begins with a seminal conference that took place at Dartmouth in 1956 [McCorduck, 1985; Russell and Norvig, 1995] .t If we    Alternate treat the attendees at that meeting as "founding fathers" (in a population    histories genetic sense!), we can attempt to track their "genetic" impact on the current field of AI.                                                                                 ?S  In an attempt to capture other significant intellectual influences beyond the advisor, the AIG questionnaire asks for committee members other than the chairman. The role of committee members varies a great deal from department to department and from campus to campus. But all of these numbers can be expected to exert some intellectual influence. Asking for committee members is a step toward other nonadvisor influencers, and it is also a matter of record. Research institutions are another way to capture intellectual interactions among collaborators.  Even if and when the AI-Ph.D. family tree is completed, it will certainly not have captured all of what we mean by "artificial intelligence research." For example:  ï  The Dartmouth "founding fathers" probably provide (direct) lineage for a minority of AI Ph.D.s currently working in the field.  ï  Ph.D. theses themselves are probably some of the worst examples of research, in AI and elsewhere. By definition, we are talking about students who are doing some of their first work. They had better improve with time!  ï  Ph.D.s account for only a fraction of AI research.  Nevertheless, science is primarily concerned with accumulating knowledge, at least as much as it is about its initial discovery. A primary argument for interest in the AIG is that traditional academic 224      FINDING OUT ABOUT  relationships, as embodied by Ph.D. genealogies, form a sort of "skeleton" around which the rest of scientific knowledge coalesces. Certainly, individuals can pursue their own research program, and corporations can fund extended investigations into areas that are not represented in academia whatsoever. But it is hard to imagine extended research programs (like that "fathered" by Roger Schank, for example) that do not involve multiple "generations" of investigators; academia is almost certainly the most successful system for such propagation. Further, clear demonstrations of intellectual lineages like this promise much more concrete evidence for memes - the hypothetical analogs of biological genes in cultural systems [Dawkins, 1976] - than analyses of text alone [Best, 2000] can ever provide.  Kuhnian [Kuhn, 1970] andpost-Kuhnian [Hull, 1988;Latour, 1987] analyses highlight the importance of the social aspects of science. "Paradigms" are extremely appealing constructs, but they're also amorphous. For all of its faults, the AI-Ph.D. tree represents incontrovertible facts, just as word frequencies do.
foa-0130	6.5 Modes of Inference  If deduction is the process of proceeding from a set of rules to the implications of those rules, and if induction is the process of forming rules based on patterns across many examples, and if abduction is the process of forming hypotheses worthy of asking, FOA can also be viewed as an inference process: the process of forming questions that elicit desirable answers.
foa-0131	6.5.1 Theorem-Proving Models for Relevance  Imagine the current state of knowledge possessed by a browsing user as a set of axioms E. Then we can model their information need as a question: Can we infer that a theorem r is true or false from our current knowledge?  ?  E \= x                                       (6.12) INFERENCE BEYOND THE INDEX      225  The fact that the user has an information need can be taken as an assumption that there must be additional knowledge, contained in some documents, that together with his of her current knowledge base does allow r (or ~i r) to be proven. Relevant documents are exactly those that support such an inference:  Rel = {d\HU d |=t}                          (6.13)  Practically speaking, of course, this model is impossible. It would demand a complete and consistent logical description of the user's cognitive state. It also requires that the full set ofall possible logical facts contained in each and every document be similarly encoded. (And then, of course, there is the minor problem of searching for the minimal set of documents that satisfy this inference!)^                                                                           Late Winograd  Still, this basic conception of existing knowledge being extended by new facts and new inferences becoming possible as new facts become known is a provocative metaphor at the least. Sperber and Wilson's notion of "connected" information corresponds exactly to such new inferences (cf. Section 8.2.2).  Van Rijsbergen has talked about this strong notion of relevance as objective relevance [van Rijsbergen, p. 147]. In more recent work, he has extended this basic model to a full relevance logic, based on a four-valued semantics over 2^T}F^ [van Rijsbergen, 1986].
foa-0132	6.5.2 Spreading Activation Search  The facts that keywords can be associated with documents or with one another, that documents become associated by citations, and so on led early IR pioneers such as Doyle and Stiles to adopt simple association as a unifying relation connecting many objects of the FOA inquiry [Doyle, 1960; Doyle, 1961; Doyle, 1962; Stiles, 1961; Maron, 1982b; Giuliano, 1963; Findler, 1979]. Integrating information across semantic networks of such associative relations has been an important example of knowledge representation within AI since the memory models of Collins and Qullian [Collins and Quillian, 1972]. In its simplest form, a simple quantity known as activity is allowed to propagate through a network like that shown in Figure 6.18 from several sources. Spreading 226      FINDING OUT ABOUT  Goal  FIGURE 6.18 Spreading Activation Search  activation search is the name for a broad range of techniques that find solutions (for example, a path from Start to Goal in Figure 6.18) by controlling the propagation of activity through associative networks like this [Preece, 1981; Salton and Buckley, 1988a; Cohen and Kjeldsen, 1987].  The Adaptive Information Retrieval (AIR) system was a prototype search engine built as part of my dissertation at the University of Michigan in the mid-1980s [Belew, 1986; Belew, 1989]. This research was one of several systems applying connectionist (neural network) learning methods to the IR search engine problem [Mozer, 1988; Bein and Smolensky, 1988; Doszkocs et al.? 1990; Belew, 1987b].  Basic Representation  In AIR, each new document first causes a corresponding document node to be generated. An author node is then generated (if it doesn't already exist) for each author of the document. Two links are then created between the document and each of its keywords (one in each direction), and two more between the document and each of its authors. Weights are assigned to these links according to an inverse frequency weighting scheme: The sum of the weights on all links going out of a node is forced to be a constant; in our system that constant is one. Figure 6.19 shows the subnet corresponding to the book Parallel Models ofAssociative INFERENCE BEYOND THE INDEX      227  [Hinton and Anderson 1984]  FIGURE 6.19 Subnet Corresponding to Each Document  Memory, by G. E. Hinton and J. A. Anderson [Hinton and Anderson,  1984].  The initial network is constructed from the superposition of many such documents' representations. Most of the experiments described in this report used a network constructed from 1600 documents, forming a network of approximately 5000 nodes. This is a trivial corpus that uses relatively crude lexical analysis and keyword weighting ideas. However, AIR requires only that the initial automatic indexing assign some weighted set of tentative keywords to each document.  There is one property of the inverse weighting scheme on which AIR does depend, however. A network built using this keyword weighting scheme, together with similar constraints on the weights assigned to author links, has the satisfying property of'conserving activity. That is, if a unit of activity is put into a node and the total outgoing associativity from that node is one, the amount of activity in the system will neither increase nor diminish. This is helpful in controlling the spreading activation dynamics of the network during querying. 228      FINDING OUT ABOUT  Querying and Retrieval  Users begin a session with AIR by describing their information need, using a very simple query language. An initial query is composed of one or more clauses. Each clause can refer to one of the three types of "features" represented in AIR's network: keywords, documents, or authors, and all but the first clause can be negated. This query causes activity to be placed on nodes in AIR's network corresponding to the features named in the query This activity is allowed to propagate throughout the network, and the system's response is the set of nodes that become most active during this propagation.  The traditional result of a query is only documents. AIR also provides keywords and authors. Keywords retrieved in this manner are considered related terms that users may use to pursue their searches. Retrieved authors are considered to be closely linked to the subject of interest. There are many ways in which a user might find related terms and centrally involved authors, a valuable information product in their own right.  Figure 6.20 shows AIR's response to a typical query:  (OTERM "ASSOCIATIVE")(:AUTH "ANDERSON, J. A."))  This is the network of keywords, documents, and authors considered relevant to this query. The nodes are drawn as a tripartite graph with keywords on the top level, documents in the middle, and authors on the bottom. Associative links that helped to cause a node to become retrieved (and only those links) are also displayed. Heavier lines imply stronger associative weights. AIR uses directed links, and this directionality is represented by the concavity of the arcs; a clockwise convention is used. For example, a link from a document node (in the middle level) to a keyword node (in the top level) goes clockwise, around to the left.  Actually, this is only a picture of the final state of the system's retrieval. The network is actually drawn incrementallyy with the first nodes to become significantly active being drawn first and in the middle of the pane. As additional nodes become active at significant levels, they are drawn farther out along the three horizontal axes and the links through which they became active are drawn as well. This dynamic display has at INFERENCE BEYOND THE INDEX      229  Qu´Ogt;  )...);   {{item lt;´saoet´ttv´gt; )( ;´wcn 'ª*wtrsofl, j** gt;  FIGURE 6.20 AIR Interface  least two real advantages. First, the fact that AIR provides the first part of its retrieval almost immediately means that the user is not impatiently waiting for the retrieval to complete (typically 5 to 10 seconds in this implementation). Second, displaying the query's dynamics helps to give the user a tangible feeling of "direct manipulation" [Rose and Belew, 1990]; the user "prods" the network in a certain place and then watches as waves of activity flow outward from that place.  relevance feedback in AIR  Queries subsequent to the first are performed differently. After AIR has  retrieved the network of features, the user responds with relevance feedback, indicating which features are considered (by that user) relevant to the query and which are not. Using a mouse, the user marks features with the symbols:  ©©, Ægt; 9, and 90, indicating that the feature was Very Relevant, Relevant, Irrelevant, or Very Irrelevant, respectively. Not all features  need be commented on (cf. Section 4.1.1). 230      FINDING OUT ABOUT  The system constructs a new query directly from this feedback. First, terms from the previous query are retained. Positively marked features are added to this query, as are the negated versions of features marked negatively. Equal weight is placed on each of these features, except that features marked Very Relevant or Very Irrelevant are counted double.  From the perspective of retrieval, this relevance feedback becomes a form of browsing: Positively marked features are directions the user wants to pursue, and negatively marked features are directions that should be pruned from the search. From the perspective of learning, this relevance feedback is exactly the training signal AIR needs to modify its representations through learning. This unification of learning (i.e., changing representations) and doing (i.e., browsing) was a central component of AIR's design. It means that the collection of feedback is not an additional onerous task for the user, but rather is a natural part of the retrieval process.  Learning in AIR  Nodes marked by the user with positive or negative feedback act as sources of a signal that then propagates backwards along the weighted links. A local learning rule then modifies the weight on links directly or indirectly involved in the query process. Several learning rules were investigated; the experiments reported here used a learning rule that correlated the activity of the presynaptic nodei with the feedback signal experienced by the postsynaptic nodef.  Wjj oc Corr(ni active, rij relevant)  AIR makes a most direct correspondence between the connectionist notion of activity and the IR notion of Pr(ReI) 5.5):  The activity level of nodes at the end of the propagation phase is considered to be a prediction of the probability that this node will be judged relevant to the query presented by the user. INFERENCE BEYOND THE INDEX      231  This interpretation constrained the AIR system design in several ways (e.g., activity is a real number bounded between zero and one, query nodes are activated fully). AIR also allows negative activity, which is interpreted as the probability that a node is not relevant. The next step of the argument is to consider a link weight wab to be the conditional probability that Nodeg is relevant given that Node a is relevant. Next, this definition must be extended inductively to include indirect transitive paths that AIR uses extensively for its retrievals.  The system's interactions with users are then considered experiments. Given a query, AIR predicts which nodes will be considered relevant and the user confirms or disconfirms this prediction. These results update the system's weights (conditional probabilities) to reflect the system's updated estimates. Thus, AIR's representation results from the combination of two completely different sources of evidence: the word frequency statistics underlying its initial indexing and the opinions of its users.  A straightforward mechanism exists to incrementally introduce new documents into AIR's database. Links are established from the new document to all of its initial keywords and to its authors; new keyword and author nodes are created as necessary. The weights on these links are distributed evenly so that they sum to a constant. Because the sum of the (outgoing) weights for all nodes is to remain constant, any associative weight to the new document must come from existing link weights. A new parameter ^CONSERVATIVE*) is introduced to control the weight given these new links at the expense of existing ones. If the network is untrained by users, this parameter can be set to zero to make the effect of an incremental addition exactly the same as if the new document had been part of the initial collection. In a trained network, setting CONSERVATIVE* near unity ensures that the system's experience incorporated in existing link weights is not sacrificed to make the new connections. Also, note that the computation required to place the new document is strictly local: Only the links directly adjacent to the new document's immediate neighbors must be changed. The major observation about the inclusion of new documents, however, is that there is an immediate "place" for new documents in AIR's existing representation.  A second source of new information to the AIR system comes from users' queries. If a query contains a term unknown to AIR, this term is 232      FINDING OUT ABOUT  held in abeyance and AIR executes the query based on the remaining terms. Then, after the user has designated which of AIR's responses are relevant to this query, a new node corresponding to the new query term is created and becomes subjected to the same learning rule used for all other nodes.  Although easily incorporating new documents and new query terms is a valuable property for any IR system, from the perspective of machine learning these are examples of simple rote learning, and they are necessarily dependent on the specifics of the IR task domain. The main focus of the AIR system is the use of general-purpose connectionist learning techniques that, once the initial document network is constructed, are quite independent of the IR task.  Generalized Representation  The standard, interkeyword and interdocument associations typically evaluated as part of keyword and document clustering are part of the broader context of the associative representation used by AIR. The system extends these pairwise clustering relations and the fundamental keyword-document Index relation to include higher-order transitive relations as well. That is, if node A is associated with node B and node B is associated with node C, then node A is also considered to be associated with node C, but to a lesser extent.  Obviously, this transitive assumption is not always valid, and this may be why most IR research does not consider this extension. But AIR is an adaptive system, and one of the critical problems facing any learning system is the generation of plausible hypotheses, i.e., theories that stand a better than average chance of being correct. Transitivity is considered a default assumption, the consequences of which will be subjected to adaptations that favor appropriate transitivities and cull out inappropriate ones.  It is interesting to contrast the adaptive changes made by AIR in response to RelFhk with the document modification strategies of G. Salton and his students [Friedman et aL, 1967; Brauen, 1969] mentioned in Section 4.2.7 (and returned to in Section 7.3). The query-document matches used as the basis of their changes consider only direct, keywordto-document associations while AIR makes use of a much wider web INFERENCE BEYOND THE INDEX      233  of indirect associations. To a first approximation the changes made by AIR to direct keyword-to-document associations are not unlike those proposed by Salton and Brauen (if I'd only known!), but AIR also makes other changes, to more indirect associations.  Salton and Buckley have analyzed the spreading activation search used in some of these systems and concluded that it is inferior to more traditional retrieval methods [Salton and Buckley, 1988a; Salton and Buckley, 1988b]. They point out:  ... the relationships between terms or documents are specified by labeled links between the nodes___the effectiveness  of the procedure is crucially dependent on the availability of a representative node association map. (pp. 4, 5, emphasis added)  In a weighted, associative representation of the semantics of indexing, interdocument and interkeyword clustering links are dropped in favor of a single, homogeneous associative relation. AIR treats all three types of weighted links equally. For example, if interdocument citation data had been available, this information could naturally be included as well; again the semantics of these relations would have been dropped in favor of a simple associative weight. The contrast between the use of spreading activation search in connectionist networks with its use in semantic networks is admittedly a subtle one, but it is also critically important [Rose and Belew, 1989]. One clear difference is that semantic networks typically make logical, deterministic use of labeled links, while connectionist networks like AIR rely on weighted links for probabilistic computations.  Figure 6.21 shows how many of the features discussed here can interact as part of a single retrieval system. This figure comes from Dan Rose's SCALIR (Symbolic and Connectionist Approach to Legal Information Retrieval) system, which was built to investigate the use of both logical, "symbolic" modes of inference and probabalistic, "subsymbolic" ones. This figure shows containment relations between document elements (like those shown in more detail in Figure 6.10), topical connections between keywords, and interdocument citations, all mixed and used as part of spreading activation-based inference. 234      FINDING OUT ABOUT  FIGURE 6.21 Semantic Net of Legal Document Relations From [Rose, 1994]. Reproduced with permission of Lawrence Erlbaum
foa-0133	6.5.3 Discovering Latent Knowledge within a Corpus  Nothing is more frustrating than spending many hours on a technical problem, unless it is finding out later that someone else had previously solved the same problem! One of the motivations shared by many people working on search engine technology is the hope that we can reduce the number of times the same wheel is reinvented.  D. R. Swanson has concentrated on the scientific and medical literature and viewed it as "a potential source of new knowledge" [Swanson, 1990; Swanson and Smalheiser, 1997]. Each new medical report contains new knowledge about some particular disease or treatment. But Swanson has taken the next step: to imagine what modes of inference might be most appropriate across a network of such papers, each of which describes potential causal relations:  Each scientific article contributes to a web of logical connections that interlace the literature of science. Some of these connections are made explicit through references from one  article to another, citations that reflect, among other things, authors' perceptions of how their own work is related to that of others and how it fits into the scheme of existing knowledge. However, there may exist many implicit logical interarticle INFERENCE BEYOND THE INDEX      235  connections that are unintended and not marked by citations; such implicit links are the focus of this paper. (The word "logical" here is used informally; a "logical connection" is formed by statements that are related by any process of scientific reasoning or argument.)  Scientific articles can be seen as clustering into more or less independent sets or "literatures." Within each set, common problems are addressed, common arguments are advanced, and articles "interact" by citing one another. Distinct literatures that are essentially unrelated are in general "noninteractive" in that they do not cite or refer directly to each other, have no articles in common, and are not cited together by other articles. On the other hand, if two literatures are linked by arguments that they respectively put forward - that is, are "logically" related or connected - one would expect them to cite each other. If they do not, then the logical connections between them would be of great interest, for such connections may be unintended, unnoticed, and unknown - therefore potential sources of new knowledge....  The number of possible pairs of literatures (that might be connected) increases very nearly as the square of the number of literatures; the number of possible connections increases at even a greater rate if triple or higher combinations are taken into account rather than just pairs. From this perspective, the significance of the "information explosion" may lie not in an explosion of quantity per se, but in an incalculably greater combinatorial explosion of unnoticed logical connections, (pp. 29, 35)  Swanson's first and most well-known example of new knowledge  discovered in this fashion identified fish oil as a treatment for Raynaud's syndrome, a circulatory problem resulting in poor blood supply to the extremities. Figure 6.22 shows a second example of Swanson's method. Beginning with a syndrome like migraine headaches, Swanson searches the literature for features mentioned in the context of migraines that have also been mentioned in a second, disjoint, "mutually oblivious" literature. Working backwards from the syndrome to be explained, a query is formed against a medical corpus like Medline. The resulting set of 236      FINDING OUT ABOUT  Step 3: Seek common cause  MAGNESIUMS  6=65  /        REDUCED_BY  tress,   Type A behavior  a            REDUCES  T  vascular tone  TYPE_OF  /    Calcium blocker  INHIBITS  pneading cortial depres  pnea  SUSCEPT_TO+  Epilepsy  INHIBIT  Platetlet aggregabilit}  .                       INHIBIT  erbtonin-induced  INHIBIT  Substance P  AGGREVATED_BY  rotoglandin release  INHBIITS  inflamation  PREVENTS  hypoxia  Step 2: Analyze results  d=63  ASSOC  Stress,   Type A behaviW  SUSCEPT_TO+        \  vascular  tone   \  PREVENTED.BY v   \  Calcium blocker \   \  CAUSED_BY        \  r cortial  depre  CAUSED_BY  Epilepsy  SYMPTOM  gp.atetlet aggregability"  SYMPTOM  s sensitive to se  CAUSED_BY  Substance  P     /  AGGREVATED_BY  protoglandin released  ASSOC          /  sterile inflamation of  cerebral blood vessel  CAUSED^BY cerebral  hypoxia  Step 1: Form query  MIGRAINES  FIGURE 6.22 Swanson's Search for Latent Knowledge  (in this case 63) documents is then analyzed and clustered into related sets; it is in this clustering that Swanson's manual, intellectual effort is most obvious. The clusters are used to suggest new additional queries. Finally, the single common "cause" of magnesium is identified. Figure 6.22 also shows an attempt to classify the relationship between common phrases in the retrieved literature and the relation to the syndrome. Further, the relationship between these same phrases (or lexical variations on them!) and the common cause magnesium is also forced into structured relationships.  It is hard to imagine a more exciting prospect for the analysis of all literature. The identification of such "undiscovered public knowledge" is almost certainly possible in many other situations. The question INFERENCE BEYOND THE INDEX      237  becomes how we might algorithmically search for all of them. Note especially the liberties taken in interpreting the literature and phrases used consistently within it as they have been transformed into the structured relations of Figure 6.22. These relations are meant to suggest more formal and powerful modes of inference between causes and effects mentioned within each paper. Some of the arrowheads show suggested causal relationships; some relationships (e.g., associated_with) are neutral with respect to correlation versus causation; others (type.of) suggest hierarchic relations between classes.  Swanson is aware of these difficulties:  ... [the] form and structure of logically connected arguments are in general recognizable by scientists irrespective of their specialty, a point that may have implications for research on futuristic, more fully automated systems. However, the simple structure of the syllogistic model does not in many respects reflect the depth or range of actual problems that would be encountered if  one tried to build a database of logical connections___  The objective, moreover, is not simply to draw mechanistic logical inferences, but rather to determine whether certain plausible connections or hypotheses appear to be worth testing. Most articles harbor, either explicitly or implicitly, an enormous number of logical connections. Which connections are relevant and important can be determined only in the light of a specific context, hypothesis, problem, or question; but such contexts and questions are always changing. The degree to which one can hope to encode logical connections in any form suitable for all future uses may therefore be quite limited. [Swanson, 1990, p. 35]  Recognizing that even if fully automatic discovery of new facts is currently too hard, Gordon and Lindsey have investigated forms of  "discovery support" [Gordon and Lindsay, 1996]. Gordon and Duraais have also explored the use of LSI techniques (cf. Section 5.2.3) as part of the literature-based discovery process [Gordon and Dumais, 1998]. The formation of an SVD, eigenstructure analysis of the relationship between a query and documents (step 2), and then the analysis of these intermediate literatures and ultimate causes (step 3) is an 238       FINDING OUT ABOUT  important extension beyond the  manual investigations  originally proposed by Swanson.
foa-0134	6,6 Deep Interfaces  Probably because interface technology did not support rich graphical presentations and pointing devices until late in the development of computer science, the human-computer interface (HCI) is often an afterthought in software engineering. This bias remains today in search engine designs, which assume the indexing and match algorithms can be separated from the presentation of results [Harman, 1992b; Rose and Belew, 1990].  Sometimes interface design is approached as a data visualization task [Veerasamy and Belkin, 1996]. Korfhage's VIBE presentation [Korfhage, 1995] highlights a user's and author's perspectives on topical areas. Rather than assuming that there is one absolute preferred perspective on keywords, Korfhage considers what the words look like from the perspective of users and authors, respectively.  In terms of the vector space model, we can think of these as projections. The huge dimensional space of keywords, or even the still large reduced dimension representation, is still far more than we can visualize on a two-dimensional computer screen. We can try to impose other dimensions (e.g., color or size), but we still must pick some projected subspace of the larger data set. Norman and Schneiderman have written extensively on the design of interfaces that are deeply connected to the user's underlying task [Norman, 1988; Schneiderman, 1992]; Marchianoni has focused particularly on interfaces for the "information seeking" task [Marchianoni, 1995]. As part of the Xerox PARC group, Hearst has explored a number of visualization techniques (cf. Figure 6.8); she has also recently provided an extensive survey of interface technologies [Hearst, 1999].
foa-0135	6.6.1 Geographical Hitlists  Singular tokens are those proper nouns - people, places, events that have a unique reference in the world. They are distinguished from general terms, which refer to categories of objects in the world. INFERENCE BEYOND THE INDEX      239  Distinctions like this have been a part of linguistic analysis since the beginning [Searle, 1980], and many with a background in AI will recall Ron Brachman's CLYDE.THE JSLEPHANT example [Brachman and Levesque, 1985; Brachman and McGuinness, 1988].  In FOA the distinction initially arose out of practical considerations. The basic morphological process of folding case - Porter's stemmer and similar tools - is designed primarily to deal with what we could, in the current context, call general terms only. Conversely, proper names (e.g., family and place names) rarely observe morphological transformation names. The capitalization that often flags singular proper nouns is thrown away rather than actually helping to ease the task of automatically identifying and parsing them.  It is no wonder, then, that name-tagging techniques that deal intelligently with singular tokens were an early area of search engine development [Rau, 1988; Jacobs, 1987]. Identifying the subclass of people singulars is an especially active area. Relatively small dictionaries of the "movers and shakers" of the modern world - politicians, captains of industry, artists, etc. - can provide an especially informative and commercially valuable set of additional indexing tokens in applications such as financial news services.  Chris Needham has proposed an interesting strategy for progressively applying stronger models of representation based on various classes of singulars (personal communication). Working on a representation for Encyclopcedia Britannicas editors, the procedure Needham and his group hit upon was to  1.  first describe places in the world;  2. then describe people who live (are born, travel through, and then die) in these places; and  3.  finally to describe events involving people at locations.  Specification of one layer of terminology provided a concrete frame of reference for the next: Events involve people, who are associated with places. This suggests one argument for focusing on place-related singulars first. However, modeling even this "simplest" class of proper names quickly required even tighter focus onto physical places about which it was quite easy to give very concrete references and distinguished from political places whose names and extents can vary dramatically. As 240      FINDING OUT ABOUT  editors of the Encyclopedia Britannicagt; these designers were especially aware of how historically and culturally sensitive resolving political place names could be.  At least for physical locations, the emergence of global positioning system (GPS) technologies that allow users to know their position within a single, reconciled geographic frame has helped to drive a growing market for geographical information systems (GIS) software, and the development of worldwide authority lists of place names (e.g., The U. S. Board on Geographic Names (BGS)20 and the earlier Federal Information Processing Standards (FIPS)21 "Countries, Dependencies, Areas of Special Sovereignty, and Their Principal Administrative Divisions" list). Like people's names, place information is important.  Further, human cognition has evolved to live in a three-dimensional world. We each have deep psychological commitments to basic features of our physical space and orientation with respect to a spatial frame of reference [Shepard, 1981; Kosslyn, 1980]. In contrast to all the other abstract, disembodied dimensions along which information often barrages a user's screen, place information is special. Our experience of time is the other important experiential dimension, as demonstrated by representations like the time line. The orientation provided by such concrete frames can be critical.  Consider, for example, the query CIVIL WAR BATTLE and its conventional retrieval, as shown in Figure 6.23. We should be able to see these retrieved items in the geographical frame they naturally suggest, as shown in Figure 6.24.  Note the steps this required: First the textual hitlist was parsed for geographical tokens. Next, the map coordinates for each of these WiW entries are collected, and a convex hull (bounding polygon) for at least a majority of them is computed. Finally the map that best contains this region is identified, zoomed into, and shifted to best fit.  Within this same frame, a user also immediately knows how to draw queries, for example, restricting a search to only those battles near the East Coast or along a particular river. With modern graphical techniques, animation of these battles as a timeline slider is slid back and forth is almost trivial. But the additional power to browsing users of visualization  1164.214.2.59/gns/html/BGN.htnil 164.214.2.59/gns/html/FIPS/PIP10-4.htnil INFERENCE BEYOND THE INDEX      241  Britannica © CD civil  vax*  bat:t:I´:  Britannica CB contains at least -40  itewtf relevant to this query. . . .  ¶  Missionary Ridge,  Battle of  (699 bytes)  in the ftroarican Civil War,  battle that ended the Confederate siege of  Union troops at Chattanooga,  Tenn.  See Chattanooga,,   Battle of.  Carthage  (1,225 bytes) city,  seat of Jasper county,  southwestern Missouri,  CT.S.   It  lies along Spring River,  just east of Joplin.  Istablished in  1S42,   it was named for ancient Carthage.  During the Civil War,   it was a centre of border warfare and was destroyed by Confederate guerrillas  in  1863;   it was rebuilt in  1$66.  Carthage  is an agricultural-trade centre   .   .   .  South Muntain  (783 bytes) northernmost section of the Blue Ridge  in the Appalachian Mountains, extending southeastward for 65  miles  (105 km)   from southern Pennsylvania to northern Virginia,  U.S.  Quirauk Mountain  (2,145 feet  54 m])  in hferyland is the highest point.   It  is crossed by the Appalachian Trail  (for hikers) .  The American Civil War Battle of South Mountain   .   .   .  Rappahannock River  (980 bytes) river  in Virginia,  U.S.   It rises near Chester Gap   in the Blue Ridge Mountains east of Iront Royal and flows southeastward past Iredericksburg  (head of navigation and of tidewater)  to enter Chesapeake Bay after a course of  184 miles  (296 km) .   Its chief tributary is the Rapidan,  v.fticl\ joins  it above Fredericksburg.   A number of American Civil   .   .   .  ï  Gordon, John Bro\m  (2,667 bytes)  (b.  Feb. 6,   1S32,  tfpson county,  6a.,  U.S.ód.  Jan.  9,   1904,  Miami, Fla.), Confederate military leader and post-American Civil War politician who syrcbolized the shift from agrarian to commercial   ideals in the Reconstruction South.  ï  Spanish Civil War (7,585 bytes)  (1936-39), military revolt against the Republican government of Spain, supported by conservative elements within the country.  Wten an  initial military coup failed to win control of the entire country,   a bloody civil war ensued,  fought with great ferocity on both sides.  The Nationalists, as the rebels were called,  received aid from Fascist   .   .  FIGURE 6.23 CIVIL WAR BATTLE Query, Standard Textual Hitlist  and direct manipulation interface techniques [Rose and Belew, 1990] such as these is enormous. The important thing is that this additional  functionality does not come at the expense of a much more complex interface of commands or menu items. People already know what space means, how to interpret itgt; and how to work within it. 242       FINDING OUT ABOUT  FIGURE 6.24 CIVIL WAR BATTLE Query, Geographical Presentation
foa-0136	6.7 FOA (The Law)  The careful reader will have noticed that there have been an unusually large number of examples drawn from the legal domain. There are a number of things it can teach us about general principles of FOA [Belew,  1987a]. The common law tradition is very old and has been connected  to an organized corpus of documents in free text for a very long time. As the Doonesbury cartoon (part of Gary Trudeau's bicentennial series) in Figure 6.25 suggests, legal documents help to demonstrate just how long prose can live beyond its drafting. Hafner was one of the first to  recognize this, manually representing a wide range of attributes for a small number of documents in the legal domain [Hafner, 1978]. This work has now become a part of a larger effort within AI to model the legal reasoning process (e.g., reasoning by analogy [Ashley, 1990]). Here INFERENCE BEYOND THE INDEX      243  "Qoonesbury  G.B.TRUDEAU  f 1 t £ IN A POSITION TO ALIEN-       BLACKMANftft A7B PINCKMB.YORtiLnwm1 RiepimiiJSk  lt;mm0/    wrd. mo 80N∞!      aciOCK IN ANPNOTA    \     j' 20Ægt;   WMORNim itiHITCLOSdR If----------------  TDRB501VIN6   |           !/^#  Yn    ^^         P WHAT P0/T5AY ABOUT  ma C0MMrrM5NT tdsquauty  FOB ALUM 5LAVW TOmCOMB INEXTRICABLY PART OF  AtAJB, I AQRBe, BUT 3 Z5AS0NA3L-I 6BOR/A  AND WB CAROUNAS WBNTALON6 WITH 7HB #=% SNTAWQhl COMPROM15B DO YOU WANT TO  RISK SBCSSIOH, JdOPARDlZIHG "     evtRYTHINe THB CONVBNIKM  TIe TB CBNIK HAS ACOQMPUSHBC?*  \  FIGURE 6.25 The Long Life of Legal Documents  Doonesbury © 1987 G. B. Trudeau. Reprinted with permission of Universal Press Syndicate. All Rights Reserved  we are most concerned with what we can learn from lawyers who have FOA the law, which might generalize to other corpora and searchers.  First, the fact that judicial opinions have been written for so long and in such a particular "voice" has meant that it is also possible to consider special linguistic characteristics of this legal genre [Goodrich, 1987; Levi, 1982].  Second, notions of citation are especially well used within this corpus. Simple concepts like impact have been described [Shapiro, 1985), and theories of legal citation proposed [Merryman, 1977; Tapper, 1980; Ggden, 1993]. Obviously, manipulation of access to the legal printed record (for example, by controlling which judges' opinions are made available!) has enormous political ramifications [Brenner, 1992]. This became even more true because recent consolidation of the media 244      FINDING OUT ABOUT  industry means that one or two corporations effectively control the entire process of legal publication.  Third, the backbone of the legal process is an adversarial argument. This dialectic is often explicit, for example, as marked by the cf. and but of. citation conventions (cf. Section 6.1). The presence of such syntactic markers makes it conceivable to analyze polarization across an entire legal literature. Of course, the arguments contained in briefs and opinions have a great deal more structure than simple opposition. The analysis of legal argument structures, in conjunction with the textual foundations of common law, is perhaps the most important feature of the legal domain to FOA. On the one hand, it is possible to model individual documents and their logical features so as to reason about them [Hafher, 1978; McCarty, 1980]. Special deontic logics have been developed especially to deal with the concepts of "rights" and "obligations" that are at the heart of many legal relations [Lowe, 1985].  Statistical analyses of large document corpora may seem contrary to logical analyses of the arguments contained in each of them, and in fact this chasm runs very deep. Not only does it suggest different technology bases from the arsenal of (roughly inductive versus deductive) AI techniques, but it also reflects a tension within the law itself. Rose [Rose, 1994, p. 95] refers to a spectrum of legal philosophies ranging from "formalism" to "realism " On the one hand, many legal documents certainly seem to function logically, with careful definitions and reasoning that Langdell [Langdell, 1887] has idealized as "mechanical jurisprudence." At the same time, analyses such as the Critical Legal Studies [Unger, 1983] have helped to demonstrate that the law is just another social process.  It is exactly this dual nature of the law, and hence of legal texts, that makes it especially interesting as an example of FOA [Nerhot, 1991]. Individual applications include litigation support systems, which allow lawyers to search through the truckloads of documents involved in extended trials. On a much larger scale, systems like West Group5s WestLaw22 and Reed Elsevier's LEXIS23 systems provide access to the bulk of statutory and case law to all practicing lawyers [Cohen, 1991; Bing, 1987].  2* www. westiaw. ogxxl / 23 www.lexis.com INFERENCE BEYOND THE INDEX      245
foa-0137	6.8 FOA (Evolution)  For some time the study of evolution has demanded an especially interdisciplinary approach, so it is no wonder that profound difficulties in communication arise as scientists trained in paradigms as varied as biology, psychology, and computer science attempt to communicate with one another [Belew and Mitchell, 1996]. Now, of course, theories of evolution are increasingly informed by huge volumes of concrete data, generated by the Human Genome Project and related efforts. Serendipitously, the field of molecular biology is also one of the first (but quite certainly not the last) disciplines to undergo a qualitative change because of the WWW. The nearly simultaneous growth of the WWW and genomic databases has meant that computational biology as a science has grown up with a very advanced notion of publication. Beyond formal publication channels, even beyond informal email and discussion groups, the genomic databases at the heart of molecular biology today may point to forms of communication among scientists that are arguably, like image-based WWW traffic, post-verbal.  The flood of biological sequence data - nucleic acid, proteins, and now gene expression networks and metabolic pathways - into sequence databases, with the related flood of molecular biology literature, represents an unprecedented opportunity to investigate how concepts learned automatically from various data sets relate to the words and phrases used by scientists to describe them. Learning this linkage - between molecular biology concepts and the genomic data relating to them - can be described as annotating the data. It is now possible to learn many of these correspondences automatically, guided by the relevance feedback of practicing scientists, as a natural by-product of their browsing through genome data and publications related to them. relevance feedback provides a key additional piece of information to learning algorithms, beyond the statistical correlations that may exist within the genome data or textual corpora treated independently: It captures the fact that a scientist who understands both the sequence data and the journal articles deeply does (or does not) believe that a particular sequence and particular keyword/concept share a common referent. Sequences are posted, annotations are often automatically constructed based on homologous relations to other sequences found in the databases. A different variety of "sequence search engines," specially developed to look for similarities among sequences rather than 246      FINDING OUT ABOUT  Query  Retrieval  relevance feedback  Documents  FIGURE 6.26 The Annotation Relation between Text and Sequence Data  among documents, has become the basis for retrievals. These retrievals can - and often do - connect the work of one scientist to that of another without a single verbal expression passing between them.  Figure 6.26 sketches the basic relations. On the bottom are the most fundamental classes of molecular data, namely, gene and protein sequences. On the top is a set of scientific documents, such as those found in MEdigital libraryINE. The primary relation connecting the raw genetic data and textual corpora are annotation links that scientists have (manually) established between articles and sequences that are both significant and useful. They are significant because they help to establish the construction of the genome as a piece of the scientific enterprise, linking it to the traditions of academic publication. They are also useful to many scientists who, for example, are interested in a particular gene or protein and want to find out all that others might know about it. But annotation is not done consistently by all participating scientists, nor has a precise semantics for what exactly an annotation should mean been established. The Entrez24 interface to MEdigital libraryINE makes it convenient for a user with  24 www.ncbi.nlm.gov/ Entrez INFERENCE BEYOND THE INDEX      247  a particular sequence in mind to find its corresponding publication, and vice versa. Together with the MESH thesaurus of medical terms (cf. Section 6.3), these features make the National Library of Medicine's resource one of the most advanced on the WWW.  In addition to expediting the searches of scientists and doctors, the identification of significant patterns in one modality (i.e., in text or in sequence data) can be used to suggest hypotheses in the other (similar to suggestions made by Swanson (cf. Section 6.5.3)). Also shown in Figure 6.26 are Sim arcs relating "similar" data. In the case of genetic or protein sequence data, these similarity measures are typically based on a notion of "edit distance" generated by string-matching tools such as BLAST and FastA, but the investigation of new methods for this problem is one of the most active areas within machine learning (cf. [Glasgow et al, 1996]). The investigation of interdocument similarities has been an important problem within the field of information retrieval (IR) for many decades. Most document similarity measures are based on correlations between "keywords" contained by pairs of documents, but other methods (e.g., those based on a bibliometric analysis of shared entries in the documents' bibliographies) have also received considerable attention.
foa-0138	6.9 Text-Based Intelligence  Knowledge representation has always been a central issue for AI, and as a subdiscipline within computer science its primary contribution is probably the beginnings of a computational theory of knowledge. Although it is still too early to speak of such a theory, some key aspects of good knowledge representation are becoming clear [Belew and Forrest, 1988].  The text captured in document corpora was not entered with the intention of being part of a knowledge base. These are documents written by someone as part of a natural communication process, and any search engine technology simply gives this document added life. Alternatively, we can say that the document was intended to become part of a "knowledge base," but one that predates (at least the AI) use of that term: People publish their documents with the explicit hope that their ideas can become part of our collective wisdom and be used by others. 248      FINDING OUT ABOUT  Note the ease with which an author-as-knowledge engineer can express his or her knowledge. Hypertext knowledge bases are accessible to every writer. In this view, hypertext solves the key AI problem of the knowledge acquisition bottleneck, providing a knowledge representation language with the ease, flexibility, and expressiveness of natural language - by actually using natural language! The cost paid is the weakness of the inferences that can be made from a textual foundation: Contrast the strong theorem-proving notions of inference of Section 6.5.1 with the many confounded associations that arise in Swanson's analysis of latent knowledge in Section 6.5.3.  Grounding Symbols in Texts  According to Hamad's grounding hypothesis, if computers are ever to understand natural language as fully as humans, they must have an equally vast corpus of experience from which to draw [Harnad, 1987]. We propose that the huge volumes of natural language text managed by hypertext systems provide exactly the corpus of "experience" needed for such understanding. Each word in every document in a hypertext system constitutes a separate experiential "data point" about what that word means. The exciting prospect of using search engines as a basis for natural language-understanding systems is that their understanding of words, and concepts built from these words, will reflect the richness of this huge base of textual "experience." There are, of course, differences between the text-based "experience" and first-person, human experience, and these imply fundamental limits on language understanding derived from this source.  In this view, the computer's experience of the world is secondhand, from documents written by people about the world and subsequently through users' queries of the system. The "trick" is to learn what words mean by interacting with users who already know what the words mean, with the documents of the textual corpus forming the common referential base of experience.  The hypertext itself is in fact only the first source of information, viz., how authors use and juxtapose words. The second, ongoing source of experience is the subsequent interactions with users, a new population of people who use these same words and then react positively or negatively to the system's interpretation of them. Both the original authors and the INFERENCE BEYOND THE INDEX      249  browsing users function as the text-based intelligent system's "eyes" into the real world and how it looks to humans. That insight is something no video camera will ever give any robot.
foa-0140	7.1 Background  Most of the techniques described in the last chapter built on representational and inference methods originally developed within AI in the 1970s and '80s. Today, these methods are sometimes called Good OldFashioned AI (GOFAI), to distinguish them from more recent advances. There are many ways to characterize this change (see Russell and Norvig's text for an alternative account [Russell and Norvig, 1995, p. 827] and cf. Sections 6.9 and 7.8), but the most important is: AI is now centrally concerned with learning the representations it uses rather than assuming that some smart knowledge engineer has entered it manually.  To be concrete, imagine that you are to act as a librarian with respect to your own email. We have assumed at several points that you are collecting vast amounts of email, but perhaps are only now starting to think how it should be classified for subsequent retrieval. If we hire a librarian, we can reasonably expect him or her to bring certain useful skills to this new job and to continue to learn ways of doing it better. As their boss we must provide regular feedback that points out both good and bad aspects of their work If this person was having their first annual review and they were no better at finding useful information than the day they were hired, we would have reason for concern.  252 ADAPTIVE INFORMATION RETRIEVAL       253  Ancillary structure  M;  Ency. Britannica WESTLAW MED LINE  index  Manual encoding effort  FIGURE 7.1 Learning Conceptual Structures  The preceding chapters have surveyed a number of techniques for supporting the FOA task, but their utility is immediately apparent and we do not expect it to improve. This chapter is concerned with adaptive techniques: those that improve their performance over time, in response to feedback they receive on prior performance. We can idealize our goal for the learning system in terms of a person - a clever, resourceful, adaptive librarian.  Figure 7.1 gives an overview of how machine learning fits into the space of existing IR techniques. The horizontal axis is meant to indicate the amount of manual effort expended to improve the corpus. These activities may include constructing a controlled vocabulary, forming good lexical index terms, including phrases, building thesauri relating the keywords to one another, etc. The vertical axis attempts to capture something like ease of use for FOA. Such ease-of-use metrics are notoriously difficult to quantify, but some indicators may include search time to find a known item.  Prior to the widespread application of search engine technologies, brought on by efforts like B. Kahle's Wide Area Information System (WAIS) and G. Salton's SMART system, to search text meant to grep across textual fields. Because grep and related search methods rely on regular expressions for queries, and because regular expressions can't 254      FINDING OUT ABOUT  be conveniently composed with Boolean operators, early search systems provided only these search techniques.  But with the introduction of search engine technologies, the goal became to build an index, much like a librarian might construct for a collection of books or documents. These indices have been at the core of our FOA discussion.  Figure 7.1 extends this progression further. While it is rare to have any textual corpus receive manual attention from a librarian or editor, and so there are very few manual indices, a very few corpora have received even more extensive editorial enhancement. The Encyclopedia Britannicagt; Westlaw, and Medline are examples of just how much the FOA activity can be supported by rich representations.  This becomes the goal for our machine learning techniques. They will turn out to form a natural extension of the statistical techniques underlying automatic index construction. Peter Turney maintains a useful bibliography of Machine Learning Applied to Information Retrieval1 references generally, and of Text Classification Resources2 in particular.  Finally, it is always a mistake to view the relationship between algorithmic (artificially intelligent) methods and the natural, human intelligent behaviors they mimic as an opposition. The most constructive systems we can build are ones that leverage editorial capabilities with new computational tools. The editor's workbench is a good metaphor for such designs.
foa-0141	7.1.1 Training against Manual Indices  We will be especially concerned with corpora that have benefited from  extensive manual indexing. For example, the articles in the Encyclopcedia Britannica have benefited from man-centuries of effort that have been applied to organize these textual passages into coherent indices, thesauri, and taxonomies. This manual attention provides two advantages in the context of machine learning.  First, the manual classification of documents to categories can be used as training data in the context of supervised learning (see  1  www. lit. nrc. ca/bibliogpaphies /ml-applied-toir. html  2  www.iit.nrc.ca/lLpiiblic/ClassiflcatIoii/resouroes.html ADAPTIVE INFORMATION RETRIEVAL       255  Section 7.4). Second, manually constructed representations provide a kind of upper bound on what we can hope our automatic learning techniques can build. Ultimately, however, we can expect that the most successful applications will not oppose manual, editorial enhancement with automatic induction but rather will integrate learning into the editorial process. Machine learning can already do much of the job that has traditionally been done by human editors; and yet, many aspects of the editorial function will remain beyond our learning techniques for the foreseeable future. Harnessing machine learning as part of an editor's workbench promises to leverage this scarce resource most effectively.  Of course, corpora that have benefited from such careful manual attention are few and far between. Much more typical is the textual corpus without any manual indexing whatsoever. The third advantage, then, of those special corpora that do have attending editorial enhancements is that if our learning techniques can generate analogous structures on these special collections, we can realistically expect the same techniques to generate useful structure on other collections as well.
foa-0142	7.1.2 Alternative Tasks for Learning  One obvious task we might ask of a learning algorithm is that it learn the Rank() function. Following the Probability Ranking Principle (Section 5.5.1), we might attempt to learn the probability that a document is relevant, given a set of features it contains. Recall from Section 4.2.1 how RelFhk information can be used to modify a query vector so as to move it closer to those documents a user has marked as relevant. As discussed in Section 4.2.2, the same technique can be used to move documents toward the query! The radically different consequence of this change is that while query modification is only useful once to the user benefiting from it, document modification changes the document's representation for all subsequent users' queries.  We will also be concerned with another task. Rather than assigning a single real number to each document (Pr (Relevance) y for example), we shall attempt (using the same set of document features) to classify a document into one of a small number of potential categories. Most simply, we may be interested in binary classification of documents into one of two categories. An obvious use of binary classification would be to classify into Relevant and Nonreievant classes. More complex classifications 256      FINDING OUT ABOUT  into one of C different classes are also possible. For example, imagine that you would like to have your email client automatically sort your incoming email into various mail folders you have used historically. Having decided how many classes to use, we must also determine whether our assignment to these classes is binary or if we should provide the probability that it belongs to a specific class. Classification techniques are discussed in Section 7.4.
foa-0143	7.1.3 Sources of Feedback  Two distinct classes of machine learning techniques can be applied to the FOA problem. These can be distinguished on the basis of the type of training feedback given to the learning system. The most powerful and best-understood are supervised learning methods, where every training instance given to the learning system comes with an explicit label as to what the learning system should do. Using an email example, suppose we want talk announcements to consistently go into one folder, mail from our family to go in another, and spam to be deleted. In terms of supervised learning, this regime requires that we first provide a training set (cf. Section 7.4). In our case, the training set would consist of email messages and the C mail categories we have used to classify them in the past. After training on this data set, we hope that our classifier generalizes to new, previously unseen messages and classifies them correctly as well.  A second class of machine learning techniques makes weaker assumptions concerning the availability of training feedback. Reinforcement learning assumes only that a positive/negative signal tells the learning system when it is doing a good/bad job. In the FOA process, for example, RelFhk generates a reinforcement signal (saying whether it was a good or bad thing that a document was retrieved).  Note that relevance feedback does not count as supervised learning; in general, we do not know all of the documents that should have been retrieved with respect to a particular query. Supervised training provides more information in the sense that every aspect of the learaer*s action (retrieval) can be contrasted with corresponding features of the correct action. Reinforcement information, on the other hand* aggregates all of these features into a single measure of performance.  The difference between these two kinds of learning is especially stark in the FOA context. To provide reinforcement information, users need ADAPTIVE INFORMATION RETRIEVAL       257  FIGURE 7.2 Browsing across Queries in Same Session  Yes!  Later  ó Stupid #@$$##! machines  only react to each document and say whether they are happy or sad it was retrieved. In order to do supervised training, the user would need to identify the perfect retrieval, requiring the user to evaluate each and every document in the corpus!  The distinction between the supervised retrieval and that shaped by relevance feedback highlights the need to be explicit about which kinds of feedback are hard for the user and which are easier. The discussion of RAVE made some of our assumptions concerning cognitive overhead clear (Section 4.4), but this is another important area for further study. Here we will continue to assume that relevance feedback is easy to acquire.  Learning from the Entire Session  Note that each iteration of the FOA conversation is but a link in the larger dialog leading from the users' initial information need to the end of their search; cf. Figure 7.2.  First, the continuity of the same search pursuing the same information need from iteration of the FOA dialog to the next iteration helps to constrain interpretations of the users' relevance feedback; we know which documents they have seen previously, we may know something about how a document retrieved in iteration i + 1 is related to one from iteration i and why it was retrieved, and so on. But perhaps the most important reason to model multiple queries as part of a single session is that it is the total, aggregate difference between their cognitive states at the beginning of the session and at the end of the session that should most concern us. Users begin the search with their best characterization of what it is they want to FOA* and they leave for one of a number of potential reasons: 258      FINDING OUT ABOUT  ï  because they ran out of time (but would like to continue this same session at a later time),  ï  because they found what they were looking for,  ï  because they reached ultimate frustration, etc.  Our interpretation of what the system could or should learn from this termination will be radically different, and so determining which of these reasons pertains becomes particularly important to establish.  Distributions over Learning Instances  Anyone familiar with basic statistics will appreciate how sensitive our estimate of an average is on the set of examples we happen to select for our sample. Learning methods are similarly sensitive to the amount and distribution of training data. For example, the ratio of the number of features to the number of instances can affect learning performance significantly [Moulinier et al., 1996; Dagan et al, 1997].  Define a positive training instance Æ to be one that has been identified as a member of a class and a negative training instance 0 to be one that is identified as not a member of that class.  It is less obvious that the ratio of positive to negative training instances is also important. In binary classification tasks we can expect an even number of positive and negative training instances. But when classifying into a larger number of categories, it is common to treat each training instance as a positive instance of one class and a negative instance of all the others. Continuing our email classification example, we can expect that every classified message in our training set corresponds to a positive instance of one classification and simultaneously as a negative example of all the others. This makes efficient use of the training data, but also generates a ^-j- skew to the positive versus negative training instance distribution.
foa-0144	7.2 Building Hypotheses about Documents  We will talk about competing "hypotheses/' for example, rules that successfully separate our spam email from our family's email. If only very simple hypotheses are to be considered, a relatively small amount of data ADAPTIVE INFORMATION RETRIEVAL       259  can be used to select between them. For example, if our hypothesis is that spam email always contains the phrase $$$$ BIG MONEY $$$$$, a small amount of training data is sufficient to confirm or disconfirm this rule [Sahami et al., 1998]. But if we wish to consider elaborate discrimination rules, for example, including many keywords and/or date information, it takes much more data to tease apart all the alternatives. The volume of training data available, then, provides a very real constraint on how complex the hypotheses we can consider can be and how statistically reliable we expect rules to perform on unseen test data.
foa-0145	7.2.1 Feature Selection  When you are thinking about how you classifiy your email, keywords contained in your email are almost certainly some of the features you think of first. Recall, however, that the keyword vocabulary can be very large. Using this feature space, then, individual document representations will be very sparse. In terms of the vector space model of Section 3.4, many of the vector elements will be zero. To use Littlestone's lovely expression, "Irrelevant attributes abound" [Littlestone, 1988], and so it should come as no surprise that his learning techniques are especially appropriate in the FOA learning applications discussed in Section 7.5.3.  Efforts to control the keyword vocabulary and make the lexical features as meaningful as possible are therefore important preconditions for good classification performance. For example, name-tagging techniques (cf. Section 6.6.1) that reliably identify proper names can provide valuable classification features. A proper name tagger would be one that was especially sophisticated about capitalization, name order, and abbreviation conventions. When both people's proper names and institutional names (government agencies, universities, corporations, etc.) are capitalized, the recognition of complex, multitoken phrases becomes possible.  In part because of the difficult issues lexical, keyword-based representations entail, it is worth thinking briefly about some of the alternatives. There are also less-obvious features we might use to classify documents. Meta-data associated with the document, for example, information about its date and place of publication, is one possibility. Geographic place information associated with a document can also be useful; cf. Section 6.6.1. Finally, recall the bibliographic citations that many documents contain (cf. Section 6.1). The set of references one 260      FINDING OUT ABOUT  document makes to others (representable as links in a graph) can be used as the basis of classification in much the same way as its keywords. In summary, while keywords provide the most obvious set of features on which classifications can be based, these result in very large and sparse learning problems. Other features are also available and may be more useful. It is important to note, however, that careful Bayesian reasoning about dependencies among keyword features is a very difficult problem, as discussed in Section 5.5.7. Attempting to extend this inference to include other heterogeneous types of features must be done carefully.  Distribution-Based Selection  Because our typical assumption has been that keywords occur independently, it should come as no surprise that when we try to reduce from the full set of all keywords in the Vocab to a smaller set, a good way to decide which features are most useful is to pick those that are most independent of any others. That is, we can hope that two keywords that are statistically dependent can be merged into a single one.  As mentioned in Section 3.3.6, entropy captures the amount of randomness in (or uncertainty about) some random variable:  H(X) = -J^Pr(X = x) log (Pr(x))                 (7.1)  When the distribution of a random variable X is conditionally dependent on that of a second random variable Y, the conditional entropy of X on Y (a.La. post-Y entropy of X) can be similarly defined [Papoulis,  1991, pp. 549-54]:  H(X\Y) = -     £     Pr(x\y)log(Pr(X = x\Y = y))       (7.2)  X=x, Y=y  If knowledge of values of Y reduces our uncertainty about the distribution of Xy it is natural to think that Y informs us about X. Mutual  information I captures this notion:  - H(X\Y)  Pr(x)log(Pr(x)) + J2Pr(x\y)log(Pr(x\y))   (7.3)  x                                                  x, y  This information is mutual in the sense that it is a symmetric relation ADAPTIVE INFORMATION RETRIEVAL       261  between the two variables; Y tells us as much about X as X does about Y:I(X,Y) = I{Y,X).  If the mutual information I(k{} kj) between all pairs of keywords in K isknown,vanRijsbergen,p. 123 recommends selecting the maximum spanning tree, which maximizes the total information across all edges of the tree:  the MST... incorporates the most significant of the dependencies between the variables subject to the global constraint that the sum of them should be a maximum, [van Rijsbergen, p. 123 ]  Note, however, that the mutual information statistic has an intrinsic bias toward keyword pairs kj, kj in which the individual keyword frequencies fi and fj are intermediate. The post-Y entropy of X can only reduce it [Papoulis, 1991]:  H(X\Y)lt; H(X)                           (7.4)  In terms of keyword frequencies, then, the mutual information of a pair of keywords is limited by their marginal entropies. This implies that very rare and very common keywords are "penalized" with respect to the mutual information measure. For this reason, it is worthwhile considering the relation between mutual information as a measure of keyword interdependencies and the eigenstructure analysis (cf. Section 5.2.3) of the keyword cross-correlation matrix /.  Mutual information considers the full joint probability between the two keywords, while methods like singular value decomposition (SVD) and principal component analysis (PCA) consider only the crosscorrelation matrix. When random variables happen to fit a normal distribution, these correlation statistics are sufficient, but that is unlikely in the case of our keywords. A second important difference is that correlationbased methods construct new feature variables, out of linear combinations of the initial keyword tokens. Mutual information-based methods (or at least van Rijsbergen, p. 123's MST-based construction) select the best variables from a constant set.  Selection Based on "Fit" to a Classification Task  Rather than using distributional statistics among the keywords themselves as the basis for feature selection, it is also possible to look for those 262      FINDING OUT ABOUT  features that are most "fit" with respect to some classification task [Lewis and Hayes, 1994] (cf. Section 7.4).  Both mutual information and correlation statistics can be used in either supervised or unsupervised learning situations, considering either the mutual information J(fc, c) of each keyword k with respect to the class c in the former case or Fisher's linear discriminant [Duda and Hart, 1973, p. 114] in the latter.  Using the classification performance criterion, Yang and Pederson considered a wide range of potential measures and found that simply measuring /*, the document frequency of keyword fc, provided an effective measure over potential keyword features [Yang and Pedersen, 1997]. In fact, using document frequency as a criterion, they were able to remove 98 percent of all keywords while retaining (even improving slightly!?) classification performance. Given the efficient way in which fk can be collected (relative to Ml, x2gt; and other potential measures), the level of performance maintained by such aggressive dimensionality reduction is indeed striking. But because the features selected are sensitive to the particular classification task considered, their utility for other purposes may be suspect. Distribution-based selection methods may therefore be more robust.
foa-0146	7.2.2 Hypothesis Spaces  However they are selected, the features discussed in the previous section must now be composed into hypotheses concerning how we might describe documents. There is an extraordinary range of alternatives represented here. Some of these are shown in Figure 7.3.  Decision trees are formed by asking a question about individual features and using the answers to these questions to navigate through a series of tests until documents are ultimately classified at the leaves. Weighted, linear combinations of the features can also be formed. Neural networks are best viewed as nonlinear compositions of weighted features [Crestani, 1993; Crestani, 1994; Gallant, 1991; Kwok, 1995; Wong et al, 1993]. Boolean formulas can be formed from sentences using simple conjunctive or disjunctive combinations. Our focus here will be on Bayesian networks, which attempt to represent probabilistic relationships among the features.  In any of these cases, machine learning techniques must be sensitive to their inductive bias. That is, given a fixed amount of data, we must ADAPTIVE INFORMATION RETRIEVAL       263  Decision trees  Inductive     Small  bias:               trees  Linear combinations  Neural networks  Few, small              Smooth  weights              mappings  FIGURE 7.3 Inductive Bias  Boolean formulas  Small con/disjuncts  Bayesian networks  Sparse,  acyclic  have some a priori preference for some kinds of hypotheses over others. For example, decision tree learning algorithms [Quinlan, 1993] prefer small trees and neural networks prefer smooth mappings [Mitchell, 1997].  A common feature of all of these learning algorithms is a general preference for parsimony, or simplicity. This preference is typically attributed first to William of Occam (ca. 1330). Occam's Razor has been used since then to cleave simpler hypotheses from more complex ones.  Another motivation for the parsimony bias has been realized more recently within machine learning: Simple hypotheses are more likely to accurately go beyond the data used to train them to classify other unseen data. That is, very complicated hypotheses have a tendency to over-fit to the training data given a learning algorithm, but the good fit they can accomplish on this set is not matched when the same classification is done on new data. The issues involved in evaluating a classifier's performance are an important topic within machine learning [Mitchell, 1997].
foa-0147	73 Learning Which Documents to Route  Arguably, the first use of relevance feedback information in an adaptive IR context  came out of the SMART group led by Gerald Salton. As was discussed in  Section 4.2.2, Salton's vector space model lends itself to a representation of documents and queries that suggest ways of producing better matches, as shown in Figure 7.4. 264      FINDING OUT ABOUT  FIGURE 7.4 Document Modifications due to relevance feedback  Rocchio's characterization of the task is best described as routing: We imagine that a stream of all the world's news is available to some institution (for example, a corporation), and many members of this institution (employees) are expected to build long-standing queries characterizing their interests. These queries are passed against the incoming stream of documents, and those matching a particular employee's query are routed to him or her [Schutze et al., 1995a].  From this employee's point of view, he or she will be receiving a stream of documents, all of which are of interest to him or her. If he or she Identifies those documents that are not relevant, the resulting corpus can be viewed as a training set and used to adjust the filter. This is a concrete application of the binary classification technology.  Brauen, in particular, considered "document vector modifications," resulting in "dynamic" document spaces [Brauen, 1969]. Documents marked as relevant can be moved closer to the query that successfully retrieved them in several ways. First and most obviously, features shared by query and document can have their weight increased. Features in the document but not in the query can have their weight decreased, and terms in the query but not in the document can be added to the document's representation with small initial weights.  From the perspective of modern machine learning, Brauen's method would be considered a type of gradient descent learning algorithm: ADAPTIVE INFORMATION RETRIEVAL       265  The disparity between document and query creates a gradient along which small changes are made. One difference, however, is that Brauen imagined a batch learning scenario, with the entire set of labeled documents available simultaneously. Online learning algorithms, on the other hand, make small, incremental adaptive changes in immediate response to every learning instance (document). Each individual step of learning (e.g., weight update in neural networks) must be very small, in order to guarantee convergence toward the globally optimal value. The size of the small change is controlled by 77, typically known as the learning rate. Stochastic approximation theory suggests that this constant should be relatively small for online learning so that the weights are small enough to allow convergence [White, 1989]. Online learning is generally preferred to avoid time delay and data collection complications. Idealizing our learning task to produce a perfect match (i.e., the dot product of query and document is 1.0) on relevant documents and no match on irrelevant documents, we can treat this behavior as the target for our error correction learning. Let R stand for this Boolean relevant/not relevant classification of each document. Then (as discussed further in Section 7.4), we can hope to have available a training set T of documents that have been labeled a priori as R^ = 0 or R = 1gt; specifying whether they are considered relevant. With such evidence as to what documents we do and don't consider relevant, we can define precisely how well we have learned. A typical error measure or loss function can be defined as the squared difference between the actual vector match and the target Rd:  ,      .,  r          Wj         n \2                          a c\     gradient runs in  Error = ^(d-q-Rd)2                           (7.5)    *oth directions  deT
foa-0148	7.3.1 Widrow-Hoff  The Widrow-Hoff (a.k.a. least mean squared (LMS)) algorithm is the  best-understood and principled approach to training a linear system to minimize this squared error loss [Widrow and Hoff, I960]. It does this  by making a small move (scaled by the parameter 77) in the direction of the gradient of error. This gradient is defined exactly by the derivative of Equation 7.5 with respect to the document vector:  (7.6) 266      FINDING OUT ABOUT  It is also important to remember that changes made to a single document in response to a single query can make no guarantees about improved performance with respect to other documents and other queries. For example, two documents might both be moved closer to a query (as proposed by Brauen and Rocchio) but their relative rankings may not change at all!
foa-0149	7.3.2 User Drift and Event Tracking  One interesting feature of the training set generated by the routing task is the odd distribution of positive and negative examples it generates. Initially, we can imagine that this filter is very inaccurate; i.e., we are likely to see many negative examples. Later, when we hope it is better trained, the filter has nearly perfect performance and the system gets very few negative examples.  Further, no user's interests remain static. As discussed in the next chapter, one common purpose for the FOA activity is to become educated (cf. Section 8.3.4), and this is an elusive, ever-changing goal. The world changes, and what users read changes their opinions of what needs to be done and what the new questions are. In brief, documents they used to find relevant become irrelevant. This has been called concept drift [Klinkenberg and Renz, 1998]. When the world changes, this corresponds to documents, and the news they contain changes too. This side of the dynamic is called topic tracking [Allan et aL, 1998; Baker et al, 1999]. Jaime Carbonell's (of CMU) approach is to first identify that a concept change has occurred and then to adjust a time window on the stream of incoming training data over which a new invariant is then identified (personal communication).  The distribution of relevance feedback generated by the filtering task, where a standing query is allowed to adapt to a stream of relevance feedback generated by users who receive and evaluate routed documents (cf. Section 4.3.9), provides an especially interesting form of learning task, because of its temporal dimension. Initially, the set of documents routed to users must depend on the same fundamental matching function shared by other search engine tasks. But as relevance feedback, in response to the first retrievals, comes to affect the users' characterizations of interest, only a skewed sample (relative to the initial distribution) of potential documents is shown to the users, and only these can be the basis of subsequent relevance feedback ADAPTIVE INFORMATION RETRIEVAL       267  This tension between exploration of the universe of potentially relevant documents and exploitation of those documents that prior relevance feedback makes it seem are most likely to be perceived as relevant by the users is familiar to other reinforcement learning situations [Sutton and Barto, 1998].
foa-0150	7.4 Classification  The classification task is ... classic), and a wide range of technologies for accomplishing it have been developed [Duda and Hart, 1973; Carlin and Louis, 1996]. Even in the relatively recent context of text-based domains, many techniques have been applied [Lewis and Hayes, 1994]. Here we will focus primarily on extending the probabilistic approach of Section 5.5, but we'll consider other alternatives in Section 7.5. Andrew McCallum and colleagues have developed a software suite called RAINBOW3 that is very useful for experiments into text classification.  We begin by assuming that we have been given a set of classes C = {c\, ci,... cq} and have been asked to produce a function that classifies a new document into one of these classes. McCallum gives a good overview of the Bayesian approach applied to text:  This approach assumes that the text data was generated by a parametric model, and uses training data to calculate the Bayesoptimal estimates of the model parameters. Then, equipped with these estimates, it classifies new documents using Bayes' rule to turn the generative model around and calculate the posterior probability that a class would have generated the test document in question. [McCallum and Nigam, 1998, p. 42, emphasis added]  The major features of the training regime are shown in Figure 7.5. During the first phase, a correspondence has been established manually between each example document d,- and some classification c\ in the training set T:  T = {(di, Ci)}                                   (7.7)  11 www.csxmu.edij/mccalliim/bow/rainbow/ 268      FINDING OUT ABOUT  Classifier 0  Automatic assignment to classes  FIGURE 7.5 Training a Classifier  (In Figure 7.5, the classifications are imagined to be part of a hierarchical classification system, as will be discussed in detail in Section 7.5.5.) These data are used somehow (for now it's okay to think of it as magic) to tune the set of parameters 0 specifying a particular classifier. The second and dominant phase is then to use this classifier to automatically assign documents to classes in an analogous manner to those manually classified in the training set.  We seek the posterior probability of a particular class, given the evidence provided by a new document, Pr{c | d). The second step is then, given these probabilities for all classes, to return the one that maximizes this likelihood. Bayes' rule is typically invoked in such situations to "invert" this conditional dependency:  Pr(c\) =  Pr(d\c)Pr(c) J2Pr(d\c)Pr(c)  (7.8)  The likelihood Pr(d | c) can be more easily estimated if we assume classes are represented as mixtures of the documents' features. We can think of hypothetical documents (that we imagine belong to the class) as generated by some model, the parameters 0 of which we hope to discover. Assuming that a document is generated by first picking a particular class and then using its parameters to select features,* the likelihood of  * More general models are also possible [McCallum and Nigam, 1998]. ADAPTIVE INFORMATION RETRIEVAL      269  a document being selected can be computed by considering the prior probability of each class Pr(c |©) and its distribution Pr(d | c; 6):  Pr(d\) = J2 Pr(c\@) ï Pr(d \c; 0)                (7.9)  These measures allow us to precisely state how well a learned model captures regularities found in the training set: The model is doing a good job if it applies the same classifications as observed in the training data. But this criterion also highlights the dependence of any learning method on the training data T used to construct it, which cannot be overemphasized. Independent of the range of hypotheses considered, and of the learning methods used to build the best possible model, our ability to inductively generalize from the training data to new examples (e.g., unclassified documents) depends entirely on how typical the training data are. Within computational learning theory this dependence is captured by the assumption that training data and subsequent trials are drawn from the same distribution [Valiant, 1984; Kearns and Vazirani, 1994].  In practice, performance of a classifier on the training set is bound to be an optimistic overestimate of how well it can generalize to previously unseen data. The most common way to guard against such overestimates is to artificially hold out some of the training set as a separate test set. Training proceeds as before on all the training data but not on the test set, and then the system is evaluated on the test set, which provides a more reasonable estimate as to how the system will fare. More sophisticated cross-validation procedures begin by partitioning the available training data into k subsets. Iteratively, each partition is used as the holdout set while the remaining ^p- balance of T is used for training. The average performance across all k tests is then used as a more statistically reliable estimate of true performance. The statistical validity derived by crossvalidation and related techniques becomes especially important when the total amount of training data is small; given the time and effort required to produce manual classifications, this is often the case.
foa-0151	7.4.1 Modeling Documents  The general framework of empirical Bayesian estimation is broad and powerful enough that it has been applied in many contexts. The hard 270      FINDING OUT ABOUT  work comes, however, in specifying just how the parametric model © is to be constructed from a set of individual parameters 0; and how these can be estimated from the training data. Principled approaches to the text classification problem require specification of explicit models of how documents are generated. Two models of the event space underlying our construction of hypothetical documents have been proposed [McCallum and Nigam, 1998], and we consider each of these below.  One critical, simplifying assumption shared by both models is that the features occur independently in the documents. As we have discussed a number of times, any such naive Bayesian model will miss a great many of the interactions arising among real words in real documents. It is somewhat curious, then, that such naive classifiers do as well as they do [Domingos and Pazzani, 1997].  Multivariate Bernoulli  Arguably the simplest model captures only the presence or absence of words in the document. That is, the document is modeled as the composition of k keywords drawn from the Vocab as so many independent Bernoulli trials. We imagine that a document d is constructed by repeatOrdered              edly selecting |d| words for each position in the document.^ sequence                   jf we associate a biased coin with each keyword k, we can decompose the desired model 0 into two sets of parameters:  6c = Pr(c)                                     (7.10)  eck = Pr(k\c)                                 (7.11)  i.e., the prior probability of each class c and the probability that a keyword is present given that we know a document containing it is in class c. Then the naive Bayesian assumption allows us to assume that the keywords occur at each positional location independently of one another:  eck                               (7.12)  Jt=i ADAPTIVE INFORMATION RETRIEVAL       271  Multinomial  An alternative model of document generation is as repeated draws from an urn of words containing all the keywords in the Vocab. This gives rise to a multinomial model that is sensitive to the frequency fa of keywords' occurrence in dy rather than just their presence or absence:*  gfdk  f]   -f-            (7.13)
foa-0152	7.4.2 Training a Classifier  The parameters © controlling the classifier could come from many places, but of course here we are concerned with learning them. In terms of the training set T:  T={(dhCi)}                             (7.14)  we seek the parameters © with the highest probability of having produced T. Depending on the model we employ, how we decompose 0 into its constituent parameters 0{ will differ.  One piece of this is easy to estimate: The prior probability of the class Pr(c) is how frequently one classification is observed in T relative to the others. Using a "twiddle" hat to distinguish estimates 0 of the probabilities from their true values 0:  S- = W\                                   lt;7'15)  Estimating 0ck is more complicated. The fact that both the multi variate Bernoulli and the multinomial models of document generation involve the product of the keywords' 0ck should make it obvious that our cumulative estimate will be very sensitive to any one of these values; consider, for example, what happens if even one of these terms is zero!  Within the Bayesian framework,^" these statistical sensitivities are ad~    Probabilists' dressed by providing priors for the underlying word-events of document    religious wars generation.  * For simplicity, we assume that the documents1 lengths are independent of the classes, i.e., that knowing a document's length tells us little about which class it should be in.
foa-0153	272      FINDING OUT ABOUT  7.43 Priors  When we are attempting to estimate 0ckgt; the most common prior applied within text classification is the m-estimate [Mitchell, 1997, p. 179]. This corresponds to pretending that the actual training data in T are augmented by NKw = \Vocab\ pseudo-trials uniformly distributed across all the keywords in Vocab. Operationally, this simply means that all keyword counters are "primed" (initialized to) one before real statistics are collected. With the priors specified, we can estimate 9ck under each of the two models discussed in Section 7.4.1.  Recall that the multivariate Bernoulli model associates a single biased coin 0ck with each keyword used by a class. The Bernoulli assumption is then that a document in the class will contain at least one occurrence of the keyword with probability 9ck but also with probability 1 ó 8ck that it will not contain any instances of the keyword. Using a Boolean indicator function Bdk to signal the presence or absence of a keyword in a document and another BC(i to signal whether the document is classified with respect to class cf  ^       1 + £ BdkBdc  Note that in this statistic the "nonoccurrence" of keywords not present in the document also affects our estimate. This somewhat odd Mark of Zero [Lewis et al., 1996] should make us feel less sanguine about what is captured by any Bernoulli model.  The multinomial alternatives (assuming the same priors as before) for these estimates are:  _            1 + £ fcdBdc  Ock = KJjr    JeJ, Ñ  ,   p                      (7.17)  NKw+ £ £ fcdBdc kevdeT  Empirically, the multinomial model seems to support better classification performance, especially when larger vocabulary sizes are considered [McCallum and Nigam, 1998].  Bdk = 1 if" A: occurs at least once in d and is zero otherwise; Bcd = liid is classified  as an instance of class c and is zero otherwise. ADAPTIVE INFORMATION RETRIEVAL       273  "Ireland"     lt;-gt;      IRELAND V  (IRA. A KILLED) v (IRA a SHOT) v  (IRA A OUT) FIGURE 7.6 RIPPER Classification Rule
foa-0155	7.5.1  Nearest-Neighbor Matching  One of the most straightforward ways to classify documents is to make a rote memory of the training set T and retrieve those documents from T that are most similar to a new document to be classified [Cost and Salzberg, 1993; Larkey and Croft, 1996; Yang and Pedersen, 1997; Larkey, 1998a]. This corresponds to using the | d ¶ d\ similarity metric discussed in Section 5.2. A weighted sum of the k most similar documents' classifications can be used to pick the best match.
foa-0156	7.5.2  Boolean Predicates  A different approach to the text classification task has been proposed by Cohen in his RIPPER system [Cohen, 1996a; Cohen, 1996b]. The space of hypothesis considered by RIPPER is a set of Boolean rules composed over a space of keywords. A simple example is shown in Figure 7.6, which extends the obvious definition of IRELAND to include documents that mention violent IRA activities. Like decision lists, RIPPER's rule sets are easier for human experts to interpret than a large system of Bayesian probabilities.  RIPPER is an example of a "covering" learning algorithm; cf. Figure 7.7. This means that it iteratively forms conjunctions of Boolean predicates that "cover" some of the positive instances of a Boolean classification while excluding all the negative instances. In the next iteration, the positive instances, which were covered previously, are removed from the training set, and a new conjunctive clause is formed, which again covers some more positive instances while excluding all negative ones. Ultimately, then, the rule set will be in disjunctive normal form (DNF).  RIPPER also includes optimizations to simplify rules by removing conditions that do not affect performance and by picking conditions 274      FINDING OUT ABOUT  +¶ Positive training instance ó Negative training instance © Seed  FIGURE 7.7 Covering Algorithms  that provide the most information gain [Quinlan, 1993]. Finally, Cohen adapted these rule learning techniques to the text domain by adding "set-valued attributes." These special attributes collapse a document's representation to be simply the set of words it contains. RIPPER's rules can then include tests for sets of words, rather than having to test the presence or absence of each word individually.
foa-0157	7.53 When Irrelevant Attributes Abound  In documents where "irrelevant attributes abound" [Littlestone, 1988] (e.g., when any one document contains a small fraction of the full vocabulary but still more than are important in a classifier), the rapid "winnowing" of features is critical. One approach is to use boosting methods, which begin with many very weak hypotheses but focus in on the most successful [Schapire et aL, 1998].  Kivinen and Warmuth's exponentiated gradient EG algorithm extends from Winnow's binary features and output to real-valued quantities [Warmuth, 1997]. The result shares much with the Widrow-Hoff model (cf. Section 7.3.1), but rather than having weight changes making an additive change in the direction of the gradient, EG recommends making a multiplicative change to each element of the document vector. Using R4 as in Equation 7.6:  d'=d  (7.18) ADAPTIVE INFORMATION RETRIEVAL       275  FIGURE 7.8 Combining Experts  where A! is the updated document vector and |d/7| is the length of the document vector after all weights have been updated. That is, weights are always renormalized so that their sum remains one (and nonnegative). Renormalization is an important feature of EG, which, in conjunction with the multiplicative increases in those "relevant" features that are shared with a query, results in quick (i.e., exponentially fast) reduction to zero for irrelevant weights [Lewis et al., 1996]. Callan has also found that rather than training the EG classifier with zero for incorrect classifications and unity for correct ones, using more moderate target values pegged to the minimum and maximum feature values is more successful [Callan, 1998].  Another very recent approach is to apply Vapnik's support vector machines (SVM) [Vapnik, 1995]. Rather than searching for dichotomizing planes within a representational space that has been predefined (e.g., the hyperplanes that gradient descent methods adjust), SVMs search in the "dual" space defined by the set of training instances for kernels (representations) wherein the classes can be conveniently separated! As Joachims [Joachims, 1998] recently emphasized, the way in which these techniques avoid searching the vast space of potential keyword features seems to make SVM a very appropriate technology for this application.
foa-0158	7.5.4 Combining Classifiers  The preceding chapters have described a wide range of potentially useful retrieval techniques. A very reasonable response is to ask if perhaps the  best possible retrieval system isn't some sort of mixture of these various techniques. For example, Bartell [Bartell et al, 1994a] has considered  simple linear combinations of experts like those shown in Figure 7.8. His experiments considered two experts, one that used a set of simple words as features and a second that did more elaborate phrase extraction. 276      FINDING OUT ABOUT  1  O  Optimal term weight (weights normalized to unit circle)  FIGURE 7.9 Optimal Weightings Distribution From [Bartell et al., 1994b]  Holding the sum of the two experts' contributions constant, their relative contribution describes a circle of fixed radius. Figure 7.9 shows a set of 228 test queries and the optimal weighting of phrase and term experts for each. That is, for a particular query, the optimal contribution of ranking information from the phrase and term expert is determined. Also shown is a line corresponding to the optimal balance between phrase and term expert across all queries. In general, the phrase expert was not very useful. On some queries, however, it was able to improve performance significantly. The way in which individual queries make special demands of the retrieval system is perhaps the most striking feature of these results.  As search engine technologies have developed, the composition of hybrid systems involving multiple systems has required a more "black box" composition. That is, rather than manipulating a single feature of the retrieval system (e.g., term versus phrase features), the combination has been of a system's net ranking [Thompson, 1990a; Thompson, 1990b].  Collection fusion refers to the problem of combining results coming from disjoint corpora [Towell et al., 1995; Yager and Rybalov, 1998]. (In the emerging environment of combined corpora and primarily ADAPTIVE INFORMATION RETRIEVAL       277  publisher-driven search engines, corpora have become confounded with the search engines allowed to search them.) Diamond (personal communication) has hypothesized several effects we might imagine from fusing multiple search engines:  Skimming effect... [when] retrieval approaches that represent [documents] differently may retrieve different relevant items, so that a combination method that takes the top-ranked items from each of the retrieval approaches will "push" non-relevant items down in the ranking.  Chorus effect... when several retrieval approaches (each representing the query differently) suggest that an item is relevant to a query, this tends to be stronger evidence for relevance than a single approach doing so. Thus, allowing several independent retrieval approaches to "vote" on the relevance of an item should enable a sharper distinction between relevant and non-relevant items. [Diamond, personal communication]  Note how the first of these focuses on differences in the systemsbased treatment of documents and the second on queries (cf. Section 3.3.3).*                                                                                                 Dark Horse  In some ways, this combination of classifiers is reminiscent of earlier effect too work using multiple query representations [BeUcin et al., 1993]. Vogt has recently performed an exhaustive analysis of linear combinations for all 61 search engines submitted to the TREC5 competition [Vogt and Cottrell, 1998]. His primary conclusion, following Lee [Lee, 1997], is that two systems are best combined linearly when there is a great deal of overlap in the set of relevant documents they identify, while their retrieval of nonrelevant documents is nearly disjoint. In terms of Diamond's qualitative expectations, linear combinations are best able to support the "chorus" effect. Schutze et al. and Larkey have considered combining various types of special-function classifiers [Schutze et al., 1995b; Larkey and Croft, 1996].
foa-0159	7.5.5 Hierarchic Classification  In many areas of careful scholarship, classification labels are not merely members of a big set, but rather they are organized hierarchically into 278       FINDING OUT ABOUT  Pr( belief)  UseNet                   pr( belief | UseNet)  alt                                Pr( belief | alt)  alt.religion                       Pr( belief | alt.religion)  q    ,        h   v    y       alt.religion.scientology    Pr( belief j alt.religion.scientology)  FIGURE 7.10 Hierarchic Classification  systems of BT/NT hypernymy (cf. Section 6.3). For example, the U.S. Patent Office has 400 top-level classifications with 135,000 subclasses [Larkey, 1998b]. These classes are part of a hierarchic tree going down 15 levels. A simple example suggested by Mitchell and others' use of the UseNet newsgroup hierarchy [Mitchell, 1997; McCallum et al., 1998] is shown in Figure 7.10. Here the probability of the keyword BELIEFS is conditionalized with respect to a series of increasingly specific newsgroups.  Let Ä}t be a hierarchic classification, meaning that it is part of a taxonomy rooted at Cq and connected via a path of ancestor classifications ^Ch'.  ©C/, =  {cqª tfi, Ca.b, . . . ª Ca.b.c.h}                      (7.19)  This notation is meant to capture the relationship shown in Figure 7.11.  McCallum et al. creatively applied the statistical technique known  as shrinkage to the problem of text classification {McCallum et al.,  1998]. Parameter estimates of child classes* which will have very few ADAPTIVE INFORMATION RETRIEVAL       279  Ca.b.....h  FIGURE 7.11 Ancestors of a Class  data instances, can be "shrunk" toward the data-rich ancestor's, and the contributions of each ancestor's classification are then linearly combined:  Okch=   £ WiPr(k\ci)                        (7.20)
foa-0161	7.6.1 Exploiting Linkage for Context*  All samples of language, including the documents indexed by Web search engines, depend heavily on shared context for comprehension. A document's author makes assumptions, often tacit, about the intended audience, and when this document appears in a "traditional" medium (conference proceedings, academic journal, etc.), it is likely that typical readers will understand it as intended. But one of the many things the Web changes is the huge new audience it brings to documents, many of whom will not share the author's intended context.  But because most search engines attempt to Index indiscriminately across the entire WWW, the global word frequency statistics they collect can only reflect gross averages. The utility of an index term, as a  * Portions previously published in |Menczer and Belew, 2000J. 280      FINDING OUT ABOUT  discriminator of relevant from irrelevant items, can become a muddy average of its application across multiple distinct subcorpora within which these words have more focused meaning [Steier and Belew, 1994a; Steier and Belew, 1994b].  Hypertext information environments like the Web contain addiQueryingfor tional structure information [Chakrabarti et al., 1998a] .t This linkage link topology information is typically exploited by browsing users. But linkage topology- the "spatial" structure imposed over documents by their hypertext links to one another - can be used to generate a concrete notion of context within which each document is understood: Two documents and the words they contain are imagined to be in the same context if they are close together in this space. Even in unstructured portions of the Web, authors tend to cluster documents about related topics by letting them point to each other via links. Such linkage topology is useful inasmuch as browsers have a better-than-random expectation that following links can provide them with guidance. If this were not the case, browsing would be a waste of time.  This suggests that agents (infobots, spiders, etc.) that navigate over such structural links might be able to discover this context. For example, agents browsing through pages about ROCK CLIMBING and ROCK 'N ROLL should attribute different weights to the word ROCK, depending on whether the query they are trying to satisfy is about music or sports. Where an agent is situated in an "environment" (neighborhood of highly interlinked documents) provides it with the local context within which to analyze word meanings - a structured, situated approach to polisemy. The words that surround links in a document provide an agent with valuable information to evaluate links and thus guide its path decisions a statistical approach to action selection.  The idea of decentralizing the index-building process is not new. Dividing the task into localized indexing, performed by a set of gatherers, and centralized searching, performed by a set of brokers, has been suggested since the early days of the Web by the Harvest project [Bowman et al., 1994]. Web Watcher [Armstrong et al., 1995] and Letizia [Lieberman, 1997] are agents that learn to mimic the user by looking over his or her shoulder while browsing. Then they perform look-ahead searches and make real-time suggestions for pages that might interest the user. Fab [Balabanovic, 1997] and Amalthaea [Moukas and Zacharia, 1997] are multiagent adaptive filtering systems inspired by genetic ADAPTIVE INFORMATION RETRIEVAL       281  algorithms, artificial life, and market models. Term weighting and relevance feedback are used to adapt a matching between a set of discovery agents (typically search engine parasites) and a set of user profiles (corresponding to single- or multiple-user interests).  Here we focus on InfoSpiders, a multiagent system developed by Fillipo Menczer [Menczer et al, 1995; Menczer, 1997; Menczer and Belew, 1998; Menczer, 1998; Menczer and Belew, 2000]. In InfoSpiders an evolving population of many agents is maintained, with each agent browsing from document to document online, making autonomous decisions about which links to follow and adjusting its strategy. Populationwide dynamics bias the search toward more promising areas and control the total amount of computing resources devoted to the search activity. Basic features of the algorithm are discussed, and then an example of how these agents perform as searchers through a hypertext version of the Encyclopaedia Britannica are presented herein.
foa-0162	7.6.2 The InfoSpiders Algorithm  InfoSpiders searches online for information relevant to the user by making autonomous decisions about what links to follow. Algorithm 7.1 is the InfoSpiders implementation of the local selection algorithm. A central part of the system is the use of optional relevance feedback. The user may assess the relevance of (some of) the documents visited by InfoSpiders up to a certain point. Such relevance assessments take place asynchronously with respect to the online search and alter the subsequent behaviors of agents online by changing the energy landscape of the environment. The process is akin to the replenishment of environmental resources; the user interacts with the environment to bias the search process. Let us first overview the algorithm at a high level; representation-dependent details will be given in the following subsections and experimental parameter values in the next section.  The user initially provides a list of keywords and a list of starting points, in the form of a bookmark file.* The search is initialized by prefetching the starting documents. Each agent is "positioned" at one of these documents and given a random behavior (depending on the representation of agents) and an initial reservoir of energy.  * This list would typically be obtained by consulting a search engine. 282      FINDING OUT ABOUT  Algorithm 7.1 An Evolutionary Algorithm for Distributed Information Agents  initialize po agents, each with energy E = 0/2 loop:  foreach alive agent a:  pick link from current document  fetch new document D  Ealt;- Ea~ c{D) + e{D)  Q-learn with reinforcement signal e(D)  if(Ea gt; 9)  af´ó mutate(recombine(clone(a))) Eagt; Eai lt;ó£fl/2 elseif(Eal lt; 0)  die(a) process optional relevance feedback from user  Each agent "senses" its local neighborhood by analyzing the text of the document where it is currently situated. This way, the relevance of all neighboring documents - those pointed to by the hyperlinks in the current document - is estimated. Based on these link relevance estimates, the agent "moves" by choosing and following one of the links from the current document.  Next, the agent's energy is updated. Energy is needed to survive and move, i.e., to continue to visit documents on behalf of the user. Agents are rewarded with energy if the visited documents appear to be relevant. The e() function is used by an agent to evaluate the relevance of documents. If a document has previously been visited and assessed by the user, the user's assessment is used; if the document has not been visited before, its relevance must be estimated. This mechanism is implemented via a cache, which also speeds up the process by minimizing duplicate transfers of documents. While in the current, client-based implementation of InfoSpiders this poses no problem; caching is a form of communication and thus is a bottleneck for the performance of distributed agents. In a distributed implementation, we imagine that agents would have local caches. When using the current implementation to simulate the performance of distributed InfoSpiders, we will simply set the cache size to zero. ADAPTIVE INFORMATION RETRIEVAL       283  Agents are charged energy costs for the network load incurred by transferring documents. The cost function c() should depend on used resources, for example, transfer latency or document size. For simplicity we will assume a constant cost for accessing any new document and a (possibly smaller) constant cost for accessing the cache; this way stationary behaviors, such as going back and forth between a pair of documents, are discouraged.  Just as for graph search, instantaneous changes of energy are used as reward and penalty signals. This way agents adapt during their lifetime by Q-learning. This adaptive process allows an agent to modify its behavior based on prior experience by learning to predict which are the best links to follow.  Depending on its current energy level, an agent may be killed or selected for reproduction. In the latter case, offspring are recombined through the use of local crossover, whereby an agent can only recombine with agents residing on the same document, if there are any. Offspring are also mutated, providing the variation necessary for adapting agents by way of evolution.  Finally, the user may provide the system with relevance feedback. It is important to stress that this process is entirely optional - InfoSpiders can search in a completely unsupervised fashion once it is given a query and a set of starting points. Relevance feedback takes place without direct online interactions between user and agents. The user may assess any visited document D with feedback lt;/gt;(D) G {-I, 0, +1}. All the words in the document are automatically assessed by updating a "feedback list" of encountered words. Each word in this list, fc, is associated with a signed integer a)k that is initialized with 0 and updated each time any document is assessed by the user:  \/ke D:a)k^~ o)k  The word feedback list is maintained to keep a global profile of which  words are relevant to the user.  The ultimate output of the algorithm is a flux of links to documents ranked according to some relevance estimate - modulo relevance assessments by the user. The algorithm terminates when the population becomes extinct for lack of relevant information resources or when it is terminated by the user.
foa-0163	284      FINDING OUT ABOUT  7.6.3 Adapting to "Spatial" Context  In order to test InfoSpiders, agents were allowed to breed in a controlled test environment previously used by Steier [Steier and Belew, 1994a; Steier, 1994]: a portion of the Encyclopedia Britannica. The advantage is that we can make use of readily available relevant sets of articles associated with a large number of queries. The subset corresponds to the HUMAN SOCIETY topic, roughly one-tenth of the entire encyclopedia. Links to other parts of the EB are removed, along with any terminal documents produced as a result. The final environment is made of N = 19,427 pages, organized in a hypertext graph (the EB is already in HTML format). Of these pages, 7859 are full articles constituting the Micropcedia. These, together with 10,585 Index pages (containing links to articles and pointed to by links in articles), form a graph with many connected components. The remaining 983 nodes form a hierarchical topical tree, called Propcedia. These nodes contain topic titles and links to children nodes, ancestor nodes, and articles. Micropaedia articles also have links to Propaedia nodes. Propaedia and index pages are included in the search set to ensure a connected graph and to be faithful to the EB information architecture - an actual subset of the Web.  Now consider two agents, A and Bgt; born at the same time and attempting to satisfy the same query, but in different "places" within this hypergraph of EB document pages.  As Table 7.1 shows, the original query words were displaced from their top positions and replaced by new terms. For example, PBTVAT and ALLEVI had relatively low weights, while FOUMDAT appeared to have the highest correlation with relevance feedback at this time.  As and B's keyword vectors are shown in Table 7.2. In the course of the evolution leading to A and B through their ancestors, some query terms were lost from both genotypes. A was a third-generation agent; its parent lost ALLEVI through a mutation in favor of HULL. At A's birth, PRIVAT was mutated into TH. B was a second-generation agent; at its birth, both ALLEVI and PBJVAT were replaced by HULL and ADDAM, respectively, via mutation and crossover. These keyword vectors demonstrate how environmental features correlated with relevance were internalized into the agents* behaviors.  The difference between A and B can be attributed to their evolutionary adaptation to spatially local context. A and B were born at documents ADAPTIVE INFORMATION RETRIEVAL       285  TABLE 7.1 Part of the Word Feedback List and Weights at Time 550  Rank New k Ik  1 ï FOIINDAT 0.335  2  RED 0.310  3 ï MISSION 0.249  4  SOCIAL 0.223  5  CROSS 0.197  6  HULL 0.184  7  HOUS 0.183  8  ORGAN 0.161  15  SERVIC 0.114  16  ACTIV 0.112  23  TH 0.094  30  PUBLIC 0.087  32  ADDAM 0.079  37  HUMAN 0.075  41  PRIVAT 0.067  44  ALLEVI 0.065  Note: Stars mark new terms not present in the original query. Note that TH does not correspond to the article the, which is a noise word and thus is removed from all documents; rather, it corresponds to the th. used for ordinal numbers and often associated with centuries.  TABLE 7.2 Keyword Vectors for Agents A and B  A                                          B  ORGAN                                QRGA2T  PUBLIC                                PUBLIC  TH                                       ADDAM  SERVIC                                SERVIC  SOCIAL                                SOCIAL  HUMAN                               HUMAN  ACTIV                                  ACTIV  HULL                                   BULL  D and D^, respectively* whose word frequency distributions are partly shown in Table 7.3. TH represented well the place where A was borngt; being the second most frequent term there; and ADDAM represented well the place where B was born, being the third most frequent term there. 286      FINDING OUT ABOUT  TABLE 7.3 Most Frequent Terms in the Documents where Agents A and B were Born  rankoA k freq(k, DA) rankD k freqik, DB)  1 WOKKHOUS 0.076 1 HOUS 0.043  2 TH 0.038 1 HULL 0.043  2 POOR 0.038 3 ADDAM 0.025  4 SOCIAL 0.030     4 CENTURI 0.030 38 AMERICAN 0.004  Note: Word frequencies are normalized by the total number of words in each document.  By internalizing these words, the two situated agents are better suited to their respective spatial contexts.
foa-0164	7.7 Other Learning Applications and Issues  This chapter has focused primarily on applications of classification techniques that are among the most successful with machine learning. There are, however, a wide range of other ways that adaptive mechanisms have been incorporated within the FOA context [Bono, 1972]. For example, Gordon has applied genetic algorithm optimization to the construction of effective queries [Gordon, 1988]. The success of social collaborative filtering, where adaptation in response to the activities of one user is extrapolated to other users [Shardaanand and Miaes, 1995], promises to become an important part of the WWW industry (as recently demonstrated by Microsoft's purchase of Firefly, for example). Learning to extract structured, database-like relations from the vast WWW of unstructured documents is another important new direction [Howe and Dreilinger, 1997; Craven et al, 1998 ]. Here we mention several other ways that adaptation mechanisms can be exploited and some new problems arising from this dynamism.
foa-0165	7.7.1 Adaptive Lenses  A discussion of the routing or filtering task should make it clear that  learning technology can be placed at many levels within the FOA system. A personal classification tool can be very useful to a single individual organizing his or her own email. These categories need not make any ADAPTIVE INFORMATION RETRIEVAL       287  FIGURE 7.12 Adaptive Lens  sense to or be consistent with those used by anyone else. But when multiple users all browse through shared corpora, it becomes possible for one person's browsing experience to benefit another's. Of course, interuser consistency in RelFhk will help to determine just how statistically correlated these training signals are. But if users are clustered as part of socially cohesive groups (for example, a research lab full of students and faculty pursuing the same research questions) and they are searching documents of shared interests (for example, reprints they have all collected on topics of mutual interest, as part of a journal club perhaps), it is not unreasonable to believe that their assessments will be very similar indeed. Figure 7.12 shows a single individual with a classification system on his or her own machine. But their searches often go through a second classifier or "adaptive lens" that they share with other members of a computer science group. The figure also shows two different social groups of users, for example, computer scientists and cognitive scientists, each learning different connotations for the phrase NEURAL NETWORK, perhaps computationally and physiologically skewed, respectively. Finally, these smaller groups are merged into progressively larger groups of users over which RelFhk assessments are pooled.
foa-0166	288      FINDING OUT ABOUT  7.7.2 Adapting to Fluid Language Use  The advantages of my benefiting from your experience training a system are obvious. Just as obvious, though, are ways in which you and I might differ in our notions of relevant documents. Even this comparison is only possible if we each evaluate exactly the same query, and how often will that happen?!  If we are to change our representations of documents based on users' opinions, should we value "expert" opinions over those of "novices"? Common sense would suggest that if we could ascertain that one user was indeed expert in a particular topical area, then their assessments of relevance should perhaps carry more weight. But based at least on the experience of lawyers searching case law [Cohen, 1991; Sprowl, 1976] experts are less likely to search in their own area of expertise, probably because they already know what they will find there. The typical user searches in areas they know less about. In that respect, perhaps the novice's relevance feedback is, in fact, a better characterization of what we should attempt to satisfy.  Once the index representation is allowed to change in response to users' relevance feedback we are faced with the question of how fast this change should occur. Some of these questions were mentioned in terms of conceptual drift by users and topic tracking in news sources (cf. Section 7.3.2). Adopting the longer-term archival perspective of a librarian perhaps, how quickly should we want our adaptive system to track current trendy terms? (cf. Wired magazine's Tired/Wired Memes4 feature). When old terms of art have been rendered obsolete, how can we nevertheless maintain an archival record of this previous ethology         terminology?*^"
foa-0167	7.8 Symbolic and Subsymbolic Learning  Most of the learning applications we have discussed apply to the Index relation between keywords and documents. But there are many other  syntactic clues associated with documents from which we can also learn.  4 www.wired.com/wired/a^ ADAPTIVE INFORMATION RETRIEVAL       289  Chapter 6 discussed a number of these heterogeneous data sources. But as we attempt to learn with and across both structured attributes and statistical features (recall the distinction of Section 1.6), it is important to keep several key differences in mind.  The distinction between symbolic features of each document (e.g., date and place of publication or author), which represent unambiguous features of the world that human experts can reliably program directly, and the much larger set of subsymbolic features from which we hope to compose our induced representation [Rose and Belew, 1989] becomes especially important as we attempt to combine both manually programmed and automatically learned knowledge as part of the same system [Belew and Forrest, 1988]. Even among these attributes, however, there is room for learning about their meaning. For example, while a scientific paper may have many nominal authors, it is often only one or two to whom most readers will attribute significant intellectual contribution. While papers often have extensive bibliographies, some of these also are more significant than others and can be considered supporting or antagonistic (see Section 6.1).  For all these reasons, FOA is an especially ripe area for AI and machine learning. The fact that documents are composed of semantically meaningful tokens allows us to make especially strong hypotheses about how they should be classified. One fundamentally important feature of the FOA activity (unless the WWW alters our world entirely!) is that there will always be more instances of document readings than of document writings. That is, while we can imagine spending a huge effort analyzing any text, there are fundamental limits to how much we can learn about it from only the features it contains. But each time a document is retrieved and read by a reader, we can potentially learn something new about the meaning of this document from this new person's perspective. Machine learning techniques are mandatory if we are to exploit the information provided by this unending stream of queries.  As discussed in Chapter 6, the histories of IR and AI have crossed many times in the past, generally in head-on collisions rather than constructively. But as AI has moved from a concern with manually constructed knowledge representations to machine learning, and as IR has begun to consider how indexing structures can change with use, these two methodologies can be expected to increasingly overlap.
foa-0168	8__________  Conclusions and Future Directions  The important thing is not to stop questioning. Curiosity has its own reason for existing. One cannot help but be in awe when he contemplates the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries to comprehend a little of this mystery everyday. - Albert Einstein [Einstein, 1955]
foa-0169	8.1 Things that Are Changing  Alta Vista was, in 1995, arguably the first search engine offered for general use, so Alta Vista's history1 is especially interesting. At that time, Alta Vista was developed by Digital Equipment Corporation to primarily demonstrate how powerful its new Alpha architecture was, especially its  then-novel 64-bit addressing and the consequentially vast data spaces. Indexing all the WWW's pages and providing a useful service to many  was simply good publicity.  Since that time Digital Equipment has been acquired by Compaq, and Alta vista has been spun off to CMGL As searching newly authored pages on the WWW has become increasingly profitable, similar  ! doc. altavlsta. com/company info /aJxgt;ut^v/baalq£round. slitml  192 CONCLUSIONS AND FUTURE DIRECTIONS       293  search technologies have been applied to existing, traditionally published corpora to form the next generation of digital libraries [Fox and Marchionini, 1998; Paepcke et al., 1998]. It is amazing how closely they resemble the vision H. G. Wells had of what a "World Encyclopedia5' might mean, as early as 1938 [Wells, 1938]!  As the Internet reaches a mass audience and these new search engine users begin to FOA in earnest, important new data are becoming available about how these real users (as opposed to most IR experimental subjects; cf. Section 4.3.1) behave. Silverstein et al. report on their analysis of approximately one billion (109) queries issued against the Alta Vista search engine during six weeks in August and September 1998 [Silverstein et al., 1999]. Another important qualification on this preliminary study is that no attempt was made to discriminate "real," human-generated queries from automatic queries generated by robots. Still, several features of this study are significant.  First, fully 15% of the queries were entirely empty; they contained no keywords! Two-thirds of these empty queries were generated within Alta Vista's "advanced query" interface. Clearly, good interface design and user education remains a fundamental issue for effective search engine design.  Second, WWW searches use very short? simple queries, averaging only 2.3 keywords/query (not including the zero-length queries in this average). Only 12.6 percent of queries used more than three keywords. Of course, the fact that Alta Vista's interface does not easily support longer relevance feedback queries (cf. Section 3.6) keeps these from occurring. Most users also avoid query syntax and issue simple queries: Only 20 percent of queries used any of Alta Vista's query operators (+, -, and, or, not, near); half of these used only one operator.  These findings are especially significant because they paint a different picture of the "typical user" from what IR has traditionally held. When IR systems were first developed, the target audience was primarily reference librarians, search intermediates who helped library patrons find what they were seeking from sophisticated systems such as DIALOG. These librarians were specially educated, in particular in the subtleties of Boolean query operators and other sophisticated techniques for constructing exactly the right "magic bullet'* query for a particular corpus. IR system design and theory therefore generally 294      FINDING OUT ABOUT  assumed that queries were fairly rich, structured expressions. At least at the moment, these assumptions do not seem to hold for most Web searching.  But despite the relatively simple form of most queries, the third  interesting fact is that Web queries are rarely repeated. Even folding case  and ignoring word order, only one third of queries appeared more than  once in the billion queries; only 14 percent occurred more than three  We mostly ask      times.t These statistics are especially significant in the face of new services  about SEX          suc]1 as Ask Jeeves,2 which focus on providing especially relevant answers  for a restricted set of anticipated queries.  Finally, Silverstein et al. attempted to analyze query sessions. Knowing when a query is part of a session is notoriously difficult, especially when some queries are being generated by robots; this study used a combination of server-set cookies and a five-minute time window to capture coherent searches by the same user. It appears that 78 percent of query sessions involve only a single query and that an average session involves only two queries! These data are preliminary, but they provide an interesting contrast to the power law, Zipfian distribution of Web surfing behavior reported by Huberman et al. [Huberman et al, 1998] (cf. Section 3.2.5).  The primary extension of the search engine technology developed so far in this text is the crawling function that must harvest Web pages prior to their indexing. The design of Web crawlers is now one of the most active areas of computer science research; we provide only a few basic references here.
foa-0170	8.1.1 WWW Crawling  One important way in which Web search engines extend beyond the  notions of FOA presented here concerns the crawlers that feed them. In all of our discussions, the corpus was imagined to be a static object. For WWW search engines, the underlying set of documents that are to be indexed and made available to users is constantly changing. Further, the task of quickly, reliably, and exhaustively visiting all WWW-linked pages is a fundamental task in and of itself. One good, accessible example of  2 aslgeeves.com/ CONCLUSIONS AND FUTURE DIRECTIONS       295  0.4  8gt;  gt;  o o  ßgt;  s  0.3  0.2  0.1  HotBot AltaVista Northern  Excite   infoSeek   Lycos Light  FIGURE 8.1 Crawler Coverage. From [Lowrence and Giles, 1998].  Reproduced with permission of American Association for the  Advancement of Science  crawler code is provided by the LibWWW Robot,3 part of the WWW Consortium (W3C) LibWWW distribution. A Perl-based crawler interface4 has also been developed by Gisle Aas; ParallelUserAgent,5 developed by Marc Langheinrich, is another Perl alternative.  Naive WWW users often seem to have the tacit belief that every Web crawler is aware of (i.e., has indexed) every document on the Web. More sophisticated users know that there is a certain lag time between the posting of a new page and its inclusion in the Web search engine's index. But the fundamental omissions by most search engine crawlers are still underappreciated. The most concrete data in this respect is due to a recent experiment done by Lawrence and Giles [Lawrence and Giles, 1998], shown in Figure 8.1. Using a statistical extrapolation from the mismatch of documents found by one of the six most important search engines suggests that at that time the Web contained approximately 320 million pages. Of this total, even the best search engine was able to capture only about a third of those documents.  The ecology of these various search engines and their co-evolutionary technological responses to one another create an extremely dynamic situation. Danny Sullivan edits an excellent newsletter, Search Engine Watch,6 that does nothing but track changes in the volatile  3 www.w5.org/Robot/  4 www.Mnpro.no/lwp/  a' http: //www.inf. ethz.ch/-laxigpieta/BaraUelUA/ 296       FINDING OUT ABOUT  marketplace of search engines and portals. The search engine business and supporting technologies can be expected to continue to foment for some time to come. In asymptote, however, current notions of search engines will go extinct for two basic reasons: Their methods do not scale to the Internet, and they only get in the way.  Search Engines Don't Scale  Scalability is a major issue limiting the effectiveness of search engines. The factors contributing to the problem are the large size of the WWW, its rapid growth, and its highly dynamic nature. In order to keep indexes up to date, crawlers periodically revisit every indexed document to see what has been changed, moved, or deleted. Heuristics are used to estimate how frequently a document is changed and needs to be revisited, but the accuracy of such statistics is highly volatile. Moreover, crawlers attempt to find newly added documents either exhaustively or based on user-supplied URLs. Yet Lawrence and Giles have shown that the coverage achieved by search engines is at best around 33 percent, and that coverage is anticorrelated with currency - the more complete an index, the staler the links [Lawrence and Giles, 1998]. More importantly, such disappointing performance comes at high costs in terms of the load imposed on the Net [Eichmann, 1994].  This becomes an important reason for investigating search agents for the WWW like those described in Section 7.6. Online agents do not have a scale problem because they search through the current environment and therefore do not run into stale information. On the other hand, they are less efficient than search engines because they cannot amortize the cost of a search over many queries.  Disintermediation  Section 8.2.1 will discuss FOA as a particular type of "language game." In brief, the FOA language game is played by three players: the text's author, its readers, and the search engine. Authors have something to say and an audience they are trying to say it to. They attempt to characterize their content to intermediates (book publishers, journal editors, WWW search engines) in ways that capture "markets" for what they have to say. Readers have an information need and some ideas about where to look for writings that might satisfy it. These readers sometimes (and now CONCLUSIONS AND FUTURE DIRECTIONS       297  much more often than in the past) characterize their information need to intermediates (librarians, paralegals, WWW search engines) in hopes of being shown documents likely to be relevant to their information needs.  The second fundamental flaw of current search engines, then, is that they are and will forever be only mediators; they neither produce content nor consume it directly. The search engine is caught in the middle of the other two players. It must somehow make the correspondence between the languages used by writers and readers. If it plays its part of the FOA language game well, it reliably connects readers with writers.  Said another way, search engines are simply noise in the channel between author and reader. If they are doing their jobs effectively, they should disappear as transparent background to facilitate easy communication of rich messages. The difficulty browsing users currently experience as they attempt to FOA documents on the WWW makes it clear just how far current search engines are from this ideal.  Traditionally, authors have made conventional assumptions about how their readers would find them. They would sell their book to a publisher, and part of this economic relationship involved the publisher putting its distribution channels at the services of the author. For magazine and newspaper reporters, as well as for fiction authors, periodical publications provided a regular audience for a magazine of contributions. Textbook authors would favor publishers with extensive connections with educational institutions. Scientists would submit articles to peer review under the supervision of editors for professional societies. In every case, multiple levels of mediation between the author and the reader are assumed by the author.  Even if the WWW were only a new technological substrate on which all of these conventional activities occurred, we might expect the level of confusion now present as search engines cross everyone's wires. But it seems likely that the change is even more fundamental: The number of content-producers (writers) is rapidly approaching the number of content-consumers (readers)! Never before has the machinery of producing and distributing media been as widely available as it is today. Our collective expectations as to just what documents are clt;out there," not to mention the care and authority with which they have been authored, are in terrific flux. 298      FINDING OUT ABOUT  Authors trying to be heard through this cacophony must fundamentally rethink their assumptions of how their content will be published. The most obvious examples of this are author-created keyword meta-tags. A wide range of meta-tags are now in use - ranging from ones that carry intellectual property information to ones that carry "decency" ratings; the HTML standard in fact allows an open-ended set of such tags to support any number of additional attributes of the document. Two meta-tags, however, are especially important from the perspective of FOA. The KEYWORDS meta-tag is designed to contain (the author's recommendation for) content descriptors, and the DESCRIPTION meta-tag to provide a proxy string. Both provide explicit mechanisms for authors to convey additional meaning in their writings, beyond words that happen to be in the text of the document itself. They have the additional advantage of being free of any morphological and weighting heuristics used by a particular search engine. Of course, this additional expressive power on the part of authors also makes it at least possible for them (or their Web masters) to attempt to spoof search engines with meta-tags designed simply to draw users to the page. Like much of the law concerning the WWW, exactly what constitutes "good faith" use of meta-tags is a matter of great debate (a recent example is Playboy v. Terri Welles7). Whether in good faith or not, attempts by authors to express themselves clearly are currently compromised by the refusal of search engines to publicly commit to some basic standards of crawling and indexing behavior. Their wide variety in operation, compounded by opaque descriptions of how each works, currently makes articulate expression by  Just how do         an author impossible.^ It is no wonder that searching users become  Alta Vista,            confused.  HotBot, ...  work?!
foa-0171	8.2 Things that Stay the Same  While some features of FOA change as quickly as Internet stock prices,  others are as old as language itself. To characterize current search engines as noise on a communication channel between author and reader is an attempt to reconsider what we'd like to have happen with WWW communications.  ^ www.terriwelles.oom/dlsiiLissal.html
foa-0172	CONCLUSIONS AND FUTURE DIRECTIONS       299  8.2.1 The FOA Language Game  Lurking at the core of the entire FOA enterprise is the fundamental question of semaBtics: What do the words in our language mean? Computer scientists are most familiar with artificial languages (formal grammars, programming languages, etc.) for which precise semantics in terms of a particular machine are absolutely necessary. Many philosophers of language, notably Frege and Ludwig Wittgenstein, have advocated that a similarly precise semantics of natural language is also possible. Words are predicates about states of the world: Either they apply or they don't  An alternative point of view says that such a precise and abstract semantics can never be achieved. What language means is what it means to us, the language users. That is, words' meaning cannot be separated from the forms of life of which they are a part. As it happens the same Ludwig Wittgenstein has argued forcefully on this side of the debate too!^  One of Wittgenstein's most useful devices for getting across his theory of language was his notion of the language game (Sprachspiele in German) [Wittgenstein, 1953]. Wittgenstein gives many varieties of language games, from chidren's games as simple as "ring-around-the-rosy" [Wittgenstein, 1953, Section 7] to such "adult" games as:  ï  forming and testing hypotheses;  ï  making up a story and reading it; and  ï  asking questions.  It is interesting to compare the multiplicity of the tools in language and of the ways they are used [Wittgenstein, 1953, Section 23]. Certainly FOA counts as another example of a language game, but one with special  rules.  Another interesting aspect of Wittgenstein's theory is how well it anticipates the models of language meaning arising from modern machine learning techniques. The common cause is that Wittgenstein was  centrally concerned with learning by children. This is evident in his aringaround-the-rosy" example, and in his explicit attention to consequences  of learning that apply equally well to our algorithms:  [Consider] two pictures, one of which consists of colour patches with vague contours, and the other of patches... with clear contours. The degree to which the sharp picture can resemble  Early versus  late  Wittgenstein 300       FINDING OUT ABOUT  the blurred one depends on the latter's degree of vagueness___Won't you then have to say: "Anything and nothing  is right." And this is the position you are in if you look for the definition corresponding to our concepts in aesthetics or ethics.  In such a difficulty always ask yourself: How did we learn the meaning of this word [vague] ? From what sort of examples? In what language games? Then it will be easier for you to see that the word must have a family of meanings. [Wittgenstein 1953, Sections 76, 77]  Our current versions of the FOA language games are tied to the technologies by which we are currently allowed to communicate with one another. For now, centralized search engines are in the center of this dialog. Authors write and sometimes try to influence the audiences their documents reach. Later, readers use a few of the first words that come to mind to tease out some possible answers. Search engines do their best to connect these two vocabularies.  Reading and writing are the primitive language games on which FOA is based. The tools available to help writers and readers are currently strong constraints in the FOA rules. People can only express what they are allowed to express. If only simple query languages are available, only simple questions will be asked. If all documents are treated interchangeably, as context-free samples of text, then the tacit context assumed by the author is not available.  And so, our abilities to automatically learn what the words really do mean to authors and to readers will change as the evidence in the WWW dialogs changes. Especially unclear are guarantees about communication privacy and security: If we believe all our words are for everyone's ears, then many things will never be said via the Net. If search engines watch over our shoulders as we browse, should we be grateful because it will understand what we mean, or should we send them a bill for the valuable training data we have provided? As companies like Amazon.com8 use new technologies that allow them to "eavesdrop" on commercial transactions^ consumers must ultimately decide what their privacy and more effective indexing are worth to them personally.  1 Ajnazon.com  * newB.cnet.oom/news/0-1007-200-1S17791 .html CONCLUSIONS AND FUTURE DIRECTIONS       301  Passion FIGURE 8.2 The Semiotic View  Semiotics  The next step toward a theory of what the FOA language game might mean involves semiotics, the subfield of linguistics centrally concerned with the ability of signs to convey meaning. Dating back at least to the American pragmatist philosopher C. S. Peirce and the French linguist Saussure, semiotics is now often associated with Umberto Eco (most famous for his popular novel Name of the Rose [Eco, 1983]). David Blair has written an excellent overview of the field [Blair, 1990, chapter 4].  In order to get away from using words as communicative signs, semioticists often use other symbols, such as the ROSE shown in Figure 8.2 In brief, semiotic theory imagines meaning being trapped by the triad of signifier, signified, and sign. Hawkes (quoted from Blair) uses a gift of ROSES as an example of how meaning can be conveyed:  ... a BUNCH OF ROSES can be used to signify passion. When  it does so, the BUNCH OF ROSES is the signifier, the passion  the signified. The relation between the two (the associative total) produces the third term, the BUNCH OF ROSES as a sign. And, as a sign it is important to understand that the BUNCH OF ROSES is quite a different thing from the BUNCH OF ROSES as a signifier, that is, as a horticultural entity. As a signifies the BUNCH OF ROSES is empty, as a sign it is full. What has filled it (with signification) is a combination of my intent and the nature of society's conventional modes and channels which offer me a range of vehicles for the purpose. The range is extensive, 302       FINDING OUT ABOUT  I'm looking for  documents  ABOUT  "Message passing interface"  "MPI"  "MPI"  "Multi-perspective interactive (video)"  FIGURE 8.3 A Semiotic Analysis of Keyword Mismatch  but conventionalized and so finite, and it offers a complex system of ways, of signing. [Hawkes, 1977, p. 131, quoted from Blair, 1990, monospace font not in original]  That is, in a successful communicative act, the sign of ROSES successfully unites what the signifier was trying to express with what the receiving listener thinks they are pointing at, i.e., the content of the sign.  Figure 8.3 applies this analysis to the case when a mediating search engine stands between linguistic sign users. When a keyword like MPI is used as part of a query by a user intent on the features of the MESSAGE PASSING INTERFACE (a communication standard used by parallel computers and language compilers), it is confounded with documents authored by people who use MPI to mean MULTI-PERSPECTIVE INTERACTIVE (video). The same signifier MPI points to two different significations. If a specialist in PARALLEL COMPUTING has MULTIMEDIA VIDEO documents returned by the search engine, the communication of a unifying sign has not been accomplished.  Speech Acts  Certainly the communication going on between sender and receiver of flowers is different from that between WWW author and browser. One important difference is that flowers, like spoken oral language, happen in the moment between two people who know one another. The WWW, CONCLUSIONS AND FUTURE DIRECTIONS       303  1. Quantity  1.1. Make your contribution as informative as required.  1.2. Do not make your contribution more informative than is required.  2. Quality  2.1. Do not say what you believe to be false.  2.2. Do not say that for which you lack adequate evidence.  3. Relation  3.1. Be relevant.  4. Manner  4.1. Avoid obscurity of expression.  4.2. Avoid ambiguity.  4.3. Be brief.  4.4. Be orderly.  FIGURE 8.4 Grice's Maxims From [Grice, 1975]. Reproduced with permission of Academic Press  like libraries, contains written language, which communicates between readers and writers separated by arbitrary amounts of time and space. Differences between orality and literacy are some of the most important to understand if FOA is to become a part of traditional linguistics [Ong, 1982].  An important dimension of the difference between oral and literate communication concerns the attentional focus of sender and receiver and how and when it is given. Before any symbols can be exchanged, the sender must apply attention to the construction of a message, and before a receiver can understand it he or she must be "listening." Communication is a demand for attention by the author and of a reader.  Grice has defined what he calls the cooperative principle to make explicit the co-dependence of sender and receiver's communicative tasks [Grice, 1957; Grice, 1975]. Grice's maxims (see Figure 8.4) help to codify ways in which this tacit contract can lead to meaningful communication or be broken."^" Although these were drafted with oral communica-    Star Trek script tion in mind, they remain (with Strunk and White's Elements of Style    generator [Strunk and White, 1979]) excellent advice to authors on how to write clearly.  A second important difference between oral and written communication is its intimacy. Spoken language is imagined to be a quiet act, between a particular speaker and a listener. The FOA communicative acts with which we are most concerned involve much more public displays of language. In 1975, Saracevic [Saracevic, 1975] talked about this as 304       FINDING OUT ABOUT  communicating public knowledge, a concept "as pertinent now as when it was written" [Sparck Jones and Willett, 1997, p. 86].  The author had an intended audience in mind when he or she wrote, but once the document is written and published, it is (like graffiti!) there for all to read. Search engines connect huge sets of authors with vast audiences of readers. The language used in queries and indexing vocabularies is bound to be loud and broad.
foa-0173	8.2.2 Sperber and Wilson's "Relevance"  The notion of relevance has been at the heart of much of the FOA enterprise, particularly its evaluation (cf. Chapter 4). By making connections to foundational theories of language games and speech acts, Sperber and Wilson offer one of the soundest definitions of what "relevance" might mean [Sperber and Wilson, 1986].  Their principle of relevance puts the onus on the communicator: Ostensive behaviors (i.e., those in which there is a manifest intention to inform another) should be taken as guarantees that the sender believes them to be relevant to the receiver. They then provide a pragmatics for the communication, i.e., why a reader or listener should pay attention: viz., to improve their knowledge. Then the most relevant information we might convey has two properties: It must be new, or we have not improved the state of knowledge. But it also must be connected to other information, or this new factoid really adds almost nothing.  The value of connected information can be made most clear in terms of a logical theorem-proving model (cf. Section 6.5). Assuming again that the user already "knows" the contents of an intitial knowledge base E and has a question whether the proposition r is true or false, relevant documents are those that most extend what they know. Sperber and Wilson's notion of connected information corresponds exactly to the set of new inferences that are now allowed.  Sperber and Wilson also draw our attention to the importance of the context within which any communicative act occurs. One aspect of this context is the mutual knowledge that sender and receiver must have in order for communication to proceed efficiently:  MutualKnowil U) = {k \Know(h k) lt;-+ Know(U, k)}          (8.1)  Mutual knowledge is that knowledge k that if I know it, you know it too. This deep knowing what you mean, and your knowing that I know, CONCLUSIONS AND FUTURE DIRECTIONS       305  and my knowing that you know that I know, etc., is what we seek in eye contact, in email responses, and in "active" listening. Relevance feedback is the fundamental communicative act that this text proposes for the process of assuring mutual knowledge.  The context within which any particular communicative act occurs has many other dimensions, too. As the earlier MPI example suggested, one of the fundamental issues in WWW searching is the confounding of contexts. The author of a journal article from the multimedia community can use MPI with his colleagues and students without any problem or confusion, because they share the same context. But when these journal articles are mingled with those from the parallel computing community, using MPI as a sign causes it to straddle two different contexts.  In summary, then, the FOA language game seems to propose a ternary predicate about connecting keywords and a document with a person who believes that the relation holds.
foa-0174	8.2.3 Argument Structures  Another important notion of context surrounding any particular document is the system of argumentation within which it participates.  The tendency of search engines to slice and dice documents into salads of their constituent words has been called the bag-of-words phenomenon. It is typically used in contrast to syntactic parsing methods, which place these words as part of well-structured, grammatical constructs. But there is another level of violence done to language when word frequency counts are collected, and that has to do with the argument structures by which sentences are strung together to form persuasive communicative acts.  Many of modern cultures' most well-documented communications involve the use of language as a persuasive device. Mathematical theorem proving, legal opinions, scientific papers, Op-Ed newspaper and magazine pages, artistic criticism, all have in common the fundamental purpose of convincing an audience.  A mathematical paper (purportedly!)* convinces by proving theorems. In legal corpora, the fundamental principle of stare decisis on which the common law tradition is based has already been mentioned. Within science, varying disciplines have wildly differing standards for what constitutes a convincing argument. Political and artistic forms of  Math proofs are more informal than you may think? 306      FINDING OUT ABOUT  persuasion are being changed as they move from the media of newspapers and magazines to the WWW.  While the modes of argument supporting each of these social activities have diverged, there remains a common thread connecting all such documents, viz., their common heritage as written artifacts. Since long strips of papyrus were first rolled onto scrolls, our expectations about a fundamentally linear progression through a text have held. The Greeks' theories of narrative and rhetoric, analyzing how good stories and persuasive arguments are constructed, are still worth knowing today. Clearly these theories were shaped by the linear media of scrolls and books that then conveyed culture. The fundamentally nonlinear capabilities of hypertext media seem to open the door to radically different notions of argument. On the other hand, many modern theories of cognition continue to highlight the fundamentally linear and sequential flow of human attention [Newell, 1990]; we can only think about one thing at a time, no matter how fast we click. It will probably be artists, using new forms of media and hypertext authorship, who teach us the most about how these new technologies can be used most expressively. It is still too early to tell how our existing social institutions will absorb these technological changes. Attempts to index musical content offer some glimpses of the future [Bakhmutova et al, 1997; Foote, 1997; Ghias et al, 1995; Wold etal, 1996].  The editors dual  representation of a document
foa-0175	8.2.4 User as Portal  How could we possibly know so much about the story or argument an author is attempting to get across?! Or about just what question a user has in mind, and why they want the answer?! In fact, more and more evidence is making itself available to make just such determinations. Virtually every author now composes using a word processor, and even if they are only interested in ultimately producing a paper document, it generally carries with it much of the intra- and interdocument structure discussed in Chapter 6. Further, it may be possible to infer even more of their thinking if the process of word processing is allowed to leave a trace with the document*  Similarly, browsers retain more and more state (i.e., permanent changes to the local computer's files that exist after the browser program is no longer running) all the time. Bookmark lists and search history CONCLUSIONS AND FUTURE DIRECTIONS       307  FIGURE 8.5 Query as Portal, Connecting Corpora  files are the most obvious examples. Less obvious to most users are the cookies left by WWW servers on the user's client, to better identify when they come to visit.  The point is that there are actually fairly rich streams of information available about both author and user, above and beyond the query and document text with which we have been primarily concerned in this book. Of course, there are some computations over these two data sets that can be done a priori, before any search commences. But the more interesting computations are those that can only be done on the occasion of a particular query. A sketch of this perspective is shown in Figure 8.5, with the user and his active seeking behaviors creating a portal through which corpora can interact.  Here we propose that in fact each user's query, each bit of relevance feedback, and each move from one document to another effectively create the opportunity to juxtapose two corpora in ways that, until that moment, had never been analyzed in that way before.
foa-0176	83 Who Needs to FOA  In attempting to survey both the most dynamic aspects of FOA and its most classic aspects, the preceding chapters have jumped over a wide 308      FINDING OUT ABOUT  range of topics. We conclude with a much more personal (Al Franken)mode of analysis: Just why are the issues discussed in this text important to you?. This question is addressed from the perspective of four special-interest user groups especially involved in FOA language games: scientists, publishers, and (as befits a textbook!) teachers and students.
foa-0177	8.3.1 Authors  Note first the new precision/recall-like trade-offs facing authors: If they are aggressively selling their documents, then putting spam-like descriptors on them will make them be retrieved in the most possible situations. Getting your document into the hands of only those readers you anticipate finding it relevant means anticipating their queries and how they'll describe their information need. Who is the audience? What words are familiar to this audience? What words in this document will be unfamiliar to them? How might you describe the unfamiliar concepts using familiar words? At their core, these questions are no different from what authors have needed to ask since the beginning.  The tendency of authors to oversell their documents is exactly what makes checkpoints in the publication process valuable. We value news editors, publishers, reviewers, and all others whose profession it is to apply discretion.  Authors should think about FOA because they need to have their documents found. Publishers have traditionally helped with this process, but only for those authors they choose to publish. The WWW opens a much larger number of useful communication channels to potential authors. Not all of this authoring activity is healthy, however. Moderation of newsgroups and the peer review process are two important checks on the flow of information that have worked traditionally in UseNet and scientific exchanges.  Authors need to know how to write to be found. The ability to separate documents explicitly with meta-data about the documents means that there is no need to compromise the work itself to publicize it to an audience. It is true, of course, that the most effective ways of automatically producing meta-descriptions of a document still involve FOA-style statistical keyword analysis. But many authors are willing to manually add useful keywords or connect their documents to relevant Web pages or search engines. For many authors and artists this explicit analysis of potential readerships is an important part of their art. CONCLUSIONS AND FUTURE DIRECTIONS       309  Of course, authors may or may not be aware of all of the ways potential readers might be interested in their work. This is where a third-party editorial role becomes most valuable. Good editors are able to span the gap between what an author is trying to say and what an audience is interested in hearing.
foa-0178	8.3.2 Scientists  Scientifically constructed knowledge can take many forms, especially in the modern age of genomic data, hypertexts, and multimedia. Knowledge networks is a term used (e.g., by the National Science Foundation) to refer to an even wider range of interconnections, among both the scientists who have created them and the representations themselves. In the new science occurring on the WWW, the speed with which a manuscript can go from author to reader has been accelerated dramatically.  Perhaps because the physical installations required to do modern physics (high-energy accelerators, telescopes, etc.) required them to come to the same geographical location (CERN, Livermore, Los Alamos), physicists have long been aware of how useful informal "preprint" communication channels can be [Latour, 1987]. This may be one reason why the server begun by P. Ginsparg at Los Alamos National Labs has been a leader in exploring how scientific publication can occur. This system, originally developed for only physicists, has been so wildly successful that it now houses many documents from many disciplines. The National Institute of Health has recently proposed extending this example with PubMed Central,10 which would disseminate research reports, both with and without peer review.  As might be expected, computer scientists have a particularly wide range of resources for searching their own literature, including:  ï  The Computing Research Repository11 (CoRR);  ï  CORAy12 a search engine developed especially for computer science research;  ï  NCSTRL13 (pronounced "ancestral"), a system for sharing archives of technical reports in computer science.  10 http://www.nih.gov/aJ3Out/director/pubm^cªntral-htin  11 xxxJanl.gov/arcMve/cs/intro.html  12 www.oorajiistresearcli.cx5m/ " www.ncstrl.org/ 310       FINDING OUT ABOUT  One ugly truth of science is that the primary task of scientists is to disagree with one another. The scientific process works because it effectively weeds out flawed arguments. One scientist can gain fame by claiming something new is true; a second scientist can gain fame by showing that it is not. If the question is sufficiently important, a third, scholarly scientist can do useful work by enumerating the many people who said all the things that were neither new nor true, as well as the very few publications that are both.  The printed record of science must contain all aspects of this ongoing debate, but the channel is not wide enough to contain all debates among all scientists, and so only some aspects of the debate can be published. Reviewers play a critical role in deciding what is worthy of publication. Searching for journal articles is only a part of what scientists must do. They are planning experiments and executing them, they may be teaching, they may be relating their basic findings to the development of a product, etc.  The browsing behaviors of scientists contribute to a philosophy of how science is actually prosecuted, but only if it deals with the larger realm of scientists' activities. A great deal has been said about the small fraction of a typical scientist's day that is in fact reflected in their publication record [Latour, 1987]. But the WWW and email suggest a new characterization of scientists' activities that includes traditional publication channels and informal artifacts similar to the letters and correspondence that have traditionally been the mainstay of historians of science [Rudwick, 1985].  Logical positivists aside, most current philosophy of science acknowledges the tremendous burden carried by the language used as scientists muddle toward a common understanding. This point has been made especially clear in biology by Keller and Lloyd's Keywords in Evolutionary Biology [Keller and Lloyd, 1992]. For these philosophers, the word "keyword" is not being used as it has in this text, as an element of an automatically assigned indexing vocabulary. Rather, they are interested in those key words that scientists use to talk to one another. As parts of scientific theories expressed in natural language, keywords  ... serve as conduits for unacknowledged, unbidden, and often unwelcome traffic between worlds. Words also have memories; they cae insinuate theoretical or cultural past into the present. Finally, they have force. Upon examination, their multiple shadows CONCLUSIONS AND FUTURE DIRECTIONS       311  and memories can be seen to perform real conceptual work, in science as in ordinary language. [Keller and Lloyd, 1992, p. 2]  Indeed, it is precisely because of the large overlap between forms of scientific thought and forms of societal thought that "keywords" - terms whose meanings chronically and insistently traverse the boundaries between ordinary and technical discourse can serve not simply as indicators of either social meanings and social change or scientific meaning and scientific change, but as indicators of the ongoing traffic between social and scientific meaning and, accordingly, between social and scientific change. [Keller and Lloyd, 1992, pp. 4-5, emphasis in original]  As suggested by Section 6.8, the emergence of vast data sets like those surrounding the Human Genome Project point to qualitatively different relationships between the keywords used as part of natural language by scientists and the data and theories to which they are meant to refer.
foa-0179	8.3.3 The Changing Economics of Publishing  Every aspect of the publishing business is undergoing radical transformation:  ï  the costs of production, creating bits rather than applying ink to paper;  ï  the means of distribution, permitting a file versus trucking pallets of books;  ï  even the fundamental social interactions, schmoozing with your agent over lunch versus contracting with someone you've never met;  ï  having all of these things occur in hours and days rather than months and years.  In such turbulent times, economic models that encompass many forms of interaction among readers, writers, editors, and publishers are very useful. M. Wellman provides one useful analysis of digital library economics.14  14 ai.eeCT.iimicli.edu/people/welJman/IJMeoon.htmI 312       FINDING OUT ABOUT  One easy dimension is the contrast between product and service. When a magazine is something you buy once a month at the grocery store, it is easiest to imagine it as a product. But when a personalized newspaper appears on your computer screen every morning, it seems more like a service.  Stefik has characterized the processing of content in terms of a refinery model [Stefik, 1995; Belew, 1985]. Crude, raw data are produced in large volumes (e.g., by satellites, video cameras, and so on). People who watch these data streams, and now automatic data mining algorithms, find interesting correlations and report on them. Individual observations and correlations are integrated into larger stories, related to prior work, and the like. At each stage of refining, the raw number of bits diminishes, but the information, and thus the knowledge (perhaps even the wisdom? I), is increased as the individual facts become integrated into more meaningful accounts. Along with each change comes increased economic value.  Another model for interaction comes from the open source model of software development. The best metaphor for this may be the hippies' notion of co-ops: Everyone contributes a little bit, so that a valuable resource is available to many. This model is most often applied to software development, but fine-grained "clipping services" and moderation-forhire newsgroups are very similar. More and more business models for such interactions are already being explored, with some paid for by  Computists is a     members and others filling more informal roles.^  good example             If publishers are to have a role in the future, the editorial enhance ment and other added value they provide beyond the authors' original content must be significant. For example, a key feature of publishing in academic journals is the authority conferred to a publication by the process of peer review. An article published in a major journal is not only made available to a wide audience, but it comes with a "seal of approval" of external validation by recognized experts in the field (viz., the editorial board and referees of their choosing) that the document represents significant work. The increased speed and reduced cost promised by electronic communication associated with electronic journals will not be realized unless social mechanisms such as these can be successfully transferred from the printed communication channel.  Another traditional distinction that electronically produced hypertext tends to blur is between the rough working notes and drafts that CONCLUSIONS AND FUTURE DIRECTIONS       313  an author maintains personally and the polished prose that is typically published. When two documents were both typed from scratch, there was every reason that they should be as different as possible. But as word processing technologies have allowed us to cut and paste, it seems that different versions of approximately the same document wind up being published independently. It is sometimes worthwhile saying the same thing more than once (e.g., with different audiences in mind), and in some fields it is common to first publish a shorter, in-progress report in a conference proceedings, with a more thorough and refined version of the same text appearing subsequently in a journal. But viewed cynically, the pressure on academics to "publish or perish," together with the increasing number of more and less formal publication channels, makes the ease of such self-plagiarism an issue. Just as funding agencies like the National Science Foundation now limit the number of publications that can be mentioned (in support of grant proposals or as part of an investigator's vita), publishers and professional societies may find it necessary to limit the raw quantity of publications in order to preserve their quality. Obviously, this text itself is an example of an experiment in electronic publishing, and hence in publishing economics. For more details about how the economics is intended to play out, see the final "(Active) Colophon."
foa-0180	8.3.4 Teachers and Students  "Finding out about" describes an activity from the perspective of the searcher, someone actively educating him- or herself. Note how similar activities like computer-aided education and distance learning are to FOA. They too present many documents to a user/student toward a similar educational goal. The critical difference is who is "driving" the educational process: In most educational settings we expect there is a teacher present, and the onus of the educational dialog is on the teacher.  In the semiotic terms of the last section, teachers and students (and sometimes authors and their readers) are involved in a special form of language game that might be called a tell/ask duality (see Figure 8.6). As author and reader, or teacher and student, become engaged in a conversation, they alternatively cede control of their shared attentional focus. A teacher explains by telling a story that takes some time to 314      FINDING OUT ABOUT  FIGURE 8.6 The Tell/Ask Duality  complete. Some of these stories are relatively self-contained, but most are only pieces of much larger stories. There must be opportunities for students to ask questions in this exchange, even as these necessarily interrupt the flow of the story.  Another way to appreciate the value of this pedagogical structure is to consider what happens when it is removed: When a query is asked of a search engine, the resulting hitlist is missing just this pedagogical structure. When trying to characterize what is most missing from hitlists, imagining a lecture that explains their relationships is one good characterization.  Our image of the classroom typically has a single teacher at the front and many, perhaps 30, students remaining quiet to hear the lecture. The teacher has prepared readings and made these available (and maybe the students have even read them:). The lecture is part of a larger curriculum, and the readings help to relate the current lecture to a larger question.  It is likely that there are simultaneously other teachers in other educational institutions teaching similar courses. For example, the NSF has sponsored the construction of a repository for Computer Science curricula.15 In every subject, there are texts other than the one selected  J 5 www. education, siggraph.org/ nsfcscr/nsfcscr.home. html CONCLUSIONS AND FUTURE DIRECTIONS       315  by a particular instructor. The student, especially the good student, may check out these other sources of information, but they must be on their guard to give special emphasis to those materials most likely to affect the grade they will ultimately receive from this teacher!  Now consider the serious (in the sense that they are there to be informed, not entertained) WWW surfer. They have a question in mind and hope that somewhere on the WWW they will find their answer. Their question is almost certainly much smaller than the question around which a course is defined. This surfer may choose, if he or she has many questions in a related area, to take the course, but the point is that one of the pieces of information this surfer may well see in his search is the curriculum for a course like the one just described.  There are many important questions about just how curricular materials available via the WWW can and should be used. They range from intellectual property issues (who owns them, the institution or the faculty?), to presentation media choices, to a reconsideration of exactly what is the importance of face-to-face meetings in the classroom.  Here we concentrate on the fundamental exchange of information: Who is attempting to learn what? In the class situation, the teacher is charged with presenting information that most efficiently allows these students to learn the concepts the instructor thinks are important. The surfer, on the other hand, is trying to make sense of the almost random set of documents in their hitlist. As weVe discussed, if the browsing user really knew the answer of their information need precisely, they wouldn't be surfing! Literate, intelligent surfers have remarkable skill at identifying documents that are likely to contain the answer to their question. In this sense they are both learner and teacher, trying to teach themselves.  Especially once they are beyond (compulsory, K-l 2) elementary education, the WWW is an excellent place for active students to construct their education. There are choices to be made between educational institutions and then between teachers of the same class. Students must pick a major discipline. Then there may be graduate school, and the process repeats itself. As the workforce becomes more Involved in continuing education, as distance learning becomes more possible and fashionable, as lifelong learning becomes a political objective, students will be actively seeking curricular units of all different sizes and scopes. Many of these questions resolve ultimately in economic issues: How much is a master^ 316      FINDING OUT ABOUT  degree worth? How much is tuition at two different schools? How much larger a salary can I earn if I have a certain education?  These changes go hand in hand with the changing institutional pressures on public and private educational systems. For example, corporations such as the Educational Testing Service (ETS) are being pressed to incorporate more holistic essay questions in place of easier-to-grade multiple-choice questions. As a consequence, textual classification techniques like those considered in Section 7.4 are being used to explore algorithmic e-rater16 computer grading of ETS essay questions [Larkey, 1998a]!  From the publisher's point of view, K-12 curricula are being divided into smaller curricular units. No longer is it necessary to buy an entire curriculum (grades K-6, mathematics) and have it adopted in toto by a school board. State guidelines have many facets, and publishers can equate units to these facets at a very fine grain of detail. Local curricular goals and teacher preferences can help to assemble units taken from various publishers and assembled like beads on strings. For the entrepreneurial teacher this provides an excellent opportunity to write curricula themselves, because they are in an excellent position to suggest ways that topics can be connected to guidelines.  As we build more and more autonomous agents (cf. Section 7.6), this interplay between teacher and student (now both software entities!) must transform our notions of who is controlling the dialog to one of mixed initiatives. Within the field of machine learning, we typically make the assumption that the learner is extremely passive. More recent analysis extends this to situations of active learning, where a large part of the problem is how informative the exemplars selected can be so as to most quickly learn.
foa-0181	8.4 Summary  And so we come full circle: This is a textbook about how to write a textbook. In writing it, I was writing the book I wished I read when I wrote my dissertation, 12 years too late.  For now, I leave you with this incomplete version. We have touched on many themes, too many to do real justice to any but the most  1 b www. ets. org/ researcii/erater. html CONCLUSIONS AND FUTURE DIRECTIONS       317  important. As the Einstein quote beginning this chapter suggests, in FOA it is not really the documents retrieved that matter, but the journey to them. The reader is encouraged to follow the bibliographic citations, and the more active pointers collected on the FOA WWW page}7 in order to really FOA (Find Out About).
foa-0182	(Active) Colophon  Production of this book would have been impossible without the use of a number of tools. Many of these were provided by their developers free of charge or as shareware. These include:  #  the emacs editor and related tools from Gnu Project;  ï  the HTjhX and TgK formatting systems, and the OzTex port to  MacQS;  ï  the gimp image manipulation package; and  *  the Linux operating system,  to name only a few. Said another way, this text should be considered a  beneficiary of the Open Source movement, as recently enunciated by E. S. Raymond's Cathedral and the Bazaar1 and earlier by GNU Manifesto2 by R. Stallman.  I firmly believe in the future importance of the Open Source model  and would like to express my appreciation in a more active way than simply listing the tools I've used. Cambridge University Press seems to believe there is at least some chance that FOA will make more money than it cost to produce; 111 believe that when I see it. Should that come  www. tuxedo, org/esr / writings / cathedralbazaar/ - www.gnu.org/gnu/the-gnu-project.html  318 (ACTIVE) COLOPHON      319  to pass, however, it is because you (the reader) bought it, and I think you should get some of the credit! I therefore propose the following, admittedly unusual, procedure:  After expenses directly related to producing FOAy I hereby commit to passing on 50 percent of all royalties I receive from FOA to organizations who have produced the tools listed above (and others). If you go to the FOA Web site 3and register your copy of the text, you will then be allowed to "vote" on which of these worthy organizations you'd like the money to go to; the Web page will contain full details.  3 http://www.cse.ucsd.edu/-rik/FOA/
iir_1	Boolean retrieval The meaning of the term information retrieval can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study, information retrieval might be defined thus: Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).   IR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term ``unstructured data'' refers to data which does not have clear, semantically overt, easy-for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly ``unstructured''. This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web pages). IR is also used to facilitate ``semistructured'' search such as finding a document where the title contains Java and the body contains threading. The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically. Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In web search , the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in webcharlink. At the other extreme is personal information retrieval . In the last few years, consumer operating systems have integrated information retrieval (such as Apple's Mac OS X Spotlight or Windows Vista's Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of enterprise, institutional, and domain-specific search , where retrieval might be provided for collections such as a corporation's internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios. In this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1 ) and the central inverted index data structure (Section 1.2 ). We will then examine the Boolean retrieval model and how Boolean queries are processed ( and 1.4 ).   Subsections An example information retrieval problem A first take at building an inverted index Processing Boolean queries The extended Boolean model versus ranked retrieval References and further reading
iir_10	XML retrieval Information retrieval systems are often contrasted with relational databases. Traditionally, IR systems have retrieved information from unstructured text - by which we mean ``raw'' text without markup. Databases are designed for querying relational data: sets of records that have values for predefined attributes such as employee number, title and salary. There are fundamental differences between information retrieval and database systems in terms of retrieval model, data structures and query language as shown in Table 10.1 .     RDB search unstructured retrieval structured retrieval objects records unstructured documents trees with text at leaves model relational model vector space   others ? main data structure table inverted index ? queries SQL free text queries ? RDB (relational database) search, unstructured information retrieval and structured information retrieval. There is no consensus yet as to which methods work best for structured retrieval although many researchers believe that XQuery (page 10.5 ) will become the standard for structured queries.  Some highly structured text search problems are most efficiently handled by a relational database, for example, if the employee table contains an attribute for short textual job descriptions and you want to find all employees who are involved with invoicing. In this case, the SQL query: select lastname from employees where job_desc like 'invoic%'; However, many structured data sources containing text are best modeled as structured documents rather than relational data. We call the search over such structured documents structured retrieval . Queries in structured retrieval can be either structured or unstructured, but we will assume in this chapter that the collection consists only of structured documents. Applications of structured retrieval include digital libraries , patent databases , , text in which entities like persons and locations have been tagged (in a process called ) and output from office suites like OpenOffice that save documents as marked up text. In all of these applications, we want to be able to run queries that combine textual criteria with structural criteria. Examples of such queries are give me a full-length article on fast fourier transforms (digital libraries), give me patents whose claims mention RSA public key encryption and that cite US patent 4,405,829 (patents), or give me articles about sightseeing tours of the Vatican and the Coliseum (entity-tagged text). These three queries are structured queries that cannot be answered well by an unranked retrieval system. As we argued in westlaw unranked retrieval models like the Boolean model suffer from low recall. For instance, an unranked system would return a potentially large number of articles that mention the Vatican, the Coliseum and sightseeing tours without ranking the ones that are most relevant for the query first. Most users are also notoriously bad at precisely stating structural constraints. For instance, users may not know for which structured elements the search system supports search. In our example, the user may be unsure whether to issue the query as sightseeing AND (COUNTRY:Vatican OR LANDMARK:Coliseum) , as sightseeing AND (STATE:Vatican OR BUILDING:Coliseum) or in some other form. Users may also be completely unfamiliar with structured search and advanced search interfaces or unwilling to use them. In this chapter, we look at how ranked retrieval methods can be adapted to structured documents to address these problems. We will only look at one standard for encoding structured documents: Extensible Markup Language or XML , which is currently the most widely used such standard. We will not cover the specifics that distinguish XML from other types of markup such as HTML and SGML. But most of what we say in this chapter is applicable to markup languages in general. In the context of information retrieval, we are only interested in XML as a language for encoding text and documents. A perhaps more widespread use of XML is to encode non-text data. For example, we may want to export data in XML format from an enterprise resource planning system and then read them into an analytics program to produce graphs for a presentation. This type of application of XML is called data-centric because numerical and non-text attribute-value data dominate and text is usually a small fraction of the overall data. Most data-centric XML is stored in databases - in contrast to the inverted index-based methods for text-centric XML that we present in this chapter. We call XML retrieval structured retrieval in this chapter. Some researchers prefer the term semistructured retrieval to distinguish XML retrieval from database querying. We have adopted the terminology that is widespread in the XML retrieval community. For instance, the standard way of referring to XML queries is structured queries , not semistructured queries . The term structured retrieval is rarely used for database querying and it always refers to XML retrieval in this book. There is a second type of information retrieval problem that is intermediate between unstructured retrieval and querying a relational database: parametric and zone search, which we discussed in Section 6.1 (page ). In the data model of parametric and zone search, there are parametric fields (relational attributes like date or file-size) and zones - text attributes that each take a chunk of unstructured text as value, e.g., author and title in Figure 6.1 (page ). The data model is flat, that is, there is no nesting of attributes. The number of attributes is small. In contrast, XML documents have the more complex tree structure that we see in Figure 10.2 in which attributes are nested. The number of attributes and nodes is greater than in parametric and zone search. After presenting the basic concepts of XML in Section 10.1 , this chapter first discusses the challenges we face in XML retrieval (Section 10.2 ). Next we describe a vector space model for XML retrieval (Section 10.3 ). Section 10.4 presents INEX, a shared task evaluation that has been held for a number of years and currently is the most important venue for XML retrieval research. We discuss the differences between data-centric and text-centric approaches to XML in Section 10.5 .   Subsections Basic XML concepts Challenges in XML retrieval A vector space model for XML retrieval Evaluation of XML retrieval Text-centric vs. data-centric XML retrieval References and further reading Exercises
iir_10_1	Basic XML concepts Figure 10.1: An XML document.  XML element  tag  XML attributes 10.1    Figure 10.2: The XML document in Figure 10.1 as a simplified DOM object. Figure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of text, e.g., Shakespeare, Macbeth, and Macbeth's castle. The tree's internal nodes encode either the structure of the document (title, act, and scene) or metadata functions (author). The standard for accessing and processing XML documents is the XML Document Object Model or DOM . The DOM represents elements, attributes and text within elements as nodes in a tree. Figure 10.2 is a simplified DOM representation of the XML document in Figure 10.1 .With a DOM API, we can process an XML document by starting at the root element and then descending down the tree from parents to children. XPath is a standard for enumerating paths in an XML document collection. We will also refer to paths as XML contexts or simply contexts in this chapter. Only a small subset of XPath is needed for our purposes. The XPath expression node selects all nodes of that name. Successive elements of a path are separated by slashes, so act/scene selects all scene elements whose parent is an act element. Double slashes indicate that an arbitrary number of elements can intervene on a path: play//scene selects all scene elements occurring in a play element. In Figure 10.2 this set consists of a single scene element, which is accessible via the path play, act, scene from the top. An initial slash starts the path at the root element. /play/title selects the play's title in Figure 10.1 , /play//title selects a set with two members (the play's title and the scene's title), and /scene/title selects no elements. For notational convenience, we allow the final element of a path to be a vocabulary term and separate it from the element path by the symbol #, even though this does not conform to the XPath standard. For example, title#"Macbeth" selects all titles containing the term Macbeth. We also need the concept of schema in this chapter. A schema puts constraints on the structure of allowable XML documents for a particular application. A schema for Shakespeare's plays may stipulate that scenes can only occur as children of acts and that only acts and scenes have the number attribute. Two standards for schemas for XML documents are XML DTD (document type definition) and XML Schema . Users can only write structured queries for an XML retrieval system if they have some minimal knowledge about the schema of the collection.  Figure 10.3: An XML query in NEXI format and its partial representation as a tree. A common format for XML queries is NEXI (Narrowed Extended XPath I). We give an example in Figure 10.3 . We display the query on four lines for typographical convenience, but it is intended to be read as one unit without line breaks. In particular, //section is embedded under //article. The query in Figure 10.3 specifies a search for sections about the summer holidays that are part of articles from 2001 or 2002. As in XPath double slashes indicate that an arbitrary number of elements can intervene on a path. The dot in a clause in square brackets refers to the element the clause modifies. The clause [.//yr = 2001 or .//yr = 2002] modifies //article. Thus, the dot refers to //article in this case. Similarly, the dot in [about(., summer holidays)] refers to the section that the clause modifies. The two yr conditions are relational attribute constraints. Only articles whose yr attribute is 2001 or 2002 (or that contain an element whose yr attribute is 2001 or 2002) are to be considered. The about clause is a ranking constraint: Sections that occur in the right type of article are to be ranked according to how relevant they are to the topic summer holidays.  Figure 10.4: Tree representation of XML documents and queries. We usually handle relational attribute constraints by prefiltering or postfiltering: We simply exclude all elements from the result set that do not meet the relational attribute constraints. In this chapter, we will not address how to do this efficiently and instead focus on the core information retrieval problem in XML retrieval, namely how to rank documents according to the relevance criteria expressed in the about conditions of the NEXI query. If we discard relational attributes, we can represent documents as trees with only one type of node: element nodes. In other words, we remove all attribute nodes from the XML document, such as the number attribute in Figure 10.1 . Figure 10.4 shows a subtree of the document in Figure 10.1 as an element-node tree (labeled ). We can represent queries as trees in the same way. This is a query-by-example approach to query language design because users pose queries by creating objects that satisfy the same formal description as documents. In Figure 10.4 , is a search for books whose titles score highly for the keywords Julius Caesar. is a search for books whose author elements score highly for Julius Caesar and whose title elements score highly for Gallic war.
iir_10_2	Challenges in XML retrieval In this section, we discuss a number of challenges that make structured retrieval more difficult than unstructured retrieval. Recall from page 10 the basic setting we assume in structured retrieval: the collection consists of structured documents and queries are either structured (as in Figure 10.3 ) or unstructured (e.g., summer holidays). The first challenge in structured retrieval is that users want us to return parts of documents (i.e., XML elements), not entire documents as IR systems usually do in unstructured retrieval. If we query Shakespeare's plays for Macbeth's castle, should we return the scene, the act or the entire play in Figure 10.2 ? In this case, the user is probably looking for the scene. On the other hand, an otherwise unspecified search for Macbeth should return the play of this name, not a subunit. One criterion for selecting the most appropriate part of a document is the structured document retrieval principle : Structured document retrieval principle. A system should always retrieve the most specific part of a document answering the query. title#"Macbeth" 10.2 Macbeth Macbeth's castle  Figure 10.5: Partitioning an XML document into non-overlapping indexing units. Parallel to the issue of which parts of a document to return to the user is the issue of which parts of a document to index. In Section 2.1.2 (page ), we discussed the need for a document unit or indexing unit in indexing and retrieval. In unstructured retrieval, it is usually clear what the right document unit is: files on your desktop, email messages, web pages on the web etc. In structured retrieval, there are a number of different approaches to defining the indexing unit. One approach is to group nodes into non-overlapping pseudodocuments as shown in Figure 10.5 . In the example, books, chapters and sections have been designated to be indexing units, but without overlap. For example, the leftmost dashed indexing unit contains only those parts of the tree dominated by book that are not already part of other indexing units. The disadvantage of this approach is that pseudodocuments may not make sense to the user because they are not coherent units. For instance, the leftmost indexing unit in Figure 10.5 merges three disparate elements, the class, author and title elements. We can also use one of the largest elements as the indexing unit, for example, the book element in a collection of books or the play element for Shakespeare's works. We can then postprocess search results to find for each book or play the subelement that is the best hit. For example, the query Macbeth's castle may return the play Macbeth, which we can then postprocess to identify act I, scene vii as the best-matching subelement. Unfortunately, this two-stage retrieval process fails to return the best subelement for many queries because the relevance of a whole book is often not a good predictor of the relevance of small subelements within it. Instead of retrieving large units and identifying subelements (top down), we can also search all leaves, select the most relevant ones and then extend them to larger units in postprocessing (bottom up). For the query Macbeth's castle in Figure 10.1 , we would retrieve the title Macbeth's castle in the first pass and then decide in a postprocessing step whether to return the title, the scene, the act or the play. This approach has a similar problem as the last one: The relevance of a leaf element is often not a good predictor of the relevance of elements it is contained in. The least restrictive approach is to index all elements. This is also problematic. Many XML elements are not meaningful search results, e.g., typographical elements like definitely or an ISBN number which cannot be interpreted without context. Also, indexing all elements means that search results will be highly redundant. For the query Macbeth's castle and the document in Figure 10.1 , we would return all of the play, act, scene and title elements on the path between the root node and Macbeth's castle. The leaf node would then occur four times in the result set, once directly and three times as part of other elements. We call elements that are contained within each other nested . Returning redundant nested elements in a list of returned hits is not very user-friendly. Because of the redundancy caused by nested elements it is common to restrict the set of elements that are eligible to be returned. Restriction strategies include: discard all small elements discard all element types that users do not look at (this requires a working XML retrieval system that logs this information) discard all element types that assessors generally do not judge to be relevant (if relevance assessments are available) only keep element types that a system designer or librarian has deemed to be useful search results  highlighting If the user knows the schema of the collection and is able to specify the desired type of element, then the problem of redundancy is alleviated as few nested elements have the same type. But as we discussed in the introduction, users often don't know what the name of an element in the collection is (Is the Vatican a country or a city?) or they may not know how to compose structured queries at all. A challenge in XML retrieval related to nesting is that we may need to distinguish different contexts of a term when we compute term statistics for ranking, in particular inverse document frequency ( idf ) statistics as defined in Section 6.2.1 (page ). For example, the term Gates under the node author is unrelated to an occurrence under a content node like section if used to refer to the plural of gate. It makes little sense to compute a single document frequency for Gates in this example. One solution is to compute idf for XML-contextterm pairs, e.g., to compute different idf weights for author#"Gates" and section#"Gates". Unfortunately, this scheme will run into sparse data problems - that is, many XML-context pairs occur too rarely to reliably estimate df (see Section 13.2 , page 13.2 , for a discussion of sparseness). A compromise is only to consider the parent node of the term and not the rest of the path from the root to to distinguish contexts. There are still conflations of contexts that are harmful in this scheme. For instance, we do not distinguish names of authors and names of corporations if both have the parent node name. But most important distinctions, like the example contrast author#"Gates" vs. section#"Gates", will be respected.  Figure 10.6: Schema heterogeneity: intervening nodes and mismatched names. In many cases, several different XML schemas occur in a collection since the XML documents in an IR application often come from more than one source. This phenomenon is called schema heterogeneity or schema diversity and presents yet another challenge. As illustrated in Figure 10.6 comparable elements may have different names: creator in vs. author in . In other cases, the structural organization of the schemas may be different: Author names are direct descendants of the node author in , but there are the intervening nodes firstname and lastname in . If we employ strict matching of trees, then will retrieve neither nor although both documents are relevant. Some form of approximate matching of element names in combination with semi-automatic matching of different document structures can help here. Human editing of correspondences of elements in different schemas will usually do better than automatic methods. Schema heterogeneity is one reason for query-document mismatches like and . Another reason is that users often are not familiar with the element names and the structure of the schemas of collections they search as mentioned. This poses a challenge for interface design in XML retrieval. Ideally, the user interface should expose the tree structure of the collection and allow users to specify the elements they are querying. If we take this approach, then designing the query interface in structured retrieval is more complex than a search box for keyword queries in unstructured retrieval. We can also support the user by interpreting all parent-child relationships in queries as descendant relationships with any number of intervening nodes allowed. We call such queries extended queries . The tree in Figure 10.3 and in Figure 10.6 are examples of extended queries. We show edges that are interpreted as descendant relationships as dashed arrows. In , a dashed arrow connects book and Gates. As a pseudo-XPath notation for , we adopt book//#"Gates": a book that somewhere in its structure contains the word Gates where the path from the book node to Gates can be arbitrarily long. The pseudo-XPath notation for the extended query that in addition specifies that Gates occurs in a section of the book is book//section//#"Gates". It is convenient for users to be able to issue such extended queries without having to specify the exact structural configuration in which a query term should occur - either because they do not care about the exact configuration or because they do not know enough about the schema of the collection to be able to specify it.  Figure 10.7: A structural mismatch between two queries and a document. In Figure 10.7 , the user is looking for a chapter entitled FFT (). Suppose there is no such chapter in the collection, but that there are references to books on FFT (). A reference to a book on FFT is not exactly what the user is looking for, but it is better than returning nothing. Extended queries do not help here. The extended query also returns nothing. This is a case where we may want to interpret the structural constraints specified in the query as hints as opposed to as strict conditions. As we will discuss in Section 10.4 , users prefer a relaxed interpretation of structural constraints: Elements that do not meet structural constraints perfectly should be ranked lower, but they should not be omitted from search results.
iir_10_3	A vector space model for XML retrieval  Figure 10.8: A mapping of an XML document (left) to a set of lexicalized subtrees (right). To take account of structure in retrieval in Figure 10.4 , we want a book entitled Julius Caesar to be a match for and no match (or a lower weighted match) for . In unstructured retrieval, there would be a single dimension of the vector space for Caesar. In XML retrieval, we must separate the title word Caesar from the author name Caesar. One way of doing this is to have each dimension of the vector space encode a word together with its position within the XML tree. Figure 10.8 illustrates this representation. We first take each text node (which in our setup is always a leaf) and break it into multiple nodes, one for each word. So the leaf node Bill Gates is split into two leaves Bill and Gates. Next we define the dimensions of the vector space to be lexicalized subtrees of documents - subtrees that contain at least one vocabulary term. A subset of these possible lexicalized subtrees is shown in the figure, but there are others - e.g., the subtree corresponding to the whole document with the leaf node Gates removed. We can now represent queries and documents as vectors in this space of lexicalized subtrees and compute matches between them. This means that we can use the vector space formalism from Chapter 6 for XML retrieval. The main difference is that the dimensions of vector space in unstructured retrieval are vocabulary terms whereas they are lexicalized subtrees in XML retrieval. There is a tradeoff between the dimensionality of the space and accuracy of query results. If we trivially restrict dimensions to vocabulary terms, then we have a standard vector space retrieval system that will retrieve many documents that do not match the structure of the query (e.g., Gates in the title as opposed to the author element). If we create a separate dimension for each lexicalized subtree occurring in the collection, the dimensionality of the space becomes too large. A compromise is to index all paths that end in a single vocabulary term, in other words, all XML-contextterm pairs. We call such an XML-contextterm pair a structural term and denote it by : a pair of XML-context and vocabulary term . The document in Figure 10.8 has nine structural terms. Seven are shown (e.g., "Bill" and Author#"Bill") and two are not shown: /Book/Author#"Bill" and /Book/Author#"Gates". The tree with the leaves Bill and Gates is a lexicalized subtree that is not a structural term. We use the previously introduced pseudo-XPath notation for structural terms. As we discussed in the last section users are bad at remembering details about the schema and at constructing queries that comply with the schema. We will therefore interpret all queries as extended queries - that is, there can be an arbitrary number of intervening nodes in the document for any parent-child node pair in the query. For example, we interpret in Figure 10.7 as . But we still prefer documents that match the query structure closely by inserting fewer additional nodes. We ensure that retrieval results respect this preference by computing a weight for each match. A simple measure of the similarity of a path in a query and a path in a document is the following context resemblance function CR: (52)        10.6            The final score for a document is computed as a variant of the cosine measure (Equation 24, page 6.3.1 ), which we call SIMNOMERGE for reasons that will become clear shortly. SIMNOMERGE is defined as follows: (53)          6    10.2  10.7  6.3.1 6.3.1   Figure 10.9: The algorithm for scoring documents with S IMN OM ERGE. IM O ERGE 10.9 normalizer 10.9  53  Figure 10.10: Scoring of a query with one structural term in S IMN OM ERGE. We give an example of how SIMNOMERGE computes query-document similarities in Figure 10.10 . is one of the structural terms in the query. We successively retrieve all postings lists for structural terms with the same vocabulary term . Three example postings lists are shown. For the first one, we have since the two contexts are identical. The next context has no context resemblance with : and the corresponding postings list is ignored. The context match of with is 0.63>0 and it will be processed. In this example, the highest ranking document is with a similarity of . To simplify the figure, the query weight of is assumed to be 1.0. The query-document similarity function in Figure 10.9 is called SIMNOMERGE because different XML contexts are kept separate for the purpose of weighting. An alternative similarity function is SIMMERGE which relaxes the matching conditions of query and document further in the following three ways. We collect the statistics used for computing and from all contexts that have a non-zero resemblance to (as opposed to just from as in SIMNOMERGE). For instance, for computing the document frequency of the structural term atl#"recognition", we also count occurrences of recognition in XML contexts fm/atl, article//atl etc. We modify Equation 53 by merging all structural terms in the document that have a non-zero context resemblance to a given query structural term. For example, the contexts /play/act/scene/title and /play/title in the document will be merged when matching against the query term /play/title#"Macbeth". The context resemblance function is further relaxed: Contexts have a non-zero resemblance in many cases where the definition of CR in Equation 52 returns 0. 10.6 These three changes alleviate the problem of sparse term statistics discussed in Section 10.2 and increase the robustness of the matching function against poorly posed structural queries. The evaluation of SIMNOMERGE and SIMMERGE in the next section shows that the relaxed matching conditions of SIMMERGE increase the effectiveness of XML retrieval. Exercises. Consider computing df for a structural term as the number of times that the structural term occurs under a particular parent node. Assume the following: the structural term author#"Herbert" occurs once as the child of the node squib; there are 10 squib nodes in the collection; occurs 1000 times as the child of article; there are 1,000,000 article nodes in the collection. The idf weight of then is when occurring as the child of squib and when occurring as the child of article. (i) Explain why this is not an appropriate weighting for . Why should not receive a weight that is three times higher in articles than in squibs? (ii) Suggest a better way of computing idf. Write down all the structural terms occurring in the XML document in Figure 10.8 . How many structural terms does the document in Figure 10.1 yield?
iir_10_4	Evaluation of XML retrieval   Table 10.2: INEX 2002 collection statistics. 12,107 number of documents 494 MB size 1995-2002 time of publication of articles 1,532 average number of XML nodes per document 6.9 average depth of a node 30 number of CAS topics 30 number of CO topics    Figure 10.11: Simplified schema of the documents in the INEX collection. The premier venue for research on XML retrieval is the INEX (INitiative for the Evaluation of XML retrieval) program, a collaborative effort that has produced reference collections, sets of queries, and relevance judgments. A yearly INEX meeting is held to present and discuss research results. The INEX 2002 collection consisted of about 12,000 articles from IEEE journals. We give collection statistics in Table 10.2 and show part of the schema of the collection in Figure 10.11 . The IEEE journal collection was expanded in 2005. Since 2006 INEX uses the much larger English Wikipedia as a test collection. The relevance of documents is judged by human assessors using the methodology introduced in Section 8.1 (page ), appropriately modified for structured documents as we will discuss shortly. Two types of information needs or in INEX are content-only or CO topics and content-and-structure (CAS) topics. CO topics are regular keyword queries as in unstructured information retrieval. CAS topics have structural constraints in addition to keywords. We already encountered an example of a CAS topic in Figure 10.3 . The keywords in this case are summer and holidays and the structural constraints specify that the keywords occur in a section that in turn is part of an article and that this article has an embedded year attribute with value 2001 or 2002. Since CAS queries have both structural and content criteria, relevance assessments are more complicated than in unstructured retrieval. INEX 2002 defined component coverage and topical relevance as orthogonal dimensions of relevance. The component coverage dimension evaluates whether the element retrieved is ``structurally'' correct, i.e., neither too low nor too high in the tree. We distinguish four cases: Exact coverage (E). The information sought is the main topic of the component and the component is a meaningful unit of information. Too small (S). The information sought is the main topic of the component, but the component is not a meaningful (self-contained) unit of information. Too large (L). The information sought is present in the component, but is not the main topic. No coverage (N). The information sought is not a topic of the component. The topical relevance dimension also has four levels: highly relevant (3), fairly relevant (2), marginally relevant (1) and nonrelevant (0). Components are judged on both dimensions and the judgments are then combined into a digit-letter code. 2S is a fairly relevant component that is too small and 3E is a highly relevant component that has exact coverage. In theory, there are 16 combinations of coverage and relevance, but many cannot occur. For example, a nonrelevant component cannot have exact coverage, so the combination 3N is not possible. The relevance-coverage combinations are quantized as follows: (54)  8.5.1 8.5.1 Q The number of relevant components in a retrieved set of components can then be computed as: (55)  8 10.6 One flaw of measuring relevance this way is that overlap is not accounted for. We discussed the concept of marginal relevance in the context of unstructured retrieval in Section 8.5.1 (page ). This problem is worse in XML retrieval because of the problem of multiple nested elements occurring in a search result as we discussed on page 10.2 . Much of the recent focus at INEX has been on developing algorithms and evaluation measures that return non-redundant results lists and evaluate them properly. See the references in Section 10.6 .   Table 10.3: INEX 2002 results of the vector space model in Section 10.3 for content-and-structure (CAS) queries and the quantization function Q. algorithm average precision SIMNOMERGE 0.242 SIMMERGE 0.271   Table 10.3 shows two INEX 2002 runs of the vector space system we described in Section 10.3 . The better run is the SIMMERGE run, which incorporates few structural constraints and mostly relies on keyword matching. SIMMERGE's median average precision (where the median is with respect to average precision numbers over topics) is only 0.147. Effectiveness in XML retrieval is often lower than in unstructured retrieval since XML retrieval is harder. Instead of just finding a document, we have to find the subpart of a document that is most relevant to the query. Also, XML retrieval effectiveness - when evaluated as described here - can be lower than unstructured retrieval effectiveness on a standard evaluation because graded judgments lower measured performance. Consider a system that returns a document with graded relevance 0.6 and binary relevance 1 at the top of the retrieved list. Then, interpolated precision at 0.00 recall (cf. page 8.4 ) is 1.0 on a binary evaluation, but can be as low as 0.6 on a graded evaluation.   Table 10.4: A comparison of content-only and full-structure search in INEX 2003/2004.   content only full structure improvement precision at 5 0.2000 0.3265 63.3% precision at 10 0.1820 0.2531 39.1% precision at 20 0.1700 0.1796 5.6% precision at 30 0.1527 0.1531 0.3%   Table 10.3 gives us a sense of the typical performance of XML retrieval, but it does not compare structured with unstructured retrieval. Table 10.4 directly shows the effect of using structure in retrieval. The results are for a language-model-based system (cf. Chapter 12 ) that is evaluated on a subset of CAS topics from INEX 2003 and 2004. The evaluation metric is precision at as defined in Chapter 8 (page 8.4 ). The discretization function used for the evaluation maps highly relevant elements (roughly corresponding to the 3E elements defined for Q) to 1 and all other elements to 0. The content-only system treats queries and documents as unstructured bags of words. The full-structure model ranks elements that satisfy structural constraints higher than elements that do not. For instance, for the query in Figure 10.3 an element that contains the phrase summer holidays in a section will be rated higher than one that contains it in an abstract. The table shows that structure helps increase precision at the top of the results list. There is a large increase of precision at and at . There is almost no improvement at . These results demonstrate the benefits of structured retrieval. Structured retrieval imposes additional constraints on what to return and documents that pass the structural filter are more likely to be relevant. Recall may suffer because some relevant documents will be filtered out, but for precision-oriented tasks structured retrieval is superior.
iir_10_5	Text-centric vs. data-centric XML retrieval  text-centric XML   In contrast, data-centric XML mainly encodes numerical and non-text attribute-value data. When querying data-centric XML, we want to impose exact match conditions in most cases. This puts the emphasis on the structural aspects of XML documents and queries. An example is: Find employees whose salary is the same this month as it was 12 months ago. Text-centric approaches are appropriate for data that are essentially text documents, marked up as XML to capture document structure. This is becoming a de facto standard for publishing text databases since most text documents have some form of interesting structure - paragraphs, sections, footnotes etc. Examples include assembly manuals, issues of journals, Shakespeare's collected works and newswire articles. Data-centric approaches are commonly used for data collections with complex structures that mainly contain non-text data. A text-centric retrieval engine will have a hard time with proteomic data in bioinformatics or with the representation of a city map that (together with street names and other textual descriptions) forms a navigational database. Two other types of queries that are difficult to handle in a text-centric structured retrieval model are joins and ordering constraints. The query for employees with unchanged salary requires a join. The following query imposes an ordering constraint: Retrieve the chapter of the book Introduction to algorithms that follows the chapter Binomial heaps.  Relational databases are better equipped to handle many structural constraints, particularly joins (but ordering is also difficult in a database framework - the tuples of a relation in the relational calculus are not ordered). For this reason, most data-centric XML retrieval systems are extensions of relational databases (see the references in Section 10.6 ). If text fields are short, exact matching meets user needs and retrieval results in form of unordered sets are acceptable, then using a relational database for XML retrieval is appropriate.
iir_10_6	References and further reading Harold and Means, 2004 10.1 van Rijsbergen, 1979 10.4 Gövert and Kazai (2003) Fuhr et al., 2003a Fuhr et al. (2003b) Fuhr et al. (2005) Fuhr et al. (2006) Fuhr et al. (2007) Fuhr and Lalmas (2007) 10.4 Kamps et al., 2006 Chu-Carroll et al. (2006) Lalmas and Tombros, 2007 Trotman et al. (2006) The structured document retrieval principle is due to Chiaramella et al. (1996). Figure 10.5 is from (Fuhr and Großjohann, 2004). Rahm and Bernstein (2001) give a survey of automatic schema matching that is applicable to XML. The vector-space based XML retrieval method in Section 10.3 is essentially IBM Haifa's JuruXML system as presented by Mass et al. (2003) and Carmel et al. (2003). Schlieder and Meuss (2002) and Grabs and Schek (2002) describe similar approaches. Carmel et al. (2003) represent queries as XML fragments . The trees that represent XML queries in this chapter are all XML fragments, but XML fragments also permit the operators , and phrase on content nodes. We chose to present the vector space model for XML retrieval because it is simple and a natural extension of the unstructured vector space model in Chapter 6 . But many other unstructured retrieval methods have been applied to XML retrieval with at least as much success as the vector space model. These methods include language models (cf. Chapter 12 , e.g., Kamps et al. (2004), Ogilvie and Callan (2005), List et al. (2005)), systems that use a relational database as a backend (Theobald et al., 2008;2005, Mihajlovic et al., 2005), probabilistic weighting (Lu et al., 2007), and fusion (Larson, 2005). There is currently no consensus as to what the best approach to XML retrieval is. Most early work on XML retrieval accomplished relevance ranking by focusing on individual terms, including their structural contexts, in query and document. As in unstructured information retrieval, there is a trend in more recent work to model relevance ranking as combining evidence from disparate measurements about the query, the document and their match. The combination function can be tuned manually (Sigurbjörnsson et al., 2004, Arvola et al., 2005) or trained using machine learning methods (Vittaut and Gallinari (2006), cf. mls). An active area of XML retrieval research is focused retrieval (Trotman et al., 2007), which aims to avoid returning nested elements that share one or more common subelements (cf. discussion in Section 10.2 , page 10.2 ). There is evidence that users dislike redundancy caused by nested elements (Betsi et al., 2006). Focused retrieval requires evaluation measures that penalize redundant results lists (Kazai and Lalmas, 2006, Lalmas et al., 2007). Trotman and Geva (2006) argue that XML retrieval is a form of passage retrieval . In passage retrieval (Kaszkiel and Zobel, 1997, Salton et al., 1993, Hearst and Plaunt, 1993, Hearst, 1997, Zobel et al., 1995), the retrieval system returns short passages instead of documents in response to a user query. While element boundaries in XML documents are cues for identifying good segment boundaries between passages, the most relevant passage often does not coincide with an XML element. In the last several years, the query format at INEX has been the NEXI standard proposed by Trotman and Sigurbjörnsson (2004). Figure 10.3 is from their paper. O'Keefe and Trotman (2004) give evidence that users cannot reliably distinguish the child and descendant axes. This justifies only permitting descendant axes in NEXI (and XML fragments). These structural constraints were only treated as ``hints'' in recent INEXes. Assessors can judge an element highly relevant even though it violates one of the structural constraints specified in a NEXI query. An alternative to structured query languages like NEXI is a more sophisticated user interface for query formulation (Tannier and Geva, 2005, van Zwol, 2006, Woodley and Geva, 2006). A broad overview of XML retrieval that covers database as well as IR approaches is given by Amer-Yahia and Lalmas (2006) and an extensive reference list on the topic can be found in (Amer-Yahia et al., 2005). Chapter 6 of Grossman and Frieder (2004) is a good introduction to structured text retrieval from a database perspective. The proposed standard for XQuery is available at http://www.w3.org/TR/xquery/ including an extension for full-text queries (Amer-Yahia et al., 2006): http://www.w3.org/TR/xquery-full-text/. Work that has looked at combining the relational database and the unstructured information retrieval approaches includes (Fuhr and Rölleke, 1997), (Navarro and Baeza-Yates, 1997), (Cohen, 1998), and (Chaudhuri et al., 2006).
iir_10_7	Exercises Exercises. Find a reasonably sized XML document collection (or a collection using a markup language different from XML like HTML) on the web and download it. Jon Bosak's XML edition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or the first 10,000 documents of the Wikipedia are good choices. Create three CAS topics of the type shown in Figure 10.3 that you would expect to do better than analogous CO topics. Explain why an XML retrieval system would be able to exploit the XML structure of the documents to achieve better retrieval results on the topics than an unstructured retrieval system. For the collection and the topics in Exercise 10.7 , (i) are there pairs of elements and , with a subelement of such that both answer one of the topics? Find one case each where (ii) (iii) is the better answer to the query. Implement the (i) SIMMERGE (ii) SIMNOMERGE algorithm in Section 10.3 and run it for the collection and the topics in Exercise 10.7 . (iii) Evaluate the results by assigning binary relevance judgments to the first five documents of the three retrieved lists for each algorithm. Which algorithm performs better? Are all of the elements in Exercise 10.7 appropriate to be returned as hits to a user or are there elements (as in the example definitely on page 10.2 ) that you would exclude? We discussed the tradeoff between accuracy of results and dimensionality of the vector space on page 10.3 . Give an example of an information need that we can answer correctly if we index all lexicalized subtrees, but cannot answer if we only index structural terms. If we index all structural terms, what is the size of the index as a function of text size? If we index all lexicalized subtrees, what is the size of the index as a function of text size? Give an example of a query-document pair for which is larger than 1.0.
iir_11	Probabilistic information retrieval During the discussion of relevance feedback in Section 9.1.2 , we observed that if we have some known relevant and nonrelevant documents, then we can straightforwardly start to estimate the probability of a term appearing in a relevant document , and that this could be the basis of a classifier that decides whether documents are relevant or not. In this chapter, we more systematically introduce this probabilistic approach to IR, which provides a different formal basis for a retrieval model and results in different techniques for setting term weights. Users start with information needs, which they translate into query representations. Similarly, there are documents, which are converted into document representations (the latter differing at least by how text is tokenized, but perhaps containing fundamentally less information, as when a non-positional index is used). Based on these two representations, a system tries to determine how well documents satisfy information needs. In the Boolean or vector space models of IR, matching is done in a formally defined but semantically imprecise calculus of index terms. Given only a query, an IR system has an uncertain understanding of the information need. Given the query and document representations, a system has an uncertain guess of whether a document has content relevant to the information need. Probability theory provides a principled foundation for such reasoning under uncertainty. This chapter provides one answer as to how to exploit this foundation to estimate how likely it is that a document is relevant to an information need. There is more than one possible retrieval model which has a probabilistic basis. Here, we will introduce probability theory and the Probability Ranking Principle (Sections 11.1 -11.2 ), and then concentrate on the Binary Independence Model (Section 11.3 ), which is the original and still most influential probabilistic retrieval model. Finally, we will introduce related but extended methods which use term counts, including the empirically successful Okapi BM25 weighting scheme, and Bayesian Network models for IR (Section 11.4 ). In Chapter 12 , we then present the alternative probabilistic language modeling approach to IR, which has been developed with considerable success in recent years.   Subsections Review of basic probability theory The Probability Ranking Principle The 1/0 loss case The PRP with retrieval costs The Binary Independence Model Deriving a ranking function for query terms Probability estimates in theory Probability estimates in practice Probabilistic approaches to relevance feedback An appraisal and some extensions An appraisal of probabilistic models Tree-structured dependencies between terms Okapi BM25: a non-binary model Bayesian network approaches to IR References and further reading
iir_11_1	Review of basic probability theory We hope that the reader has seen a little basic probability theory previously. We will give a very quick review; some references for further reading appear at the end of the chapter. A variable represents an event (a subset of the space of possible outcomes). Equivalently, we can represent the subset via a random variable , which is a function from outcomes to real numbers; the subset is the domain over which the random variable has a particular value. Often we will not know with certainty whether an event is true in the world. We can ask the probability of the event . For two events and , the joint event of both events occurring is described by the joint probability . The conditional probability expresses the probability of event given that event occurred. The fundamental relationship between joint and conditional probabilities is given by the chain rule : (56)  Writing for the complement of an event, we similarly have: (57)   partition rule    (58)  From these we can derive Bayes' Rule for inverting conditional probabilities: (59)    prior probability   posterior probability    likelihood    Finally, it is often useful to talk about the odds of an event, which provide a kind of multiplier for how probabilities change: (60)
iir_11_2_1	The 1/0 loss case We assume a ranked retrieval setup as in Section 6.3 , where there is a collection of documents, the user issues a query, and an ordered list of documents is returned. We also assume a binary notion of relevance as in Chapter 8 . For a query and a document in the collection, let be an indicator random variable that says whether is relevant with respect to a given query . That is, it takes on a value of 1 when the document is relevant and 0 otherwise. In context we will often write just for . Using a probabilistic model, the obvious order in which to present documents to the user is to rank documents by their estimated probability of relevance with respect to the information need: . This is the basis of the Probability Ranking Principle (PRP) (van Rijsbergen, 1979, 113-114): ``If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose, the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data.'' accuracy  1/0 loss     Bayes Optimal Decision Rule  (61)  Theorem. The PRP is optimal, in the sense that it minimizes the expected loss (also known as the Bayes risk ) under 1/0 loss. End theorem. The proof can be found in Ripley (1996). However, it requires that all probabilities are known correctly. This is never the case in practice. Nevertheless, the PRP still provides a very useful foundation for developing models of IR.
iir_11_2_2	The PRP with retrieval costs Suppose, instead, that we assume a model of retrieval costs. Let be the cost of not retrieving a relevant document and the cost of retrieval of a nonrelevant document. Then the Probability Ranking Principle says that if for a specific document and for all documents not yet retrieved (62)   8.6
iir_11_3	The Binary Independence Model The Binary Independence Model (BIM) we present in this section is the model that has traditionally been used with the PRP. It introduces some simple assumptions, which make estimating the probability function practical. Here, ``binary'' is equivalent to Boolean: documents and queries are both represented as binary term incidence vectors. That is, a document is represented by the vector where if term is present in document and if is not present in . With this representation, many possible documents have the same vector representation. Similarly, we represent by the incidence vector (the distinction between and is less central since commonly is in the form of a set of words). ``Independence'' means that terms are modeled as occurring in documents independently. The model recognizes no association between terms. This assumption is far from correct, but it nevertheless often gives satisfactory results in practice; it is the ``naive'' assumption of Naive Bayes models, discussed further in Section 13.4 (page ). Indeed, the Binary Independence Model is exactly the same as the multivariate Bernoulli Naive Bayes model presented in Section 13.3 (page ). In a sense this assumption is equivalent to an assumption of the vector space model, where each term is a dimension that is orthogonal to all other terms. We will first present a model which assumes that the user has a single step information need. As discussed in Chapter 9 , seeing a range of results might let the user refine their information need. Fortunately, as mentioned there, it is straightforward to extend the Binary Independence Model so as to provide a framework for relevance feedback, and we present this model in Section 11.3.4 . To make a probabilistic retrieval strategy precise, we need to estimate how terms in documents contribute to relevance, specifically, we wish to know how term frequency, document frequency, document length, and other statistics that we can compute influence judgments about document relevance, and how they can be reasonably combined to estimate the probability of document relevance. We then order documents by decreasing estimated probability of relevance. We assume here that the relevance of each document is independent of the relevance of other documents. As we noted in Section 8.5.1 (page ), this is incorrect: the assumption is especially harmful in practice if it allows a system to return duplicate or near duplicate documents. Under the BIM, we model the probability that a document is relevant via the probability in terms of term incidence vectors . Then, using Bayes rule, we have: (63) (64)            (65)    Subsections Deriving a ranking function for query terms Probability estimates in theory Probability estimates in practice Probabilistic approaches to relevance feedback
iir_11_3_1	Deriving a ranking function for query terms Given a query , we wish to order returned documents by descending . Under the BIM, this is modeled as ordering by . Rather than estimating this probability directly, because we are interested only in the ranking of documents, we work with some other quantities which are easier to compute and which give the same ordering of documents. In particular, we can rank documents by their odds of relevance (as the odds of relevance is monotonic with the probability of relevance). This makes things easier, because we can ignore the common denominator in Rxq-bayes, giving: (66)  The left term in the rightmost expression of Equation 66 is a constant for a given query. Since we are only ranking documents, there is thus no need for us to estimate it. The right-hand term does, however, require estimation, and this initially appears to be difficult: How can we accurately estimate the probability of an entire term incidence vector occurring? It is at this point that we make the Naive Bayes conditional independence assumption that the presence or absence of a word in a document is independent of the presence or absence of any other word (given the query): (67)   (68)    (69)       Let us make an additional simplifying assumption that terms not occurring in the query are equally likely to occur in relevant and nonrelevant documents: that is, if then . (This assumption can be changed, as when doing relevance feedback in Section 11.3.4 .) Then we need only consider terms in the products that appear in the query, and so, (70)  We can manipulate this expression by including the query terms found in the document into the right product, but simultaneously dividing through by them in the left product, so the value is unchanged. Then we have: (71)    Retrieval Status Value  (72)  So everything comes down to computing the . Define : (73)      odds ratio   7.1
iir_11_3_2	Probability estimates in theory For each term , what would these numbers look like for the whole collection? odds-ratio-ct-contingency gives a contingency table of counts of documents in the collection, where is the number of documents that contain term : Using this, and and (74)   add   (75)  Adding in this way is a simple form of smoothing. For trials with categorical outcomes (such as noting the presence or absence of a term), one way to estimate the probability of an event from data is simply to count the number of times an event occurred divided by the total number of trials. This is referred to as the relative frequency of the event. Estimating the probability as the relative frequency is the maximum likelihood estimate (or MLE ), because this value makes the observed data maximally likely. However, if we simply use the MLE, then the probability given to events we happened to see is usually too high, whereas other events may be completely unseen and giving them as a probability estimate their relative frequency of 0 is both an underestimate, and normally breaks our models, since anything multiplied by 0 is 0. Simultaneously decreasing the estimated probability of seen events and increasing the probability of unseen events is referred to as smoothing . One simple way of smoothing is to add a number to each of the observed counts. These pseudocounts correspond to the use of a uniform distribution over the vocabulary as a Bayesian prior , following Equation 59. We initially assume a uniform distribution over events, where the size of denotes the strength of our belief in uniformity, and we then update the probability based on observed events. Since our belief in uniformity is weak, we use . This is a form of maximum a posteriori ( MAP ) estimation, where we choose the most likely point value for probabilities based on the prior and the observed evidence, following Equation 59. We will further discuss methods of smoothing estimated counts to give probability models in Section 12.2.2 (page ); the simple method of adding to each observed count will do for now.
iir_11_3_3	Probability estimates in practice    (76)   idf 6.2.1 The approximation technique in Equation 76 cannot easily be extended to relevant documents. The quantity can be estimated in various ways: We can use the frequency of term occurrence in known relevant documents (if we know some). This is the basis of probabilistic approaches to relevance feedback weighting in a feedback loop, discussed in the next subsection. Croft and Harper (1979) proposed using a constant in their combination match model. For instance, we might assume that is constant over all terms in the query and that . This means that each term has even odds of appearing in a relevant document, and so the and factors cancel out in the expression for . Such an estimate is weak, but doesn't disagree violently with our hopes for the search terms appearing in many but not all relevant documents. Combining this method with our earlier approximation for , the document ranking is determined simply by which query terms occur in documents scaled by their idf weighting. For short documents (titles or abstracts) in situations in which iterative searching is undesirable, using this weighting term alone can be quite satisfactory, although in many other circumstances we would like to do better. Greiff (1998) argues that the constant estimate of in the Croft and Harper (1979) model is theoretically problematic and not observed empirically: as might be expected, is shown to rise with . Based on his data analysis, a plausible proposal would be to use the estimate . Iterative methods of estimation, which combine some of the above ideas, are discussed in the next subsection.
iir_11_3_4	Probabilistic approaches to relevance feedback We can use (pseudo-)relevance feedback, perhaps in an iterative process of estimation, to get a more accurate estimate of . The probabilistic approach to relevance feedback works as follows: Guess initial estimates of and . This can be done using the probability estimates of the previous section. For instance, we can assume that is constant over all in the query, in particular, perhaps taking . Use the current estimates of and to determine a best guess at the set of relevant documents . Use this model to retrieve a set of candidate relevant documents, which we present to the user. We interact with the user to refine the model of . We do this by learning from the user relevance judgments for some subset of documents . Based on relevance judgments, is partitioned into two subsets: and , which is disjoint from . We reestimate and on the basis of known relevant and nonrelevant documents. If the sets and are large enough, we may be able to estimate these quantities directly from these documents as maximum likelihood estimates: (77) (where is the set of documents in containing ). In practice, we usually need to smooth these estimates. We can do this by adding to both the count and to the number of relevant documents not containing the term, giving: (78) However, the set of documents judged by the user () is usually very small, and so the resulting statistical estimate is quite unreliable (noisy), even if the estimate is smoothed. So it is often better to combine the new information with the original guess in a process of Bayesian updating . In this case we have: (79) Here is the estimate for in an iterative updating process and is used as a Bayesian prior in the next iteration with a weighting of . Relating this equation back to Equation 59 requires a bit more probability theory than we have presented here (we need to use a beta distribution prior, conjugate to the Bernoulli random variable ). But the form of the resulting equation is quite straightforward: rather than uniformly distributing pseudocounts, we now distribute a total of pseudocounts according to the previous estimate, which acts as the prior distribution. In the absence of other evidence (and assuming that the user is perhaps indicating roughly 5 relevant or nonrelevant documents) then a value of around is perhaps appropriate. That is, the prior is strongly weighted so that the estimate does not change too much from the evidence provided by a very small number of documents. Repeat the above process from step 2, generating a succession of approximations to and hence , until the user is satisfied. It is also straightforward to derive a pseudo-relevance feedback version of this algorithm, where we simply pretend that . More briefly: Assume initial estimates for and as above. Determine a guess for the size of the relevant document set. If unsure, a conservative (too small) guess is likely to be best. This motivates use of a fixed size set of highest ranked documents. Improve our guesses for and . We choose from the methods of and 79 for re-estimating , except now based on the set instead of . If we let be the subset of documents in containing and use add smoothing , we get: (80) and if we assume that documents that are not retrieved are nonrelevant then we can update our estimates as: (81) Go to step 2 until the ranking of the returned results converges. Once we have a real estimate for then the weights used in the value look almost like a tf-idf value. For instance, using Equation 73, Equation 76, and Equation 80, we have: (82)     (83)  adding Exercises. Work through the derivation of Equation 74 from and 3()I . What are the differences between standard vector space tf-idf weighting and the BIM probabilistic retrieval model (in the case where no document relevance information is available)? Let be a random variable indicating whether the term appears in a document. Suppose we have relevant documents in the document collection and that in of the documents. Take the observed data to be just these observations of for each document in . Show that the MLE for the parameter , that is, the value for which maximizes the probability of the observed data, is . Describe the differences between vector space relevance feedback and probabilistic relevance feedback.
iir_11_4	An appraisal and some extensions   Subsections An appraisal of probabilistic models Tree-structured dependencies between terms Okapi BM25: a non-binary model Bayesian network approaches to IR
iir_11_4_1	An appraisal of probabilistic models Probabilistic methods are one of the oldest formal models in IR. Already in the 1970s they were held out as an opportunity to place IR on a firmer theoretical footing, and with the resurgence of probabilistic methods in computational linguistics in the 1990s, that hope has returned, and probabilistic methods are again one of the currently hottest topics in IR. Traditionally, probabilistic IR has had neat ideas but the methods have never won on performance. Getting reasonable approximations of the needed probabilities for a probabilistic IR model is possible, but it requires some major assumptions. In the BIM these are: a Boolean representation of documents/queries/relevance term independence terms not in the query don't affect the outcome document relevance values are independent Things started to change in the 1990s when the BM25 weighting scheme, which we discuss in the next section, showed very good performance, and started to be adopted as a term weighting scheme by many groups. The difference between ``vector space'' and ``probabilistic'' IR systems is not that great: in either case, you build an information retrieval scheme in the exact same way that we discussed in Chapter 7 . For a probabilistic IR system, it's just that, at the end, you score queries not by cosine similarity and tf-idf in a vector space, but by a slightly different formula motivated by probability theory. Indeed, sometimes people have changed an existing vector-space IR system into an effectively probabilistic system simply by adopted term weighting formulas from probabilistic models. In this section, we briefly present three extensions of the traditional probabilistic model, and in the next chapter, we look at the somewhat different probabilistic language modeling approach to IR.
iir_11_4_2	Tree-structured dependencies between terms   Some of the assumptions of the BIM can be removed. For example, we can remove the assumption that terms are independent. This assumption is very far from true in practice. A case that particularly violates this assumption is term pairs like Hong and Kong, which are strongly dependent. But dependencies can occur in various complex configurations, such as between the set of terms New, York, England, City, Stock, Exchange, and University. van Rijsbergen (1979) proposed a simple, plausible model which allowed a tree structure of term dependencies, as in Figure 11.1 . In this model each term can be directly dependent on only one other term, giving a tree structure of dependencies. When it was invented in the 1970s, estimation problems held back the practical success of this model, but the idea was reinvented as the Tree Augmented Naive Bayes model by Friedman and Goldszmidt (1996), who used it with some success on various machine learning data sets.
iir_11_4_3	Okapi BM25: a non-binary model The BIM was originally designed for short catalog records and abstracts of fairly consistent length, and it works reasonably in these contexts, but for modern full-text search collections, it seems clear that a model should pay attention to term frequency and document length, as in Chapter 6 . The BM25 weighting scheme , often called Okapi weighting , after the system in which it was first implemented, was developed as a way of building a probabilistic model sensitive to these quantities while not introducing too many additional parameters into the model (Spärck Jones et al., 2000). We will not develop the full theory behind the model here, but just present a series of forms that build up to the standard form now used for document scoring. The simplest score for document is just idf weighting of the query terms present, as in Equation 76: (84)   idf 75    (85)  We can improve on Equation 84 by factoring in the frequency of each term and document length: (86)               If the query is long, then we might also use similar weighting for query terms. This is appropriate if the queries are paragraph long information needs, but unnecessary for short queries. (87)       8.1    If we have relevance judgments available, then we can use the full form of smoothed-rf in place of the approximation introduced in prob-idf: (88)     (89)      11.3.4 Rather than just providing a term weighting method for terms in a user's query, relevance feedback can also involve augmenting the query (automatically or with manual review) with some (say, 10-20) of the top terms in the known-relevant documents as ordered by the relevance factor from Equation 75, and the above formula can then be used with such an augmented query vector . The BM25 term weighting formulas have been used quite widely and quite successfully across a range of collections and search tasks. Especially in the TREC evaluations, they performed well and were widely adopted by many groups. See Spärck Jones et al. (2000) for extensive motivation and discussion of experimental results.
iir_11_4_4	Bayesian network approaches to IR Turtle and Croft (1989;1991) introduced into information retrieval the use of Bayesian networks (Jensen and Jensen, 2001), a form of probabilistic graphical model. We skip the details because fully introducing the formalism of Bayesian networks would require much too much space, but conceptually, Bayesian networks use directed graphs to show probabilistic dependencies between variables, as in Figure 11.1 , and have led to the development of sophisticated algorithms for propagating influence so as to allow learning and inference with arbitrary knowledge within arbitrary directed acyclic graphs. Turtle and Croft used a sophisticated network to better model the complex dependencies between a document and a user's information need. The model decomposes into two parts: a document collection network and a query network. The document collection network is large, but can be precomputed: it maps from documents to terms to concepts. The concepts are a thesaurus-based expansion of the terms appearing in the document. The query network is relatively small but a new network needs to be built each time a query comes in, and then attached to the document network. The query network maps from query terms, to query subexpressions (built using probabilistic or ``noisy'' versions of AND and OR operators), to the user's information need. The result is a flexible probabilistic network which can generalize various simpler Boolean and probabilistic models. Indeed, this is the primary case of a statistical ranked retrieval model that naturally supports structured query operators. The system allowed efficient large-scale retrieval, and was the basis of the InQuery text retrieval system, built at the University of Massachusetts. This system performed very well in TREC evaluations and for a time was sold commercially. On the other hand, the model still used various approximations and independence assumptions to make parameter estimation and computation possible. There has not been much follow-on work along these lines, but we would note that this model was actually built very early on in the modern era of using Bayesian networks, and there have been many subsequent developments in the theory, and the time is perhaps right for a new generation of Bayesian network-based information retrieval systems.
iir_11_5	References and further reading Longer introductions to probability theory can be found in most introductory probability and statistics books, such as (Ross, 2006, Grinstead and Snell, 1997, Rice, 2006). An introduction to Bayesian utility theory can be found in (Ripley, 1996). The probabilistic approach to IR originated in the UK in the 1950s. The first major presentation of a probabilistic model is Maron and Kuhns (1960). Robertson and Jones (1976) introduce the main foundations of the BIM and van Rijsbergen (1979) presents in detail the classic BIM probabilistic model. The idea of the PRP is variously attributed to S. E. Robertson, M. E. Maron and W. S. Cooper (the term ``Probabilistic Ordering Principle'' is used in Robertson and Jones (1976), but PRP dominates in later work). Fuhr (1992) is a more recent presentation of probabilistic IR, which includes coverage of other approaches such as probabilistic logics and Bayesian networks. Crestani et al. (1998) is another survey. Spärck Jones et al. (2000) is the definitive presentation of probabilistic IR experiments by the ``London school'', and Robertson (2005) presents a retrospective on the group's participation in TREC evaluations, including detailed discussion of the Okapi BM25 scoring function and its development. Robertson et al. (2004) extend BM25 to the case of multiple weighted fields. The open-source Indri search engine, which is distributed with the Lemur toolkit (http://www.lemurproject.org/) merges ideas from Bayesian inference networks and statistical language modeling approaches (see Chapter 12 ), in particular preserving the former's support for structured query operators.
iir_12	Language models for information retrieval A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. This approach thus provides a different realization of some of the basic ideas for document ranking which we saw in Section 6.2 (page ). Instead of overtly modeling the probability of relevance of a document to a query , as in the traditional probabilistic approach to IR (Chapter 11 ), the basic language modeling approach instead builds a probabilistic language model from each document , and ranks documents based on the probability of the model generating the query: . In this chapter, we first introduce the concept of language models (Section 12.1 ) and then describe the basic and most commonly used language modeling approach to IR, the Query Likelihood Model (Section 12.2 ). After some comparisons between the language modeling approach and other approaches to IR (Section 12.3 ), we finish by briefly describing various extensions to the language modeling approach (Section 12.4 ).   Subsections Language models Finite automata and language models Types of language models Multinomial distributions over words The query likelihood model Using query likelihood language models in IR Estimating the query generation probability Ponte and Croft's Experiments Language modeling versus other approaches in IR Extended language modeling approaches References and further reading
iir_12_1_1	Finite automata and language models   What do we mean by a document model generating a query? A traditional generative model of a language, of the kind familiar from formal language theory, can be used either to recognize or to generate strings. For example, the finite automaton shown in Figure 12.1 can generate strings that include the examples shown. The full set of strings that can be generated is called the language of the automaton.   If instead each node has a probability distribution over generating different terms, we have a language model. The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary. That is, for a language model over an alphabet : (90)   12.2 Worked example. To find the probability of a word sequence, we just multiply the probabilities which the model gives to each word in the sequence, together with the probability of continuing or stopping after producing each word. For example, (91)     (92)   (93)   frog 90 STOP   likelihood ratio  12.1.3 End worked example.  Figure 12.3: Partial specification of two unigram language models. Worked example. Suppose, now, that we have two language models and , shown partially in Figure 12.3 . Each gives a probability estimate to a sequence of terms, as already illustrated in m1probability. The language model that gives the higher probability to the sequence of terms is more likely to have generated the term sequence. This time, we will omit STOP probabilities from our calculations. For the sequence shown, we get: and we see that . We present the formulas here in terms of products of probabilities, but, as is common in probabilistic applications, in practice it is usually best to work with sums of log probabilities (cf. page 13.2 ). End worked example.
iir_12_1_2	Types of language models How do we build probabilities over sequences of terms? We can always use the chain rule from Equation 56 to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events: (94)   unigram language model  (95)   bigram language models  (96)   speech recognition  spelling correction  machine translation  sparseness 13.2  bias-variance tradeoff 11 11.4.2
iir_12_1_3	Multinomial distributions over words Under the unigram language model the order of words is irrelevant, and so such models are often called ``bag of words'' models, as discussed in Chapter 6 (page 6.2 ). Even though there is no conditioning on preceding context, this model nevertheless still gives the probability of a particular ordering of terms. However, any other ordering of this bag of terms will have the same probability. So, really, we have a multinomial distribution over words. So long as we stick to unigram models, the language model name and motivation could be viewed as historical rather than necessary. We could instead just refer to the model as a multinomial model. From this perspective, the equations presented above do not present the multinomial probability of a bag of words, since they do not sum over all possible orderings of those words, as is done by the multinomial coefficient (the first term on the right-hand side) in the standard presentation of a multinomial model: (97)     STOP 13.2  The fundamental problem in designing language models is that we do not know what exactly we should use as the model . However, we do generally have a sample of text that is representative of that model. This problem makes a lot of sense in the original, primary uses of language models. For example, in speech recognition, we have a training sample of (spoken) text. But we have to expect that, in the future, users will use different words and in different sequences, which we have never observed before, and so the model has to generalize beyond the observed data to allow unknown words and sequences. This interpretation is not so clear in the IR case, where a document is finite and usually fixed. The strategy we adopt in IR is as follows. We pretend that the document is only a representative sample of text drawn from a model distribution, treating it like a fine-grained topic. We then estimate a language model from this sample, and use that model to calculate the probability of observing any word sequence, and, finally, we rank documents according to their probability of generating the query. Exercises. Including stop probabilities in the calculation, what will the sum of the probability estimates of all strings in the language of length 1 be? Assume that you generate a word and then decide whether to stop or not (i.e., the null string is not part of the language). If the stop probability is omitted from calculations, what will the sum of the scores assigned to strings in the language of length 1 be? What is the likelihood ratio of the document according to and in m1m2compare? No explicit STOP probability appeared in m1m2compare. Assuming that the STOP probability of each model is 0.1, does this change the likelihood ratio of a document according to the two models? How might a language model be used in a spelling correction system? In particular, consider the case of context-sensitive spelling correction, and correcting incorrect usages of words, such as their in Are you their? (See Section 3.5 (page ) for pointers to some literature on this topic.)
iir_12_2_1	Using query likelihood language models in IR Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model . In it, we construct from each document in the collection a language model . Our goal is to rank documents by , where the probability of a document is interpreted as the likelihood that it is relevant to the query. Using Bayes rule (as introduced in probirsec), we have: (98)        The most common way to do this is using the multinomial unigram language model, which is equivalent to a multinomial Naive Bayes model (page 13.3 ), where the documents are the classes, each treated in the estimation as a separate ``language''. Under this model, we have that: (99)    For retrieval based on a language model (henceforth LM ), we treat the generation of queries as a random process. The approach is to Infer a LM for each document. Estimate , the probability of generating the query according to each of these document models. Rank the documents according to these probabilities.
iir_12_2_2	Estimating the query generation probability In this section we describe how to estimate . The probability of producing the query given the LM of document using maximum likelihood estimation ( MLE ) and the unigram assumption is: (100)          11.3.2  The classic problem with using language models is one of estimation (the symbol on the P's is used above to stress that the model is estimated): terms appear very sparsely in documents. In particular, some words will not have appeared in the document at all, but are possible words for the information need, which the user may have used in the query. If we estimate for a term missing from a document , then we get a strict conjunctive semantics: documents will only give a query non-zero probability if all of the query terms appear in the document. Zero probabilities are clearly a problem in other uses of language models, such as when predicting the next word in a speech recognition application, because many words will be sparsely represented in the training data. It may seem rather less clear whether this is problematic in an IR application. This could be thought of as a human-computer interface issue: vector space systems have generally preferred more lenient matching, though recent web search developments have tended more in the direction of doing searches with such conjunctive semantics. Regardless of the approach here, there is a more general problem of estimation: occurring words are also badly estimated; in particular, the probability of words occurring once in the document is normally overestimated, since their one occurrence was partly by chance. The answer to this (as we saw in probtheory) is smoothing. But as people have come to understand the LM approach better, it has become apparent that the role of smoothing in this model is not only to avoid zero probabilities. The smoothing of terms actually implements major parts of the term weighting component (Exercise 12.2.3 ). It is not just that an unsmoothed model has conjunctive semantics; an unsmoothed model works badly because it lacks parts of the term weighting component. Thus, we need to smooth probabilities in our document language models: to discount non-zero probabilities and to give some probability mass to unseen words. There's a wide space of approaches to smoothing probability distributions to deal with this problem. In Section 11.3.2 (page ), we already discussed adding a number (1, 1/2, or a small ) to the observed counts and renormalizing to give a probability distribution.In this section we will mention a couple of other smoothing methods, which involve combining observed counts with a more general reference probability distribution. The general approach is that a non-occurring term should be possible in a query, but its probability should be somewhat close to but no more likely than would be expected by chance from the whole collection. That is, if then (101)     (102)     linear interpolation   An alternative is to use a language model built from the whole collection as a prior distribution in a Bayesian updating process (rather than a uniform distribution, as we saw in Section 11.3.2 ). We then get the following equation: (103)  Both of these smoothing methods have been shown to perform well in IR experiments; we will stick with the linear interpolation smoothing method for the rest of this section. While different in detail, they are both conceptually similar: in both cases the probability estimate for a word present in the document combines a discounted MLE and a fraction of the estimate of its prevalence in the whole collection, while for words not present in a document, the estimate is just a fraction of the estimate of the prevalence of the word in the whole collection. The role of smoothing in LMs for IR is not simply or principally to avoid estimation problems. This was not clear when the models were first proposed, but it is now understood that smoothing is essential to the good properties of the models. The reason for this is explored in Exercise 12.2.3 . The extent of smoothing in these two models is controlled by the and parameters: a small value of or a large value of means more smoothing. This parameter can be tuned to optimize performance using a line search (or, for the linear interpolation model, by other methods, such as the expectation maximimization algorithm; see modelclustering). The value need not be a constant. One approach is to make the value a function of the query size. This is useful because a small amount of smoothing (a ``conjunctive-like'' search) is more suitable for short queries, while a lot of smoothing is more suitable for long queries. To summarize, the retrieval ranking for a query under the basic LM for IR we have been considering is given by: (104)   Worked example. Suppose the document collection contains two documents: : Xyzzy reports a profit but revenue is down : Quorus narrows quarter loss but revenue decreases further  Suppose the query is revenue down. Then: (105)   (106) (107)   (108)    End worked example.
iir_12_2_3	Ponte and Croft's Experiments   Ponte and Croft (1998) present the first experiments on the language modeling approach to information retrieval. Their basic approach is the model that we have presented until now. However, we have presented an approach where the language model is a mixture of two multinomials, much as in (Miller et al., 1999, Hiemstra, 2000) rather than Ponte and Croft's multivariate Bernoulli model. The use of multinomials has been standard in most subsequent work in the LM approach and experimental results in IR, as well as evidence from text classification which we consider in Section 13.3 (page ), suggests that it is superior. Ponte and Croft argued strongly for the effectiveness of the term weights that come from the language modeling approach over traditional tf-idf weights. We present a subset of their results in Figure 12.4 where they compare tf-idf to language modeling by evaluating TREC topics 202-250 over TREC disks 2 and 3. The queries are sentence-length natural language queries. The language modeling approach yields significantly better results than their baseline tf-idf based term weighting approach. And indeed the gains shown here have been extended in subsequent work. Exercises. Consider making a language model from the following training text: the martian has landed on the latin pop sensation ricky martin Under a MLE-estimated unigram probability model, what are and ? Under a MLE-estimated bigram model, what are and ? Suppose we have a collection that consists of the 4 documents given in the below table. docID Document text 1 click go the shears boys click click click 2 click click 3 metal here 4 metal shears click here Build a query likelihood language model for this document collection. Assume a mixture model between the documents and the collection, with both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. Work out the model probabilities of the queries click, shears, and hence click shears for each document, and use those probabilities to rank the documents returned by each query. Fill in these probabilities in the below table: Query Doc 1 Doc 2 Doc 3 Doc 4 click         shears         click shears         What is the final ranking of the documents for the query click shears? Using the calculations in Exercise 12.2.3 as inspiration or as examples where appropriate, write one sentence each describing the treatment that the model in Equation 102 gives to each of the following quantities. Include whether it is present in the model or not and whether the effect is raw or scaled. Term frequency in a document Collection frequency of a term Document frequency of a term Length normalization of a term In the mixture model approach to the query likelihood model (Equation 104), the probability estimate of a term is based on the term frequency of a word in a document, and the collection frequency of the word. Doing this certainly guarantees that each term of a query (in the vocabulary) has a non-zero chance of being generated by each document. But it has a more subtle but important effect of implementing a form of term weighting, related to what we saw in Chapter 6 . Explain how this works. In particular, include in your answer a concrete numeric example showing this term weighting at work.
iir_12_3	Language modeling versus other approaches in IR The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech and language processing. As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used, and hence the performance of the model. The major issue is estimation of the document model, such as choices of how to smooth it effectively. The model has achieved very good retrieval results. Compared to other probabilistic approaches, such as the BIM from Chapter 11 , the main difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the BIM approach). But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss. The LM approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the tools and methods of language modeling from speech and natural language processing. The resulting model is mathematically precise, conceptually simple, computationally tractable, and intuitively appealing. This seems similar to the situation with XML retrieval (Chapter 10 ): there the approaches that assume queries and documents are objects of the same type are also among the most successful. On the other hand, like all IR models, you can also raise objections to the model. The assumption of equivalence between document and information need representation is unrealistic. Current LM approaches use very simple models of language, usually unigram models. Without an explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences. It also seems necessary to move beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators. Subsequent work in the LM approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch between the query language and the document language. The model has significant relations to traditional tf-idf models. Term frequency is directly represented in tf-idf models, and much recent work has recognized the importance of document length normalization. The effect of doing a mixture of document generation probability with collection generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents. In most concrete realizations, the models share treating terms as if they were independent. On the other hand, the intuitions are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term frequency and document length are used differ. If you are concerned mainly with performance numbers, recent work has shown the LM approach to be very effective in retrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is perhaps still insufficient evidence that its performance so greatly exceeds that of a well-tuned traditional vector space retrieval system as to justify changing an existing implementation.
iir_12_4	Extended language modeling approaches In this section we briefly mention some of the work that extends the basic language modeling approach. There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model generating the query, you can look at the probability of a query language model generating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less text available to estimate a language model based on the query text, and so the model will be worse estimated, and will have to depend more on being smoothed with some other language model. On the other hand, it is easy to see how to incorporate relevance feedback into such a model: you can expand the query with terms taken from relevant documents in the usual way and hence update the language model (Zhai and Lafferty, 2001a). Indeed, with appropriate modeling choices, this approach leads to the BIM model of Chapter 11 . The relevance model of Lavrenko and Croft (2001) is an instance of a document likelihood model, which incorporates pseudo-relevance feedback into a language modeling approach. It achieves very strong empirical results.  Figure 12.5: Three ways of developing the language modeling approach: (a) query likelihood, (b) document likelihood, and (c) model comparison. Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other. Lafferty and Zhai (2001) lay out these three ways of thinking about the problem, which we show in Figure 12.5 , and develop a general risk minimization approach for document retrieval. For instance, one way to model the risk of returning a document as relevant to a query is to use the Kullback-Leibler (KL) divergence between their respective language models: (109)    Manning and Schütze, 1999 Cover and Thomas, 1991 Lafferty and Zhai (2001) Kraaij and Spitters (2003)  Basic LMs do not address issues of alternate expression, that is, synonymy, or any deviation in use of language between queries and documents. Berger and Lafferty (1999) introduce translation models to bridge this query-document gap. A translation model lets you generate query words not in a document by translation to alternate terms with similar meaning. This also provides a basis for performing cross-language IR. We assume that the translation model can be represented by a conditional probability distribution between vocabulary terms. The form of the translation query generation model is then: (110)    Building extended LM approaches remains an active area of research. In general, translation models, relevance feedback models, and model comparison approaches have all been demonstrated to improve performance over the basic query likelihood LM.
iir_12_5	References and further reading For more details on the basic concepts of probabilistic language models and techniques for smoothing, see either Manning and Schütze (1999, Chapter 6) or Jurafsky and Martin (2008, Chapter 4). The important initial papers that originated the language modeling approach to IR are: (Berger and Lafferty, 1999, Ponte and Croft, 1998, Miller et al., 1999, Hiemstra, 1998). Other relevant papers can be found in the next several years of SIGIR proceedings. (Croft and Lafferty, 2003) contains a collection of papers from a workshop on language modeling approaches and Hiemstra and Kraaij (2005) review one prominent thread of work on using language modeling approaches for TREC tasks. Zhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR and present detailed empirical comparisons of different smoothing methods. Zaragoza et al. (2003) advocate using full Bayesian predictive distributions rather than MAP point estimates, but while they outperform Bayesian smoothing, they fail to outperform a linear interpolation. Zhai and Lafferty (2002) argue that a two-stage smoothing model with first Bayesian smoothing followed by linear interpolation gives a good model of the task, and performs better and more stably than a single form of smoothing. A nice feature of the LM approach is that it provides a convenient and principled way to put various kinds of prior information into the model; Kraaij et al. (2002) demonstrate this by showing the value of link information as a prior in improving web entry page retrieval performance. As briefly discussed in Chapter 16 (page 16.1 ), Liu and Croft (2004) show some gains by smoothing a document LM with estimates from a cluster of similar documents; Tao et al. (2006) report larger gains by doing document-similarity based smoothing. Hiemstra and Kraaij (2005) present TREC results showing a LM approach beating use of BM25 weights. Recent work has achieved some gains by going beyond the unigram model, providing the higher order models are smoothed with lower order models (Cao et al., 2005, Gao et al., 2004), though the gains to date remain modest. Spärck Jones (2004) presents a critical viewpoint on the rationale for the language modeling approach, but Lafferty and Zhai (2003) argue that a unified account can be given of the probabilistic semantics underlying both the language modeling approach presented in this chapter and the classical probabilistic information retrieval approach of Chapter 11 . The Lemur Toolkit (http://www.lemurproject.org/) provides a flexible open source framework for investigating language modeling approaches to IR.
iir_13	Text classification and Naive Bayes Thus far, this book has mainly discussed the process of ad hoc retrieval , where users have transient information needs that they try to address by posing one or more queries to a search engine. However, many users have ongoing information needs. For example, you might need to track developments in multicore computer chips. One way of doing this is to issue the query multicore and computer and chip against an index of recent newswire articles each morning. In this and the following two chapters we examine the question: How can this repetitive task be automated? To this end, many systems support standing queries . A standing query is like any other query except that it is periodically executed on a collection to which new documents are incrementally added over time. If your standing query is just multicore and computer and chip, you will tend to miss many relevant new articles which use other terms such as multicore processors. To achieve good recall, standing queries thus have to be refined over time and can gradually become quite complex. In this example, using a Boolean search engine with stemming, you might end up with a query like (multicore or multi-core) and (chip or processor or microprocessor). To capture the generality and scope of the problem space to which standing queries belong, we now introduce the general notion of a classification problem. Given a set of classes, we seek to determine which class(es) a given object belongs to. In the example, the standing query serves to divide new newswire articles into the two classes: documents about multicore computer chips and documents not about multicore computer chips. We refer to this as two-class classification. Classification using standing queries is also called routing or filtering and will be discussed further in Section 15.3.1 (page ). A class need not be as narrowly focused as the standing query multicore computer chips. Often, a class is a more general subject area like China or coffee. Such more general classes are usually referred to as topics , and the classification task is then called text classification , text categorization , topic classification , or topic spotting . An example for China appears in Figure 13.1 . Standing queries and topics differ in their degree of specificity, but the methods for solving routing, filtering, and text classification are essentially the same. We therefore include routing and filtering under the rubric of text classification in this and the following chapters. The notion of classification is very general and has many applications within and beyond information retrieval (IR). For instance, in computer vision, a classifier may be used to divide images into classes such as landscape, portrait, and neither. We focus here on examples from information retrieval such as:  Several of the preprocessing steps necessary for indexing as discussed in Chapter 2 : detecting a document's encoding (ASCII, Unicode UTF-8 etc; page 2.1.1 ); word segmentation (Is the white space between two letters a word boundary or not? page 24 ) ; truecasing (page 2.2.3 ); and identifying the language of a document (page 2.5 ). The automatic detection of spam pages (which then are not included in the search engine index). The automatic detection of sexually explicit content (which is included in search results only if the user turns an option such as SafeSearch off). Sentiment detection or the automatic classification of a movie or product review as positive or negative. An example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems. Personal email sorting . A user may have folders like talk announcements, electronic bills, email from family and friends, and so on, and may want a classifier to classify each incoming email and automatically move it to the appropriate folder. It is easier to find messages in sorted folders than in a very large inbox. The most common case of this application is a spam folder that holds all suspected spam messages. Topic-specific or vertical search. Vertical search engines restrict searches to a particular topic. For example, the query computer science on a vertical search engine for the topic China will return a list of Chinese computer science departments with higher precision and recall than the query computer science China on a general purpose search engine. This is because the vertical search engine does not include web pages in its index that contain the term china in a different sense (e.g., referring to a hard white ceramic), but does include relevant pages even if they do not explicitly mention the term China. Finally, the ranking function in ad hoc information retrieval can also be based on a document classifier as we will explain in Section 15.4 (page ). This list shows the general importance of classification in IR. Most retrieval systems today contain multiple components that use some form of classifier. The classification task we will use as an example in this book is text classification. A computer is not essential for classification. Many classification tasks have traditionally been solved manually. Books in a library are assigned Library of Congress categories by a librarian. But manual classification is expensive to scale. The multicore computer chips example illustrates one alternative approach: classification by the use of standing queries - which can be thought of as rules - most commonly written by hand. As in our example (multicore or multi-core) and (chip or processor or microprocessor), rules are sometimes equivalent to Boolean expressions. A rule captures a certain combination of keywords that indicates a class. Hand-coded rules have good scaling properties, but creating and maintaining them over time is labor intensive. A technically skilled person (e.g., a domain expert who is good at writing regular expressions) can create rule sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly; however, it can be hard to find someone with this specialized skill. Apart from manual classification and hand-crafted rules, there is a third approach to text classification, namely, machine learning-based text classification. It is the approach that we focus on in the next several chapters. In machine learning, the set of rules or, more generally, the decision criterion of the text classifier, is learned automatically from training data. This approach is also called statistical text classification if the learning method is statistical. In statistical text classification, we require a number of good example documents (or training documents) for each class. The need for manual classification is not eliminated because the training documents come from a person who has labeled them - where labeling refers to the process of annotating each document with its class. But labeling is arguably an easier task than writing rules. Almost anybody can look at a document and decide whether or not it is related to China. Sometimes such labeling is already implicitly part of an existing workflow. For instance, you may go through the news articles returned by a standing query each morning and give relevance feedback (cf. Chapter 9 ) by moving the relevant articles to a special folder like multicore-processors. We begin this chapter with a general introduction to the text classification problem including a formal definition (Section 13.1 ); we then cover Naive Bayes, a particularly simple and effective classification method (Sections 13.2-13.4). All of the classification algorithms we study represent documents in high-dimensional spaces. To improve the efficiency of these algorithms, it is generally desirable to reduce the dimensionality of these spaces; to this end, a technique known as feature selection is commonly applied in text classification as discussed in Section 13.5 . Section 13.6 covers evaluation of text classification. In the following chapters, Chapters 14 15 , we look at two other families of classification methods, vector space classifiers and support vector machines.   Subsections The text classification problem Naive Bayes text classification Relation to multinomial unigram language model The Bernoulli model Properties of Naive Bayes A variant of the multinomial model Feature selection Mutual information Feature selectionChi2 Feature selection Assessing as a feature selection methodAssessing chi-square as a feature selection method Frequency-based feature selection Feature selection for multiple classifiers Comparison of feature selection methods Evaluation of text classification References and further reading
iir_13_1	The text classification problem In text classification, we are given a description of a document, where is the document space ; and a fixed set of classes . Classes are also called categories or labels . Typically, the document space is some type of high-dimensional space, and the classes are human defined for the needs of an application, as in the examples China and documents that talk about multicore computer chips above. We are given a training set of labeled documents , where . For example: (111)  Using a learning method or learning algorithm , we then wish to learn a classifier or classification function that maps documents to classes:  (112)  This type of learning is called supervised learning because a supervisor (the human who defines the classes and labels training documents) serves as a teacher directing the learning process. We denote the supervised learning method by and write . The learning method takes the training set as input and returns the learned classification function . Most names for learning methods are also used for classifiers . We talk about the Naive Bayes (NB) learning method when we say that ``Naive Bayes is robust,'' meaning that it can be applied to many different learning problems and is unlikely to produce classifiers that fail catastrophically. But when we say that ``Naive Bayes had an error rate of 20%,'' we are describing an experiment in which a particular NB classifier (which was produced by the NB learning method) had a 20% error rate in an application. Figure 13.1 shows an example of text classification from the Reuters-RCV1 collection, introduced in Section 4.2 , page 4.2 . There are six classes (UK, China, ..., sports), each with three training documents. We show a few mnemonic words for each document's content. The training set provides some typical examples for each class, so that we can learn the classification function . Once we have learned , we can apply it to the test set (or test data ), for example, the new document first private Chinese airline whose class is unknown. In Figure 13.1 , the classification function assigns the new document to class China, which is the correct assignment. The classes in text classification often have some interesting structure such as the hierarchy in Figure 13.1 . There are two instances each of region categories, industry categories, and subject area categories. A hierarchy can be an important aid in solving a classification problem; see Section 15.3.2 for further discussion. Until then, we will make the assumption in the text classification chapters that the classes form a set with no subset relationships between them.  Figure 13.1: Classes, training set, and test set in text classification . Definition eqn:gammadef stipulates that a document is a member of exactly one class. This is not the most appropriate model for the hierarchy in Figure 13.1 . For instance, a document about the 2008 Olympics should be a member of two classes: the China class and the sports class. This type of classification problem is referred to as an any-of problem and we will return to it in Section 14.5 (page ). For the time being, we only consider one-of problems where a document is a member of exactly one class. Our goal in text classification is high accuracy on test data or new data - for example, the newswire articles that we will encounter tomorrow morning in the multicore chip example. It is easy to achieve high accuracy on the training set (e.g., we can simply memorize the labels). But high accuracy on the training set in general does not mean that the classifier will work well on new data in an application. When we use the training set to learn a classifier for test data, we make the assumption that training data and test data are similar or from the same distribution. We defer a precise definition of this notion to Section 14.6 (page ).
iir_13_2	Naive Bayes text classification  multinomial Naive Bayes  multinomial NB        (113)                   In text classification, our goal is to find the best class for the document. The best class in NB classification is the most likely or maximum a posteriori ( MAP ) class : (114)      In Equation 114, many conditional probabilities are multiplied, one for each position . This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable; and the logarithm function is monotonic. Hence, the maximization that is actually done in most implementations of NB is:     (115)   Equation 115 has a simple interpretation. Each conditional parameter is a weight that indicates how good an indicator is for . Similarly, the prior is a weight that indicates the relative frequency of . More frequent classes are more likely to be the correct class than infrequent classes. The sum of log prior and term weights is then a measure of how much evidence there is for the document being in the class, and Equation 115 selects the class for which we have the most evidence. We will initially work with this intuitive interpretation of the multinomial NB model and defer a formal derivation to Section 13.4 . How do we estimate the parameters and ? We first try the maximum likelihood estimate (MLE; probtheory), which is simply the relative frequency and corresponds to the most likely value of each parameter given the training data. For the priors this estimate is:     (116)       We estimate the conditional probability as the relative frequency of term in documents belonging to class : (117)      positional independence assumption      The problem with the MLE estimate is that it is zero for a term-class combination that did not occur in the training data. If the term WTO in the training data only occurred in China documents, then the MLE estimates for the other classes, for example UK, will be zero: (118)  113    sparseness  Figure 13.2: Naive Bayes algorithm (multinomial model): Training and testing. To eliminate zeros, we use add-one or Laplace smoothing, which simply adds one to each count (cf. Section 11.3.2 ): (119)   term class 116 We have now introduced all the elements we need for training and applying an NB classifier. The complete algorithm is described in Figure 13.2 .   Table 13.1: Data for parameter estimation examples.     docID words in document in China?     training set 1 Chinese Beijing Chinese yes       2 Chinese Chinese Shanghai yes       3 Chinese Macao yes       4 Tokyo Japan Chinese no     test set 5 Chinese Chinese Chinese Tokyo Japan ?    Worked example. For the example in Table 13.1 , the multinomial parameters we need to classify the test document are the priors and and the following conditional probabilities:          119 We then get:       End worked example.   Table 13.2: Training and test times for NB.   mode time complexity     training     testing          We use as a notation for here, where is the length of the training collection. This is nonstandard; is not defined for an average. We prefer expressing the time complexity in terms of and because these are the primary statistics used to characterize training collections. The time complexity of APPLYMULTINOMIALNB in Figure 13.2 is . and are the numbers of tokens and types, respectively, in the test document . APPLYMULTINOMIALNB can be modified to be (Exercise 13.6 ). Finally, assuming that the length of test documents is bounded, because for a fixed constant . Table 13.2 summarizes the time complexities. In general, we have , so both training and testing complexity are linear in the time it takes to scan the data. Because we have to look at the data at least once, NB can be said to have optimal time complexity. Its efficiency is one reason why NB is a popular text classification method.   Subsections Relation to multinomial unigram language model
iir_13_2_1	Relation to multinomial unigram language model 12.2.1 12.2.1 113 104 12.2.1       (120)    113 120   120  We also used MLE estimates in Section 12.2.2 (page ) and encountered the problem of zero estimates owing to sparse data (page 12.2.2 ); but instead of add-one smoothing, we used a mixture of two distributions to address the problem there. Add-one smoothing is closely related to add- smoothing in Section 11.3.4 (page ). Exercises. Why is in Table 13.2 expected to hold for most text collections ?
iir_13_3	The Bernoulli model There are two different ways we can set up an NB classifier. The model we introduced in the previous section is the multinomial model . It generates one term from the vocabulary in each position of the document, where we assume a generative model that will be discussed in more detail in Section 13.4 (see also page 12.1.1 ). An alternative to the multinomial model is the multivariate Bernoulli model or Bernoulli model . It is equivalent to the binary independence model of Section 11.3 (page ), which generates an indicator for each term of the vocabulary, either indicating presence of the term in the document or indicating absence. Figure 13.3 presents training and testing algorithms for the Bernoulli model. The Bernoulli model has the same time complexity as the multinomial model.   The different generation models imply different estimation strategies and different classification rules. The Bernoulli model estimates as the fraction of documents of class that contain term (Figure 13.3 , TRAINBERNOULLINB, line 8). In contrast, the multinomial model estimates as the fraction of tokens or fraction of positions in documents of class that contain term (Equation 119). When classifying a test document, the Bernoulli model uses binary occurrence information, ignoring the number of occurrences, whereas the multinomial model keeps track of multiple occurrences. As a result, the Bernoulli model typically makes many mistakes when classifying long documents. For example, it may assign an entire book to the class China because of a single occurrence of the term China. The models also differ in how nonoccurring terms are used in classification. They do not affect the classification decision in the multinomial model; but in the Bernoulli model the probability of nonoccurrence is factored in when computing (Figure 13.3 , APPLYBERNOULLINB, Line 7). This is because only the Bernoulli NB model models absence of terms explicitly. Worked example. Applying the Bernoulli model to the example in Table 13.1 , we have the same estimates for the priors as before: , . The conditional probabilities are:      The denominators are and because there are three documents in and one document in and because the constant in Equation 119 is 2 - there are two cases to consider for each term, occurrence and nonoccurrence. The scores of the test document for the two classes are               End worked example.
iir_13_4	Properties of Naive Bayes 11 12   (121)   (122)   (123)   59 59 122  We can interpret Equation 123 as a description of the generative process we assume in Bayesian text classification. To generate a document, we first choose class with probability (top nodes in and 13.5 ). The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution : (124) (125)        It should now be clearer why we introduced the document space in Equation 112 when we defined the classification problem. A critical step in solving a text classification problem is to choose the document representation. and are two different document representations. In the first case, is the set of all term sequences (or, more precisely, sequences of term tokens). In the second case, is . We cannot use and 125 for text classification directly. For the Bernoulli model, we would have to estimate different parameters, one for each possible combination of values and a class. The number of parameters in the multinomial case has the same order of magnitude.This being a very large quantity, estimating these parameters reliably is infeasible. To reduce the number of parameters, we make the Naive Bayes conditional independence assumption . We assume that attribute values are independent of each other given the class: (126) (127)                    Figure 13.4: The multinomial NB model.  Figure 13.5: The Bernoulli NB model. We illustrate the conditional independence assumption in and 13.5 . The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes. The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing. In reality, the conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. But as we will discuss shortly, NB models perform well despite the conditional independence assumption. Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position in the document. The position of a term in a document by itself does not carry information about the class. Although there is a difference between China sues France and France sues China, the occurrence of China in position 1 versus position 3 of the document is not useful in NB classification because we look at each term separately. The conditional independence assumption commits us to this way of processing the evidence. Also, if we assumed different term distributions for each position , we would have to estimate a different set of parameters for each . The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on. This again causes problems in estimation owing to data sparseness. For these reasons, we make a second independence assumption for the multinomial model, positional independence : The conditional probabilities for a term are the same independent of position in the document. (128)         bag of words 6 6.2 With conditional and positional independence assumptions, we only need to estimate parameters (multinomial model) or (Bernoulli model), one for each term-class combination, rather than a number that is at least exponential in , the size of the vocabulary. The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude. To summarize, we generate a document in the multinomial model (Figure 13.4 ) by first picking a class with where is a random variable taking values from as values. Next we generate term in position with for each of the positions of the document. The all have the same distribution over terms for a given . In the example in Figure 13.4 , we show the generation of , corresponding to the one-sentence document Beijing and Taipei join WTO. For a completely specified document generation model, we would also have to define a distribution over lengths. Without it, the multinomial model is a token generation model rather than a document generation model. We generate a document in the Bernoulli model (Figure 13.5 ) by first picking a class with and then generating a binary indicator for each term of the vocabulary ( ). In the example in Figure 13.5 , we show the generation of , corresponding, again, to the one-sentence document Beijing and Taipei join WTO where we have assumed that and is a stop word.   Table 13.3: Multinomial versus Bernoulli model.     multinomial model Bernoulli model     event model generation of token generation of document     random variable(s) iff occurs at given pos iff occurs in doc     document representation                  parameter estimation     decision rule: maximize     multiple occurrences taken into account ignored     length of docs can handle longer docs works best for short docs     # features can handle more works best with fewer     estimate for term the    We compare the two models in Table 13.3 , including estimation equations and decision rules. Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class. This is hardly ever true for terms in documents. In many cases, the opposite is true. The pairs hong and kong or london and english in Figure 13.7 are examples of highly dependent terms. In addition, the multinomial model makes an assumption of positional independence. The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence. This bag-of-words model discards all information that is communicated by the order of words in natural language sentences. How can NB be a good text classifier when its model of natural language is so oversimplified?   Table 13.4: Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation.     class selected     true probability 0.6 0.4     (Equation 126) 0.00099 0.00001       NB estimate 0.99 0.01    The answer is that even though the probability estimates of NB are of low quality, its classification decisions are surprisingly good. Consider a document with true probabilities and as shown in Table 13.4 . Assume that contains many terms that are positive indicators for and many terms that are negative indicators for . Thus, when using the multinomial model in Equation 126, will be much larger than (0.00099 vs. 0.00001 in the table). After division by 0.001 to get well-formed probabilities for , we end up with one estimate that is close to 1.0 and one that is close to 0.0. This is common: The winning class in NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. But the classification decision is based on which class gets the highest score. It does not matter how accurate the estimates are. Despite the bad estimates, NB estimates a higher probability for and therefore assigns to the correct class in Table 13.4 . Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification. It excels if there are many equally important features that jointly contribute to the classification decision. It is also somewhat robust to noise features (as defined in the next section) and concept drift - the gradual change over time of the concept underlying a class like US president from Bill Clinton to George W. Bush (see Section 13.7 ). Classifiers like kNN knn can be carefully tuned to idiosyncratic properties of a particular time period. This will then hurt them when documents in the following time period have slightly different properties. The Bernoulli model is particularly robust with respect to concept drift. We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms. The most important indicators for a class are less likely to change. Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift. NB's main strength is its efficiency: Training and classification can be accomplished with one pass over the data. Because it combines efficiency with good accuracy it is often used as a baseline in text classification research. It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited.   Table 13.5: A set of documents for which the NB independence assumptions are problematic.   (1) He moved from London, Ontario, to London, England.     (2) He moved from London, England, to London, Ontario.     (3) He moved from England to London, Ontario.    In this book, we discuss NB as a classifier for text. The independence assumptions do not hold for text. However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for data where the independence assumptions do hold.   Subsections A variant of the multinomial model
iir_13_4_1	A variant of the multinomial model         99 12.2.1  (129)  99 99 Equation 129 is equivalent to the sequence model in Equation 113 as for terms that do not occur in ( ) and a term that occurs times will contribute factors both in Equation 113 and in Equation 129. Exercises. Which of the documents in Table 13.5 have identical and different bag of words representations for (i) the Bernoulli model (ii) the multinomial model? If there are differences, describe them. The rationale for the positional independence assumption is that there is no useful information in the fact that a term occurs in position of a document. Find exceptions. Consider formulaic documents with a fixed document structure. Table 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the difference.
iir_13_5	Feature selection Feature selection    noise feature   overfitting  Figure: Basic feature selection algorithm for selecting the best features. We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features). It may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification, but when discussing the bias-variance tradeoff in Section 14.6 (page ), we will see that weaker models are often preferable when limited training data are available. The basic feature selection algorithm is shown in Figure 13.6 . For a given class , we compute a utility measure for each term of the vocabulary and select the terms that have the highest values of . All other terms are discarded and not used in classification. We will introduce three different utility measures in this section: mutual information, ; the test, ; and frequency, . Of the two NB models, the Bernoulli model is particularly sensitive to noise features. A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low. This section mainly addresses feature selection for two-class classification tasks like China versus not-China. Section 13.5.5 briefly discusses optimizations for systems with more than two classes.   Subsections Mutual information Feature selectionChi2 Feature selection Assessing as a feature selection methodAssessing chi-square as a feature selection method Frequency-based feature selection Feature selection for multiple classifiers Comparison of feature selection methods
iir_13_5_1	Mutual information A common feature selection method is to compute as the expected mutual information (MI) of term and class . MI measures how much information the presence/absence of a term contributes to making the correct classification decision on . Formally: (130)        13.4          For MLEs of the probabilities, Equation 130 is equivalent to Equation 131: (131)     (132)                130 131  Worked example. Consider the class poultry and the term export in Reuters-RCV1. The counts of the number of documents with the four possible combinations of indicator values are as follows:   131     End worked example. To select terms for a given class, we use the feature selection algorithm in Figure 13.6 : We compute the utility measure as and select the terms with the largest values. Mutual information measures how much information - in the information-theoretic sense - a term contains about the class. If a term's distribution is the same in the class as it is in the collection as a whole, then . MI reaches its maximum value if the term is a perfect indicator for class membership, that is, if the term is present in a document if and only if the document is in the class.  Figure 13.7: Features with high mutual information scores for six Reuters-RCV1 classes. Figure 13.7 shows terms with high mutual information scores for the six classes in Figure 13.1 . The selected terms (e.g., london, uk, british for the class UK) are of obvious utility for making classification decisions for their respective classes. At the bottom of the list for UK we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding whether the document is in the class. As you might expect, keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier's accuracy.  Figure 13.8: Effect of feature set size on accuracy for multinomial and Bernoulli models. 13.8
iir_13_5_2	Feature selectionChi2 Feature selection          independent         (133)     130  observed   expected    Worked example. We first compute for the data in Example 13.5.1:  (134)   (135)    We compute the other in the same way:                                Plugging these values into Equation 133, we get a value of 284:     (136)   End worked example. is a measure of how much expected counts and observed counts deviate from each other. A high value of indicates that the hypothesis of independence, which implies that expected and observed counts are similar, is incorrect. In our example, . Based on Table 13.6 , we can reject the hypothesis that poultry and export are independent with only a 0.001 chance of being wrong.Equivalently, we say that the outcome is statistically significant at the 0.001 level. If the two events are dependent, then the occurrence of the term makes the occurrence of the class more likely (or less likely), so it should be helpful as a feature. This is the rationale of feature selection.   Table 13.6: Critical values of the distribution with one degree of freedom. For example, if the two events are independent, then . So for the assumption of independence can be rejected with 99% confidence.   critical value     0.1 2.71     0.05 3.84     0.01 6.63     0.005 7.88     0.001 10.83    An arithmetically simpler way of computing is the following: (137)  133 13.6   Subsections Assessing as a feature selection methodAssessing chi-square as a feature selection method
iir_13_5_3	Frequency-based feature selection  frequency-based feature selection       Frequency-based feature selection selects some frequent terms that have no specific information about the class, for example, the days of the week (Monday, Tuesday, ...), which are frequent across classes in newswire text. When many thousands of features are selected, then frequency-based feature selection often does well. Thus, if somewhat suboptimal accuracy is acceptable, then frequency-based feature selection can be a good alternative to more complex methods. However, Figure 13.8 is a case where frequency-based feature selection performs a lot worse than MI and and should not be used.
iir_13_5_4	Feature selection for multiple classifiers     More commonly, feature selection statistics are first computed separately for each class on the two-class classification task versus and then combined. One combination method computes a single figure of merit for each feature, for example, by averaging the values for feature , and then selects the features with highest figures of merit. Another frequently used combination method selects the top features for each of classifiers and then combines these sets into one global feature set. Classification accuracy often decreases when selecting common features for a system with classifiers as opposed to different sets of size . But even if it does, the gain in efficiency owing to a common document representation may be worth the loss in accuracy .
iir_13_5_5	Comparison of feature selection methods       Despite the differences between the two methods, the classification accuracy of feature sets selected with and MI does not seem to differ systematically. In most text classification problems, there are a few strong indicators and many weak indicators. As long as all strong indicators and a large number of weak indicators are selected, accuracy is expected to be good. Both methods do this. Figure 13.8 compares MI and feature selection for the multinomial model. Peak effectiveness is virtually the same for both methods. reaches this peak later, at 300 features, probably because the rare, but highly significant features it selects initially do not cover all documents in the class. However, features selected later (in the range of 100-300) are of better quality than those selected by MI. All three methods - MI, and frequency based - are greedy methods. They may select features that contribute no incremental information over previously selected features. In Figure 13.7 , kong is selected as the seventh term even though it is highly correlated with previously selected hong and therefore redundant. Although such redundancy can negatively impact accuracy, non-greedy methods (see Section 13.7 for references) are rarely used in text classification due to their computational cost. Exercises. Consider the following frequencies for the class coffee for four terms in the first 100,000 documents of Reuters-RCV1:               term     brazil 98,012 102 1835 51     council 96,322 133 3525 20     producers 98,524 119 1118 34     roasted 99,824 143 23 10               Select two of these four terms based on (i) , (ii) mutual information, (iii) frequency .
iir_13_6	Evaluation of text classification Historically, the classic Reuters-21578 collection was the main benchmark for text classification evaluation. This is a collection of 21,578 newswire articles, originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text classification system. It is much smaller than and predates the Reuters-RCV1 collection discussed in Chapter 4 (page 4.2 ). The articles are assigned classes from a set of 118 topic categories. A document may be assigned several classes or none, but the commonest case is single assignment (documents with at least one class received an average of 1.24 classes). The standard approach to this any-of problem (Chapter 14 , page 14.5 ) is to learn 118 two-class classifiers, one for each class, where the two-class classifier for class is the classifier for the two classes and its complement .   Table 13.7: The ten largest classes in the Reuters-21578 collection with number of documents in training and test sets.   class # train # test   class # train # test     earn 2877 1087   trade 369 119     acquisitions 1650 179   interest 347 131     money-fx 538 179   ship 197 89     grain 433 149   wheat 212 71     crude 389 189   corn 182 56    For each of these classifiers, we can measure recall, precision, and accuracy. In recent work, people almost invariably use the ModApte split , which includes only documents that were viewed and assessed by a human indexer, and comprises 9,603 training documents and 3,299 test documents. The distribution of documents in classes is very uneven, and some work evaluates systems on only documents in the ten largest classes. They are listed in Table 13.7 . A typical document with topics is shown in Figure 13.9 . In Section 13.1 , we stated as our goal in text classification the minimization of classification error on test data. Classification error is 1.0 minus classification accuracy, the proportion of correct decisions, a measure we introduced in Section 8.3 (page 8.3 ). This measure is appropriate if the percentage of documents in the class is high, perhaps 10% to 20% and higher. But as we discussed in Section 8.3 , accuracy is not a good measure for ``small'' classes because always saying no, a strategy that defeats the purpose of building a classifier, will achieve high accuracy. The always-no classifier is 99% accurate for a class with relative frequency 1%. For small classes, precision, recall and are better measures. We will use effectiveness as a generic term for measures that evaluate the quality of classification decisions, including precision, recall, , and accuracy. Performance refers to the computational efficiency of classification and IR systems in this book. However, many researchers mean effectiveness, not efficiency of text classification when they use the term performance.  Figure 13.9: A sample document from the Reuters-21578 collection. When we process a collection with several two-class classifiers (such as Reuters-21578 with its 118 classes), we often want to compute a single aggregate measure that combines the measures for individual classifiers. There are two methods for doing this. Macroaveraging computes a simple average over classes. Microaveraging pools per-document decisions across classes, and then computes an effectiveness measure on the pooled contingency table. Table 13.8 gives an example. The differences between the two methods can be large. Macroaveraging gives equal weight to each class, whereas microaveraging gives equal weight to each per-document classification decision. Because the measure ignores true negatives and its magnitude is mostly determined by the number of true positives, large classes dominate small classes in microaveraging. In the example, microaveraged precision (0.83) is much closer to the precision of (0.9) than to the precision of (0.5) because is five times larger than . Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection. To get a sense of effectiveness on small classes, you should compute macroaveraged results.   Table 13.8: Macro- and microaveraging. ``Truth'' is the true class and ``call'' the decision of the classifier. In this example, macroaveraged precision is . Microaveraged precision is .   class 1   truth: truth:   yes no call: yes 10 10 call: no 10 970 class 2   truth: truth:   yes no call: yes 90 10 call: no 10 890 pooled table   truth: truth:   yes no call: yes 100 20 call: no 20 1860      Table 13.9: Text classification effectiveness numbers on Reuters-21578 for F (in percent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais et al. (1998) (b: NB, Rocchio, trees, SVM).   (a)   NB Rocchio kNN   SVM       micro-avg-L (90 classes) 80 85 86   89       macro-avg (90 classes) 47 59 60   60             (b)   NB Rocchio kNN trees SVM     earn 96 93 97 98 98       acq 88 65 92 90 94       money-fx 57 47 78 66 75       grain 79 68 82 85 95       crude 80 70 86 85 89       trade 64 65 77 73 76       interest 65 63 74 67 78       ship 85 49 79 74 86       wheat 70 69 77 93 92       corn 65 48 78 92 90     micro-avg (top 10) 82 65 82 88 92       micro-avg-D (118 classes) 75 62 n/a n/a 87    In one-of classification (more-than-two-classes), microaveraged is the same as accuracy (Exercise 13.6 ). Table 13.9 gives microaveraged and macroaveraged effectiveness of Naive Bayes for the ModApte split of Reuters-21578. To give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15 ), one of the most effective classifiers, but also one that is more expensive to train than NB. NB has a microaveraged of 80%, which is 9% less than the SVM (89%), a 10% relative decrease (row ``micro-avg-L (90 classes)''). So there is a surprisingly small effectiveness penalty for its simplicity and efficiency. However, on small classes, some of which only have on the order of ten positive examples in the training set, NB does much worse. Its macroaveraged is 13% below the SVM, a 22% relative decrease (row ``macro-avg (90 classes)'' ). The table also compares NB with the other classifiers we cover in this book: Rocchio and kNN. In addition, we give numbers for decision trees , an important classification method we do not cover. The bottom part of the table shows that there is considerable variation from class to class. For instance, NB beats kNN on ship, but is much worse on money-fx. Comparing parts (a) and (b) of the table, one is struck by the degree to which the cited papers' results differ. This is partly due to the fact that the numbers in (b) are break-even scores (cf. page 8.4 ) averaged over 118 classes, whereas the numbers in (a) are true scores (computed without any knowledge of the test set) averaged over ninety classes. This is unfortunately typical of what happens when comparing different results in text classification: There are often differences in the experimental setup or the evaluation that complicate the interpretation of the results. These and other results have shown that the average effectiveness of NB is uncompetitive with classifiers like SVMs when trained and tested on independent and identically distributed ( i.i.d. ) data, that is, uniform data with all the good properties of statistical sampling. However, these differences may often be invisible or even reverse themselves when working in the real world where, usually, the training sample is drawn from a subset of the data to which the classifier will be applied, the nature of the data drifts over time rather than being stationary (the problem of concept drift we mentioned on page 13.4 ), and there may well be errors in the data (among other problems). Many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than NB. Our conclusion from the results in Table 13.9 is that, although most researchers believe that an SVM is better than kNN and kNN better than NB, the ranking of classifiers ultimately depends on the class, the document collection, and the experimental setup. In text classification, there is always more to know than simply which machine learning algorithm was used, as we further discuss in Section 15.3 (page ). When performing evaluations like the one in Table 13.9 , it is important to maintain a strict separation between the training set and the test set . We can easily make correct classification decisions on the test set by using information we have gleaned from the test set, such as the fact that a particular term is a good predictor in the test set (even though this is not the case in the training set). A more subtle example of using knowledge about the test set is to try a large number of values of a parameter (e.g., the number of selected features) and select the value that is best for the test set. As a rule, accuracy on new data - the type of data we will encounter when we use the classifier in an application - will be much lower than accuracy on a test set that the classifier has been tuned for. We discussed the same problem in ad hoc retrieval in Section 8.1 (page 8.1 ). In a clean statistical text classification experiment, you should never run any program on or even look at the test set while developing a text classification system. Instead, set aside a development set for testing while you develop your method. When such a set serves the primary purpose of finding a good value for a parameter, for example, the number of selected features, then it is also called held-out data . Train the classifier on the rest of the training set with different parameter values, and then select the value that gives best results on the held-out part of the training set. Ideally, at the very end, when all parameters have been set and the method is fully specified, you run one final experiment on the test set and publish the results. Because no information about the test set was used in developing the classifier, the results of this experiment should be indicative of actual performance in practice. This ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a period of several years. But it is nevertheless highly important to not look at the test data and to run systems on it as sparingly as possible. Beginners often violate this rule, and their results lose validity because they have implicitly tuned their system to the test data simply by running many variant systems and keeping the tweaks to the system that worked best on the test set. Exercises. Assume a situation where every document in the test collection has been assigned exactly one class, and that a classifier also assigns exactly one class to each document. This setup is called one-of classification more-than-two-classes. Show that in one-of classification (i) the total number of false positive decisions equals the total number of false negative decisions and (ii) microaveraged and accuracy are identical. The class priors in Figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class. Why? The function APPLYMULTINOMIALNB in Figure 13.2 has time complexity . How would you modify the function so that its time complexity is ? Table 13.10: Data for parameter estimation exercise.     docID words in document in China?     training set 1 Taipei Taiwan yes       2 Macao Taiwan Shanghai yes       3 Japan Sapporo no       4 Sapporo Osaka Taiwan no     test set 5 Taiwan Taiwan Sapporo ?   Based on the data in Table 13.10 , (i) estimate a multinomial Naive Bayes classifier, (ii) apply the classifier to the test document, (iii) estimate a Bernoulli NB classifier, (iv) apply the classifier to the test document. You need not estimate parameters that you don't need for classifying the test document. Your task is to classify words as English or not English. Words are generated by a source with the following distribution:   event word English? probability     1 ozb no 4/9     2 uzu no 4/9     3 zoo yes 1/18     4 bun yes 1/18   (i) Compute the parameters (priors and conditionals) of a multinomial NB classifier that uses the letters b, n, o, u, and z as features. Assume a training set that reflects the probability distribution of the source perfectly. Make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text classification. Compute parameters using smoothing, in which computed-zero probabilities are smoothed into probability 0.01, and computed-nonzero probabilities are untouched. (This simplistic smoothing may cause . Solutions are not required to correct this.) (ii) How does the classifier classify the word zoo? (iii) Classify the word zoo using a multinomial classifier as in part (i), but do not make the assumption of positional independence. That is, estimate separate parameters for each position in a word. You only need to compute the parameters you need for classifying zoo. What are the values of and if term and class are completely independent? What are the values if they are completely dependent? The feature selection method in Equation 130 is most appropriate for the Bernoulli model. Why? How could one modify it for the multinomial model? Features can also be selected according to information gain (IG), which is defined as: (138) where is entropy, is the training set, and , and are the subset of with term , and the subset of without term , respectively. is the class distribution in (sub)collection , e.g., if a quarter of the documents in are in class . Show that mutual information and information gain are equivalent. Show that the two formulas ( and 137 ) are equivalent. In the example on page 13.5.2 we have . Show that this holds in general. and mutual information do not distinguish between positively and negatively correlated features. Because most good text classification features are positively correlated (i.e., they occur more often in than in ), one may want to explicitly rule out the selection of negative indicators. How would you do this?
iir_13_7	References and further reading General introductions to statistical classification and machine learning can be found in (Hastie et al., 2001), (Mitchell, 1997), and (Duda et al., 2000), including many important methods (e.g., decision trees and boosting ) that we do not cover. A comprehensive review of text classification methods and results is (Sebastiani, 2002). Manning and Schütze (1999, Chapter 16) give an accessible introduction to text classification with coverage of decision trees, perceptrons and maximum entropy models. More information on the superlinear time complexity of learning methods that are more accurate than Naive Bayes can be found in (Perkins et al., 2003) and (Joachims, 2006a). Maron and Kuhns (1960) described one of the first NB text classifiers. Lewis (1998) focuses on the history of NB classification. Bernoulli and multinomial models and their accuracy for different collections are discussed by McCallum and Nigam (1998). Eyheramendy et al. (2003) present additional NB models. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu (2001) analyze why NB performs well although its probability estimates are poor. The first paper also discusses NB's optimality when the independence assumptions are true of the data. Pavlov et al. (2004) propose a modified document representation that partially addresses the inappropriateness of the independence assumptions. Bennett (2000) attributes the tendency of NB probability estimates to be close to either 0 or 1 to the effect of document length. Ng and Jordan (2001) show that NB is sometimes (although rarely) superior to discriminative methods because it more quickly reaches its optimal error rate. The basic NB model presented in this chapter can be tuned for better effectiveness (Rennie et al. 2003;Kocz and Yih 2007). The problem of concept drift and other reasons why state-of-the-art classifiers do not always excel in practice are discussed by Forman (2006) and Hand (2006). Early uses of mutual information and for feature selection in text classification are Lewis and Ringuette (1994) and Schütze et al. (1995), respectively. Yang and Pedersen (1997) review feature selection methods and their impact on classification effectiveness. They find that pointwise mutual information is not competitive with other methods. Yang and Pedersen refer to expected mutual information (Equation 130) as information gain (see Exercise 13.6 , page 13.6 ). (Snedecor and Cochran, 1989) is a good reference for the test in statistics, including the Yates' correction for continuity for tables. Dunning (1993) discusses problems of the test when counts are small. Nongreedy feature selection techniques are described by Hastie et al. (2001). Cohen (1995) discusses the pitfalls of using multiple significance tests and methods to avoid them. Forman (2004) evaluates different methods for feature selection for multiple classifiers. David D. Lewis defines the ModApte split at www.daviddlewis.com/resources/testcollections/reuters21578/readme.txtbased on Apté et al. (1994). Lewis (1995) describes utility measures for the evaluation of text classification systems. Yang and Liu (1999) employ significance tests in the evaluation of text classification methods. Lewis et al. (2004) find that SVMs (Chapter 15 ) perform better on Reuters-RCV1 than kNN and Rocchio (Chapter 14 ).
iir_14	Vector space classification The document representation in Naive Bayes is a sequence of terms or a binary vector . In this chapter we adopt a different representation for text classification, the vector space model, developed in Chapter 6 . It represents each document as a vector with one real-valued component, usually a tf-idf weight, for each term. Thus, the document space , the domain of the classification function , is . This chapter introduces a number of classification methods that operate on real-valued vectors. The basic hypothesis in using the vector space model for classification is the contiguity hypothesis . Contiguity hypothesis. Documents in the same class form a contiguous region and regions of different classes do not overlap. 13 14.1  Figure 14.1: Vector space classification into three classes. Whether or not a set of documents is mapped into a contiguous region depends on the particular choices we make for the document representation: type of weighting, stop list etc. To see that the document representation is crucial, consider the two classes written by a group vs. written by a single person. Frequent occurrence of the first person pronoun I is evidence for the single-person class. But that information is likely deleted from the document representation if we use a stop list. If the document representation chosen is unfavorable, the contiguity hypothesis will not hold and successful vector space classification is not possible. The same considerations that led us to prefer weighted representations, in particular length-normalized tf-idf representations, in Chapters 6 7 also apply here. For example, a term with 5 occurrences in a document should get a higher weight than a term with one occurrence, but a weight 5 times larger would give too much emphasis to the term. Unweighted and unnormalized counts should not be used in vector space classification. We introduce two vector space classification methods in this chapter, Rocchio and kNN. Rocchio classification (Section 14.2 ) divides the vector space into regions centered on centroids or prototypes , one for each class, computed as the center of mass of all documents in the class. Rocchio classification is simple and efficient, but inaccurate if classes are not approximately spheres with similar radii. kNN or nearest neighbor classification (Section 14.3 ) assigns the majority class of the nearest neighbors to a test document. kNN requires no explicit training and can use the unprocessed training set directly in classification. It is less efficient than other classification methods in classifying documents. If the training set is large, then kNN can handle non-spherical and other complex classes better than Rocchio. A large number of text classifiers can be viewed as linear classifiers - classifiers that classify based on a simple linear combination of the features (Section 14.4 ). Such classifiers partition the space of features into regions separated by linear decision hyperplanes , in a manner to be detailed below. Because of the bias-variance tradeoff (Section 14.6 ) more complex nonlinear models are not systematically better than linear models. Nonlinear models have more parameters to fit on a limited amount of training data and are more likely to make mistakes for small and noisy data sets. When applying two-class classifiers to problems with more than two classes, there are one-of tasks - a document must be assigned to exactly one of several mutually exclusive classes - and any-of tasks - a document can be assigned to any number of classes as we will explain in Section 14.5 . Two-class classifiers solve any-of problems and can be combined to solve one-of problems.   Subsections Document representations and measures of relatedness in vector spaces Rocchio classification k nearest neighbor Time complexity and optimality of kNN Linear versus nonlinear classifiers Classification with more than two classes The bias-variance tradeoff References and further reading Exercises
iir_14_1	Document representations and measures of relatedness in vector spaces   As in Chapter 6 , we represent documents as vectors in in this chapter. To illustrate properties of document vectors in vector classification, we will render these vectors as points in a plane as in the example in Figure 14.1 . In reality, document vectors are length-normalized unit vectors that point to the surface of a hypersphere. We can view the 2D planes in our figures as projections onto a plane of the surface of a (hyper-)sphere as shown in Figure 14.2 . Distances on the surface of the sphere and on the projection plane are approximately the same as long as we restrict ourselves to small areas of the surface and choose an appropriate projection (Exercise 14.1 ). Decisions of many vector space classifiers are based on a notion of distance, e.g., when computing the nearest neighbors in kNN classification. We will use Euclidean distance in this chapter as the underlying distance measure. We observed earlier (Exercise 6.4.4 , page ) that there is a direct correspondence between cosine similarity and Euclidean distance for length-normalized vectors. In vector space classification, it rarely matters whether the relatedness of two documents is expressed in terms of similarity or distance. However, in addition to documents, centroids or averages of vectors also play an important role in vector space classification. Centroids are not length-normalized. For unnormalized vectors, dot product, cosine similarity and Euclidean distance all have different behavior in general (Exercise 14.8 ). We will be mostly concerned with small local regions when computing the similarity between a document and a centroid, and the smaller the region the more similar the behavior of the three measures is. Exercises. For small areas, distances on the surface of the hypersphere are approximated well by distances on its projection (Figure 14.2 ) because for small angles. For what size angle is the distortion (i) 1.01, (ii) 1.05 and (iii) 1.1?
iir_14_2	Rocchio classification 14.1  decision boundaries  Figure 14.3: Rocchio classification. Perhaps the best-known way of computing good class boundaries is Rocchio classification , which uses centroids to define the boundaries. The centroid of a class is computed as the vector average or center of mass of its members:     (139)          25 6.3.1 14.3 The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids. For example, , , and in the figure. This set of points is always a line. The generalization of a line in -dimensional space is a hyperplane, which we define as the set of points that satisfy:     (140)      normal vector     Thus, the boundaries of class regions in Rocchio classification are hyperplanes. The classification rule in Rocchio is to classify a point in accordance with the region it falls into. Equivalently, we determine the centroid that the point is closest to and then assign it to . As an example, consider the star in Figure 14.3 . It is located in the China region of the space and Rocchio therefore assigns it to China. We show the Rocchio algorithm in pseudocode in Figure 14.4 .   Table 14.1: Vectors and class centroids for the data in Table 13.1 .   term weights vector Chinese Japan Tokyo Macao Beijing Shanghai 0 0 0 0 1.0 0 0 0 0 0 0 1.0 0 0 0 1.0 0 0 0 0.71 0.71 0 0 0 0 0.71 0.71 0 0 0 0 0 0 0.33 0.33 0.33 0 0.71 0.71 0 0 0   Worked example. Table 14.1 shows the tf-idf vector representations of the five documents in Table 13.1 (page 13.1 ), using the formula if (Equation 29, page 6.4.1 ). The two class centroids are and . The distances of the test document from the centroids are and . Thus, Rocchio assigns to . The separating hyperplane in this case has the following parameters:     14.8            End worked example. The assignment criterion in Figure 14.4 is Euclidean distance (APPLYROCCHIO, line 1). An alternative is cosine similarity: (141)  14.1  16.4 16.4  Figure 14.4: Rocchio classification: Training and testing. Rocchio classification is a form of Rocchio relevance feedback (Section 9.1.1 , page 9.1.1 ). The average of the relevant documents, corresponding to the most important component of the Rocchio vector in relevance feedback (Equation 49, page 49 ), is the centroid of the ``class'' of relevant documents. We omit the query component of the Rocchio formula in Rocchio classification since there is no query in text classification. Rocchio classification can be applied to classes whereas Rocchio relevance feedback is designed to distinguish only two classes, relevant and nonrelevant. In addition to respecting contiguity, the classes in Rocchio classification must be approximate spheres with similar radii. In Figure 14.3 , the solid square just below the boundary between UK and Kenya is a better fit for the class UK since UK is more scattered than Kenya. But Rocchio assigns it to Kenya because it ignores details of the distribution of points in a class and only uses distance from the centroid for classification.   The assumption of sphericity also does not hold in Figure 14.5 . We cannot represent the ``a'' class well with a single prototype because it has two clusters. Rocchio often misclassifies this type of multimodal class . A text classification example for multimodality is a country like Burma, which changed its name to Myanmar in 1989. The two clusters before and after the name change need not be close to each other in space. We also encountered the problem of multimodality in relevance feedback (Section 9.1.2 , page 9.1.3 ). Two-class classification is another case where classes are rarely distributed like spheres with similar radii. Most two-class classifiers distinguish between a class like China that occupies a small region of the space and its widely scattered complement. Assuming equal radii will result in a large number of false positives. Most two-class classification problems therefore require a modified decision rule of the form: (142)        mode time complexity training testing Training and test times for Rocchio classification. is the average number of tokens per document. and are the numbers of tokens and types, respectively, in the test document. Computing Euclidean distance between the class centroids and a document is .  Table 14.2 gives the time complexity of Rocchio classification. Adding all documents to their respective (unnormalized) centroid is (as opposed to ) since we need only consider non-zero entries. Dividing each vector sum by the size of its class to compute the centroid is . Overall, training time is linear in the size of the collection (cf. Exercise 13.2.1 ). Thus, Rocchio classification and Naive Bayes have the same linear training time complexity. In the next section, we will introduce another vector space classification method, kNN, that deals better with classes that have non-spherical, disconnected or other irregular shapes.   Exercises. Show that Rocchio classification can assign a label to a document that is different from its training set label.
iir_14_3	k nearest neighbor Unlike Rocchio, nearest neighbor or kNN classification determines the decision boundary locally. For 1NN we assign each document to the class of its closest neighbor. For kNN we assign each document to the majority class of its closest neighbors where is a parameter. The rationale of kNN classification is that, based on the contiguity hypothesis, we expect a test document to have the same label as the training documents located in the local region surrounding . Decision boundaries in 1NN are concatenated segments of the Voronoi tessellation as shown in Figure 14.6 . The Voronoi tessellation of a set of objects decomposes space into Voronoi cells, where each object's cell consists of all points that are closer to the object than to other objects. In our case, the objects are documents. The Voronoi tessellation then partitions the plane into convex polygons, each containing its corresponding document (and no other) as shown in Figure 14.6 , where a convex polygon is a convex region in 2-dimensional space bounded by lines. For general in kNN, consider the region in the space for which the set of nearest neighbors is the same. This again is a convex polygon and the space is partitioned into convex polygons , within each of which the set of nearest neighbors is invariant (Exercise 14.8 ). 1NN is not very robust. The classification decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical. kNN for is more robust. It assigns documents to the majority class of their closest neighbors, with ties broken randomly. There is a probabilistic version of this kNN classification algorithm. We can estimate the probability of membership in class as the proportion of the nearest neighbors in . Figure 14.6 gives an example for . Probability estimates for class membership of the star are , , and . The 3nn estimate ( ) and the 1nn estimate ( ) differ with 3nn preferring the X class and 1nn preferring the circle class . The parameter in kNN is often chosen based on experience or knowledge about the classification problem at hand. It is desirable for to be odd to make ties less likely. and are common choices, but much larger values between 50 and 100 are also used. An alternative way of setting the parameter is to select the that gives best results on a held-out portion of the training set.   We can also weight the ``votes'' of the nearest neighbors by their cosine similarity. In this scheme, a class's score is computed as: (143)         Figure 14.7 summarizes the kNN algorithm. Worked example. The distances of the test document from the four training documents in Table 14.1 are and . 's nearest neighbor is therefore and 1NN assigns to 's class, . End worked example.   Subsections Time complexity and optimality of kNN
iir_14_3_1	Time complexity and optimality of kNN   kNN with preprocessing of training set training testing kNN without preprocessing of training set training testing Training and test times for kNN classification. is the average size of the vocabulary of documents in the collection.  Table 14.3 gives the time complexity of kNN. kNN has properties that are quite different from most other classification algorithms. Training a kNN classifier simply consists of determining and preprocessing documents. In fact, if we preselect a value for and do not preprocess, then kNN requires no training at all. In practice, we have to perform preprocessing steps like tokenization. It makes more sense to preprocess training documents once as part of the training phase rather than repeatedly every time we classify a new test document. Test time is for kNN. It is linear in the size of the training set as we need to compute the distance of each training document from the test document. Test time is independent of the number of classes . kNN therefore has a potential advantage for problems with large . In kNN classification, we do not perform any estimation of parameters as we do in Rocchio classification (centroids) or in Naive Bayes (priors and conditional probabilities). kNN simply memorizes all examples in the training set and then compares the test document to them. For this reason, kNN is also called memory-based learning or instance-based learning . It is usually desirable to have as much training data as possible in machine learning. But in kNN large training sets come with a severe efficiency penalty in classification. Can kNN testing be made more efficient than or, ignoring the length of documents, more efficient than ? There are fast kNN algorithms for small dimensionality (Exercise 14.8 ). There are also approximations for large that give error bounds for specific efficiency gains (see Section 14.7 ). These approximations have not been extensively tested for text classification applications, so it is not clear whether they can achieve much better efficiency than without a significant loss of accuracy. The reader may have noticed the similarity between the problem of finding nearest neighbors of a test document and ad hoc retrieval, where we search for the documents with the highest similarity to the query (Section 6.3.2 , page 6.3.2 ). In fact, the two problems are both nearest neighbor problems and only differ in the relative density of (the vector of) the test document in kNN (10s or 100s of non-zero entries) versus the sparseness of (the vector of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries). We introduced the inverted index for efficient ad hoc retrieval in Section 1.1 (page 1.1 ). Is the inverted index also the solution for efficient kNN? An inverted index restricts a search to those documents that have at least one term in common with the query. Thus in the context of kNN, the inverted index will be efficient if the test document has no term overlap with a large number of training documents. Whether this is the case depends on the classification problem. If documents are long and no stop list is used, then less time will be saved. But with short documents and a large stop list, an inverted index may well cut the average test time by a factor of 10 or more. The search time in an inverted index is a function of the length of the postings lists of the terms in the query. Postings lists grow sublinearly with the length of the collection since the vocabulary increases according to Heaps' law - if the probability of occurrence of some terms increases, then the probability of occurrence of others must decrease. However, most new terms are infrequent. We therefore take the complexity of inverted index search to be (as discussed in Section 2.4.2 , page 2.4.2 ) and, assuming average document length does not change over time, . As we will see in the next chapter, kNN's effectiveness is close to that of the most accurate learning methods in text classification (Table 15.2 , page 15.2 ). A measure of the quality of a learning method is its Bayes error rate , the average error rate of classifiers learned by it for a particular problem. kNN is not optimal for problems with a non-zero Bayes error rate - that is, for problems where even the best possible classifier has a non-zero classification error. The error of 1NN is asymptotically (as the training set increases) bounded by twice the Bayes error rate. That is, if the optimal classifier has an error rate of , then 1NN has an asymptotic error rate of less than . This is due to the effect of noise - we already saw one example of noise in the form of noisy features in Section 13.5 (page 13.5 ), but noise can also take other forms as we will discuss in the next section. Noise affects two components of kNN: the test document and the closest training document. The two sources of noise are additive, so the overall error of 1NN is twice the optimal error rate. For problems with Bayes error rate 0, the error rate of 1NN will approach 0 as the size of the training set increases. Exercises. Explain why kNN handles multimodal classes better than Rocchio.
iir_14_4	Linear versus nonlinear classifiers In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers. To simplify the discussion, we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class membership by comparing a linear combination of the features to a threshold.  Figure 14.8: There are an infinite number of hyperplanes that separate two linearly separable classes. In two dimensions, a linear classifier is a line. Five examples are shown in Figure 14.8 . These lines have the functional form . The classification rule of a linear classifier is to assign a document to if and to if . Here, is the two-dimensional vector representation of the document and is the parameter vector that defines (together with ) the decision boundary. An alternative geometric interpretation of a linear classifier is provided in Figure 15.7 (page ). We can generalize this 2D linear classifier to higher dimensions by defining a hyperplane as we did in Equation 140, repeated here as Equation 144: (144)        decision hyperplane  Figure 14.9: Linear classification algorithm. The corresponding algorithm for linear classification in dimensions is shown in Figure 14.9 . Linear classification at first seems trivial given the simplicity of this algorithm. However, the difficulty is in training the linear classifier, that is, in determining the parameters and based on the training set. In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data. We now show that Rocchio and Naive Bayes are linear classifiers. To see this for Rocchio, observe that a vector is on the decision boundary if it has equal distance to the two class centroids:     (145)     14.8 We can derive the linearity of Naive Bayes from its decision rule, which chooses the category with the largest (Figure 13.2 , page 13.2 ) where: (146)         (147)   We choose class if the odds are greater than 1 or, equivalently, if the log odds are greater than 0. It is easy to see that Equation 147 is an instance of Equation 144 for , number of occurrences of in , and . Here, the index , , refers to terms of the vocabulary (not to positions in as does; cf. variantmultinomial) and and are -dimensional vectors. So in log space, Naive Bayes is a linear classifier.   prime 0.70 0 1 dlrs -0.71 1 1 rate 0.67 1 0 world -0.35 1 0 interest 0.63 0 0 sees -0.33 0 0 rates 0.60 0 0 year -0.25 0 0 discount 0.46 1 0 group -0.24 0 0 bundesbank 0.43 0 0 dlr -0.24 0 0 A linear classifier. The dimensions and parameters of a linear classifier for the class interest (as in interest rate) in Reuters-21578. The threshold is . Terms like dlr and world have negative weights because they are indicators for the competing class currency.  Worked example. Table 14.4 defines a linear classifier for the category interest in Reuters-21578 (see Section 13.6 , page 13.6 ). We assign document ``rate discount dlrs world'' to interest since . We assign ``prime dlrs'' to the complement class (not in interest) since . For simplicity, we assume a simple binary vector representation in this example: 1 for occurring terms, 0 for non-occurring terms. End worked example.  A linear problem with noise. In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese-English web pages are squares. The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows). Figure 14.10 is a graphical example of a linear problem, which we define to mean that the underlying distributions and of the two classes are separated by a line. We call this separating line the class boundary . It is the ``true'' boundary of the two classes and we distinguish it from the decision boundary that the learning method computes to approximate the class boundary. As is typical in text classification, there are some noise documents in Figure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes. In Section 13.5 (page 13.5 ), we defined a noise feature as a misleading feature that, when included in the document representation, on average increases the classification error. Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error. Intuitively, the underlying distribution partitions the representation space into areas with mostly homogeneous class assignments. A document that does not conform with the dominant class in its area is a noise document. Noise documents are one reason why training a linear classifier is hard. If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data. More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading. If there exists a hyperplane that perfectly separates the two classes, then we call the two classes linearly separable . In fact, if linear separability holds, then there is an infinite number of linear separators (Exercise 14.4 ) as illustrated by Figure 14.8 , where the number of possible separating hyperplanes is infinite. Figure 14.8 illustrates another challenge in training a linear classifier. If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data. In general, some of these hyperplanes will do well on new data, some will not.  Figure 14.11: A nonlinear problem. An example of a nonlinear classifier is kNN. The nonlinearity of kNN is intuitively clear when looking at examples like Figure 14.6 . The decision boundaries of kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions. Figure 14.11 is another example of a nonlinear problem: there is no good linear separator between the distributions and because of the circular ``enclave'' in the upper left part of the graph. Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough. If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier. Exercises. Prove that the number of linear separators of two classes is either infinite or zero.
iir_14_5	Classification with more than two classes  Classification for classes that are not mutually exclusive is called any-of , multilabel , or multivalue classification . In this case, a document can belong to several classes simultaneously, or to a single class, or to none of the classes. A decision on one class leaves all options open for the others. It is sometimes said that the classes are independent of each other, but this is misleading since the classes are rarely statistically independent in the sense defined on page 13.5.2 . In terms of the formal definition of the classification problem in Equation 112 (page 112 ), we learn different classifiers in any-of classification, each returning either or : . Solving an any-of classification task with linear classifiers is straightforward: Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. The decision of one classifier has no influence on the decisions of the other classifiers. The second type of classification with more than two classes is one-of classification . Here, the classes are mutually exclusive. Each document must belong to exactly one of the classes. One-of classification is also called multinomial , polytomous , multiclass , or single-label classification . Formally, there is a single classification function in one-of classification whose range is , i.e., . kNN is a (nonlinear) one-of classifier. True one-of problems are less common in text classification than any-of problems. With classes like UK, China, poultry, or coffee, a document can be relevant to many topics simultaneously - as when the prime minister of the UK visits China to talk about the coffee and poultry trade. Nevertheless, we will often make a one-of assumption, as we did in Figure 14.1 , even if classes are not really mutually exclusive. For the classification problem of identifying the language of a document, the one-of assumption is a good approximation as most text is written in only one language. In such cases, imposing a one-of constraint can increase the classifier's effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated.  Figure 14.12: hyperplanes do not divide space into disjoint regions.    14.12   Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. Assign the document to the class with the maximum score, the maximum confidence value, or the maximum probability.     assigned class money-fx trade interest wheat corn grain true class               money-fx   95 0 10 0 0 0 trade   1 1 90 0 1 0 interest   13 0 0 0 0 0 wheat   0 0 1 34 3 7 corn   1 0 2 13 26 5 grain   0 0 2 14 5 10 A confusion matrix for Reuters-21578.For example, 14 documents from grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006).  An important tool for analyzing the performance of a classifier for classes is the confusion matrix . The confusion matrix shows for each pair of classes , how many documents from were incorrectly assigned to . In Table 14.5 , the classifier manages to distinguish the three financial classes money-fx, trade, and interest from the three agricultural classes wheat, corn, and grain, but makes many errors within these two groups. The confusion matrix can help pinpoint opportunities for improving the accuracy of the system. For example, to address the second largest error in Table 14.5 (14 in the row grain), one could attempt to introduce features that distinguish wheat documents from grain documents. Exercises. Create a training set of 300 documents, 100 each from three different languages (e.g., English, French, Spanish). Create a test set by the same procedure, but also add 100 documents from a fourth language. Train (i) a one-of classifier (ii) an any-of classifier on this training set and evaluate it on the test set. (iii) Are there any interesting differences in how the two classifiers behave on this task?
iir_14_6	The bias-variance tradeoff Nonlinear classifiers are more powerful than linear classifiers. For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier. Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification? To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning. The tradeoff helps explain why there is no universally optimal learning method. Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem. Throughout this section, we use linear and nonlinear classifiers as prototypical examples of ``less powerful'' and ``more powerful'' learning, respectively. This is a simplification for a number of reasons. First, many nonlinear models subsume linear models as a special case. For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier. Second, there are nonlinear models that are less complex than linear models. For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier. Third, the complexity of learning is not really a property of the classifier because there are many aspects of learning (such as feature selection, cf. feature, regularization, and constraints such as margin maximization in Chapter 15 ) that make a learning method either more powerful or less powerful without affecting the type of classifier that is the final result of learning - regardless of whether that classifier is linear or nonlinear. We refer the reader to the publications listed in Section 14.7 for a treatment of the bias-variance tradeoff that takes into account these complexities. In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification. We first need to state our objective in text classification more precisely. In Section 13.1 (page ), we said that we want to minimize classification error on the test set. The implicit assumption was that training documents and test documents are generated according to the same underlying distribution. We will denote this distribution where is the document and its label or class. graphclassmodelbernoulligraph were examples of generative models that decompose into the product of and . typicallineartypicalnonlinear depict generative models for with and . In this section, instead of using the number of correctly classified test documents (or, equivalently, the error rate on test documents) as evaluation measure, we adopt an evaluation measure that addresses the inherent uncertainty of labeling. In many text classification problems, a given document representation can arise from documents belonging to different classes. This is because documents from different classes can be mapped to the same document representation. For example, the one-sentence documents China sues France and France sues China are mapped to the same document representation in a bag of words model. But only the latter document is relevant to the class legal actions brought by France (which might be defined, for example, as a standing query by an international trade lawyer). To simplify the calculations in this section, we do not count the number of errors on the test set when evaluating a classifier, but instead look at how well the classifier estimates the conditional probability of a document being in a class. In the above example, we might have . Our goal in text classification then is to find a classifier such that, averaged over documents , is as close as possible to the true probability . We measure this using mean squared error:     (148)      We define a classifier to be optimal for a distribution if it minimizes . Minimizing MSE is a desideratum for classifiers. We also need a criterion for learning methods. Recall that we defined a learning method as a function that takes a labeled training set as input and returns a classifier . For learning methods, we adopt as our goal to find a that, averaged over training sets, learns classifiers with minimal MSE. We can formalize this as minimizing learning error :     (149)      We can use learning error as a criterion for selecting a learning method in statistical text classification. A learning method is optimal for a distribution if it minimizes the learning error.  (150)   (151)     (152)   (153)     (154)   (155)       (156) (157)   (158)     (159)   157   150   Writing for for better readability, we can transform Equation 149 as follows: (160)   (161)   (162) (163) (164)   162 157 14.6       Bias is the squared difference between , the true conditional probability of being in , and , the prediction of the learned classifier, averaged over training sets. Bias is large if the learning method produces classifiers that are consistently wrong. Bias is small if (i) the classifiers are consistently right or (ii) different training sets cause errors on different documents or (iii) different training sets cause positive and negative errors on the same documents, but that average out to close to 0. If one of these three conditions holds, then , the expectation over all training sets, is close to . Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary, a linear hyperplane. If the generative model has a complex nonlinear class boundary, the bias term in Equation 162 will be high because a large number of points will be consistently misclassified. For example, the circular enclave in Figure 14.11 does not fit a linear model and will be misclassified consistently by linear classifiers. We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier. If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method. But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average. Nonlinear methods like kNN have low bias. We can see in Figure 14.6 that the decision boundaries of kNN are variable - depending on the distribution of documents in the training set, learned decision boundaries can vary greatly. As a result, each document has a chance of being classified correctly for some training sets. The average prediction is therefore closer to and bias is smaller than for a linear learning method. Variance is the variation of the prediction of learned classifiers: the average squared difference between and its average . Variance is large if different training sets give rise to very different classifiers . It is small if the training set has a minor effect on the classification decisions makes, be they correct or incorrect. Variance measures how inconsistent the decisions are, not whether they are correct or incorrect. Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes. The decision lines produced by linear learning methods in and 14.11 will deviate slightly from the main class boundaries, depending on the training set, but the class assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected. The circular enclave in Figure 14.11 will be consistently misclassified. Nonlinear methods like kNN have high variance. It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes. It is therefore sensitive to noise documents of the sort depicted in Figure 14.10 . As a result the variance term in Equation 162 is large for kNN: Test documents are sometimes misclassified - if they happen to be close to a noise document in the training set - and sometimes correctly classified - if there are no noise documents in the training set near them. This results in high variation from training set to training set. High-variance learning methods are prone to overfitting the training data. The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution . In overfitting, the learning method also learns from noise. Overfitting increases MSE and frequently is a problem for high-variance learning methods. We can also think of variance as the model complexity or, equivalently, memory capacity of the learning method - how detailed a characterization of the training set it can remember and then apply to new data. This capacity corresponds to the number of independent parameters available to fit the training set. Each kNN neighborhood makes an independent classification decision. The parameter in this case is the estimate from Figure 14.7 . Thus, kNN's capacity is only limited by the size of the training set. It can memorize arbitrarily large training sets. In contrast, the number of parameters of Rocchio is fixed - parameters per dimension, one for each centroid - and independent of the size of the training set. The Rocchio classifier (in form of the centroids defining it) cannot ``remember'' fine-grained details of the distribution of the documents in the training set. According to Equation 149, our goal in selecting a learning method is to minimize learning error. The fundamental insight captured by Equation 162, which we can succinctly state as: learning-error = bias + variance, is that the learning error has two components, bias and variance, which in general cannot be minimized simultaneously. When comparing two learning methods and , in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance. The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias). Instead, we have to weigh the respective merits of bias and variance in our application and choose accordingly. This tradeoff is called the bias-variance tradeoff . Figure 14.10 provides an illustration, which is somewhat contrived, but will be useful as an example for the tradeoff. Some Chinese text contains English words written in the Roman alphabet like CPU, ONLINE, and GPS. Consider the task of distinguishing Chinese-only web pages from mixed Chinese-English web pages. A search engine might offer Chinese users without knowledge of English (but who understand loanwords like CPU) the option of filtering out mixed pages. We use two features for this classification task: number of Roman alphabet characters and number of Chinese characters on the web page. As stated earlier, the distribution ) of the generative model generates most mixed (respectively, Chinese) documents above (respectively, below) the short-dashed line, but there are a few noise documents. In Figure 14.10 , we see three classifiers: One-feature classifier. Shown as a dotted horizontal line. This classifier uses only one feature, the number of Roman alphabet characters. Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents). So a learning method producing this type of classifier has low variance. But its bias is high since it will consistently misclassify squares in the lower left corner and ``solid circle'' documents with more than 50 Roman characters. Linear classifier. Shown as a dashed line with long dashes. Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified. The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets. Thus, very few documents (documents close to the class boundary) will be inconsistently classified. ``Fit-training-set-perfectly'' classifier. Shown as a solid line. Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set. This method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test set right. But the variance of this learning method is high. Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified - something that a linear learning method is unlikely to do. It is perhaps surprising that so many of the best-known text classification algorithms are linear. Some of these methods, in particular linear SVMs, regularized logistic regression and regularized linear regression, are among the most effective known methods. The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.8 ). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases.
iir_14_7	References and further reading As discussed in Chapter 9 , Rocchio relevance feedback is due to Rocchio (1971). Joachims (1997) presents a probabilistic analysis of the method. Rocchio classification was widely used as a classification method in in the 1990s (Buckley et al., 1994b;a, Voorhees and Harman, 2005). Initially, it was used as a form of routing . Routing merely ranks documents according to relevance to a class without assigning them. Early work on filtering , a true classification approach that makes an assignment decision on each document, was published by Ittner et al. (1995) and Schapire et al. (1998). The definition of routing we use here should not be confused with another sense. Routing can also refer to the electronic distribution of documents to subscribers, the so-called push model of document distribution. In a pull model , each transfer of a document to the user is initiated by the user - for example, by means of search or by selecting it from a list of documents on a news aggregation website. Some authors restrict the name Roccchio classification to two-class problems and use the terms cluster-based (Iwayama and Tokunaga, 1995) and centroid-based classification (Han and Karypis, 2000, Tan and Cheng, 2007) for Rocchio classification with . A more detailed treatment of kNN can be found in (Hastie et al., 2001), including methods for tuning the parameter . An example of an approximate fast kNN algorithm is locality-based hashing (Andoni et al., 2006). Kleinberg (1997) presents an approximate kNN algorithm (where is the dimensionality of the space and the number of data points), but at the cost of exponential storage requirements: . Indyk (2004) surveys nearest neighbor methods in high-dimensional spaces. Early work on kNN in text classification was motivated by the availability of massively parallel hardware architectures (Creecy et al., 1992). Yang (1994) uses an inverted index to speed up kNN classification. The optimality result for 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart (1967). The effectiveness of Rocchio classification and kNN is highly dependent on careful parameter tuning (in particular, the parameters for Rocchio on page 14.2 and for kNN), feature engineering svm-text and feature selection feature. Buckley and Salton (1995), Yang and Kisiel (2003), Schapire et al. (1998) and Moschitti (2003) address these issues for Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al. (2000) compare feature selection methods for kNN. The bias-variance tradeoff was introduced by Geman et al. (1992). The derivation in Section 14.6 is for , but the tradeoff applies to many loss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995) and Lewis et al. (1996) discuss linear classifiers for text and Hastie et al. (2001) linear classifiers in general. Readers interested in the algorithms mentioned, but not described in this chapter may wish to consult Bishop (2006) for neural networks, Hastie et al. (2001) for linear and logistic regression, and Minsky and Papert (1988) for the perceptron algorithm . Anagnostopoulos et al. (2006) show that an inverted index can be used for highly efficient document classification with any linear classifier, provided that the classifier is still effective when trained on a modest number of features via feature selection. We have only presented the simplest method for combining two-class classifiers into a one-of classifier. Another important method is the use of error-correcting codes, where a vector of decisions of different two-class classifiers is constructed for each document. A test document's decision vector is then ``corrected'' based on the distribution of decision vectors in the training set, a procedure that incorporates information from all two-class classifiers and their correlations into the final classification decision (Dietterich and Bakiri, 1995). Ghamrawi and McCallum (2005) also exploit dependencies between classes in any-of classification. Allwein et al. (2000) propose a general framework for combining two-class classifiers.
iir_14_8	Exercises   Exercises. In Figure 14.13 , which of the three vectors , , and is (i) most similar to according to dot product similarity, (ii) most similar to according to cosine similarity, (iii) closest to according to Euclidean distance? Download Reuters-21578 and train and test Rocchio and kNN classifiers for the classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Use the ModApte split. You may want to use one of a number of software packages that implement Rocchio classification and kNN classification, for example, the Bow toolkit (McCallum, 1996). Download 20 Newgroups (page 8.2 ) and train and test Rocchio and kNN classifiers for its 20 classes. Show that the decision boundaries in Rocchio classification are, as in kNN, given by the Voronoi tessellation. Computing the distance between a dense centroid and a sparse vector is for a naive implementation that iterates over all dimensions. Based on the equality and assuming that has been precomputed, write down an algorithm that is instead, where is the number of distinct terms in the test document. Prove that the region of the plane consisting of all points with the same nearest neighbors is a convex polygon. Design an algorithm that performs an efficient 1NN search in 1 dimension (where efficiency is with respect to the number of documents ). What is the time complexity of the algorithm? Design an algorithm that performs an efficient 1NN search in 2 dimensions with at most polynomial (in ) preprocessing time. Can one design an exact efficient algorithm for 1NN for very large along the ideas you used to solve the last exercise? Show that Equation 145 defines a hyperplane with and . Figure 14.14: A simple non-separable set of points. We can easily construct non-separable data sets in high dimensions by embedding a non-separable set like the one shown in Figure 14.14 . Consider embedding Figure 14.14 in 3D and then perturbing the 4 points slightly (i.e., moving them a small distance in a random direction). Why would you expect the resulting configuration to be linearly separable? How likely is then a non-separable set of points in -dimensional space? Assuming two classes, show that the percentage of non-separable assignments of the vertices of a hypercube decreases with dimensionality for . For example, for the proportion of non-separable assignments is 0, for , it is . One of the two non-separable cases for is shown in Figure 14.14 , the other is its mirror image. Solve the exercise either analytically or by simulation. Although we point out the similarities of Naive Bayes with linear vector space classifiers, it does not make sense to represent count vectors (the document representations in NB) in a continuous vector space. There is however a formalization of NB that is analogous to Rocchio. Show that NB assigns a document to the class (represented as a parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4 , page 12.4 ) to the document (represented as a count vector as in Section 13.4.1 (page ), normalized to sum to 1) is smallest.
iir_15	Support vector machines and machine learning on documents Improving classifier effectiveness has been an area of intensive machine-learning research over the last two decades, and this work has led to a new generation of state-of-the-art classifiers, such as support vector machines, boosted decision trees, regularized logistic regression, neural networks, and random forests. Many of these methods, including support vector machines (SVMs), the main topic of this chapter, have been applied with success to information retrieval problems, particularly text classification. An SVM is a kind of large-margin classifier: it is a vector space based machine learning method where the goal is to find a decision boundary between two classes that is maximally far from any point in the training data (possibly discounting some points as outliers or noise). We will initially motivate and develop SVMs for the case of two-class data sets that are separable by a linear classifier (Section 15.1 ), and then extend the model in Section 15.2 to non-separable data, multi-class problems, and nonlinear models, and also present some additional discussion of SVM performance. The chapter then moves to consider the practical deployment of text classifiers in Section 15.3 : what sorts of classifiers are appropriate when, and how can you exploit domain-specific text features in classification? Finally, we will consider how the machine learning technology that we have been building for text classification can be applied back to the problem of learning how to rank documents in ad hoc retrieval (Section 15.4 ). While several machine learning methods have been applied to this task, use of SVMs has been prominent. Support vector machines are not necessarily better than other machine learning methods (except perhaps in situations with little training data), but they perform at the state-of-the-art level and have much current theoretical and empirical appeal.   Subsections Support vector machines: The linearly separable case Extensions to the SVM model Soft margin classification Multiclass SVMs Nonlinear SVMs Experimental results Issues in the classification of text documents Choosing what kind of classifier to use Improving classifier performance Large and difficult category taxonomies Features for text Document zones in text classification Upweighting document zones. Separate feature spaces for document zones. Connections to text summarization. Machine learning methods in ad hoc information retrieval A simple example of machine-learned scoring Result ranking by machine learning References and further reading
iir_15_1	Support vector machines: The linearly separable case  Figure 15.1: The support vectors are the 5 points right up against the margin of the classifier. For two-class, separable training data sets, such as the one in Figure 14.8 (page ), there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the perceptron algorithm (see references in vclassfurther) find just any linear separator, others, like Naive Bayes, search for the best linear separator according to some criterion. The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision surface to the closest data point determines the margin of the classifier. This method of construction necessarily means that the decision function for an SVM is fully specified by a (usually small) subset of the data which defines the position of the separator. These points are referred to as the support vectors (in a vector space, a point can be thought of as a vector between the origin and that point). Figure 15.1 shows the margin and support vectors for a sample problem. Other data points play no part in determining the decision surface that is chosen.  An intuition for large-margin classification.Insisting on a large margin reduces the capacity of the model: the range of angles at which the fat decision surface can be placed is smaller than for a decision hyperplane (cf. vclassline). Maximizing the margin seems good because points near the decision surface represent very uncertain classification decisions: there is almost a 50% chance of the classifier deciding either way. A classifier with a large margin makes no low certainty classification decisions. This gives you a classification safety margin: a slight error in measurement or a slight document variation will not cause a misclassification. Another intuition motivating SVMs is shown in Figure 15.2 . By construction, an SVM classifier insists on a large margin around the decision boundary. Compared to a decision hyperplane, if you have to place a fat separator between classes, you have fewer choices of where it can be put. As a result of this, the memory capacity of the model has been decreased, and hence we expect that its ability to correctly generalize to test data is increased (cf. the discussion of the bias-variance tradeoff in Chapter 14 , page 14.6 ). Let us formalize an SVM with algebra. A decision hyperplane (page 14.4 ) can be defined by an intercept term and a decision hyperplane normal vector which is perpendicular to the hyperplane. This vector is commonly referred to in the machine learning literature as the weight vector . To choose among all the hyperplanes that are perpendicular to the normal vector, we specify the intercept term . Because the hyperplane is perpendicular to the normal vector, all points on the hyperplane satisfy . Now suppose that we have a set of training data points , where each member is a pair of a point and a class label corresponding to it.For SVMs, the two data classes are always named and (rather than 1 and 0), and the intercept term is always explicitly represented as (rather than being folded into the weight vector by adding an extra always-on feature). The math works out much more cleanly if you do things this way, as we will see almost immediately in the definition of functional margin. The linear classifier is then: (165)    We are confident in the classification of a point if it is far away from the decision boundary. For a given data set and decision hyperplane, we define the functional margin of the example with respect to a hyperplane as the quantity . The functional margin of a data set with respect to a decision surface is then twice the functional margin of any of the points in the data set with minimal functional margin (the factor of 2 comes from measuring across the whole width of the margin, as in Figure 15.3 ). However, there is a problem with using this definition as is: the value is underconstrained, because we can always make the functional margin as big as we wish by simply scaling up and . For example, if we replace by and by then the functional margin is five times as large. This suggests that we need to place some constraint on the size of the vector. To get a sense of how to do that, let us look at the actual geometry.  Figure 15.3: The geometric margin of a point ( ) and a decision boundary ( ). What is the Euclidean distance from a point to the decision boundary? In Figure 15.3 , we denote by this distance. We know that the shortest distance between a point and a hyperplane is perpendicular to the plane, and hence, parallel to . A unit vector in this direction is . The dotted line in the diagram is then a translation of the vector . Let us label the point on the hyperplane closest to as . Then: (166)       (167)     (168)   geometric margin  168 15.2       6  Since we can scale the functional margin as we please, for convenience in solving large SVMs, let us choose to require that the functional margin of all data points is at least 1 and that it is equal to 1 for at least one data vector. That is, for all items in the data: (169)      is maximized For all ,      We are now optimizing a quadratic function subject to linear constraints. Quadratic optimization problems are a standard, well-known class of mathematical optimization problems, and many algorithms exist for solving them. We could in principle build our SVM using standard quadratic programming (QP) libraries, but there has been much recent research in this area aiming to exploit the structure of the kind of QP that emerges from an SVM. As a result, there are more intricate but much faster and more scalable libraries available especially for building SVMs, which almost everyone uses to build models. We will not present the details of such algorithms here. However, it will be helpful to what follows to understand the shape of the solution of such an optimization problem. The solution involves constructing a dual problem where a Lagrange multiplier is associated with each constraint in the primal problem: The solution is then of the form: In the solution, most of the are zero. Each non-zero indicates that the corresponding is a support vector. The classification function is then: (170)  dot product     To recap, we start with a training data set. The data set uniquely defines the best separating hyperplane, and we feed the data through a quadratic optimization procedure to find this plane. Given a new point to classify, the classification function in either Equation 165 or Equation 170 is computing the projection of the point onto the hyperplane normal. The sign of this function determines the class to assign to the point. If the point is within the margin of the classifier (or another confidence threshold that we might have determined to minimize classification mistakes) then the classifier can return ``don't know'' rather than one of the two classes. The value of may also be transformed into a probability of classification; fitting a sigmoid to transform the values is standard (Platt, 2000). Also, since the margin is constant, if the model includes dimensions from various sources, careful rescaling of some dimensions may be required. However, this is not a problem if our documents (points) are on the unit hypersphere.  Figure 15.4: A tiny 3 data point training set for an SVM. Worked example. Consider building an SVM over the (very little) data set shown in Figure 15.4 . Working geometrically, for an example like this, the maximum margin weight vector will be parallel to the shortest line connecting points of the two classes, that is, the line between and , giving a weight vector of . The optimal decision surface is orthogonal to that line and intersects it at the halfway point. Therefore, it passes through . So, the SVM decision boundary is: (171)  Working algebraically, with the standard constraint that , we seek to minimize . This happens when this constraint is satisfied with equality by the two support vectors. Further we know that the solution is for some . So we have that:         The margin is . This answer can be confirmed geometrically by examining Figure 15.4 . End worked example. Exercises. What is the minimum number of support vectors that there can be for a data set (which contains instances of each class)? The basis of being able to use kernels in SVMs (see Section 15.2.3 ) is that the classification function can be written in the form of Equation 170 (where, for large problems, most are 0). Show explicitly how the classification function could be written in this form for the data set from small-svm-eg. That is, write as a function where the data points appear and the only variable is . Install an SVM package such as SVMlight (http://svmlight.joachims.org/), and build an SVM for the data set discussed in small-svm-eg. Confirm that the program gives the same solution as the text. For SVMlight, or another package that accepts the same training data format, the training file would be: 1 1:2 2:3 1 1:2 2:0 1 1:1 2:1 The training command for SVMlight is then: svm_learn -c 1 -a alphas.dat train.dat model.dat The -c 1 option is needed to turn off use of the slack variables that we discuss in Section 15.2.1 . Check that the norm of the weight vector agrees with what we found in small-svm-eg. Examine the file alphas.dat which contains the values, and check that they agree with your answers in Exercise 15.1 .
iir_15_2_1	Soft margin classification For the very high dimensional problems common in text classification, sometimes the data are linearly separable. But in the general case they are not, and even if they are, we might prefer a solution that better separates the bulk of the data while ignoring a few weird noise documents.  Figure 15.5: Large margin classification with slack variables. If the training set is not linearly separable, the standard approach is to allow the fat decision margin to make a few mistakes (some points - outliers or noisy examples - are inside or on the wrong side of the margin). We then pay a cost for each misclassified example, which depends on how far it is from meeting the margin requirement given in Equation 169. To implement this, we introduce slack variables . A non-zero value for allows to not meet the margin requirement at a cost proportional to the value of . See Figure 15.5 . The formulation of the SVM optimization problem with slack variables is: The optimization problem is then trading off how fat it can make the margin versus how many points have to be moved around to allow this margin. The margin can be less than 1 for a point by setting , but then one pays a penalty of in the minimization for having done that. The sum of the gives an upper bound on the number of training errors. Soft-margin SVMs minimize training error traded off against margin. The parameter is a regularization term, which provides a way to control overfitting: as becomes large, it is unattractive to not respect the data at the cost of reducing the geometric margin; when it is small, it is easy to account for some data points with the use of slack variables and to have a fat margin placed so it models the bulk of the data. The dual problem for soft margin classification becomes: Neither the slack variables nor Lagrange multipliers for them appear in the dual problem. All we are left with is the constant bounding the possible size of the Lagrange multipliers for the support vector data points. As before, the with non-zero will be the support vectors. The solution of the dual problem is of the form: Again is not needed explicitly for classification, which can be done in terms of dot products with data points, as in Equation 170. Typically, the support vectors will be a small proportion of the training data. However, if the problem is non-separable or with small margin, then every data point which is misclassified or within the margin will have a non-zero . If this set of points becomes large, then, for the nonlinear case which we turn to in Section 15.2.3 , this can be a major slowdown for using SVMs at test time.   Classifier Mode Method Time complexity NB training   NB testing   Rocchio training   Rocchio testing   kNN training preprocessing kNN testing preprocessing kNN training no preprocessing kNN testing no preprocessing SVM training conventional ;       , empirically SVM training cutting planes SVM testing   Training and testing complexity of various classifiers including SVMs. Training is the time the learning method takes to learn a classifier over , while testing is the time it takes a classifier to classify one document. For SVMs, multiclass classification is assumed to be done by a set of one-versus-rest classifiers. is the average number of tokens per document, while is the average vocabulary (number of non-zero features) of a document. and are the numbers of tokens and types, respectively, in the test document.  The complexity of training and testing with linear SVMs is shown in Table 15.1 . The time for training an SVM is dominated by the time for solving the underlying QP, and so the theoretical and empirical complexity varies depending on the method used to solve it. The standard result for solving QPs is that it takes time cubic in the size of the data set (Kozlov et al., 1979). All the recent work on SVM training has worked to reduce that complexity, often by being satisfied with approximate solutions. Standardly, empirical complexity is about (Joachims, 2006a). Nevertheless, the super-linear training time of traditional SVM algorithms makes them difficult or impossible to use on very large training data sets. Alternative traditional SVM solution algorithms which are linear in the number of training examples scale badly with a large number of features, which is another standard attribute of text problems. However, a new training algorithm based on cutting plane techniques gives a promising answer to this issue by having running time linear in the number of training examples and the number of non-zero features in examples (Joachims, 2006a). Nevertheless, the actual speed of doing quadratic optimization remains much slower than simply counting terms as is done in a Naive Bayes model. Extending SVM algorithms to nonlinear SVMs, as in the next section, standardly increases training complexity by a factor of (since dot products between examples need to be calculated), making them impractical. In practice it can often be cheaper to materialize the higher-order features and to train a linear SVM.
iir_15_2_2	Multiclass SVMs SVMs are inherently two-class classifiers. The traditional way to do multiclass classification with SVMs is to use one of the methods discussed in Section 14.5 (page 14.5 ). In particular, the most common technique in practice has been to build one-versus-rest classifiers (commonly referred to as ``one-versus-all'' or OVA classification), and to choose the class which classifies the test datum with greatest margin. Another strategy is to build a set of one-versus-one classifiers, and to choose the class that is selected by the most classifiers. While this involves building classifiers, the time for training classifiers may actually decrease, since the training data set for each classifier is much smaller. However, these are not very elegant approaches to solving multiclass problems. A better alternative is provided by the construction of multiclass SVMs, where we build a two-class classifier over a feature vector derived from the pair consisting of the input features and the class of the datum. At test time, the classifier chooses the class . The margin during training is the gap between this value for the correct class and for the nearest other class, and so the quadratic program formulation will require that . This general method can be extended to give a multiclass formulation of various kinds of linear classifiers. It is also a simple instance of a generalization of classification where the classes are not just a set of independent, categorical labels, but may be arbitrary structured objects with relationships defined between them. In the SVM world, such work comes under the label of structural SVMs . We mention them again in Section 15.4.2 .
iir_15_2_3	Nonlinear SVMs  Figure 15.6: Projecting data that is not linearly separable into a higher dimensional space can make it linearly separable. With what we have presented so far, data sets that are linearly separable (perhaps with a few exceptions or some noise) are well-handled. But what are we going to do if the data set just doesn't allow classification by a linear classifier? Let us look at a one-dimensional case. The top data set in Figure 15.6 is straightforwardly classified by a linear classifier but the middle data set is not. We instead need to be able to pick out an interval. One way to solve this problem is to map the data on to a higher dimensional space and then to use a linear classifier in the higher dimensional space. For example, the bottom part of the figure shows that a linear separator can easily classify the data if we use a quadratic function to map the data into two dimensions (a polar coordinates projection would be another possibility). The general idea is to map the original feature space to some higher-dimensional feature space where the training set is separable. Of course, we would want to do so in ways that preserve relevant dimensions of relatedness between data points, so that the resultant classifier should still generalize well. SVMs, and also a number of other linear classifiers, provide an easy and efficient way of doing this mapping to a higher dimensional space, which is referred to as ``the kernel trick ''. It's not really a trick: it just exploits the math that we have seen. The SVM linear classifier relies on a dot product between data point vectors. Let . Then the classifier we have seen so far is: (172)      172  kernel function  Worked example. The quadratic kernel in two dimensions.quad-kernel For 2-dimensional vectors , , consider . We wish to show that this is a kernel, i.e., that for some . Consider . Then: (173)   (174)   (175)   (176)   End worked example. In the language of functional analysis, what kinds of functions are valid kernel functions ? Kernel functions are sometimes more precisely referred to as Mercer kernels , because they must satisfy Mercer's condition: for any such that is finite, we must have that: (177)     15.5 The two commonly used families of kernels are polynomial kernels and radial basis functions. Polynomial kernels are of the form . The case of is a linear kernel, which is what we had before the start of this section (the constant 1 just changing the threshold). The case of gives a quadratic kernel, and is very commonly used. We illustrated the quadratic kernel in quad-kernel. The most common form of radial basis function is a Gaussian distribution, calculated as: (178)  15.5 The world of SVMs comes with its own language, which is rather different from the language otherwise used in machine learning. The terminology does have deep roots in mathematics, but it's important not to be too awed by that terminology. Really, we are talking about some quite simple things. A polynomial kernel allows us to model feature conjunctions (up to the order of the polynomial). That is, if we want to be able to model occurrences of pairs of words, which give distinctive information about topic classification, not given by the individual words alone, like perhaps operating and system or ethnic and cleansing, then we need to use a quadratic kernel. If occurrences of triples of words give distinctive information, then we need to use a cubic kernel. Simultaneously you also get the powers of the basic features - for most text applications, that probably isn't useful, but just comes along with the math and hopefully doesn't do harm. A radial basis function allows you to have features that pick out circles (hyperspheres) - although the decision boundaries become much more complex as multiple such features interact. A string kernel lets you have features that are character subsequences of terms. All of these are straightforward notions which have also been used in many other places under different names.
iir_15_2_4	Experimental results       Roc- Dec.   linear SVM rbf-SVM   NB chio Trees kNN earn 96.0 96.1 96.1 97.8 98.0 98.2 98.1 acq 90.7 92.1 85.3 91.8 95.5 95.6 94.7 money-fx 59.6 67.6 69.4 75.4 78.8 78.5 74.3 grain 69.8 79.5 89.1 82.6 91.9 93.1 93.4 crude 81.2 81.5 75.5 85.8 89.4 89.4 88.7 trade 52.2 77.4 59.2 77.9 79.2 79.2 76.6 interest 57.6 72.5 49.1 76.7 75.6 74.8 69.1 ship 80.9 83.1 80.9 79.8 87.4 86.5 85.8 wheat 63.4 79.4 85.5 72.9 86.6 86.8 82.4 corn 45.2 62.2 87.7 71.4 87.5 87.8 84.6 microavg. 72.3 79.9 79.4 82.6 86.7 87.5 86.4 SVM classifier break-even F from ( Joachims, 2002a, p. 114). Results are shown for the 10 largest categories and for microaveraged performance over all 90 categories on the Reuters-21578 data set.  We presented results in Section 13.6 showing that an SVM is a very effective text classifier. The results of Dumais et al. (1998) given in Table 13.9 show SVMs clearly performing the best. This was one of several pieces of work from this time that established the strong reputation of SVMs for text classification. Another pioneering work on scaling and evaluating SVMs for text classification was (Joachims, 1998). We present some of his results from (Joachims, 2002a) in Table 15.2 .Joachims used a large number of term features in contrast to Dumais et al. (1998), who used MI feature selection (Section 13.5.1 , page 13.5.1 ) to build classifiers with a much more limited number of features. The success of the linear SVM mirrors the results discussed in Section 14.6 (page ) on other linear approaches like Naive Bayes. It seems that working with simple term features can get one a long way. It is again noticeable the extent to which different papers' results for the same machine learning methods differ. In particular, based on replications by other researchers, the Naive Bayes results of (Joachims, 1998) appear too weak, and the results in Table 13.9 should be taken as representative.
iir_15_3	Issues in the classification of text documents There are lots of applications of text classification in the commercial world; email spam filtering is perhaps now the most ubiquitous. Jackson and Moulinier (2002) write: ``There is no question concerning the commercial value of being able to classify documents automatically by content. There are myriad potential applications of such a capability for corporate Intranets, government departments, and Internet publishers.'' Most of our discussion of classification has focused on introducing various machine learning methods rather than discussing particular features of text documents relevant to classification. This bias is appropriate for a textbook, but is misplaced for an application developer. It is frequently the case that greater performance gains can be achieved from exploiting domain-specific text features than from changing from one machine learning method to another. Jackson and Moulinier (2002) suggest that ``Understanding the data is one of the keys to successful categorization, yet this is an area in which most categorization tool vendors are extremely weak. Many of the `one size fits all' tools on the market have not been tested on a wide range of content types.'' In this section we wish to step back a little and consider the applications of text classification, the space of possible solutions, and the utility of application-specific heuristics.   Subsections Choosing what kind of classifier to use Improving classifier performance Large and difficult category taxonomies Features for text Document zones in text classification Upweighting document zones. Separate feature spaces for document zones. Connections to text summarization.
iir_15_3_1	Choosing what kind of classifier to use When confronted with a need to build a text classifier, the first question to ask is how much training data is there currently available? None? Very little? Quite a lot? Or a huge amount, growing every day? Often one of the biggest practical challenges in fielding a machine learning classifier in real applications is creating or obtaining enough training data. For many problems and algorithms, hundreds or thousands of examples from each class are required to produce a high performance classifier and many real world contexts involve large sets of categories. We will initially assume that the classifier is needed as soon as possible; if a lot of time is available for implementation, much of it might be spent on assembling data resources. If you have no labeled training data, and especially if there are existing staff knowledgeable about the domain of the data, then you should never forget the solution of using hand-written rules. That is, you write standing queries, as we touched on at the beginning of Chapter 13 . For example: if (wheat or grain) and not (whole or bread) then Jacobs and Rau (1990) Hayes and Weinstein (1990)  13.4 If you have fairly little data and you are going to train a supervised classifier, then machine learning theory says you should stick to a classifier with high bias, as we discussed in Section 14.6 (page ). For example, there are theoretical and empirical results that Naive Bayes does well in such circumstances (Forman and Cohen, 2004, Ng and Jordan, 2001), although this effect is not necessarily observed in practice with regularized models over textual data (Klein and Manning, 2002). At any rate, a very low bias model like a nearest neighbor model is probably counterindicated. Regardless, the quality of the model will be adversely affected by the limited training data. Here, the theoretically interesting answer is to try to apply semi-supervised training methods . This includes methods such as bootstrapping or the EM algorithm, which we will introduce in Section 16.5 (page ). In these methods, the system gets some labeled documents, and a further large supply of unlabeled documents over which it can attempt to learn. One of the big advantages of Naive Bayes is that it can be straightforwardly extended to be a semi-supervised learning algorithm, but for SVMs, there is also semi-supervised learning work which goes under the title of transductive SVMs . See the references for pointers. Often, the practical answer is to work out how to get more labeled data as quickly as you can. The best way to do this is to insert yourself into a process where humans will be willing to label data for you as part of their natural tasks. For example, in many cases humans will sort or route email for their own purposes, and these actions give information about classes. The alternative of getting human labelers expressly for the task of training classifiers is often difficult to organize, and the labeling is often of lower quality, because the labels are not embedded in a realistic task context. Rather than getting people to label all or a random sample of documents, there has also been considerable research on active learning , where a system is built which decides which documents a human should label. Usually these are the ones on which a classifier is uncertain of the correct classification. This can be effective in reducing annotation costs by a factor of 2-4, but has the problem that the good documents to label to train one type of classifier often are not the good documents to label to train a different type of classifier. If there is a reasonable amount of labeled data, then you are in the perfect position to use everything that we have presented about text classification. For instance, you may wish to use an SVM. However, if you are deploying a linear classifier such as an SVM, you should probably design an application that overlays a Boolean rule-based classifier over the machine learning classifier. Users frequently like to adjust things that do not come out quite right, and if management gets on the phone and wants the classification of a particular document fixed right now, then this is much easier to do by hand-writing a rule than by working out how to adjust the weights of an SVM without destroying the overall classification accuracy. This is one reason why machine learning models like decision trees which produce user-interpretable Boolean-like models retain considerable popularity. If a huge amount of data are available, then the choice of classifier probably has little effect on your results and the best choice may be unclear (cf. Banko and Brill, 2001). It may be best to choose a classifier based on the scalability of training or even runtime efficiency. To get to this point, you need to have huge amounts of data. The general rule of thumb is that each doubling of the training data size produces a linear increase in classifier performance, but with very large amounts of data, the improvement becomes sub-linear.
iir_15_3_2	Improving classifier performance For any particular application, there is usually significant room for improving classifier effectiveness through exploiting features specific to the domain or document collection. Often documents will contain zones which are especially useful for classification. Often there will be particular subvocabularies which demand special treatment for optimal classification effectiveness.   Subsections Large and difficult category taxonomies Features for text Document zones in text classification Upweighting document zones. Separate feature spaces for document zones. Connections to text summarization.
iir_15_4	Machine learning methods in ad hoc information retrieval Rather than coming up with term and document weighting functions by hand, as we primarily did in Chapter 6 , we can view different sources of relevance signal (cosine score, title match, etc.) as features in a learning problem. A classifier that has been fed examples of relevant and nonrelevant documents for each of a set of queries can then figure out the relative weights of these signals. If we configure the problem so that there are pairs of a document and a query which are assigned a relevance judgment of relevant or nonrelevant, then we can think of this problem too as a text classification problem. Taking such a classification approach is not necessarily best, and we present an alternative in Section 15.4.2 . Nevertheless, given the material we have covered, the simplest place to start is to approach this problem as a classification problem, by ordering the documents according to the confidence of a two-class classifier in its relevance decision. And this move is not purely pedagogical; exactly this approach is sometimes used in practice.   Subsections A simple example of machine-learned scoring Result ranking by machine learning
iir_15_4_1	A simple example of machine-learned scoring In this section we generalize the methodology of Section 6.1.2 (page ) to machine learning of the scoring function. In Section 6.1.2 we considered a case where we had to combine Boolean indicators of relevance; here we consider more general factors to further develop the notion of machine-learned relevance . In particular, the factors we now consider go beyond Boolean functions of query term presence in document zones, as in Section 6.1.2 . We develop the ideas in a setting where the scoring function is a linear combination of two factors: (1) the vector space cosine similarity between query and document and (2) the minimum window width within which the query terms lie. As we noted in Section 7.2.2 (page ), query term proximity is often very indicative of a document being on topic, especially with longer documents and on the web. Among other things, this quantity gives us an implementation of implicit phrases. Thus we have one factor that depends on the statistics of query terms in the document as a bag of words, and another that depends on proximity weighting. We consider only two features in the development of the ideas because a two-feature exposition remains simple enough to visualize. The technique can be generalized to many more features. As in Section 6.1.2 , we are provided with a set of training examples, each of which is a pair consisting of a query and a document, together with a relevance judgment for that document on that query that is either relevant or nonrelevant. For each such example we can compute the vector space cosine similarity, as well as the window width . The result is a training set as shown in Table 15.3 , which resembles Figure 6.5 (page ) from Section 6.1.2 .   Table 15.3: Training examples for machine-learned scoring. Example DocID Query Cosine score Judgment 37 linux operating system 0.032 3 relevant 37 penguin logo 0.02 4 nonrelevant 238 operating system 0.043 2 relevant 238 runtime environment 0.004 2 nonrelevant 1741 kernel layer 0.022 3 relevant 2094 device driver 0.03 2 relevant 3191 device driver 0.027 5 nonrelevant   Here, the two features (cosine score denoted and window width ) are real-valued predictors. If we once again quantify the judgment relevant as 1 and nonrelevant as 0, we seek a scoring function that combines the values of the features to generate a value that is (close to) 0 or 1. We wish this function to be in agreement with our set of training examples as far as possible. Without loss of generality, a linear classifier will use a linear combination of features of the form (179)   6.1.2 179 15.3   15.7  A collection of training examples.Each R denotes a training example labeled relevant, while each N is a training example labeled nonrelevant. In this setting, the function from Equation 179 represents a plane ``hanging above'' Figure 15.7 . Ideally this plane (in the direction perpendicular to the page containing Figure 15.7 ) assumes values close to 1 above the points marked R, and values close to 0 above the points marked N. Since a plane is unlikely to assume only values close to 0 or 1 above the training sample points, we make use of thresholding: given any query and document for which we wish to determine relevance, we pick a value and if we declare the document to be relevant, else we declare the document to be nonrelevant. As we know from Figure 14.8 (page ), all points that satisfy form a line (shown as a dashed line in Figure 15.7 ) and we thus have a linear classifier that separates relevant from nonrelevant instances. Geometrically, we can find the separating line as follows. Consider the line passing through the plane whose height is above the page containing Figure 15.7 . Project this line down onto Figure 15.7 ; this will be the dashed line in Figure 15.7 . Then, any subsequent query/document pair that falls below the dashed line in Figure 15.7 is deemed nonrelevant; above the dashed line, relevant. Thus, the problem of making a binary relevant/nonrelevant judgment given training examples as above turns into one of learning the dashed line in Figure 15.7 separating relevant training examples from the nonrelevant ones. Being in the - plane, this line can be written as a linear equation involving and , with two parameters (slope and intercept). The methods of linear classification that we have already looked at in classificationsvm provide methods for choosing this line. Provided we can build a sufficiently rich collection of training samples, we can thus altogether avoid hand-tuning score functions as in Section 7.2.3 (page ). The bottleneck of course is the ability to maintain a suitably representative set of training examples, whose relevance assessments must be made by experts.
iir_15_4_2	Result ranking by machine learning The above ideas can be readily generalized to functions of many more than two variables. There are lots of other scores that are indicative of the relevance of a document to a query, including static quality (PageRank-style measures, discussed in Chapter 21 ), document age, zone contributions, document length, and so on. Providing that these measures can be calculated for a training document collection with relevance judgments, any number of such measures can be used to train a machine learning classifier. For instance, we could train an SVM over binary relevance judgments, and order documents based on their probability of relevance, which is monotonic with the documents' signed distance from the decision boundary. However, approaching IR result ranking like this is not necessarily the right way to think about the problem. Statisticians normally first divide problems into classification problems (where a categorical variable is predicted) versus regression problems (where a real number is predicted). In between is the specialized field of ordinal regression where a ranking is predicted. Machine learning for ad hoc retrieval is most properly thought of as an ordinal regression problem, where the goal is to rank a set of documents for a query, given training data of the same sort. This formulation gives some additional power, since documents can be evaluated relative to other candidate documents for the same query, rather than having to be mapped to a global scale of goodness, while also weakening the problem space, since just a ranking is required rather than an absolute measure of relevance. Issues of ranking are especially germane in web search, where the ranking at the very top of the results list is exceedingly important, whereas decisions of relevance of a document to a query may be much less important. Such work can and has been pursued using the structural SVM framework which we mentioned in Section 15.2.2 , where the class being predicted is a ranking of results for a query, but here we will present the slightly simpler ranking SVM. The construction of a ranking SVM proceeds as follows. We begin with a set of judged queries. For each training query , we have a set of documents returned in response to the query, which have been totally ordered by a person for relevance to the query. We construct a vector of features for each document/query pair, using features such as those discussed in Section 15.4.1 , and many more. For two documents and , we then form the vector of feature differences: (180)  By hypothesis, one of and has been judged more relevant. If is judged more relevant than , denoted ( should precede in the results ordering), then we will assign the vector the class ; otherwise . The goal then is to build a classifier which will return (181)       Both of the methods that we have just looked at use a linear weighting of document features that are indicators of relevance, as has most work in this area. It is therefore perhaps interesting to note that much of traditional IR weighting involves nonlinear scaling of basic measurements (such as log-weighting of term frequency, or idf). At the present time, machine learning is very good at producing optimal weights for features in a linear combination (or other similar restricted model classes), but it is not good at coming up with good nonlinear scalings of basic measurements. This area remains the domain of human feature engineering. The idea of learning ranking functions has been around for a number of years, but it is only very recently that sufficient machine learning knowledge, training document collections, and computational power have come together to make this method practical and exciting. It is thus too early to write something definitive on machine learning approaches to ranking in information retrieval, but there is every reason to expect the use and importance of machine learned ranking approaches to grow over time. While skilled humans can do a very good job at defining ranking functions by hand, hand tuning is difficult, and it has to be done again for each new document collection and class of users. Exercises. Plot the first 7 rows of Table 15.3 in the - plane to produce a figure like that in Figure 15.7 . Write down the equation of a line in the - plane separating the Rs from the Ns. Give a training example (consisting of values for and the relevance judgment) that when added to the training set makes it impossible to separate the R's from the N's using a line in the - plane.
iir_15_5	References and further reading The somewhat quirky name support vector machine originates in the neural networks literature, where learning algorithms were thought of as architectures, and often referred to as ``machines''. The distinctive element of this model is that the decision boundary to use is completely decided (``supported'') by a few training data points, the support vectors. For a more detailed presentation of SVMs, a good, well-known article-length introduction is (Burges, 1998). Chen et al. (2005) introduce the more recent -SVM, which provides an alternative parameterization for dealing with inseparable problems, whereby rather than specifying a penalty , you specify a parameter which bounds the number of examples which can appear on the wrong side of the decision surface. There are now also several books dedicated to SVMs, large margin learning, and kernels: (Cristianini and Shawe-Taylor, 2000) and (Schölkopf and Smola, 2001) are more mathematically oriented, while (Shawe-Taylor and Cristianini, 2004) aims to be more practical. For the foundations by their originator, see (Vapnik, 1998). Some recent, more general books on statistical learning, such as (Hastie et al., 2001) also give thorough coverage of SVMs. The construction of multiclass SVMs is discussed in (Weston and Watkins, 1999), (Crammer and Singer, 2001), and (Tsochantaridis et al., 2005). The last reference provides an introduction to the general framework of structural SVMs. The kernel trick was first presented in (Aizerman et al., 1964). For more about string kernels and other kernels for structured data, see (Lodhi et al., 2002) and (Gaertner et al., 2002). The Advances in Neural Information Processing (NIPS) conferences have become the premier venue for theoretical machine learning work, such as on SVMs. Other venues such as SIGIR are much stronger on experimental methodology and using text-specific features to improve classifier effectiveness. A recent comparison of most current machine learning classifiers (though on problems rather different from typical text problems) can be found in (Caruana and Niculescu-Mizil, 2006). (Li and Yang, 2003), discussed in Section 13.6 , is the most recent comparative evaluation of machine learning classifiers on text classification. Older examinations of classifiers on text problems can be found in (Yang and Liu, 1999, Dumais et al., 1998, Yang, 1999). Joachims (2002a) presents his work on SVMs applied to text problems in detail. Zhang and Oles (2001) present an insightful comparison of Naive Bayes, regularized logistic regression and SVM classifiers. Joachims (1999) discusses methods of making SVM learning practical over large text data sets. Joachims (2006a) improves on this work. A number of approaches to hierarchical classification have been developed in order to deal with the common situation where the classes to be assigned have a natural hierarchical organization (Weigend et al., 1999, Dumais and Chen, 2000, Koller and Sahami, 1997, McCallum et al., 1998). In a recent large study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005) conclude that hierarchical classification noticeably if still modestly outperforms flat classification. Classifier effectiveness remains limited by the very small number of training documents for many classes. For a more general approach that can be applied to modeling relations between classes, which may be arbitrary rather than simply the case of a hierarchy, see Tsochantaridis et al. (2005). Moschitti and Basili (2004) investigate the use of complex nominals, proper nouns and word senses as features in text classification. Dietterich (2002) overviews ensemble methods for classifier combination, while Schapire (2003) focuses particularly on boosting, which is applied to text classification in (Schapire and Singer, 2000). Chapelle et al. (2006) present an introduction to work in semi-supervised methods, including in particular chapters on using EM for semi-supervised text classification (Nigam et al., 2006) and on transductive SVMs (Joachims, 2006b). Sindhwani and Keerthi (2006) present a more efficient implementation of a transductive SVM for large data sets. Tong and Koller (2001) explore active learning with SVMs for text classification; Baldridge and Osborne (2004) point out that examples selected for annotation with one classifier in an active learning context may be no better than random examples when used with another classifier. Machine learning approaches to ranking for ad hoc retrieval were pioneered in (Wong et al., 1988), (Fuhr, 1992), and (Gey, 1994). But limited training data and poor machine learning techniques meant that these pieces of work achieved only middling results, and hence they only had limited impact at the time. Taylor et al. (2006) study using machine learning to tune the parameters of the BM25 family of ranking functions okapi-bm25 so as to maximize NDCG (Section 8.4 , page 8.4 ). Machine learning approaches to ordinal regression appear in (Herbrich et al., 2000) and (Burges et al., 2005), and are applied to clickstream data in (Joachims, 2002b). Cao et al. (2006) study how to make this approach effective in IR, and Qin et al. (2007) suggest an extension involving using multiple hyperplanes. Yue et al. (2007) study how to do ranking with a structural SVM approach, and in particular show how this construction can be effectively used to directly optimize for MAP ranked-evaluation, rather than using surrogate measures like accuracy or area under the ROC curve. Geng et al. (2007) study feature selection for the ranking problem. Other approaches to learning to rank have also been shown to be effective for web search, such as (Richardson et al., 2006, Burges et al., 2005).
iir_16	Flat clustering Clustering algorithms group a set of documents into subsets or clusters . The algorithms' goal is to create clusters that are coherent internally, but clearly different from each other. In other words, documents within a cluster should be as similar as possible; and documents in one cluster should be as dissimilar as possible from documents in other clusters.  Figure 16.1: An example of a data set with a clear cluster structure. Clustering is the most common form of unsupervised learning . No supervision means that there is no human expert who has assigned documents to classes. In clustering, it is the distribution and makeup of the data that will determine cluster membership. A simple example is Figure 16.1 . It is visually clear that there are three distinct clusters of points. This chapter and Chapter 17 introduce algorithms that find such clusters in an unsupervised fashion. The difference between clustering and classification may not seem great at first. After all, in both cases we have a partition of a set of documents into groups. But as we will see the two problems are fundamentally different. Classification is a form of supervised learning (Chapter 13 , page 13.1 ): our goal is to replicate a categorical distinction that a human supervisor imposes on the data. In unsupervised learning, of which clustering is the most important example, we have no such teacher to guide us. The key input to a clustering algorithm is the distance measure. In Figure 16.1 , the distance measure is distance in the 2D plane. This measure suggests three different clusters in the figure. In document clustering, the distance measure is often also Euclidean distance. Different distance measures give rise to different clusterings. Thus, the distance measure is an important means by which we can influence the outcome of clustering. Flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters and will be covered in Chapter 17 . Chapter 17 also addresses the difficult problem of labeling clusters automatically. A second important distinction can be made between hard and soft clustering algorithms. Hard clustering computes a hard assignment - each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft - a document's assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Latent semantic indexing, a form of dimensionality reduction, is a soft clustering algorithm (Chapter 18 , page 18.4 ). This chapter motivates the use of clustering in information retrieval by introducing a number of applications (Section 16.1 ), defines the problem we are trying to solve in clustering (Section 16.2 ) and discusses measures for evaluating cluster quality (Section 16.3 ). It then describes two flat clustering algorithms, -means (Section 16.4 ), a hard clustering algorithm, and the Expectation-Maximization (or EM) algorithm (Section 16.5 ), a soft clustering algorithm. -means is perhaps the most widely used flat clustering algorithm due to its simplicity and efficiency. The EM algorithm is a generalization of -means and can be applied to a large variety of document representations and distributions.   Subsections Clustering in information retrieval Problem statement A note on terminology. Cardinality - the number of clusters Evaluation of clustering K-means Cluster cardinality in K-means Model-based clustering References and further reading Exercises
iir_16_1	Clustering in information retrieval  cluster hypothesis Cluster hypothesis. Documents in the same cluster behave similarly with respect to relevance to information needs. 14 14   Table 16.1: Some applications of clustering in information retrieval. Application What is Benefit Example   clustered?     Search result clustering search results more effective information presentation to user Figure 16.2 Scatter-Gather (subsets of) collection alternative user interface: ``search without typing'' Figure 16.3 Collection clustering collection effective information presentation for exploratory browsing McKeown et al. (2002), http://news.google.com Language modeling collection increased precision and/or recall Liu and Croft (2004) Cluster-based retrieval collection higher efficiency: faster search Salton (1971a)   Table 16.1 shows some of the main applications of clustering in information retrieval. They differ in the set of documents that they cluster - search results, collection or subsets of the collection - and the aspect of an information retrieval system they try to improve - user experience, user interface, effectiveness or efficiency of the search system. But they are all based on the basic assumption stated by the cluster hypothesis.  Clustering of search results to improve recall. None of the top hits cover the animal sense of jaguar, but users can easily access it by clicking on the cat cluster in the Clustered Results panel on the left (third arrow from the top). The first application mentioned in Table 16.1 is search result clustering where by search results we mean the documents that were returned in response to a query. The default presentation of search results in information retrieval is a simple list. Users scan the list from top to bottom until they have found the information they are looking for. Instead, search result clustering clusters the search results, so that similar documents appear together. It is often easier to scan a few coherent groups than many individual documents. This is particularly useful if a search term has different word senses. The example in Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the animal and an Apple operating system. The Clustered Results panel returned by the Vivísimo search engine (http://vivisimo.com) can be a more effective user interface for understanding what is in the search results than a simple list of documents.  An example of a user session in Scatter-Gather. A collection of New York Times news stories is clustered (``scattered'') into eight clusters (top row). The user manually gathers three of these into a smaller collection International Stories and performs another scattering operation. This process repeats until a small cluster with relevant documents is found (e.g., Trinidad). A better user interface is also the goal of Scatter-Gather , the second application in Table 16.1 . Scatter-Gather clusters the whole collection to get groups of documents that the user can select or gather. The selected groups are merged and the resulting set is again clustered. This process is repeated until a cluster of interest is found. An example is shown in Figure 16.3 . Automatically generated clusters like those in Figure 16.3 are not as neatly organized as a manually constructed hierarchical tree like the Open Directory at http://dmoz.org. Also, finding descriptive labels for clusters automatically is a difficult problem (Section 17.7 , page 17.7 ). But cluster-based navigation is an interesting alternative to keyword searching, the standard information retrieval paradigm. This is especially true in scenarios where users prefer browsing over searching because they are unsure about which search terms to use. As an alternative to the user-mediated iterative clustering in Scatter-Gather, we can also compute a static hierarchical clustering of a collection that is not influenced by user interactions (``Collection clustering'' in Table 16.1 ). Google News and its precursor, the Columbia NewsBlaster system, are examples of this approach. In the case of news, we need to frequently recompute the clustering to make sure that users can access the latest breaking stories. Clustering is well suited for access to a collection of news stories since news reading is not really search, but rather a process of selecting a subset of stories about recent events. The fourth application of clustering exploits the cluster hypothesis directly for improving search results, based on a clustering of the entire collection. We use a standard inverted index to identify an initial set of documents that match the query, but we then add other documents from the same clusters even if they have low similarity to the query. For example, if the query is car and several car documents are taken from a cluster of automobile documents, then we can add documents from this cluster that use terms other than car (automobile, vehicle etc). This can increase recall since a group of documents with high mutual similarity is often relevant as a whole. More recently this idea has been used for language modeling. Equation 102 , page 102 , showed that to avoid sparse data problems in the language modeling approach to IR, the model of document can be interpolated with a collection model. But the collection contains many documents with terms untypical of . By replacing the collection model with a model derived from 's cluster, we get more accurate estimates of the occurrence probabilities of terms in . Clustering can also speed up search. As we saw in Section 6.3.2 ( page 6.3.2 ) search in the vector space model amounts to finding the nearest neighbors to the query. The inverted index supports fast nearest-neighbor search for the standard IR setting. However, sometimes we may not be able to use an inverted index efficiently, e.g., in latent semantic indexing (Chapter 18 ). In such cases, we could compute the similarity of the query to every document, but this is slow. The cluster hypothesis offers an alternative: Find the clusters that are closest to the query and only consider documents from these clusters. Within this much smaller set, we can compute similarities exhaustively and rank documents in the usual way. Since there are many fewer clusters than documents, finding the closest cluster is fast; and since the documents matching a query are all similar to each other, they tend to be in the same clusters. While this algorithm is inexact, the expected decrease in search quality is small. This is essentially the application of clustering that was covered in Section 7.1.6 (page 7.1.6 ). Exercises. Define two documents as similar if they have at least two proper names like Clinton or Sarkozy in common. Give an example of an information need and two documents, for which the cluster hypothesis does not hold for this notion of similarity. Make up a simple one-dimensional example (i.e. points on a line) with two clusters where the inexactness of cluster-based retrieval shows up. In your example, retrieving clusters close to the query should do worse than direct nearest neighbor search.
iir_16_2	Problem statement     objective function    The objective function is often defined in terms of similarity or distance between documents. Below, we will see that the objective in -means clustering is to minimize the average distance between documents and their centroids or, equivalently, to maximize the similarity between documents and their centroids. The discussion of similarity measures and distance metrics in Chapter 14 (page 14.1 ) also applies to this chapter. As in Chapter 14 , we use both similarity and distance to talk about relatedness between documents. For documents, the type of similarity we want is usually topic similarity or high values on the same dimensions in the vector space model. For example, documents about China have high values on dimensions like Chinese, Beijing, and Mao whereas documents about the UK tend to have high values for London, Britain and Queen. We approximate topic similarity with cosine similarity or Euclidean distance in vector space (Chapter 6 ). If we intend to capture similarity of a type other than topic, for example, similarity of language, then a different representation may be appropriate. When computing topic similarity, stop words can be safely ignored, but they are important cues for separating clusters of English (in which the occurs frequently and la infrequently) and French documents (in which the occurs infrequently and la frequently).   Subsections A note on terminology. Cardinality - the number of clusters
iir_16_2_1	Cardinality - the number of clusters  cardinality       16.3  Since our goal is to optimize an objective function, clustering is essentially a search problem. The brute force solution would be to enumerate all possible clusterings and pick the best. However, there are exponentially many partitions, so this approach is not feasible. For this reason, most flat clustering algorithms refine an initial partitioning iteratively. If the search starts at an unfavorable initial point, we may miss the global optimum. Finding a good starting point is therefore another important problem we have to solve in flat clustering.
iir_16_3	Evaluation of clustering Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low inter-cluster similarity (documents from different clusters are dissimilar). This is an internal criterion for the quality of a clustering. But good scores on an internal criterion do not necessarily translate into good effectiveness in an application. An alternative to internal criteria is direct evaluation in the application of interest. For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms. This is the most direct evaluation, but it is expensive, especially if large user studies are necessary. As a surrogate for user judgments, we can use a set of classes in an evaluation benchmark or gold standard (see Section 8.5 , page 8.5 , and Section 13.6 , page 13.6 ). The gold standard is ideally produced by human judges with a good level of inter-judge agreement (see Chapter 8 , page 8.1 ). We can then compute an external criterion that evaluates how well the clustering matches the gold standard classes. For example, we may want to say that the optimal clustering of the search results for jaguar in Figure 16.2 consists of three classes corresponding to the three senses car, animal, and operating system. In this type of evaluation, we only use the partition provided by the gold standard, not the class labels. This section introduces four external criteria of clustering quality. Purity is a simple and transparent evaluation measure. Normalized mutual information can be information-theoretically interpreted. The Rand index penalizes both false positive and false negative decisions during clustering. The F measure in addition supports differential weighting of these two types of errors.   To compute purity , each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by . Formally: (182)          182 We present an example of how to compute purity in Figure 16.4 . Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1 . Purity is compared with the other three measures discussed in this chapter in Table 16.2 .   Table 16.2: The four external evaluation measures applied to the clustering in Figure 16.4 .   purity NMI RI lower bound 0.0 0.0 0.0 0.0 maximum 1 1 1 1 value for Figure 16.4 0.71 0.36 0.68 0.46   High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. A measure that allows us to make this tradeoff is normalized mutual information or NMI : (183)    13 13.5.1  (184)   (185)          185 184 is entropy as defined in Chapter 5 (page 5.3.2 ): (186)   (187)   in Equation 184 measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are. The minimum of is 0 if the clustering is random with respect to class membership. In that case, knowing that a document is in a particular cluster does not give us any new information about what its class might be. Maximum mutual information is reached for a clustering that perfectly recreates the classes - but also if clusters in are further subdivided into smaller clusters (Exercise 16.7 ). In particular, a clustering with one-document clusters has maximum MI. So MI has the same problem as purity: it does not penalize large cardinalities and thus does not formalize our bias that, other things being equal, fewer clusters are better. The normalization by the denominator in Equation 183 fixes this problem since entropy tends to increase with the number of clusters. For example, reaches its maximum for , which ensures that NMI is low for . Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters. The particular form of the denominator is chosen because is a tight upper bound on (Exercise 16.7 ). Thus, NMI is always a number between 0 and 1. An alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions, one for each of the pairs of documents in the collection. We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters. The Rand index ( ) measures the percentage of decisions that are correct. That is, it is simply accuracy (Section 8.3 , page 8.3 ).     As an example, we compute RI for Figure 16.4 . We first compute . The three clusters contain 6, 6, and 5 points, respectively, so the total number of ``positives'' or pairs of documents that are in the same cluster is: (188)    (189)   and are computed similarly, resulting in the following contingency table:   Same cluster Different clusters Same class Different classes   The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the F measure measuresperf to penalize false negatives more strongly than false positives by selecting a value , thus giving more weight to recall.            Exercises. Replace every point in Figure 16.4 with two identical copies of in the same class. (i) Is it less difficult, equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in Figure 16.4 ? (ii) Compute purity, NMI, RI, and for the clustering with 34 points. Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings?
iir_16_4	K-means  6 6.4.4  centroid    (190)  The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way. We used centroids for Rocchio classification in Chapter 14 (page 14.2 ). They play a similar role here. The ideal cluster in -means is a sphere with the centroid as its center of gravity. Ideally, the clusters should not overlap. Our desiderata for classes in Rocchio classification were the same. The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster. A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS , the squared distance of each vector from its centroid summed over all vectors:          (191)    objective function          seeds 16.5 16.6  17.2 17.2 We can apply one of the following termination conditions. A fixed number of iterations has been completed. This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations. Assignment of documents to clusters (the partitioning function ) does not change between iterations. Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long. Centroids do not change between iterations. This is equivalent to not changing (Exercise 16.4.1 ). Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. In practice, we need to combine it with a bound on the number of iterations to guarantee termination. Terminate when the decrease in RSS falls below a threshold . For small , this indicates that we are close to convergence. Again, we need to combine it with a bound on the number of iterations to prevent very long runtimes. We now show that -means converges by proving that monotonically decreases in each iteration. We will use decrease in the meaning decrease or does not change in this section. First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to decreases. Second, it decreases in the recomputation step because the new centroid is the vector for which reaches its minimum. (192) (193)           (194)      Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum. Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids. Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost.   While this proves the convergence of -means, there is unfortunately no guarantee that a global minimum in the objective function will be reached. This is a particular problem if a document set contains many outliers , documents that are far from any other documents and therefore do not fit well into any cluster. Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations. Thus, we end up with a singleton cluster (a cluster with only one document) even though there is probably a clustering with lower RSS. Figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds. Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise 16.7 ). Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering. Since deterministic hierarchical clustering methods are more predictable than -means, a hierarchical clustering of a small random sample of size (e.g., for or ) often provides good seeds (see the description of the Buckshot algorithm, Chapter 17 , page 17.8 ). Other initialization methods compute seeds that are not selected from the vectors to be clustered. A robust method that works well for a large variety of document distributions is to select (e.g., ) random vectors for each cluster and use their centroid as the seed for this cluster. See Section 16.6 for more sophisticated initializations. What is the time complexity of -means? Most of the time is spent on computing vector distances. One such operation costs . The reassignment step computes distances, so its overall complexity is . In the recomputation step, each vector gets added to a centroid once, so the complexity of this step is . For a fixed number of iterations , the overall complexity is therefore . Thus, -means is linear in all relevant factors: iterations, number of clusters, number of vectors and dimensionality of the space. This means that -means is more efficient than the hierarchical algorithms in Chapter 17 . We had to fix the number of iterations , which can be tricky in practice. But in most cases, -means quickly reaches either complete convergence or a clustering that is close to convergence. In the latter case, a few documents would switch membership if further iterations were computed, but this has a small effect on the overall quality of the clustering. There is one subtlety in the preceding argument. Even a linear algorithm can be quite slow if one of the arguments of is large, and usually is large. High dimensionality is not a problem for computing the distance between two documents. Their vectors are sparse, so that only a small fraction of the theoretically possible componentwise differences need to be computed. Centroids, however, are dense since they pool all terms that occur in any of the documents of their clusters. As a result, distance computations are time consuming in a naive implementation of -means. However, there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities. Truncating centroids to the most significant terms (e.g., ) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in Section 16.6 ). The same efficiency problem is addressed by K-medoids , a variant of -means that computes medoids instead of centroids as cluster centers. We define the medoid of a cluster as the document vector that is closest to the centroid. Since medoids are sparse document vectors, distance computations are fast.  Estimated minimal residual sum of squares as a function of the number of clusters in -means. In this clustering of 1203 Reuters-RCV1 documents, there are two points where the curve flattens: at 4 clusters and at 9 clusters. The documents were selected from the categories China, Germany, Russia and Sports, so the clustering is closest to the Reuters classification.   Subsections Cluster cardinality in K-means
iir_16_4_1	Cluster cardinality in K-means 16.2   A naive approach would be to select the optimal value of according to the objective function, namely the value of that minimizes RSS. Defining as the minimal RSS of all clusterings with clusters, we observe that is a monotonically decreasing function in (Exercise 16.7 ), which reaches its minimum 0 for where is the number of documents. We would end up with each document being in its own cluster. Clearly, this is not an optimal clustering. A heuristic method that gets around this problem is to estimate as follows. We first perform (e.g., ) clusterings with clusters (each with a different initialization) and compute the RSS of each. Then we take the minimum of the RSS values. We denote this minimum by . Now we can inspect the values as increases and find the ``knee'' in the curve - the point where successive decreases in become noticeably smaller. There are two such points in Figure 16.8 , one at , where the gradient flattens slightly, and a clearer flattening at . This is typical: there is seldom a single best number of clusters. We still need to employ an external constraint to choose from a number of possible values of (4 and 9 in this case). A second type of criterion for cluster cardinality imposes a penalty for each new cluster - where conceptually we start with a single cluster containing all documents and then search for the optimal number of clusters by successively incrementing by one. To determine the cluster cardinality in this way, we create a generalized objective function that combines two elements: distortion , a measure of how much documents deviate from the prototype of their clusters (e.g., RSS for -means); and a measure of model complexity . We interpret a clustering here as a model of the data. Model complexity in clustering is usually the number of clusters or a function thereof. For -means, we then get this selection criterion for :     (195)       The obvious difficulty with Equation 195 is that we need to determine . Unless this is easier than determining directly, then we are back to square one. In some cases, we can choose values of that have worked well for similar data sets in the past. For example, if we periodically cluster news stories from a newswire, there is likely to be a fixed value of that gives us the right in each successive clustering. In this application, we would not be able to determine based on past experience since changes. A theoretical justification for Equation 195 is the Akaike Information Criterion or AIC, an information-theoretic measure that trades off distortion against model complexity. The general form of AIC is: (196)      For -means, the AIC can be stated as follows: (197)  197 195  To derive Equation 197 from Equation 196 observe that in -means since each element of the centroids is a parameter that can be varied independently; and that (modulo a constant) if we view the model underlying -means as a Gaussian mixture with hard assignment, uniform cluster priors and identical spherical covariance matrices (see Exercise 16.7 ). The derivation of AIC is based on a number of assumptions, e.g., that the data are . These assumptions are only approximately true for data sets in information retrieval. As a consequence, the AIC can rarely be applied without modification in text clustering. In Figure 16.8 , the dimensionality of the vector space is 50,000. Thus, dominates the smaller RSS-based term ( , not shown in the figure) and the minimum of the expression is reached for . But as we know, (corresponding to the four classes China, Germany, Russia and Sports) is a better choice than . In practice, Equation 195 is often more useful than Equation 197 - with the caveat that we need to come up with an estimate for . Exercises. Why are documents that do not use the same term for the concept car likely to end up in the same cluster in -means clustering? Two of the possible termination conditions for -means were (1) assignment does not change, (2) centroids do not change (page 16.4 ). Do these two conditions imply each other?
iir_16_5	Model-based clustering In this section, we describe a generalization of -means, the EM algorithm. It can be applied to a larger variety of document representations and distributions than -means. In -means, we attempt to find centroids that are good representatives. We can view the set of centroids as a model that generates the data. Generating a document in this model consists of first picking a centroid at random and then adding some noise. If the noise is normally distributed, this procedure will result in clusters of spherical shape. Model-based clustering assumes that the data were generated by a model and tries to recover the original model from the data. The model that we recover from the data then defines clusters and an assignment of documents to clusters. A commonly used criterion for estimating the model parameters is maximum likelihood. In -means, the quantity is proportional to the likelihood that a particular model (i.e., a set of centroids) generated the data. For -means, maximum likelihood and minimal RSS are equivalent criteria. We denote the model parameters by . In -means, . More generally, the maximum likelihood criterion is to select the parameters that maximize the log-likelihood of generating the data : (198)    This is the same approach we took in Chapter 12 (page 12.1.1 ) for language modeling and in Section 13.1 (page 13.4 ) for text classification. In text classification, we chose the class that maximizes the likelihood of generating a particular document. Here, we choose the clustering that maximizes the likelihood of generating a given set of documents. Once we have , we can compute an assignment probability for each document-cluster pair. This set of assignment probabilities defines a soft clustering. An example of a soft assignment is that a document about Chinese cars may have a fractional membership of 0.5 in each of the two clusters China and automobiles, reflecting the fact that both topics are pertinent. A hard clustering like -means cannot model this simultaneous relevance to two topics. Model-based clustering provides a framework for incorporating our knowledge about a domain. -means and the hierarchical algorithms in Chapter 17 make fairly rigid assumptions about the data. For example, clusters in -means are assumed to be spheres. Model-based clustering offers more flexibility. The clustering model can be adapted to what we know about the underlying distribution of the data, be it Bernoulli (as in the example in Table 16.3 ), Gaussian with non-spherical variance (another model that is important in document clustering) or a member of a different family. A commonly used algorithm for model-based clustering is the Expectation-Maximization algorithm or EM algorithm . EM clustering is an iterative algorithm that maximizes . EM can be applied to many different types of probabilistic modeling. We will work with a mixture of multivariate Bernoulli distributions here, the distribution we know from Section 11.3 (page 11.3 ) and Section 13.3 (page 13.3 ):     (199)               The mixture model then is:     (200)       How do we use EM to infer the parameters of the clustering from the data? That is, how do we choose parameters that maximize ? EM is similar to -means in that it alternates between an expectation step , corresponding to reassignment, and a maximization step , corresponding to recomputation of the parameters of the model. The parameters of -means are the centroids, the parameters of the instance of EM in this section are the and . The maximization step recomputes the conditional parameters and the priors as follows:      (201)        13.3 13.3 The expectation step computes the soft assignment of documents to clusters given the current parameters and :      (202)   200   13.3   (a) docID document text docID document text   1 hot chocolate cocoa beans 7 sweet sugar   2 cocoa ghana africa 8 sugar cane brazil   3 beans harvest ghana 9 sweet sugar beet   4 cocoa butter 10 sweet cake icing   5 butter truffles 11 cake black forest   6 sweet chocolate     (b) Parameter Iteration of clustering     0 1 2 3 4 5 15 25     0.50 0.45 0.53 0.57 0.58 0.54 0.45     1.00 1.00 1.00 1.00 1.00 1.00 1.00     0.50 0.79 0.99 1.00 1.00 1.00 1.00     0.50 0.84 1.00 1.00 1.00 1.00 1.00     0.50 0.75 0.94 1.00 1.00 1.00 1.00     0.50 0.52 0.66 0.91 1.00 1.00 1.00   1.00 1.00 1.00 1.00 1.00 1.00 0.83 0.00   0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.50 0.40 0.14 0.01 0.00 0.00 0.00     0.50 0.57 0.58 0.41 0.07 0.00 0.00     0.000 0.100 0.134 0.158 0.158 0.169 0.200     0.000 0.083 0.042 0.001 0.000 0.000 0.000     0.000 0.000 0.000 0.000 0.000 0.000 0.000     0.000 0.167 0.195 0.213 0.214 0.196 0.167     0.000 0.400 0.432 0.465 0.474 0.508 0.600     0.000 0.167 0.090 0.014 0.001 0.000 0.000     0.000 0.000 0.000 0.000 0.000 0.000 0.000     1.000 0.500 0.585 0.640 0.642 0.589 0.500     1.000 0.300 0.238 0.180 0.159 0.153 0.000     1.000 0.417 0.507 0.610 0.640 0.608 0.667 The EM clustering algorithm.The table shows a set of documents (a) and parameter values for selected iterations during EM clustering (b). Parameters shown are prior , soft assignment scores (both omitted for cluster 2), and lexical parameters for a few terms. The authors initially assigned document 6 to cluster 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For smoothing, the in Equation  201 were replaced with where .  We clustered a set of 11 documents into two clusters using EM in Table 16.3 . After convergence in iteration 25, the first 5 documents are assigned to cluster 1 ( ) and the last 6 to cluster 2 (). Somewhat atypically, the final assignment is a hard assignment here. EM usually converges to a soft assignment. In iteration 25, the prior for cluster 1 is because 5 of the 11 documents are in cluster 1. Some terms are quickly associated with one cluster because the initial assignment can ``spread'' to them unambiguously. For example, membership in cluster 2 spreads from document 7 to document 8 in the first iteration because they share sugar ( in iteration 1). For parameters of terms occurring in ambiguous contexts, convergence takes longer. Seed documents 6 and 7 both contain sweet. As a result, it takes 25 iterations for the term to be unambiguously associated with cluster 2. ( in iteration 25.) Finding good seeds is even more critical for EM than for -means. EM is prone to get stuck in local optima if the seeds are not chosen well. This is a general problem that also occurs in other applications of EM.Therefore, as with -means, the initial assignment of documents to clusters is often computed by a different algorithm. For example, a hard -means clustering may provide the initial assignment, which EM can then ``soften up.'' Exercises. We saw above that the time complexity of -means is . What is the time complexity of EM?
iir_16_6	References and further reading Berkhin (2006b) gives a general up-to-date survey of clustering methods with special attention to scalability. The classic reference for clustering in pattern recognition, covering both -means and EM, is (Duda et al., 2000). Rasmussen (1992) introduces clustering from an information retrieval perspective. Anderberg (1973) provides a general introduction to clustering for applications. In addition to Euclidean distance and cosine similarity , Kullback-Leibler divergence is often used in clustering as a measure of how (dis)similar documents and clusters are (Xu and Croft, 1999, Muresan and Harper, 2004, Kurland and Lee, 2004). The cluster hypothesis is due to Jardine and van Rijsbergen (1971) who state it as follows: Associations between documents convey information about the relevance of documents to requests. Croft (1978), Can and Ozkarahan (1990), Voorhees (1985a), Salton (1975), Cacheda et al. (2003), Salton (1971a), Singitham et al. (2004), Can et al. (2004) and Altingövde et al. (2008) investigate the efficiency and effectiveness of cluster-based retrieval. While some of these studies show improvements in effectiveness, efficiency or both, there is no consensus that cluster-based retrieval works well consistently across scenarios. Cluster-based language modeling was pioneered by Liu and Croft (2004). There is good evidence that clustering of search results improves user experience and search result quality (Hearst and Pedersen, 1996, Zamir and Etzioni, 1999, Käki, 2005, Toda and Kataoka, 2005, Tombros et al., 2002), although not as much as search result structuring based on carefully edited category hierarchies (Hearst, 2006). The Scatter-Gather interface for browsing collections was presented by Cutting et al. (1992). A theoretical framework for analyzing the properties of Scatter/Gather and other information seeking user interfaces is presented by Pirolli (2007). Schütze and Silverstein (1997) evaluate LSI (Chapter 18 ) and truncated representations of centroids for efficient -means clustering. The Columbia NewsBlaster system (McKeown et al., 2002), a forerunner to the now much more famous and refined Google News (http://news.google.com), used hierarchical clustering (Chapter 17 ) to give two levels of news topic granularity. See Hatzivassiloglou et al. (2000) for details, and Chen and Lin (2000) and Radev et al. (2001) for related systems. Other applications of clustering in information retrieval are duplicate detection (Yang and Callan (2006), shingling), novelty detection (see references in hclstfurther) and metadata discovery on the semantic web (Alonso et al., 2006). The discussion of external evaluation measures is partially based on Strehl (2002). Dom (2002) proposes a measure that is better motivated theoretically than NMI. is the number of bits needed to transmit class memberships assuming cluster memberships are known. The Rand index is due to Rand (1971). Hubert and Arabie (1985) propose an adjusted that ranges between and 1 and is 0 if there is only chance agreement between clusters and classes (similar to in Chapter 8 , page 8.2 ). Basu et al. (2004) argue that the three evaluation measures NMI, Rand index and F measure give very similar results. Stein et al. (2003) propose expected edge density as an internal measure and give evidence that it is a good predictor of the quality of a clustering. Kleinberg (2002) and Meila (2005) present axiomatic frameworks for comparing clusterings. Authors that are often credited with the invention of the -means algorithm include Lloyd (1982) (first distributed in 1957), Ball (1965), MacQueen (1967), and Hartigan and Wong (1979). Arthur and Vassilvitskii (2006) investigate the worst-case complexity of -means. Bradley and Fayyad (1998), Pelleg and Moore (1999) and Davidson and Satyanarayana (2003) investigate the convergence properties of -means empirically and how it depends on initial seed selection. Dhillon and Modha (2001) compare -means clusters with SVD -based clusters (Chapter 18 ). The K-medoid algorithm was presented by Kaufman and Rousseeuw (1990). The EM algorithm was originally introduced by Dempster et al. (1977). An in-depth treatment of EM is (McLachlan and Krishnan, 1996). See Section 18.5 (page ) for publications on latent analysis, which can also be viewed as soft clustering. AIC is due to Akaike (1974) (see also Burnham and Anderson (2002)). An alternative to AIC is BIC, which can be motivated as a Bayesian model selection procedure (Schwarz, 1978). Fraley and Raftery (1998) show how to choose an optimal number of clusters based on BIC. An application of BIC to -means is (Pelleg and Moore, 2000). Hamerly and Elkan (2003) propose an alternative to BIC that performs better in their experiments. Another influential Bayesian approach for determining the number of clusters (simultaneously with cluster assignment) is described by Cheeseman and Stutz (1996). Two methods for determining cardinality without external criteria are presented by Tibshirani et al. (2001). We only have space here for classical completely unsupervised clustering. An important current topic of research is how to use prior knowledge to guide clustering (e.g., Ji and Xu (2006)) and how to incorporate interactive feedback during clustering (e.g., Huang and Mitchell (2006)). Fayyad et al. (1998) propose an initialization for EM clustering. For algorithms that can cluster very large data sets in one scan through the data see Bradley et al. (1998). The applications in Table 16.1 all cluster documents. Other information retrieval applications cluster words (e.g., Crouch, 1988), contexts of words (e.g., Schütze and Pedersen, 1995) or words and documents simultaneously (e.g., Tishby and Slonim, 2000, Zha et al., 2001, Dhillon, 2001). Simultaneous clustering of words and documents is an example of co-clustering or biclustering .
iir_16_7	Exercises Exercises. Let be a clustering that exactly reproduces a class structure and a clustering that further subdivides some clusters in . Show that . Show that . Mutual information is symmetric in the sense that its value does not change if the roles of clusters and classes are switched: . Which of the other three evaluation measures are symmetric in this sense? Compute RSS for the two clusterings in Figure 16.7 . (i) Give an example of a set of points and three initial centroids (which need not be members of the set of points) for which 3-means converges to a clustering with an empty cluster. (ii) Can a clustering with an empty cluster be the global optimum with respect to RSS? Download Reuters-21578. Discard documents that do not occur in one of the 10 classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Discard documents that occur in two of these 10 classes. (i) Compute a -means clustering of this subset into 10 clusters. There are a number of software packages that implement -means, such as WEKA (Witten and Frank, 2005) and R (R Development Core Team, 2005). (ii) Compute purity, normalized mutual information, and RI for the clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Table 14.5 , page 14.5 ) for the 10 classes and 10 clusters. Identify classes that give rise to false positives and false negatives. Prove that is monotonically decreasing in . There is a soft version of -means that computes the fractional membership of a document in a cluster as a monotonically decreasing function of the distance from its centroid, e.g., as . Modify reassignment and recomputation steps of hard -means for this soft version. In the last iteration in Table 16.3 , document 6 is in cluster 2 even though it was the initial seed for cluster 1. Why does the document change membership? The values of the parameters in iteration 25 in Table 16.3 are rounded. What are the exact values that EM will converge to? Perform a -means clustering for the documents in Table 16.3 . After how many iterations does -means converge? Compare the result with the EM clustering in Table 16.3 and discuss the differences. Modify the expectation and maximization steps of EM for a Gaussian mixture. The maximization step computes the maximum likelihood parameter estimates , , and for each of the clusters. The expectation step computes for each vector a soft assignment to clusters (Gaussians) based on their current parameters. Write down the equations for Gaussian mixtures corresponding to and 202 . Show that -means can be viewed as the limiting case of EM for Gaussian mixtures if variance is very small and all covariances are 0. The within-point scatter of a clustering is defined as . Show that minimizing RSS and minimizing within-point scatter are equivalent. Derive an AIC criterion for the multivariate Bernoulli mixture model from Equation 196.
iir_17	Hierarchical clustering Flat clustering is efficient and conceptually simple, but as we saw in Chapter 16 it has a number of drawbacks. The algorithms introduced in Chapter 16 return a flat unstructured set of clusters, require a prespecified number of clusters as input and are nondeterministic. Hierarchical clustering (or hierarchic clustering ) outputs a hierarchy, a structure that is more informative than the unstructured set of clusters returned by flat clustering.Hierarchical clustering does not require us to prespecify the number of clusters and most hierarchical algorithms that have been used in IR are deterministic. These advantages of hierarchical clustering come at the cost of lower efficiency. The most common hierarchical clustering algorithms have a complexity that is at least quadratic in the number of documents compared to the linear complexity of -means and EM (cf. Section 16.4 , page 16.4 ). This chapter first introduces agglomerative hierarchical clustering (Section 17.1 ) and presents four different agglomerative algorithms, in Sections 17.2 -17.4 , which differ in the similarity measures they employ: single-link, complete-link, group-average, and centroid similarity. We then discuss the optimality conditions of hierarchical clustering in Section 17.5 . Section 17.6 introduces top-down (or divisive) hierarchical clustering. Section 17.7 looks at labeling clusters automatically, a problem that must be solved whenever humans interact with the output of clustering. We discuss implementation issues in Section 17.8 . Section 17.9 provides pointers to further reading, including references to soft hierarchical clustering, which we do not cover in this book. There are few differences between the applications of flat and hierarchical clustering in information retrieval. In particular, hierarchical clustering is appropriate for any of the applications shown in Table 16.1 (page 16.1 ; see also Section 16.6 , page 16.6 ). In fact, the example we gave for collection clustering is hierarchical. In general, we select flat clustering when efficiency is important and hierarchical clustering when one of the potential problems of flat clustering (not enough structure, predetermined number of clusters, non-determinism) is a concern. In addition, many researchers believe that hierarchical clustering produces better clusters than flat clustering. However, there is no consensus on this issue (see references in Section 17.9 ).   Subsections Hierarchical agglomerative clustering Single-link and complete-link clustering Time complexity of HAC Group-average agglomerative clustering Centroid clustering Optimality of HAC Divisive clustering Cluster labeling Implementation notes References and further reading Exercises
iir_17_1	Hierarchical agglomerative clustering  agglomerate  hierarchical agglomerative clustering  HAC 17.6  A dendrogram of a single-link clustering of 30 documents from Reuters-RCV1. Two possible cuts of the dendrogram are shown: at 0.4 into 24 clusters and at 0.1 into 12 clusters. Before looking at specific similarity measures used in HAC in Sections 17.2 -17.4 , we first introduce a method for depicting hierarchical clusterings graphically, discuss a few key properties of HACs and present a simple algorithm for computing an HAC. An HAC clustering is typically visualized as a dendrogram as shown in Figure 17.1 . Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where documents are viewed as singleton clusters. We call this similarity the combination similarity of the merged cluster. For example, the combination similarity of the cluster consisting of Lloyd's CEO questioned and Lloyd's chief / U.S. grilling in Figure 17.1 is . We define the combination similarity of a singleton cluster as its document's self-similarity (which is 1.0 for cosine similarity). By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. For example, we see that the two documents entitled War hero Colin Powell were merged first in Figure 17.1 and that the last merge added Ag trade reform to a cluster consisting of the other 29 documents. A fundamental assumption in HAC is that the merge operation is monotonic . Monotonic means that if are the combination similarities of the successive merges of an HAC, then holds. A non-monotonic hierarchical clustering contains at least one inversion and contradicts the fundamental assumption that we chose the best merge available at each step. We will see an example of an inversion in Figure 17.12 . Hierarchical clustering does not require a prespecified number of clusters. However, in some applications we want a partition of disjoint clusters just as in flat clustering. In those cases, the hierarchy needs to be cut at some point. A number of criteria can be used to determine the cutting point: Cut at a prespecified level of similarity. For example, we cut the dendrogram at 0.4 if we want clusters with a minimum combination similarity of 0.4. In Figure 17.1 , cutting the diagram at yields 24 clusters (grouping only documents with high similarity together) and cutting it at yields 12 clusters (one large financial news cluster and 11 smaller clusters). Cut the dendrogram where the gap between two successive combination similarities is largest. Such large gaps arguably indicate ``natural'' clusterings. Adding one more cluster decreases the quality of the clustering significantly, so cutting before this steep decrease occurs is desirable. This strategy is analogous to looking for the knee in the -means graph in Figure 16.8 (page 16.8 ). Apply Equation 195 (page 16.4.1 ): where refers to the cut of the hierarchy that results in clusters, RSS is the residual sum of squares and is a penalty for each additional cluster. Instead of RSS, another measure of distortion can be used. As in flat clustering, we can also prespecify the number of clusters and select the cutting point that produces clusters.  Figure 17.2: A simple, but inefficient HAC algorithm.   A simple, naive HAC algorithm is shown in Figure 17.2 . We first compute the similarity matrix . The algorithm then executes steps of merging the currently most similar clusters. In each iteration, the two most similar clusters are merged and the rows and columns of the merged cluster in are updated.The clustering is stored as a list of merges in . indicates which clusters are still available to be merged. The function SIM computes the similarity of cluster with the merge of clusters and . For some HAC algorithms, SIM is simply a function of and , for example, the maximum of these two values for single-link. We will now refine this algorithm for the different similarity measures of single-link and complete-link clustering (Section 17.2 ) and group-average and centroid clustering ( and 17.4 ). The merge criteria of these four variants of HAC are shown in Figure 17.3 .
iir_17_10	Exercises Exercises. A single-link clustering can also be computed from the minimum spanning tree of a graph. The minimum spanning tree connects the vertices of a graph at the smallest possible cost, where cost is defined as the sum over all edges of the graph. In our case the cost of an edge is the distance between two documents. Show that if are the costs of the edges of a minimum spanning tree, then these edges correspond to the merges in constructing a single-link clustering. Show that single-link clustering is best-merge persistent and that GAAC and centroid clustering are not best-merge persistent. Consider running 2-means clustering on a collection with documents from two different languages. What result would you expect? Would you expect the same result when running an HAC algorithm? Download Reuters-21578. Keep only documents that are in the classes crude, interest, and grain. Discard documents that are members of more than one of these three classes. Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid clustering of the documents. (v) Cut each dendrogram at the second branch from the top to obtain clusters. Compute the Rand index for each of the 4 clusterings. Which clustering method performs best? Suppose a run of HAC finds the clustering with to have the highest value on some prechosen goodness measure of clustering. Have we found the highest-value clustering among all clusterings with ? Consider the task of producing a single-link clustering of points on a line: Show that we only need to compute a total of about similarities. What is the overall complexity of single-link clustering for a set of points on a line? Prove that single-link, complete-link, and group-average clustering are monotonic in the sense defined on page 17.1 . For points, there are different flat clusterings into clusters (Section 16.2 , page 16.2.1 ). What is the number of different hierarchical clusterings (or dendrograms) of documents? Are there more flat clusterings or more hierarchical clusterings for given and ?
iir_17_2	Single-link and complete-link clustering In single-link clustering or single-linkage clustering , the similarity of two clusters is the similarity of their most similar members (see Figure 17.3 , (a)). This single-link merge criterion is local. We pay attention solely to the area where the two clusters come closest to each other. Other, more distant parts of the cluster and the clusters' overall structure are not taken into account. In complete-link clustering or complete-linkage clustering , the similarity of two clusters is the similarity of their most dissimilar members (see Figure 17.3 , (b)). This is equivalent to choosing the cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions. This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering.  A dendrogram of a complete-link clustering.The same 30 documents were clustered with single-link clustering in Figure 17.1 . Figure 17.4 depicts a single-link and a complete-link clustering of eight documents. The first four steps, each producing a cluster consisting of a pair of two documents, are identical. Then single-link clustering joins the upper two pairs (and after that the lower two pairs) because on the maximum-similarity definition of cluster similarity, those two clusters are closest. Complete-link clustering joins the left two pairs (and then the right two pairs) because those are the closest pairs according to the minimum-similarity definition of cluster similarity. Figure 17.1 is an example of a single-link clustering of a set of documents and Figure 17.5 is the complete-link clustering of the same set. When cutting the last merge in Figure 17.5 , we obtain two clusters of similar size (documents 1-16, from NYSE closing averages to Lloyd's chief / U.S. grilling, and documents 17-30, from Ohio Blue Cross to Clinton signs law). There is no cut of the dendrogram in Figure 17.1 that would give us an equally balanced clustering. Both single-link and complete-link clustering have graph-theoretic interpretations. Define to be the combination similarity of the two clusters merged in step , and the graph that links all data points with a similarity of at least . Then the clusters after step in single-link clustering are the connected components of and the clusters after step in complete-link clustering are maximal cliques of . A connected component is a maximal set of connected points such that there is a path connecting each pair. A clique is a set of points that are completely linked with each other. These graph-theoretic interpretations motivate the terms single-link and complete-link clustering. Single-link clusters at step are maximal sets of points that are linked via at least one link (a single link) of similarity ; complete-link clusters at step are maximal sets of points that are completely linked with each other via links of similarity .   Single-link and complete-link clustering reduce the assessment of cluster quality to a single similarity between a pair of documents: the two most similar documents in single-link clustering and the two most dissimilar documents in complete-link clustering. A measurement based on one pair cannot fully reflect the distribution of documents in a cluster. It is therefore not surprising that both algorithms often produce undesirable clusters. Single-link clustering can produce straggling clusters as shown in Figure 17.6 . Since the merge criterion is strictly local, a chain of points can be extended for long distances without regard to the overall shape of the emerging cluster. This effect is called chaining . The chaining effect is also apparent in Figure 17.1 . The last eleven merges of the single-link clustering (those above the line) add on single documents or pairs of documents, corresponding to a chain. The complete-link clustering in Figure 17.5 avoids this problem. Documents are split into two groups of roughly equal size when we cut the dendrogram at the last merge. In general, this is a more useful organization of the data than a clustering with chains.   However, complete-link clustering suffers from a different problem. It pays too much attention to outliers, points that do not fit well into the global structure of the cluster. In the example in Figure 17.7 the four documents are split because of the outlier at the left edge (Exercise 17.2.1 ). Complete-link clustering does not find the most intuitive cluster structure in this example.   Subsections Time complexity of HAC
iir_17_2_1	Time complexity of HAC The complexity of the naive HAC algorithm in Figure 17.2 is because we exhaustively scan the matrix for the largest similarity in each of iterations.   For the four HAC methods discussed in this chapter a more efficient algorithm is the priority-queue algorithm shown in Figure 17.8 . Its time complexity is . The rows of the similarity matrix are sorted in decreasing order of similarity in the priority queues . then returns the cluster in that currently has the highest similarity with , where we use to denote the cluster as in Chapter 16 . After creating the merged cluster of and , is used as its representative. The function SIM computes the similarity function for potential merge pairs: largest similarity for single-link, smallest similarity for complete-link, average similarity for GAAC (Section 17.3 ), and centroid similarity for centroid clustering (Section 17.4 ). We give an example of how a row of is processed (Figure 17.8 , bottom panel). The loop in lines 1-7 is and the loop in lines 9-21 is for an implementation of priority queues that supports deletion and insertion in . The overall complexity of the algorithm is therefore . In the definition of the function SIM, and are the vector sums of and , respectively, and and are the number of documents in and , respectively. The argument of EFFICIENTHAC in Figure 17.8 is a set of vectors (as opposed to a set of generic documents) because GAAC and centroid clustering ( and 17.4 ) require vectors as input. The complete-link version of EFFICIENTHAC can also be applied to documents that are not represented as vectors.   For single-link, we can introduce a next-best-merge array (NBM) as a further optimization as shown in Figure 17.9 . NBM keeps track of what the best merge is for each cluster. Each of the two top level for-loops in Figure 17.9 are , thus the overall complexity of single-link clustering is . Can we also speed up the other three HAC algorithms with an NBM array? We cannot because only single-link clustering is best-merge persistent . Suppose that the best merge cluster for is in single-link clustering. Then after merging with a third cluster , the merge of and will be 's best merge cluster (Exercise 17.10 ). In other words, the best-merge candidate for the merged cluster is one of the two best-merge candidates of its components in single-link clustering. This means that can be updated in in each iteration - by taking a simple max of two values on line 14 in Figure 17.9 for each of the remaining clusters.   Figure 17.10 demonstrates that best-merge persistence does not hold for complete-link clustering, which means that we cannot use an NBM array to speed up clustering. After merging 's best merge candidate with cluster , an unrelated cluster becomes the best merge candidate for . This is because the complete-link merge criterion is non-local and can be affected by points at a great distance from the area where two merge candidates meet. In practice, the efficiency penalty of the algorithm is small compared with the single-link algorithm since computing the similarity between two documents (e.g., as a dot product) is an order of magnitude slower than comparing two scalars in sorting. All four HAC algorithms in this chapter are with respect to similarity computations. So the difference in complexity is rarely a concern in practice when choosing one of the algorithms. Exercises. Show that complete-link clustering creates the two-cluster clustering depicted in Figure 17.7 .
iir_17_3	Group-average agglomerative clustering Group-average agglomerative clustering  GAAC 17.3 all  group-average clustering  average-link clustering SIM-GA  (203)         The motivation for GAAC is that our goal in selecting two clusters and as the next merge in HAC is that the resulting merge cluster should be coherent. To judge the coherence of , we need to look at all document-document similarities within , including those that occur within and those that occur within . We can compute the measure SIM-GA efficiently because the sum of individual vector similarities is equal to the similarities of their sums:     (204)    (205)         SIM FFICIENT 17.8 205 Equation 204 relies on the distributivity of the dot product with respect to vector addition. Since this is crucial for the efficient computation of a GAAC clustering, the method cannot be easily applied to representations of documents that are not real-valued vectors. Also, Equation 204 only holds for the dot product. While many algorithms introduced in this book have near-equivalent descriptions in terms of dot product, cosine similarity and Euclidean distance (cf. simdisfigs), Equation 204 can only be expressed using the dot product. This is a fundamental difference between single-link/complete-link clustering and GAAC. The first two only require a square matrix of similarities as input and do not care how these similarities were computed. To summarize, GAAC requires (i) documents represented as vectors, (ii) length normalization of vectors, so that self-similarities are 1.0, and (iii) the dot product as the measure of similarity between vectors and sums of vectors. The merge algorithms for GAAC and complete-link clustering are the same except that we use Equation 205 as similarity function in Figure 17.8 . Therefore, the overall time complexity of GAAC is the same as for complete-link clustering: . Like complete-link clustering, GAAC is not best-merge persistent (Exercise 17.10 ). This means that there is no algorithm for GAAC that would be analogous to the algorithm for single-link in Figure 17.9 . We can also define group-average similarity as including self-similarities:     (206)    139 139   Self-similarities are always equal to 1.0, the maximum possible value for length-normalized vectors. The proportion of self-similarities in Equation 206 is for a cluster of size . This gives an unfair advantage to small clusters since they will have proportionally more self-similarities. For two documents , with a similarity , we have . In contrast, . This similarity of two documents is the same as in single-link, complete-link and centroid clustering. We prefer the definition in Equation 205, which excludes self-similarities from the average, because we do not want to penalize large clusters for their smaller proportion of self-similarities and because we want a consistent similarity value for document pairs in all four HAC algorithms. Exercises. Apply group-average clustering to the points in and 17.7 . Map them onto the surface of the unit sphere in a three-dimensional space to get length-normalized vectors. Is the group-average clustering different from the single-link and complete-link clusterings?
iir_17_4	Centroid clustering   (207)   (208)   (209)   207 209 different 17.3 17.3 Figure 17.11 shows the first three steps of a centroid clustering. The first two iterations form the clusters with centroid and with centroid because the pairs and have the highest centroid similarities. In the third iteration, the highest centroid similarity is between and producing the cluster with centroid . Like GAAC, centroid clustering is not best-merge persistent and therefore (Exercise 17.10 ).   In contrast to the other three HAC algorithms, centroid clustering is not monotonic. So-called inversions can occur: Similarity can increase during clustering as in the example in Figure 17.12 , where we define similarity as negative distance. In the first merge, the similarity of and is . In the second merge, the similarity of the centroid of and (the circle) and is . This is an example of an inversion: similarity increases in this sequence of two clustering steps. In a monotonic HAC algorithm, similarity is monotonically decreasing from iteration to iteration. Increasing similarity in a series of HAC clustering steps contradicts the fundamental assumption that small clusters are more coherent than large clusters. An inversion in a dendrogram shows up as a horizontal merge line that is lower than the previous merge line. All merge lines in and 17.5 are higher than their predecessors because single-link and complete-link clustering are monotonic clustering algorithms. Despite its non-monotonicity, centroid clustering is often used because its similarity measure - the similarity of two centroids - is conceptually simpler than the average of all pairwise similarities in GAAC. Figure 17.11 is all one needs to understand centroid clustering. There is no equally simple graph that would explain how GAAC works. Exercises. For a fixed set of documents there are up to distinct similarities between clusters in single-link and complete-link clustering. How many distinct cluster similarities are there in GAAC and centroid clustering?
iir_17_5	Optimality of HAC To state the optimality conditions of hierarchical clustering precisely, we first define the combination similarity COMB-SIM of a clustering as the smallest combination similarity of any of its clusters: (210)       17.1 We then define to be optimal if all clusterings with clusters, , have lower combination similarities: (211)  Figure 17.12 shows that centroid clustering is not optimal. The clustering (for ) has combination similarity and (for ) has combination similarity -3.46. So the clustering produced in the first merge is not optimal since there is a clustering with fewer clusters ( ) that has higher combination similarity. Centroid clustering is not optimal because inversions can occur. The above definition of optimality would be of limited use if it was only applicable to a clustering together with its merge history. However, we can show (Exercise 17.5 ) that for the three non-inversion algorithms can be read off from the cluster without knowing its history. These direct definitions of combination similarity are as follows. single-link The combination similarity of a cluster is the smallest similarity of any bipartition of the cluster, where the similarity of a bipartition is the largest similarity between any two documents from the two parts: (212) where each is a bipartition of . complete-link The combination similarity of a cluster is the smallest similarity of any two points in : . GAAC The combination similarity of a cluster is the average of all pairwise similarities in (where self-similarities are not included in the average): Equation  205. We can now prove the optimality of single-link clustering by induction over the number of clusters . We will give a proof for the case where no two pairs of documents have the same similarity, but it can easily be extended to the case with ties. The inductive basis of the proof is that a clustering with clusters has combination similarity 1.0, which is the largest value possible. The induction hypothesis is that a single-link clustering with clusters is optimal: for all . Assume for contradiction that the clustering we obtain by merging the two most similar clusters in is not optimal and that instead a different sequence of merges leads to the optimal clustering with clusters. We can write the assumption that is optimal and that is not as . Case 1: The two documents linked by are in the same cluster in . They can only be in the same cluster if a merge with similarity smaller than has occurred in the merge sequence producing . This implies . Thus, . Contradiction. Case 2: The two documents linked by are not in the same cluster in . But , so the single-link merging rule should have merged these two clusters when processing . Contradiction. Thus, is optimal. In contrast to single-link clustering, complete-link clustering and GAAC are not optimal as this example shows:  Both algorithms merge the two points with distance 1 ( and ) first and thus cannot find the two-cluster clustering . But is optimal on the optimality criteria of complete-link clustering and GAAC. However, the merge criteria of complete-link clustering and GAAC approximate the desideratum of approximate sphericity better than the merge criterion of single-link clustering. In many applications, we want spherical clusters. Thus, even though single-link clustering may seem preferable at first because of its optimality, it is optimal with respect to the wrong criterion in many document clustering applications.   Table 17.1: Comparison of HAC algorithms. method combination similarity time compl. optimal? comment single-link max inter-similarity of any 2 docs yes chaining effect complete-link min inter-similarity of any 2 docs no sensitive to outliers group-average average of all sims no best choice for most applications centroid average inter-similarity no inversions can occur   Table 17.1 summarizes the properties of the four HAC algorithms introduced in this chapter. We recommend GAAC for document clustering because it is generally the method that produces the clustering with the best properties for applications. It does not suffer from chaining, from sensitivity to outliers and from inversions. There are two exceptions to this recommendation. First, for non-vector representations, GAAC is not applicable and clustering should typically be performed with the complete-link method. Second, in some applications the purpose of clustering is not to create a complete hierarchy or exhaustive partition of the entire document set. For instance, first story detection or novelty detection is the task of detecting the first occurrence of an event in a stream of news stories. One approach to this task is to find a tight cluster within the documents that were sent across the wire in a short period of time and are dissimilar from all previous documents. For example, the documents sent over the wire in the minutes after the World Trade Center attack on September 11, 2001 form such a cluster. Variations of single-link clustering can do well on this task since it is the structure of small parts of the vector space - and not global structure - that is important in this case. Similarly, we will describe an approach to duplicate detection on the web in Section 19.6 (page 19.6 ) where single-link clustering is used in the guise of the union-find algorithm . Again, the decision whether a group of documents are duplicates of each other is not influenced by documents that are located far away and single-link clustering is a good choice for duplicate detection. Exercises. Show the equivalence of the two definitions of combination similarity: the process definition on page 17.1 and the static definition on page 17.5 .
iir_17_6	Divisive clustering  top-down clustering  divisive clustering Top-down clustering is conceptually more complex than bottom-up clustering since we need a second, flat clustering algorithm as a ``subroutine''. It has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves. For a fixed number of top levels, using an efficient flat algorithm like -means, top-down algorithms are linear in the number of documents and clusters. So they run much faster than HAC algorithms, which are at least quadratic. There is evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms in some circumstances. See the references on bisecting -means in Section 17.9 . Bottom-up methods make clustering decisions based on local patterns without initially taking into account the global distribution. These early decisions cannot be undone. Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions.
iir_17_7	Cluster labeling In many applications of flat clustering and hierarchical clustering, particularly in analysis tasks and in user interfaces (see applications in Table 16.1 , page 16.1 ), human users interact with clusters. In such settings, we must label clusters, so that users can see what a cluster is about. Differential cluster labeling selects cluster labels by comparing the distribution of terms in one cluster with that of other clusters. The feature selection methods we introduced in Section 13.5 (page ) can all be used for differential cluster labeling. In particular, mutual information (MI) (Section 13.5.1 , page 13.5.1 ) or, equivalently, information gain and the -test (Section 13.5.2 , page 13.5.2 ) will identify cluster labels that characterize one cluster in contrast to other clusters. A combination of a differential test with a penalty for rare terms often gives the best labeling results because rare terms are not necessarily representative of the cluster as a whole.       labeling method   # docs centroid mutual information title 4 622 oil plant mexico production crude power 000 refinery gas bpd plant oil production barrels crude bpd mexico dolly capacity petroleum MEXICO: Hurricane Dolly heads for Mexico coast 9 1017 police security russian people military peace killed told grozny court police killed military security peace told troops forces rebels people RUSSIA: Russia's Lebed meets rebel chief in Chechnya 10 1259 00 000 tonnes traders futures wheat prices cents september tonne delivery traders futures tonne tonnes desk wheat prices 000 00 USA: Export Business - Grain/oilseeds complex Automatically computed cluster labels.This is for three of ten clusters (4, 9, and 10) in a -means clustering of the first 10,000 documents in Reuters-RCV1. The last three columns show cluster summaries computed by three labeling methods: most highly weighted terms in centroid (centroid), mutual information, and the title of the document closest to the centroid of the cluster (title). Terms selected by only one of the first two methods are in bold.  We apply three labeling methods to a -means clustering in Table 17.2 . In this example, there is almost no difference between MI and . We therefore omit the latter. Cluster-internal labeling computes a label that solely depends on the cluster itself, not on other clusters. Labeling a cluster with the title of the document closest to the centroid is one cluster-internal method. Titles are easier to read than a list of terms. A full title can also contain important context that didn't make it into the top 10 terms selected by MI. On the web, anchor text can play a role similar to a title since the anchor text pointing to a page can serve as a concise summary of its contents. In Table 17.2 , the title for cluster 9 suggests that many of its documents are about the Chechnya conflict, a fact the MI terms do not reveal. However, a single document is unlikely to be representative of all documents in a cluster. An example is cluster 4, whose selected title is misleading. The main topic of the cluster is oil. Articles about hurricane Dolly only ended up in this cluster because of its effect on oil prices. We can also use a list of terms with high weights in the centroid of the cluster as a label. Such highly weighted terms (or, even better, phrases, especially noun phrases) are often more representative of the cluster than a few titles can be, even if they are not filtered for distinctiveness as in the differential methods. However, a list of phrases takes more time to digest for users than a well crafted title. Cluster-internal methods are efficient, but they fail to distinguish terms that are frequent in the collection as a whole from those that are frequent only in the cluster. Terms like year or Tuesday may be among the most frequent in a cluster, but they are not helpful in understanding the contents of a cluster with a specific topic like oil. In Table 17.2 , the centroid method selects a few more uninformative terms (000, court, cents, september) than MI (forces, desk), but most of the terms selected by either method are good descriptors. We get a good sense of the documents in a cluster from scanning the selected terms. For hierarchical clustering, additional complications arise in cluster labeling. Not only do we need to distinguish an internal node in the tree from its siblings, but also from its parent and its children. Documents in child nodes are by definition also members of their parent node, so we cannot use a naive differential method to find labels that distinguish the parent from its children. However, more complex criteria, based on a combination of overall collection frequency and prevalence in a given cluster, can determine whether a term is a more informative label for a child node or a parent node (see Section 17.9 ).
iir_17_8	Implementation notes In low dimensions, more aggressive optimizations are possible that make the computation of most pairwise similarities unnecessary (Exercise 17.10 ). However, no such algorithms are known in higher dimensions. We encountered the same problem in kNN classification (see Section 14.7 , page 14.7 ). When using GAAC on a large document set in high dimensions, we have to take care to avoid dense centroids. For dense centroids, clustering can take time where is the size of the vocabulary, whereas complete-link clustering is where is the average size of the vocabulary of a document. So for large vocabularies complete-link clustering can be more efficient than an unoptimized implementation of GAAC. We discussed this problem in the context of -means clustering in Chapter 16 (page 16.4 ) and suggested two solutions: truncating centroids (keeping only highly weighted terms) and representing clusters by means of sparse medoids instead of dense centroids. These optimizations can also be applied to GAAC and centroid clustering. Even with these optimizations, HAC algorithms are all or and therefore infeasible for large sets of 1,000,000 or more documents. For such large sets, HAC can only be used in combination with a flat clustering algorithm like -means. Recall that -means requires a set of seeds as initialization (Figure 16.5 , page 16.5 ). If these seeds are badly chosen, then the resulting clustering will be of poor quality. We can employ an HAC algorithm to compute seeds of high quality. If the HAC algorithm is applied to a document subset of size , then the overall runtime of -means cum HAC seed generation is . This is because the application of a quadratic algorithm to a sample of size has an overall complexity of . An appropriate adjustment can be made for an algorithm to guarantee linearity. This algorithm is referred to as the Buckshot algorithm . It combines the determinism and higher reliability of HAC with the efficiency of -means.
iir_17_9	References and further reading An excellent general review of clustering is (Jain et al., 1999). Early references for specific HAC algorithms are (King, 1967) (single-link), (Sneath and Sokal, 1973) (complete-link, GAAC) and (Lance and Williams, 1967) (discussing a large variety of hierarchical clustering algorithms). The single-link algorithm in Figure 17.9 is similar to Kruskal's algorithm for constructing a minimum spanning tree. A graph-theoretical proof of the correctness of Kruskal's algorithm (which is analogous to the proof in Section 17.5 ) is provided by Cormen et al. (1990, Theorem 23.1). See Exercise 17.10 for the connection between minimum spanning trees and single-link clusterings. It is often claimed that hierarchical clustering algorithms produce better clusterings than flat algorithms (Jain and Dubes (1988, p. 140), Cutting et al. (1992), Larsen and Aone (1999)) although more recently there have been experimental results suggesting the opposite (Zhao and Karypis, 2002). Even without a consensus on average behavior, there is no doubt that results of EM and -means are highly variable since they will often converge to a local optimum of poor quality. The HAC algorithms we have presented here are deterministic and thus more predictable. The complexity of complete-link, group-average and centroid clustering is sometimes given as (Day and Edelsbrunner, 1984, Murtagh, 1983, Voorhees, 1985b) because a document similarity computation is an order of magnitude more expensive than a simple comparison, the main operation executed in the merging steps after the similarity matrix has been computed. The centroid algorithm described here is due to Voorhees (1985b). Voorhees recommends complete-link and centroid clustering over single-link for a retrieval application. The Buckshot algorithm was originally published by Cutting et al. (1993). Allan et al. (1998) apply single-link clustering to first story detection . An important HAC technique not discussed here is Ward's method (El-Hamdouchi and Willett, 1986, Ward Jr., 1963), also called minimum variance clustering . In each step, it selects the merge with the smallest RSS (Chapter 16 , page 191 ). The merge criterion in Ward's method (a function of all individual distances from the centroid) is closely related to the merge criterion in GAAC (a function of all individual similarities to the centroid). Despite its importance for making the results of clustering useful, comparatively little work has been done on labeling clusters. Popescul and Ungar (2000) obtain good results with a combination of and collection frequency of a term. Glover et al. (2002b) use information gain for labeling clusters of web pages. Stein and zu Eissen's approach is ontology-based (2004). The more complex problem of labeling nodes in a hierarchy (which requires distinguishing more general labels for parents from more specific labels for children) is tackled by Glover et al. (2002a) and Treeratpituk and Callan (2006). Some clustering algorithms attempt to find a set of labels first and then build (often overlapping) clusters around the labels, thereby avoiding the problem of labeling altogether (Osinski and Weiss, 2005, Zamir and Etzioni, 1999, Käki, 2005). We know of no comprehensive study that compares the quality of such ``label-based'' clustering to the clustering algorithms discussed in this chapter and in Chapter 16 . In principle, work on multi-document summarization (McKeown and Radev, 1995) is also applicable to cluster labeling, but multi-document summaries are usually longer than the short text fragments needed when labeling clusters (cf. snippets). Presenting clusters in a way that users can understand is a UI problem. We recommend reading (Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) for an introduction to user interfaces in IR. An example of an efficient divisive algorithm is bisecting -means (Steinbach et al., 2000). Spectral clustering algorithms (Kannan et al., 2000, Dhillon, 2001, Zha et al., 2001, Ng et al., 2001a), including principal direction divisive partitioning (PDDP) (whose bisecting decisions are based on SVD , see Chapter 18 ) (Boley, 1998, Savaresi and Boley, 2004), are computationally more expensive than bisecting -means, but have the advantage of being deterministic. Unlike -means and EM, most hierarchical clustering algorithms do not have a probabilistic interpretation. Model-based hierarchical clustering (Kamvar et al., 2002, Vaithyanathan and Dom, 2000, Castro et al., 2004) is an exception. The evaluation methodology described in Section 16.3 (page 16.3 ) is also applicable to hierarchical clustering. Specialized evaluation measures for hierarchies are discussed by Fowlkes and Mallows (1983), Larsen and Aone (1999) and Sahoo et al. (2006). The R environment (R Development Core Team, 2005) offers good support for hierarchical clustering. The R function hclust implements single-link, complete-link, group-average, and centroid clustering; and Ward's method. Another option provided is median clustering which represents each cluster by its medoid (cf. k-medoids in Chapter 16 , page 16.4 ). Support for clustering vectors in high-dimensional spaces is provided by the software package CLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto).
iir_18	Matrix decompositions and latent semantic indexing On page 6.3.1 we introduced the notion of a term-document matrix: an matrix , each of whose rows represents a term and each of whose columns represents a document in the collection. Even for a collection of modest size, the term-document matrix is likely to have several tens of thousands of rows and columns. In Section 18.1.1 we first develop a class of operations from linear algebra, known as matrix decomposition. In Section 18.2 we use a special form of matrix decomposition to construct a low-rank approximation to the term-document matrix. In Section 18.3 we examine the application of such low-rank approximations to indexing and retrieving documents, a technique referred to as latent semantic indexing. While latent semantic indexing has not been established as a significant force in scoring and ranking for information retrieval, it remains an intriguing approach to clustering in a number of domains including for collections of text documents (Section 16.6 , page 16.6 ). Understanding its full potential remains an area of active research. Readers who do not require a refresher on linear algebra may skip Section 18.1 , although Example 18.1 is especially recommended as it highlights a property of eigenvalues that we exploit later in the chapter.   Subsections Linear algebra review Matrix decompositions Term-document matrices and singular value decompositions Low-rank approximations Latent semantic indexing References and further reading
iir_18_1	Linear algebra review   The rank of a matrix is the number of linearly independent rows (or columns) in it; thus, . A square matrix all of whose off-diagonal entries are zero is called a diagonal matrix; its rank is equal to the number of non-zero diagonal entries. If all diagonal entries of such a diagonal matrix are , it is called the identity matrix of dimension and represented by . For a square matrix and a vector that is not all zeros, the values of satisfying (213)   eigenvalues    213  right eigenvector principal eigenvector. left eigenvectors     (214)    The eigenvalues of a matrix are found by solving the characteristic equation, which is obtained by rewriting Equation 213 in the form . The eigenvalues of are then the solutions of , where denotes the determinant of a square matrix . The equation is an th order polynomial equation in and can have at most roots, which are the eigenvalues of . These eigenvalues can in general be complex, even if all entries of are real. We now examine some further properties of eigenvalues and eigenvectors, to set up the central idea of singular value decompositions in Section 18.2 below. First, we look at the relationship between matrix-vector multiplication and eigenvalues. Worked example. Consider the matrix (215)      (216)       (217)     (218)   (219)   (220)   (221)   End worked example. Example 18.1 shows that even though is an arbitrary vector, the effect of multiplication by is determined by the eigenvalues and eigenvectors of . Furthermore, it is intuitively apparent from Equation 221 that the product is relatively unaffected by terms arising from the small eigenvalues of ; in our example, since , the contribution of the third term on the right hand side of Equation 221 is small. In fact, if we were to completely ignore the contribution in Equation 221 from the third eigenvector corresponding to , then the product would be computed to be rather than the correct product which is ; these two vectors are relatively close to each other by any of various metrics one could apply (such as the length of their vector difference). This suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector product is small. We will carry forward this intuition when studying matrix decompositions and low-rank approximations in Section 18.2 . Before doing so, we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us. For a symmetric matrix , the eigenvectors corresponding to distinct eigenvalues are orthogonal. Further, if is both real and symmetric, the eigenvalues are all real. Worked example. Consider the real, symmetric matrix (222)        End worked example.   Subsections Matrix decompositions
iir_18_1_1	Matrix decompositions factored  matrix decomposition  18.3 18.2 We begin by giving two theorems on the decomposition of a square matrix into the product of three matrices of a special form. The first of these, Theorem 18.1.1, gives the basic factorization of a square real-valued matrix into three factors. The second, Theorem 18.1.1, applies to square symmetric matrices and is the basis of the singular value decomposition described in Theorem 18.2. Theorem. (Matrix diagonalization theorem) Let be a square real-valued matrix with linearly independent eigenvectors. Then there exists an eigen decomposition (223)       (224)  End theorem. To understand how Theorem 18.1.1 works, we note that has the eigenvectors of as columns (225)   (226)   (227)   (228)     We next state a closely related decomposition of a symmetric square matrix into the product of matrices derived from its eigenvectors. This will pave the way for the development of our main tool for text analysis, the singular value decomposition (Section 18.2 ). Theorem. (Symmetric diagonalization theorem) Let be a square, symmetric real-valued matrix with linearly independent eigenvectors. Then there exists a symmetric diagonal decomposition (229)        End theorem. We will build on this symmetric diagonal decomposition to build low-rank approximations to term-document matrices. Exercises. What is the rank of the diagonal matrix below? (230) Show that is an eigenvalue of (231) Find the corresponding eigenvector. Compute the unique eigen decomposition of the matrix in (222).
iir_18_2	Term-document matrices and singular value decompositions      singular value decomposition 18.3  18.2  18.1.1          Theorem. Let be the rank of the matrix . Then, there is a singular-value decomposition ( SVD for short) of of the form (232)  The eigenvalues of are the same as the eigenvalues of ; For , let , with . Then the matrix is composed by setting for , and zero otherwise. End theorem. The values are referred to as the singular values of . It is instructive to examine the relationship of Theorem 18.2 to Theorem 18.1.1; we do this rather than derive the general proof of Theorem 18.2, which is beyond the scope of this book. By multiplying Equation 232 by its transposed version, we have (233)  Note now that in Equation 233, the left-hand side is a square symmetric matrix real-valued matrix, and the right-hand side represents its symmetric diagonal decomposition as in Theorem 18.1.1. What does the left-hand side represent? It is a square matrix with a row and a column corresponding to each of the terms. The entry in the matrix is a measure of the overlap between the th and th terms, based on their co-occurrence in documents. The precise mathematical meaning depends on the manner in which is constructed based on term weighting. Consider the case where is the term-document incidence matrix of page 1.1 , illustrated in Figure 1.1 . Then the entry in is the number of documents in which both term and term occur.   When writing down the numerical values of the SVD, it is conventional to represent as an matrix with the singular values on the diagonals, since all its entries outside this sub-matrix are zeros. Accordingly, it is conventional to omit the rightmost columns of corresponding to these omitted rows of ; likewise the rightmost columns of are omitted since they correspond in to the rows that will be multiplied by the columns of zeros in . This written form of the SVD is sometimes known as the reduced SVD or truncated SVD and we will encounter it again in Exercise 18.3 . Henceforth, our numerical examples and exercises will use this reduced form. Worked example. We now illustrate the singular-value decomposition of a matrix of rank 2; the singular values are and .  (234)  End worked example. As with the matrix decompositions defined in Section 18.1.1 , the singular value decomposition of a matrix can be computed by a variety of algorithms, many of which have been publicly available software implementations; pointers to these are given in Section 18.5 . Exercises. Let (235) be the term-document incidence matrix for a collection. Compute the co-occurrence matrix . What is the interpretation of the diagonal entries of when is a term-document incidence matrix? Verify that the SVD of the matrix in Equation 235 is (236) by verifying all of the properties in the statement of Theorem 18.2. Suppose that is a binary term-document incidence matrix. What do the entries of represent? Let (237) be a term-document matrix whose entries are term frequencies; thus term 1 occurs 2 times in document 2 and once in document 3. Compute ; observe that its entries are largest where two terms have their most frequent occurrences together in the same document.
iir_18_3	Low-rank approximations We next state a matrix approximation problem that at first seems to have little to do with information retrieval. We describe a solution to this matrix problem using singular-value decompositions, then develop its application to information retrieval. Given an matrix and a positive integer , we wish to find an matrix of rank at most , so as to minimize the Frobenius norm of the matrix difference , defined to be (238)               low-rank approximation The singular value decomposition can be used to solve the low-rank matrix approximation problem. We then derive from it an application to approximating term-document matrices. We invoke the following three-step procedure to this end: Given , construct its SVD in the form shown in (232); thus, . Derive from the matrix formed by replacing by zeros the smallest singular values on the diagonal of . Compute and output as the rank- approximation to .     18.1   Theorem. (239)  End theorem. Recalling that the singular values are in decreasing order , we learn from Theorem 18.3 that is the best rank- approximation to , incurring an error (measured by the Frobenius norm of ) equal to . Thus the larger is, the smaller this error (and in particular, for , the error is zero since ; provided , then and thus ).   To derive further insight into why the process of truncating the smallest singular values in helps generate a rank- approximation of low error, we examine the form of : (240)   (241)   (242)              Exercises. Compute a rank 1 approximation to the matrix in Example 235, using the SVD as in Exercise 236. What is the Frobenius norm of the error of this approximation? Consider now the computation in Exercise 18.3 . Following the schematic in Figure 18.2 , notice that for a rank 1 approximation we have being a scalar. Denote by the first column of and by the first column of . Show that the rank-1 approximation to can then be written as . reduced can be generalized to rank approximations: we let and denote the ``reduced'' matrices formed by retaining only the first columns of and , respectively. Thus is an matrix while is a matrix. Then, we have (243) where is the square submatrix of with the singular values on the diagonal. The primary advantage of using (243) is to eliminate a lot of redundant columns of zeros in and , thereby explicitly eliminating multiplication by columns that do not affect the low-rank approximation; this version of the SVD is sometimes known as the reduced SVD or truncated SVD and is a computationally simpler representation from which to compute the low rank approximation. For the matrix in Example 18.2, write down both and .
iir_18_4	Latent semantic indexing    latent semantic indexing But first, we motivate such an approximation. Recall the vector space representation of documents and queries introduced in Section 6.3 (page ). This vector space representation enjoys a number of advantages including the uniform treatment of queries and documents as vectors, the induced score computation based on cosine similarity, the ability to weight different terms differently, and its extension beyond document retrieval to such applications as clustering and classification. The vector space representation suffers, however, from its inability to cope with two classic problems arising in natural languages: synonymy and polysemy. Synonymy refers to a case where two different words (say car and automobile) have the same meaning. Because the vector space representation fails to capture the relationship between synonymous terms such as car and automobile - according each a separate dimension in the vector space. Consequently the computed similarity between a query (say, car) and a document containing both car and automobile underestimates the true similarity that a user would perceive. Polysemy on the other hand refers to the case where a term such as charge has multiple meanings, so that the computed similarity overestimates the similarity that a user would perceive. Could we use the co-occurrences of terms (whether, for instance, charge occurs in a document containing steed versus in a document containing electron) to capture the latent semantic associations of terms and alleviate these problems? Even for a collection of modest size, the term-document matrix is likely to have several tens of thousand of rows and columns, and a rank in the tens of thousands as well. In latent semantic indexing (sometimes referred to as latent semantic analysis (LSA) ), we use the SVD to construct a low-rank approximation to the term-document matrix, for a value of that is far smaller than the original rank of . In the experimental work cited later in this section, is generally chosen to be in the low hundreds. We thus map each row/column (respectively corresponding to a term/document) to a -dimensional space; this space is defined by the principal eigenvectors (corresponding to the largest eigenvalues) of and . Note that the matrix is itself still an matrix, irrespective of . Next, we use the new -dimensional LSI representation as we did the original representation - to compute similarities between vectors. A query vector is mapped into its representation in the LSI space by the transformation (244)  6.3.1  244  244 The fidelity of the approximation of to leads us to hope that the relative values of cosine similarities are preserved: if a query is close to a document in the original space, it remains relatively close in the -dimensional space. But this in itself is not sufficiently interesting, especially given that the sparse query vector turns into a dense query vector in the low-dimensional space. This has a significant computational cost, when compared with the cost of processing in its native form. Worked example. Consider the term-document matrix         ship 1 0 1 0 0 0     boat 0 1 0 0 0 0     ocean 1 1 0 0 0 0     voyage 1 0 0 1 1 0     trip 0 0 0 1 0 1   Its singular value decomposition is the product of three matrices as below. First we have which in this example is:     1 2 3 4 5     ship     boat 0.00 0.73     ocean 0.00     voyage 0.35 0.15 0.16     trip 0.65 0.58   When applying the SVD to a term-document matrix, is known as the SVD term matrix. The singular values are 2.16 0.00 0.00 0.00 0.00 0.00 1.59 0.00 0.00 0.00 0.00 0.00 1.28 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.39 Finally we have , which in the context of a term-document matrix is known as the SVD document matrix:         1     2 0.63 0.22 0.41     3 0.28 0.45 0.12     4 0.00 0.00 0.58 0.00 0.58     5 0.29 0.63 0.19 0.41   By ``zeroing out'' all but the two largest singular values of , we obtain 2.16 0.00 0.00 0.00 0.00 0.00 1.59 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 From this, we compute         1     2 1.00 0.35 0.65     3 0.00 0.00 0.00 0.00 0.00 0.00     4 0.00 0.00 0.00 0.00 0.00 0.00     5 0.00 0.00 0.00 0.00 0.00 0.00   Notice that the low-rank approximation, unlike the original matrix , can have negative entries. End worked example. Examination of and in Example 18.4 shows that the last 3 rows of each of these matrices are populated entirely by zeros. This suggests that the SVD product in Equation 241 can be carried out with only two rows in the representations of and ; we may then replace these matrices by their truncated versions and . For instance, the truncated SVD document matrix in this example is:         1     2 1.00 0.35 0.65   Figure 18.3 illustrates the documents in in two dimensions. Note also that is dense relative to .  Figure 18.3: The documents of Example  18.4 reduced to two dimensions in . We may in general view the low-rank approximation of by as a constrained optimization problem: subject to the constraint that have rank at most , we seek a representation of the terms and documents comprising with low Frobenius norm for the error . When forced to squeeze the terms/documents down to a -dimensional space, the SVD should bring together terms with similar co-occurrences. This intuition suggests, then, that not only should retrieval quality not suffer too much from the dimension reduction, but in fact may improve. Dumais (1993) and Dumais (1995) conducted experiments with LSI on TREC documents and tasks, using the commonly-used Lanczos algorithm to compute the SVD. At the time of their work in the early 1990's, the LSI computation on tens of thousands of documents took approximately a day on one machine. On these experiments, they achieved precision at or above that of the median TREC participant. On about 20% of TREC topics their system was the top scorer, and reportedly slightly better on average than standard vector spaces for LSI at about 350 dimensions. Here are some conclusions on LSI first suggested by their work, and subsequently verified by many other experiments.  The computational cost of the SVD is significant; at the time of this writing, we know of no successful experiment with over one million documents. This has been the biggest obstacle to the widespread adoption to LSI. One approach to this obstacle is to build the LSI representation on a randomly sampled subset of the documents in the collection, following which the remaining documents are ``folded in'' as detailed with Equation 244. As we reduce , recall tends to increase, as expected. Most surprisingly, a value of in the low hundreds can actually increase precision on some query benchmarks. This appears to suggest that for a suitable value of , LSI addresses some of the challenges of synonymy. LSI works best in applications where there is little overlap between queries and documents. The experiments also documented some modes where LSI failed to match the effectiveness of more traditional indexes and score computations. Most notably (and perhaps obviously), LSI shares two basic drawbacks of vector space retrieval: there is no good way of expressing negations (find documents that contain german but not shepherd), and no way of enforcing Boolean conditions. LSI can be viewed as soft clustering by interpreting each dimension of the reduced space as a cluster and the value that a document has on that dimension as its fractional membership in that cluster.
iir_18_5	References and further reading Strang (1986) provides an excellent introductory overview of matrix decompositions including the singular value decomposition. Theorem 18.3 is due to Eckart and Young (1936). The connection between information retrieval and low-rank approximations of the term-document matrix was introduced in Deerwester et al. (1990), with a subsequent survey of results in Berry et al. (1995). Dumais (1993) and Dumais (1995) describe experiments on TREC benchmarks giving evidence that at least on some benchmarks, LSI can produce better precision and recall than standard vector-space retrieval. http://www.cs.utk.edu/~berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.htmloffer comprehensive pointers to the literature and software of LSI. Schütze and Silverstein (1997) evaluate LSI and truncated representations of centroids for efficient -means clustering (Section 16.4 ). Bast and Majumdar (2005) detail the role of the reduced dimension in LSI and how different pairs of terms get coalesced together at differing values of . Applications of LSI to cross-language information retrieval (where documents in two or more different languages are indexed, and a query posed in one language is expected to retrieve documents in other languages) are developed in Berry and Young (1995) and Littman et al. (1998). LSI (referred to as LSA in more general settings) has been applied to host of other problems in computer science ranging from memory modeling to computer vision. Hofmann (1999a;b) provides an initial probabilistic extension of the basic latent semantic indexing technique. A more satisfactory formal basis for a probabilistic latent variable model for dimensionality reduction is the Latent Dirichlet Allocation ( LDA ) model (Blei et al., 2003), which is generative and assigns probabilities to documents outside of the training set. This model is extended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft (2006) present the first large scale evaluation of LDA, finding it to significantly outperform the query likelihood model of Section 12.2 (page ), but to not perform quite as well as the relevance model mentioned in Section 12.4 (page ) - but the latter does additional per-query processing unlike LDA. Teh et al. (2006) generalize further by presenting Hierarchical Dirichlet Processes , a probabilistic model which allows a group (for us, a document) to be drawn from an infinite mixture of latent topics, while still allowing these topics to be shared across documents. Exercises. Assume you have a set of documents each of which is in either English or in Spanish. The collection is given in Figure 18.4 . Figure: Documents for Exercise  18.5. Figure 18.5 gives a glossary relating the Spanish and English words above for your own information. This glossary is NOT available to the retrieval system: Figure 18.5: Glossary for Exercise  18.5. Construct the appropriate term-document matrix to use for a collection consisting of these documents. For simplicity, use raw term frequencies rather than normalized tf-idf weights. Make sure to clearly label the dimensions of your matrix. Write down the matrices and and from these derive the rank 2 approximation . State succinctly what the entry in the matrix represents. State succinctly what the entry in the matrix represents, and why it differs from that in .
iir_19	Web search basics 19.1 19.4 19.5 19.6   Subsections Background and history Web characteristics The web graph Spam Advertising as the economic model The search user experience User query needs Index size and estimation Near-duplicates and shingling References and further reading
iir_19_1	Background and history The invention of hypertext, envisioned by Vannevar Bush in the 1940's and first realized in working systems in the 1970's, significantly precedes the formation of the World Wide Web (which we will simply refer to as the Web), in the 1990's. Web usage has shown tremendous growth to the point where it now claims a good fraction of humanity as participants, by relying on a simple, open client-server design: (1) the server communicates with the client via a protocol (the http or hypertext transfer protocol) that is lightweight and simple, asynchronously carrying a variety of payloads (text, images and - over time - richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language); (2) the client - generally a browser, an application within a graphical user environment - can ignore what it does not understand. Each of these seemingly innocuous features has contributed enormously to the growth of the Web, so it is worthwhile to examine them further. The basic operation is as follows: a client (such as a browser) sends an http request to a web server. The browser specifies a URL (for Uniform Resource Locator) such as http://www.stanford.edu/home/atoz/contact.html. In this example URL, the string http refers to the protocol to be used for transmitting the data. The string www.stanford.edu is known as the domain and specifies the root of a hierarchy of web pages (typically mirroring a filesystem hierarchy underlying the web server). In this example, /home/atoz/contact.html is a path in this hierarchy with a file contact.html that contains the information to be returned by the web server at www.stanford.edu in response to this request. The HTML-encoded file contact.html holds the hyperlinks and the content (in this instance, contact information for Stanford University), as well as formatting rules for rendering this content in a browser. Such an http request thus allows us to fetch the content of a page, something that will prove to be useful to us for crawling and indexing documents (Chapter 20 ). The designers of the first browsers made it easy to view the HTML markup tags on the content of a URL. This simple convenience allowed new users to create their own HTML content without extensive training or experience; rather, they learned from example content that they liked. As they did so, a second feature of browsers supported the rapid proliferation of web content creation and usage: browsers ignored what they did not understand. This did not, as one might fear, lead to the creation of numerous incompatible dialects of HTML. What it did promote was amateur content creators who could freely experiment with and learn from their newly created web pages without fear that a simple syntax error would ``bring the system down.'' Publishing on the Web became a mass activity that was not limited to a few trained programmers, but rather open to tens and eventually hundreds of millions of individuals. For most users and for most information needs, the Web quickly became the best way to supply and consume information on everything from rare ailments to subway schedules. The mass publishing of information on the Web is essentially useless unless this wealth of information can be discovered and consumed by other users. Early attempts at making web information ``discoverable'' fell into two broad categories: (1) full-text index search engines such as Altavista, Excite and Infoseek and (2) taxonomies populated with web pages in categories, such as Yahoo! The former presented the user with a keyword search interface supported by inverted indexes and ranking mechanisms building on those introduced in earlier chapters. The latter allowed the user to browse through a hierarchical tree of category labels. While this is at first blush a convenient and intuitive metaphor for finding web pages, it has a number of drawbacks: first, accurately classifying web pages into taxonomy tree nodes is for the most part a manual editorial process, which is difficult to scale with the size of the Web. Arguably, we only need to have ``high-quality'' web pages in the taxonomy, with only the best web pages for each category. However, just discovering these and classifying them accurately and consistently into the taxonomy entails significant human effort. Furthermore, in order for a user to effectively discover web pages classified into the nodes of the taxonomy tree, the user's idea of what sub-tree(s) to seek for a particular topic should match that of the editors performing the classification. This quickly becomes challenging as the size of the taxonomy grows; the Yahoo! taxonomy tree surpassed 1000 distinct nodes fairly early on. Given these challenges, the popularity of taxonomies declined over time, even though variants (such as About.com and the Open Directory Project) sprang up with subject-matter experts collecting and annotating web pages for each category. The first generation of web search engines transported classical search techniques such as those in the preceding chapters to the web domain, focusing on the challenge of scale. The earliest web search engines had to contend with indexes containing tens of millions of documents, which was a few orders of magnitude larger than any prior information retrieval system in the public domain. Indexing, query serving and ranking at this scale required the harnessing together of tens of machines to create highly available systems, again at scales not witnessed hitherto in a consumer-facing search application. The first generation of web search engines was largely successful at solving these challenges while continually indexing a significant fraction of the Web, all the while serving queries with sub-second response times. However, the quality and relevance of web search results left much to be desired owing to the idiosyncrasies of content creation on the Web that we discuss in Section 19.2 . This necessitated the invention of new ranking and spam-fighting techniques in order to ensure the quality of the search results. While classical information retrieval techniques (such as those covered earlier in this book) continue to be necessary for web search, they are not by any means sufficient. A key aspect (developed further in Chapter 21 ) is that whereas classical techniques measure the relevance of a document to a query, there remains a need to gauge the authoritativeness of a document based on cues such as which website hosts it.
iir_19_2	Web characteristics What about the substance of the text in web pages? The democratization of content creation on the web meant a new level of granularity in opinion on virtually any subject. This meant that the web contained truth, lies, contradictions and suppositions on a grand scale. This gives rise to the question: which web pages does one trust? In a simplistic approach, one might argue that some publishers are trustworthy and others not - begging the question of how a search engine is to assign such a measure of trust to each website or web page. In Chapter 21 we will examine approaches to understanding this question. More subtly, there may be no universal, user-independent notion of trust; a web page whose contents are trustworthy to one user may not be so to another. In traditional (non-web) publishing this is not an issue: users self-select sources they find trustworthy. Thus one reader may find the reporting of The New York Times to be reliable, while another may prefer The Wall Street Journal. But when a search engine is the only viable means for a user to become aware of (let alone select) most content, this challenge becomes significant. While the question ``how big is the Web?'' has no easy answer (see Section 19.5 ), the question ``how many web pages are in a search engine's index'' is more precise, although, even this question has issues. By the end of 1995, Altavista reported that it had crawled and indexed approximately 30 million static web pages . Static web pages are those whose content does not vary from one request for that page to the next. For this purpose, a professor who manually updates his home page every week is considered to have a static web page, but an airport's flight status page is considered to be dynamic. Dynamic pages are typically mechanically generated by an application server in response to a query to a database, as show in Figure 19.1 . One sign of such a page is that the URL has the character "?" in it. Since the number of static web pages was believed to be doubling every few months in 1995, early web search engines such as Altavista had to constantly add hardware and bandwidth for crawling and indexing web pages. A dynamically generated web page.The browser sends a request for flight information on flight AA129 to the web application, that fetches the information from back-end databases then creates a dynamic web page that it returns to the browser.   Subsections The web graph Spam
iir_19_2_1	The web graph  Figure 19.2: Two nodes of the web graph joined by a link. Figure 19.2 shows two nodes A and B from the web graph, each corresponding to a web page, with a hyperlink from A to B. We refer to the set of all such nodes and directed edges as the web graph. Figure 19.2 also shows that (as is the case with most links on web pages) there is some text surrounding the origin of the hyperlink on page A. This text is generally encapsulated in the href attribute of the  (for anchor) tag that encodes the hyperlink in the HTML code of page A, and is referred to as anchor text . As one might suspect, this directed graph is not strongly connected: there are pairs of pages such that one cannot proceed from one page of the pair to the other by following hyperlinks. We refer to the hyperlinks into a page as in-links and those out of a page as out-links . The number of in-links to a page (also known as its in-degree) has averaged from roughly 8 to 15, in a range of studies. We similarly define the out-degree of a web page to be the number of links out of it. These notions are represented in Figure 19.3 .  A sample small web graph.In this example we have six pages labeled A-F. Page B has in-degree 3 and out-degree 1. This example graph is not strongly connected: there is no path from any of pages B-F to page A. There is ample evidence that these links are not randomly distributed; for one thing, the distribution of the number of links into a web page does not follow the Poisson distribution one would expect if every web page were to pick the destinations of its links uniformly at random. Rather, this distribution is widely reported to be a power law , in which the total number of web pages with in-degree is proportional to ; the value of typically reported by studies is 2.1. Furthermore, several studies have suggested that the directed graph connecting web pages has a bowtie shape: there are three major categories of web pages that are sometimes referred to as IN, OUT and SCC. A web surfer can pass from any page in IN to any page in SCC, by following hyperlinks. Likewise, a surfer can pass from page in SCC to any page in OUT. Finally, the surfer can surf from any page in SCC to any other page in SCC. However, it is not possible to pass from a page in SCC to any page in IN, or from a page in OUT to a page in SCC (or, consequently, IN). Notably, in several studies IN and OUT are roughly equal in size, whereas SCC is somewhat larger; most web pages fall into one of these three sets. The remaining pages form into tubes that are small sets of pages outside SCC that lead directly from IN to OUT, and tendrils that either lead nowhere from IN, or from nowhere to OUT. Figure 19.4 illustrates this structure of the Web. The bowtie structure of the Web.Here we show one tube and three tendrils.
iir_19_2_2	Spam  spam spammers At its root, spam stems from the heterogeneity of motives in content creation on the Web. In particular, many web content creators have commercial motives and therefore stand to gain from manipulating search engine results. You might argue that this is no different from a company that uses large fonts to list its phone numbers in the yellow pages; but this generally costs the company more and is thus a fairer mechanism. A more apt analogy, perhaps, is the use of company names beginning with a long string of A's to be listed early in a yellow pages category. In fact, the yellow pages' model of companies paying for larger/darker fonts has been replicated in web search: in many search engines, it is possible to pay to have one's web page included in the search engine's index - a model known as paid inclusion . Different search engines have different policies on whether to allow paid inclusion, and whether such a payment has any effect on ranking in search results. Search engines soon became sophisticated enough in their spam detection to screen out a large number of repetitions of particular keywords. Spammers responded with a richer set of spam techniques, the best known of which we now describe. The first of these techniques is cloaking, shown in Figure 19.5 . Here, the spammer's web server returns different pages depending on whether the http request comes from a web search engine's crawler (the part of the search engine that gathers web pages, to be described in Chapter 20 ), or from a human user's browser. The former causes the web page to be indexed by the search engine under misleading keywords. When the user searches for these keywords and elects to view the page, he receives a web page that has altogether different content than that indexed by the search engine. Such deception of search indexers is unknown in the traditional world of information retrieval; it stems from the fact that the relationship between page publishers and web search engines is not completely collaborative.  Figure 19.5: Cloaking as used by spammers. A doorway page contains text and metadata carefully chosen to rank highly on selected search keywords. When a browser requests the doorway page, it is redirected to a page containing content of a more commercial nature. More complex spamming techniques involve manipulation of the metadata related to a page including (for reasons we will see in Chapter 21 ) the links into a web page. Given that spamming is inherently an economically motivated activity, there has sprung around it an industry of Search Engine Optimizers , or SEOs to provide consultancy services for clients who seek to have their web pages rank highly on selected keywords. Web search engines frown on this business of attempting to decipher and adapt to their proprietary ranking techniques and indeed announce policies on forms of SEO behavior they do not tolerate (and have been known to shut down search requests from certain SEOs for violation of these). Inevitably, the parrying between such SEOs (who gradually infer features of each web search engine's ranking methods) and the web search engines (who adapt in response) is an unending struggle; indeed, the research sub-area of adversarial information retrieval has sprung up around this battle. To combat spammers who manipulate the text of their web pages is the exploitation of the link structure of the Web - a technique known as link analysis. The first web search engine known to apply link analysis on a large scale (to be detailed in Chapter 21 ) was Google, although all web search engines currently make use of it (and correspondingly, spammers now invest considerable effort in subverting it - this is known as link spam ). Exercises. If the number of pages with in-degree is proportional to , what is the probability that a randomly chosen web page has in-degree ? If the number of pages with in-degree is proportional to , what is the average in-degree of a web page? If the number of pages with in-degree is proportional to , then as the largest in-degree goes to infinity, does the fraction of pages with in-degree grow, stay the same, or diminish? How would your answer change for values of the exponent other than ? The average in-degree of all nodes in a snapshot of the web graph is 9. What can we say about the average out-degree of all nodes in this snapshot?
iir_19_3	Advertising as the economic model branding cost per mil  CPM impressions clicked on cost per click  CPC The pioneer in this direction was a company named Goto, which changed its name to Overture prior to eventual acquisition by Yahoo! Goto was not, in the traditional sense, a search engine; rather, for every query term it accepted bids from companies who wanted their web page shown on the query . In response to the query , Goto would return the pages of all advertisers who bid for , ordered by their bids. Furthermore, when the user clicked on one of the returned results, the corresponding advertiser would make a payment to Goto (in the initial implementation, this payment equaled the advertiser's bid for ). Several aspects of Goto's model are worth highlighting. First, a user typing the query into Goto's search interface was actively expressing an interest and intent related to the query . For instance, a user typing golf clubs is more likely to be imminently purchasing a set than one who is simply browsing news on golf. Second, Goto only got compensated when a user actually expressed interest in an advertisement - as evinced by the user clicking the advertisement. Taken together, these created a powerful mechanism by which to connect advertisers to consumers, quickly raising the annual revenues of Goto/Overture into hundreds of millions of dollars. This style of search engine came to be known variously as sponsored search or search advertising . Given these two kinds of search engines - the ``pure'' search engines such as Google and Altavista, versus the sponsored search engines - the logical next step was to combine them into a single user experience. Current search engines follow precisely this model: they provide pure search results (generally known as algorithmic search results) as the primary response to a user's search, together with sponsored search results displayed separately and distinctively to the right of the algorithmic results. This is shown in Figure 19.6 . Retrieving sponsored search results and ranking them in response to a query has now become considerably more sophisticated than the simple Goto scheme; the process entails a blending of ideas from information retrieval and microeconomics, and is beyond the scope of this book. For advertisers, understanding how search engines do this ranking and how to allocate marketing campaign budgets to different keywords and to different sponsored search engines has become a profession known as search engine marketing (SEM).  Search advertising triggered by query keywords.Here the query A320 returns algorithmic search results about the Airbus aircraft, together with advertisements for various non-aircraft goods numbered A320, that advertisers seek to market to those querying on this query. The lack of advertisements for the aircraft reflects the fact that few marketers attempt to sell A320 aircraft on the web. The inherently economic motives underlying sponsored search give rise to attempts by some participants to subvert the system to their advantage. This can take many forms, one of which is known as click spam . There is currently no universally accepted definition of click spam. It refers (as the name suggests) to clicks on sponsored search results that are not from bona fide search users. For instance, a devious advertiser may attempt to exhaust the advertising budget of a competitor by clicking repeatedly (through the use of a robotic click generator) on that competitor's sponsored search advertisements. Search engines face the challenge of discerning which of the clicks they observe are part of a pattern of click spam, to avoid charging their advertiser clients for such clicks. Exercises. The Goto method ranked advertisements matching a query by bid: the highest-bidding advertiser got the top position, the second-highest the next, and so on. What can go wrong with this when the highest-bidding advertiser places an advertisement that is irrelevant to the query? Why might an advertiser with an irrelevant advertisement bid high in this manner? Suppose that, in addition to bids, we had for each advertiser their click-through rate: the ratio of the historical number of times users click on their advertisement to the number of times the advertisement was shown. Suggest a modification of the Goto scheme that exploits this data to avoid the problem in Exercise 19.3 above.
iir_19_4	The search user experience It is clear that the more user traffic a web search engine can attract, the more revenue it stands to earn from sponsored search. How do search engines differentiate themselves and grow their traffic? Here Google identified two principles that helped it grow at the expense of its competitors: (1) a focus on relevance, specifically precision rather than recall in the first few results; (2) a user experience that is lightweight, meaning that both the search query page and the search results page are uncluttered and almost entirely textual, with very few graphical elements. The effect of the first was simply to save users time in locating the information they sought. The effect of the second is to provide a user experience that is extremely responsive, or at any rate not bottlenecked by the time to load the search query or results page.  Subsections User query needs
iir_19_4_1	User query needs Informational queries seek general information on a broad topic, such as leukemia or Provence. There is typically not a single web page that contains all the information sought; indeed, users with informational queries typically try to assimilate information from multiple web pages. Navigational queries seek the website or home page of a single entity that the user has in mind, say Lufthansa airlines. In such cases, the user's expectation is that the very first search result should be the home page of Lufthansa. The user is not interested in a plethora of documents containing the term Lufthansa; for such a user, the best measure of user satisfaction is precision at 1. A transactional query is one that is a prelude to the user performing a transaction on the Web - such as purchasing a product, downloading a file or making a reservation. In such cases, the search engine should return results listing services that provide form interfaces for such transactions. Discerning which of these categories a query falls into can be challenging. The category not only governs the algorithmic search results, but the suitability of the query for sponsored search results (since the query may reveal an intent to purchase). For navigational queries, some have argued that the search engine should return only a single result or even the target web page directly. Nevertheless, web search engines have historically engaged in a battle of bragging rights over which one indexes more web pages. Does the user really care? Perhaps not, but the media does highlight estimates (often statistically indefensible) of the sizes of various search engines. Users are influenced by these reports and thus, search engines do have to pay attention to how their index sizes compare to competitors'. For informational (and to a lesser extent, transactional) queries, the user does care about the comprehensiveness of the search engine. Figure 19.7 shows a composite picture of a web search engine including the crawler, as well as both the web page and advertisement indexes. The portion of the figure under the curved dashed line is internal to the search engine.  Figure 19.7: The various components of a web search engine.
iir_19_5	Index size and estimation http://www.yahoo.com/any_string  spider traps 20 We could ask the following better-defined question: given two search engines, what are the relative sizes of their indexes? Even this question turns out to be imprecise, because: In response to queries a search engine can return web pages whose contents it has not (fully or even partially) indexed. For one thing, search engines generally index only the first few thousand words in a web page. In some cases, a search engine is aware of a page that is linked to by pages it has indexed, but has not indexed itself. As we will see in Chapter 21 , it is still possible to meaningfully return in search results. Search engines generally organize their indexes in various tiers and partitions, not all of which are examined on every search (recall tiered indexes from Section 7.2.1 ). For instance, a web page deep inside a website may be indexed but not retrieved on general web searches; it is however retrieved as a result on a search that a user has explicitly restricted to that website (such site-specific search is offered by most web search engines).   20  capture-recapture method Suppose that we could pick a random page from the index of and test whether it is in 's index and symmetrically, test whether a random page from is in . These experiments give us fractions and such that our estimate is that a fraction of the pages in are in , while a fraction of the pages in are in . Then, letting denote the size of the index of search engine , we have (245)   (246)    246   from outside the search engine To implement the sampling phase, we might generate a random page from the entire (idealized, finite) Web and test it for presence in each search engine. Unfortunately, picking a web page uniformly at random is a difficult problem. We briefly outline several attempts to achieve such a sample, pointing out the biases inherent to each; following this we describe in some detail one technique that much research has built on. Random searches: Begin with a search log of web searches; send a random search from this log to and a random page from the results. Since such logs are not widely available outside a search engine, one implementation is to trap all search queries going out of a work group (say scientists in a research center) that agrees to have all its searches logged. This approach has a number of issues, including the bias from the types of searches made by the work group. Further, a random document from the results of such a random search to is not the same as a random document from . Random IP addresses: A second approach is to generate random IP addresses and send a request to a web server residing at the random address, collecting all pages at that server. The biases here include the fact that many hosts might share one IP (due to a practice known as virtual hosting) or not accept http requests from the host where the experiment is conducted. Furthermore, this technique is more likely to hit one of the many sites with few pages, skewing the document probabilities; we may be able to correct for this effect if we understand the distribution of the number of pages on websites. Random walks: If the web graph were a strongly connected directed graph, we could run a random walk starting at an arbitrary web page. This walk would converge to a steady state distribution (see Chapter 21 , Section 21.2.1 for more background material on this), from which we could in principle pick a web page with a fixed probability. This method, too has a number of biases. First, the Web is not strongly connected so that, even with various corrective rules, it is difficult to argue that we can reach a steady state distribution starting from any page. Second, the time it takes for the random walk to settle into this steady state is unknown and could exceed the length of the experiment. Clearly each of these approaches is far from perfect. We now describe a fourth sampling approach, random queries. This approach is noteworthy for two reasons: it has been successfully built upon for a series of increasingly refined estimates, and conversely it has turned out to be the approach most likely to be misinterpreted and carelessly implemented, leading to misleading measurements. The idea is to pick a page (almost) uniformly at random from a search engine's index by posing a random query to it. It should be clear that picking a set of random terms from (say) Webster's dictionary is not a good way of implementing this idea. For one thing, not all vocabulary terms occur equally often, so this approach will not result in documents being chosen uniformly at random from the search engine. For another, there are a great many terms in web documents that do not occur in a standard dictionary such as Webster's. To address the problem of vocabulary terms not in a standard dictionary, we begin by amassing a sample web dictionary. This could be done by crawling a limited portion of the Web, or by crawling a manually-assembled representative subset of the Web such as Yahoo! (as was done in the earliest experiments with this method). Consider a conjunctive query with two or more randomly chosen words from this dictionary. Operationally, we proceed as follows: we use a random conjunctive query on and pick from the top 100 returned results a page at random. We then test for presence in by choosing 6-8 low-frequency terms in and using them in a conjunctive query for . We can improve the estimate by repeating the experiment a large number of times. Both the sampling process and the testing process have a number of issues. Our sample is biased towards longer documents. Picking from the top 100 results of induces a bias from the ranking algorithm of . Picking from all the results of makes the experiment slower. This is particularly so because most web search engines put up defenses against excessive robotic querying. During the checking phase, a number of additional biases are introduced: for instance, may not handle 8-word conjunctive queries properly. Either or may refuse to respond to the test queries, treating them as robotic spam rather than as bona fide queries. There could be operational problems like connection time-outs. A sequence of research has built on this basic paradigm to eliminate some of these issues; there is no perfect solution yet, but the level of sophistication in statistics for understanding the biases is increasing. The main idea is to address biases by estimating, for each document, the magnitude of the bias. From this, standard statistical sampling methods can generate unbiased samples. In the checking phase, the newer work moves away from conjunctive queries to phrase and other queries that appear to be better-behaved. Finally, newer experiments use other sampling methods besides random queries. The best known of these is document random walk sampling, in which a document is chosen by a random walk on a virtual graph derived from documents. In this graph, nodes are documents; two documents are connected by an edge if they share two or more words in common. The graph is never instantiated; rather, a random walk on it can be performed by moving from a document to another by picking a pair of keywords in , running a query on a search engine and picking a random document from the results. Details may be found in the references in Section 19.7 . Exercises. Two web search engines A and B each generate a large number of pages uniformly at random from their indexes. 30% of A's pages are present in B's index, while 50% of B's pages are present in A's index. What is the number of pages in A's index relative to B's?
iir_19_6	Near-duplicates and shingling 19.5 duplication The simplest approach to detecting duplicates is to compute, for each web page, a fingerprint that is a succinct (say 64-bit) digest of the characters on that page. Then, whenever the fingerprints of two web pages are equal, we test whether the pages themselves are equal and if so declare one of them to be a duplicate copy of the other. This simplistic approach fails to capture a crucial and widespread phenomenon on the Web: near duplication. In many cases, the contents of one web page are identical to those of another except for a few characters - say, a notation showing the date and time at which the page was last modified. Even in such cases, we want to be able to declare the two pages to be close enough that we only index one copy. Short of exhaustively comparing all pairs of web pages, an infeasible task at the scale of billions of pages, how can we detect and filter out such near duplicates? We now describe a solution to the problem of detecting near-duplicate web pages. The answer lies in a technique known as shingling . Given a positive integer and a sequence of terms in a document , define the -shingles of to be the set of all consecutive sequences of terms in . As an example, consider the following text: a rose is a rose is a rose. The 4-shingles for this text ( is a typical value used in the detection of near-duplicate web pages) are a rose is a, rose is a rose and is a rose is. The first two of these shingles each occur twice in the text. Intuitively, two documents are near duplicates if the sets of shingles generated from them are nearly the same. We now make this intuition precise, then develop a method for efficiently computing and comparing the sets of shingles for all web pages. Let denote the set of shingles of document . Recall the Jaccard coefficient from page 3.3.4 , which measures the degree of overlap between the sets and as ; denote this by . Our test for near duplication between and is to compute this Jaccard coefficient; if it exceeds a preset threshold (say, ), we declare them near duplicates and eliminate one from indexing. However, this does not appear to have simplified matters: we still have to compute Jaccard coefficients pairwise. To avoid this, we use a form of hashing. First, we map every shingle into a hash value over a large space, say 64 bits. For , let be the corresponding set of 64-bit hash values derived from . We now invoke the following trick to detect document pairs whose sets have large Jaccard overlaps. Let be a random permutation from the 64-bit integers to the 64-bit integers. Denote by the set of permuted hash values in ; thus for each , there is a corresponding value .  Let be the smallest integer in . Then Theorem. (247)  End theorem. Proof. We give the proof in a slightly more general setting: consider a family of sets whose elements are drawn from a common universe. View the sets as columns of a matrix , with one row for each element in the universe. The element if element is present in the set that the th column represents. Let be a random permutation of the rows of ; denote by the column that results from applying to the th column. Finally, let be the index of the first row in which the column has a . We then prove that for any two columns , (248)   Figure 19.9: Two sets and ; their Jaccard coefficient is . Consider two columns as shown in Figure 19.9 . The ordered pairs of entries of and partition the rows into four types: those with 0's in both of these columns, those with a 0 in and a 1 in , those with a 1 in and a 0 in , and finally those with 1's in both of these columns. Indeed, the first four rows of Figure 19.9 exemplify all of these four types of rows. Denote by the number of rows with 0's in both columns, the second, the third and the fourth. Then, (249)  249    249 End proof. Thus, our test for the Jaccard coefficient of the shingle sets is probabilistic: we compare the computed values from different documents. If a pair coincides, we have candidate near duplicates. Repeat the process independently for 200 random permutations (a choice suggested in the literature). Call the set of the 200 resulting values of the sketch of . We can then estimate the Jaccard coefficient for any pair of documents to be ; if this exceeds a preset threshold, we declare that and are similar. How can we quickly compute for all pairs ? Indeed, how do we represent all pairs of documents that are similar, without incurring a blowup that is quadratic in the number of documents? First, we use fingerprints to remove all but one copy of identical documents. We may also remove common HTML tags and integers from the shingle computation, to eliminate shingles that occur very commonly in documents without telling us anything about duplication. Next we use a union-find algorithm to create clusters that contain documents that are similar. To do this, we must accomplish a crucial step: going from the set of sketches to the set of pairs such that and are similar. To this end, we compute the number of shingles in common for any pair of documents whose sketches have any members in common. We begin with the list sorted by pairs. For each , we can now generate all pairs for which is present in both their sketches. From these we can compute, for each pair with non-zero sketch overlap, a count of the number of values they have in common. By applying a preset threshold, we know which pairs have heavily overlapping sketches. For instance, if the threshold were 80%, we would need the count to be at least 160 for any . As we identify such pairs, we run the union-find to group documents into near-duplicate ``syntactic clusters''. This is essentially a variant of the single-link clustering algorithm introduced in Section 17.2 (page ). One final trick cuts down the space needed in the computation of for pairs , which in principle could still demand space quadratic in the number of documents. To remove from consideration those pairs whose sketches have few shingles in common, we preprocess the sketch for each document as follows: sort the in the sketch, then shingle this sorted sequence to generate a set of super-shingles for each document. If two documents have a super-shingle in common, we proceed to compute the precise value of . This again is a heuristic but can be highly effective in cutting down the number of pairs for which we accumulate the sketch overlap counts. Exercises. Web search engines A and B each crawl a random subset of the same size of the Web. Some of the pages crawled are duplicates - exact textual copies of each other at different URLs. Assume that duplicates are distributed uniformly amongst the pages crawled by A and B. Further, assume that a duplicate is a page that has exactly two copies - no pages have more than two copies. A indexes pages without duplicate elimination whereas B indexes only one copy of each duplicate page. The two random subsets have the same size before duplicate elimination. If, 45% of A's indexed URLs are present in B's index, while 50% of B's indexed URLs are present in A's index, what fraction of the Web consists of pages that do not have a duplicate? Instead of using the process depicted in Figure 19.8 , consider instead the following process for estimating the Jaccard coefficient of the overlap between two sets and . We pick a random subset of the elements of the universe from which and are drawn; this corresponds to picking a random subset of the rows of the matrix in the proof. We exhaustively compute the Jaccard coefficient of these random subsets. Why is this estimate an unbiased estimator of the Jaccard coefficient for and ? Explain why this estimator would be very difficult to use in practice.
iir_19_7	References and further reading Bush (1945) memex Berners-Lee et al. (1992) Kumar et al. (2000) Broder et al. (2000) McBryan (1994) 19.4 Broder (2002) 19.2.1 Kumar et al. (1999) Chakrabarti (2002) The estimation of web search index sizes has a long history of development covered by Bharat and Broder (1998), Lawrence and Giles (1998), Rusmevichientong et al. (2001), Lawrence and Giles (1999), Henzinger et al. (2000), Bar-Yossef and Gurevich (2006). The state of the art is Bar-Yossef and Gurevich (2006), including several of the bias-removal techniques mentioned at the end of Section 19.5 . Shingling was introduced by Broder et al. (1997) and used for detecting websites (rather than simply pages) that are identical by Bharat et al. (2000).
iir_1_1	An example information retrieval problem A fat book which many people own is Shakespeare's Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar and NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as grepping through text, after the Unix command grep, which performs this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of . With modern computers, for simple querying of modest collections (the size of Shakespeare's Collected Works is a bit under one million words of text in total), you really need nothing more. But for many purposes, you do need more: To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words. To allow more flexible matching operations. For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as ``within 5 words'' or ``within the same sentence''. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words. The way to avoid linearly scanning the texts for each query is to index the documents in advance. Let us stick with Shakespeare's Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document - here a play of Shakespeare's - whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document incidence matrix , as in Figure 1.1 . Terms are the indexed units (further discussed in Section 2.2 ); they are usually words, and for the moment you can think of them as words, but the information retrieval literature normally speaks of terms because some of them, such as perhaps I-9 or Hong Kong are not usually thought of as words. Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it.   To answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND: 110100 AND 110111 AND 101111 = 100100 1.2 The Boolean retrieval model is a model for information retrieval in which we can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators and, or, and not. The model views each document as just a set of words.  Figure: Results from Shakespeare for the query Brutus AND Caesar AND NOT Calpurnia. Let us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation. Suppose we have documents. By documents we mean whatever units we have decided to build a retrieval system over. They might be individual memos or chapters of a book (see Section 2.1.2 (page ) for further discussion). We will refer to the group of documents over which we perform retrieval as the (document) collection . It is sometimes also referred to as a corpus (a body of texts). Suppose each document is about 1000 words long (2-3 book pages). If we assume an average of 6 bytes per word including spaces and punctuation, then this is a document collection about 6 GB in size. Typically, there might be about distinct terms in these documents. There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle. We will discuss and model these size assumptions in Section 5.1 (page ). Our goal is to develop a system to address the ad hoc retrieval task. This is the most standard IR task. In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query. An information need is the topic about which the user desires to know more, and is differentiated from a query , which is what the user conveys to the computer in an attempt to communicate the information need. A document is relevant if it is one that the user perceives as containing information of value with respect to their personal information need. Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like ``pipeline leaks'' and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture. To assess the effectiveness of an IR system (i.e., the quality of its search results), a user will usually want to know two key statistics about the system's returned results for a query: Precision : What fraction of the returned results are relevant to the information need? Recall : What fraction of the relevant documents in the collection were returned by the system? 8 We now cannot build a term-document matrix in a naive way. A matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory. But the crucial observation is that the matrix is extremely sparse, that is, it has few non-zero entries. Because each document is 1000 words long, the matrix has no more than one billion 1's, so a minimum of 99.8% of the cells are zero. A much better representation is to record only the things that do occur, that is, the 1 positions. This idea is central to the first major concept in information retrieval, the inverted index . The name is actually redundant: an index always maps back from terms to the parts of a document where they occur. Nevertheless, inverted index, or sometimes inverted file , has become the standard term in information retrieval.The basic idea of an inverted index is shown in Figure 1.3 . We keep a dictionary of terms (sometimes also referred to as a vocabulary or lexicon ; in this book, we use dictionary for the data structure and vocabulary for the set of terms). Then for each term, we have a list that records which documents the term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called a posting .The list is then called a postings list (or ), and all the postings lists taken together are referred to as the postings . The dictionary in Figure 1.3 has been sorted alphabetically and each postings list is sorted by document ID. We will see why this is useful in Section 1.3 , below, but later we will also consider alternatives to doing this (Section 7.1.5 ).
iir_1_2	A first take at building an inverted index To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are: Collect the documents to be indexed: ... Tokenize the text, turning each document into a list of tokens: ... Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms: ... Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings. 2.2  tokens normalized tokens words  sort-based indexing   Within a document collection, we assume that each document has a unique serial number, known as the document identifier ( docID ). During index construction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4 . The core indexing step is sorting this list so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4 . Multiple occurrences of the same term from the same document are then merged.Instances of the same term are then grouped, and the result is split into a dictionary and postings , as shown in the right column of Figure 1.4 . Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the document frequency , which is here also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search. In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3 ), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3 ), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory. Exercises. Draw the inverted index that would be built for the following document collection. (See Figure 1.3 for an example.) Doc 1    new home sales top forecasts Doc 2    home sales rise in july Doc 3    increase in home sales in july Doc 4    july new home sales rise Consider these documents: Doc 1    breakthrough drug for schizophrenia Doc 2    new schizophrenia drug Doc 3    new approach for treatment of schizophrenia Doc 4    new hopes for schizophrenia patients Draw the term-document incidence matrix for this document collection. Draw the inverted index representation for this collection, as in Figure 1.3 (page ). For the document collection shown in Exercise 1.2 , what are the returned results for these queries: schizophrenia AND drug for AND NOT(drug OR approach)
iir_1_3	Processing Boolean queries How do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the simple conjunctive query : over the inverted index partially shown in Figure 1.3 (page ). We: Locate Brutus in the Dictionary Retrieve its postings Locate Calpurnia in the Dictionary Retrieve its postings Intersect the two postings lists, as shown in Figure 1.5 .  intersection  merging  merge algorithm  Figure: Intersecting the postings lists for Brutus and Calpurnia from Figure 1.3 .  Figure 1.6: Algorithm for the intersection of two postings lists and . There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6 ): we maintain pointers into both lists and walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are and , the intersection takes operations. Formally, the complexity of querying is ,where is the number of documents in the collection.Our indexing methods gain us just a constant, not a difference in time complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries like: Query optimization is the process of selecting how to organize the work of answering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of terms, for instance: For each of the terms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document frequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 (page ), we execute the above query as: This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list. Consider now the optimization of more general queries, such as: As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term.  Figure 1.7: Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms. For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7 . The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings intersection can still be done by the algorithm in Figure 1.6 , but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5 . Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common. Exercises. For the queries below, can we still run through the intersection in time , where and are the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve? Brutus and not Caesar Brutus or not Caesar Extend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider: c. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra) Can we always merge in linear time? Linear in what? Can we do better than this? We can use distributive laws for and and or to rewrite queries. Show how to rewrite the query in Exercise 1.3 into disjunctive normal form using the distributive laws. Would the resulting query be more or less efficiently evaluated than the original form of this query? Is this result true in general or does it depend on the words and the contents of the document collection? Recommend a query processing order for d. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes) given the following postings list sizes: Term Postings size eyes 213312 kaleidoscope 87009 marmalade 107913 skies 271658 tangerine 46653 trees 316812 If the query is: e. friends AND romans AND (NOT countrymen) how could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing. For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn't. Write out a postings merge algorithm, in the style of Figure 1.6 (page ), for an OR query. How should the Boolean query AND NOT be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently.
iir_1_4	The extended Boolean model versus ranked retrieval The Boolean retrieval model contrasts with ranked retrieval models such as the vector space model (Section 6.3 ), in which users largely use free text queries , that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boolean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A proximity operator is a way of specifying that two terms in a query must occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph.  Worked example. Commercial Boolean searching: Westlaw.westlaw Westlaw (http://www.westlaw.com/) is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called ``Terms and Connectors'' by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called ``Natural Language'' by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw:  Information need: Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company. Query: "trade secret" /s disclos! /s prevent /s employe!  Information need: Requirements for disabled people to be able to access a workplace. Query: disab! /p access! /s work-site work-place (employment /3 place)    Information need: Cases about a host's responsibility for drunk guests. Query: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest Note the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator),   is AND and /s, /p, and / ask for matches in the same sentence, same paragraph or within words respectively. Double quotes give a phrase search (consecutive words); see Section 2.4 (page ). The exclamation mark (!) gives a trailing wildcard query wildcard; thus liab! matches all words starting with liab. Additionally work-site matches any of worksite, work-site or work site; see Section 2.2.1 (page ). Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user. Many users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw's own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground. End worked example. In this chapter, we have looked at the structure and construction of a basic inverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In dictionaryranking-ir-system we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do: We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words. It is often useful to search for compounds or phrases that denote a concept such as ``operating system''. As the Westlaw examples show, we might also wish to do proximity queries such as Gates near Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need term frequency information (the number of times a term occurs in a document) in postings lists. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or ``rank'') the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query. With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying , most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance. Exercises. Write a query using Westlaw syntax which would find any of the words professor, teacher, or lecturer in the same sentence as a form of the verb explain. Try using the Boolean search features on a couple of major web search engines. For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and top hits. Do they make sense in terms of Boolean logic? Often they haven't for major search engines. Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer. What bound should the number of results from the first two queries place on the third query? Is this bound observed?
iir_1_5	References and further reading The practical pursuit of computerized information retrieval began in the late 1940s (Cleverdon, 1991, Liddy, 2005). A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval. However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later. The article of Bush (1945) provided lasting inspiration for the new field: ``Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, `memex' will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.'' Information Retrieval Mooers, 1950 In 1958, much newspaper attention was paid to demonstrations at a conference (see Taube and Wooster, 1958) of IBM ``auto-indexing'' machines, based primarily on the work of H. P. Luhn. Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems. For example Mooers (1961) dissented: ``It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design. This view is as widely and uncritically accepted as it is wrong.'' Lee and Fox, 1988 The book (Witten et al., 1999) is the standard reference for an in-depth comparison of the space and time efficiency of the inverted index versus other possible data structures; a more succinct and up-to-date presentation appears in Zobel and Moffat (2006). We further discuss several approaches in Chapter 5 . Friedl (2006) covers the practical usage of regular expressions for searching. The underlying computer science appears in (Hopcroft et al., 2000).
iir_2	The term vocabulary and postings lists Recall the major steps in inverted index construction: Collect the documents to be indexed. Tokenize the text. Do linguistic preprocessing of tokens. Index the documents that each term occurs in. 2.1 2.2  tokens  terms 1 4 2.3 2.4   Subsections Document delineation and character sequence decoding Obtaining the character sequence in a document Choosing a document unit Determining the vocabulary of terms Tokenization Dropping common terms: stop words Normalization (equivalence classing of terms) Accents and diacritics. Capitalization/case-folding. Other issues in English. Other languages. Stemming and lemmatization Faster postings list intersection via skip pointers Positional postings and phrase queries Biword indexes Positional indexes Positional index size. Combination schemes References and further reading
iir_20	Web crawling and indexes   Subsections Overview Features a crawler must provide Features a crawler should provide Crawling Crawler architecture Distributing the crawler DNS resolution The URL frontier Distributing indexes Connectivity servers References and further reading
iir_20_1	Overview 19 19.7  web crawler  spider The goal of this chapter is not to describe how to build the crawler for a full-scale commercial web search engine. We focus instead on a range of issues that are generic to crawling from the student project scale to substantial research projects. We begin (Section 20.1.1 ) by listing desiderata for web crawlers, and then discuss in Section 20.2 how each of these issues is addressed. The remainder of this chapter describes the architecture and some implementation details for a distributed web crawler that satisfies these features. Section 20.3 discusses distributing indexes across many machines for a web-scale implementation.  Subsections Features a crawler must provide Features a crawler should provide
iir_20_1_1	Features a crawler must provide must should Robustness: The Web contains servers that create spider traps, which are generators of web pages that mislead crawlers into getting stuck fetching an infinite number of pages in a particular domain. Crawlers must be designed to be resilient to such traps. Not all such traps are malicious; some are the inadvertent side-effect of faulty website development. Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them. These politeness policies must be respected.
iir_20_1_2	Features a crawler should provide Distributed: The crawler should have the ability to execute in a distributed fashion across multiple machines. Scalable: The crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth. Performance and efficiency: The crawl system should make efficient use of various system resources including processor, storage and network bandwidth. Quality: Given that a significant fraction of all web pages are of poor utility for serving user query needs, the crawler should be biased towards fetching ``useful'' pages first. Freshness: In many applications, the crawler should operate in continuous mode: it should obtain fresh copies of previously fetched pages. A search engine crawler, for instance, can thus ensure that the search engine's index contains a fairly current representation of each indexed web page. For such continuous crawling, a crawler should be able to crawl a page with a frequency that approximates the rate of change of that page. Extensible: Crawlers should be designed to be extensible in many ways - to cope with new data formats, new fetch protocols, and so on. This demands that the crawler architecture be modular.
iir_20_2	Crawling seed set 4 5 URL frontier 19 This seemingly simple recursive traversal of the web graph is complicated by the many demands on a practical web crawling system: the crawler has to be distributed, scalable, efficient, polite, robust and extensible while fetching pages of high quality. We examine the effects of each of these issues. Our treatment follows the design of the Mercator crawler that has formed the basis of a number of research and commercial crawlers. As a reference point, fetching a billion pages (a small fraction of the static Web at present) in a month-long crawl requires fetching several hundred pages each second. We will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate. Before proceeding to this detailed description, we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy: Only one connection should be open to any given host at a time. A waiting time of a few seconds should occur between successive requests to a host. Politeness restrictions detailed in Section 20.2.1 should be obeyed.   Subsections Crawler architecture Distributing the crawler DNS resolution The URL frontier
iir_20_2_1	Crawler architecture The simple scheme outlined above for crawling demands several modules that fit together as shown in Figure 20.1 . The URL frontier, containing URLs yet to be fetched in the current crawl (in the case of continuous crawling, a URL may have been fetched previously but is back in the frontier for re-fetching). We describe this further in Section 20.2.3 . A DNS resolution module that determines the web server from which to fetch the page specified by a URL. We describe this further in Section 20.2.2 . A fetch module that uses the http protocol to retrieve the web page at a URL. A parsing module that extracts the text and set of links from a fetched web page. A duplicate elimination module that determines whether an extracted link is already in the URL frontier or has recently been fetched.  Figure 20.1: The basic crawler architecture. Crawling is performed by anywhere from one to potentially hundreds of threads, each of which loops through the logical cycle in Figure 20.1 . These threads may be run in a single process, or be partitioned amongst multiple processes running at different nodes of a distributed system. We begin by assuming that the URL frontier is in place and non-empty and defer our description of the implementation of the URL frontier to Section 20.2.3 . We follow the progress of a single URL through the cycle of being fetched, passing through various checks and filters, then finally (for continuous crawling) being returned to the URL frontier. A crawler thread begins by taking a URL from the frontier and fetching the web page at that URL, generally using the http protocol. The fetched page is then written into a temporary store, where a number of operations are performed on it. Next, the page is parsed and the text as well as the links in it are extracted. The text (with any tag information - e.g., terms in boldface) is passed on to the indexer. Link information including anchor text is also passed on to the indexer for use in ranking in ways that are described in Chapter 21 . In addition, each extracted link goes through a series of tests to determine whether the link should be added to the URL frontier. First, the thread tests whether a web page with the same content has already been seen at another URL. The simplest implementation for this would use a simple fingerprint such as a checksum (placed in a store labeled "Doc FP's" in Figure 20.1 ). A more sophisticated test would use shingles instead of fingerprints, as described in Chapter 19 . Next, a URL filter is used to determine whether the extracted URL should be excluded from the frontier based on one of several tests. For instance, the crawl may seek to exclude certain domains (say, all .com URLs) - in this case the test would simply filter out the URL if it were from the .com domain. A similar test could be inclusive rather than exclusive. Many hosts on the Web place certain portions of their websites off-limits to crawling, under a standard known as the Robots Exclusion Protocol , except for the robot called ``searchengine''.  User-agent: * Disallow: /yoursite/temp/  User-agent: searchengine Disallow: The robots.txt file must be fetched from a website in order to test whether the URL under consideration passes the robot restrictions, and can therefore be added to the URL frontier. Rather than fetch it afresh for testing on each URL to be added to the frontier, a cache can be used to obtain a recently fetched copy of the file for the host. This is especially important since many of the links extracted from a page fall within the host from which the page was fetched and therefore can be tested against the host's robots.txt file. Thus, by performing the filtering during the link extraction process, we would have especially high locality in the stream of hosts that we need to test for robots.txt files, leading to high cache hit rates. Unfortunately, this runs afoul of webmasters' politeness expectations. A URL (particularly one referring to a low-quality or rarely changing document) may be in the frontier for days or even weeks. If we were to perform the robots filtering before adding such a URL to the frontier, its robots.txt file could have changed by the time the URL is dequeued from the frontier and fetched. We must consequently perform robots-filtering immediately before attempting to fetch a web page. As it turns out, maintaining a cache of robots.txt files is still highly effective; there is sufficient locality even in the stream of URLs dequeued from the URL frontier. Next, a URL should be normalized in the following sense: often the HTML encoding of a link from a web page indicates the target of that link relative to the page . Thus, there is a relative link encoded thus in the HTML of the page en.wikipedia.org/wiki/Main_Page: Disclaimers http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer Finally, the URL is checked for duplicate elimination: if the URL is already in the frontier or (in the case of a non-continuous crawl) already crawled, we do not add it to the frontier. When the URL is added to the frontier, it is assigned a priority based on which it is eventually removed from the frontier for fetching. The details of this priority queuing are in Section 20.2.3 . Certain housekeeping tasks are typically performed by a dedicated thread. This thread is generally quiescent except that it wakes up once every few seconds to log crawl progress statistics (URLs crawled, frontier size, etc.), decide whether to terminate the crawl, or (once every few hours of crawling) checkpoint the crawl. In checkpointing, a snapshot of the crawler's state (say, the URL frontier) is committed to disk. In the event of a catastrophic crawler failure, the crawl is restarted from the most recent checkpoint.   Subsections Distributing the crawler
iir_20_2_2	DNS resolution  IP address  DNS resolution Domain Name Service  DNS server en.wikipedia.org/wiki/Domain_Name_System DNS resolution is a well-known bottleneck in web crawling. Due to the distributed nature of the Domain Name Service, DNS resolution may entail multiple requests and round-trips across the internet, requiring seconds and sometimes even longer. Right away, this puts in jeopardy our goal of fetching several hundred documents a second. A standard remedy is to introduce caching: URLs for which we have recently performed DNS lookups are likely to be found in the DNS cache, avoiding the need to go to the DNS servers on the internet. However, obeying politeness constraints (see Section 20.2.3 ) limits the of cache hit rate. There is another important difficulty in DNS resolution; the lookup implementations in standard libraries (likely to be used by anyone developing a crawler) are generally synchronous. This means that once a request is made to the Domain Name Service, other crawler threads at that node are blocked until the first request is completed. To circumvent this, most web crawlers implement their own DNS resolver as a component of the crawler. Thread executing the resolver code sends a message to the DNS server and then performs a timed wait: it resumes either when being signaled by another thread or when a set time quantum expires. A single, separate DNS thread listens on the standard DNS port (port 53) for incoming response packets from the name service. Upon receiving a response, it signals the appropriate crawler thread (in this case, ) and hands it the response packet if has not yet resumed because its time quantum has expired. A crawler thread that resumes because its wait time quantum has expired retries for a fixed number of attempts, sending out a new message to the DNS server and performing a timed wait each time; the designers of Mercator recommend of the order of five attempts. The time quantum of the wait increases exponentially with each of these attempts; Mercator started with one second and ended with roughly 90 seconds, in consideration of the fact that there are host names that take tens of seconds to resolve.
iir_20_2_3	The URL frontier The second consideration is politeness: we must avoid repeated fetch requests to a host within a short time span. The likelihood of this is exacerbated because of a form of locality of reference: many URLs link to other URLs at the same host. As a result, a URL frontier implemented as a simple priority queue might result in a burst of fetch requests to a host. This might occur even if we were to constrain the crawler so that at most one thread could fetch from any single host at any time. A common heuristic is to insert a gap between successive fetch requests to a host that is an order of magnitude larger than the time taken for the most recent fetch from that host.   Figure 20.3 shows a polite and prioritizing implementation of a URL frontier. Its goals are to ensure that (i) only one connection is open at a time to any host; (ii) a waiting time of a few seconds occurs between successive requests to a host and (iii) high-priority pages are crawled preferentially. The two major sub-modules are a set of front queues in the upper portion of the figure, and a set of back queues in the lower part; all of these are FIFO queues. The front queues implement the prioritization, while the back queues implement politeness. In the flow of a URL added to the frontier as it makes its way through the front and back queues, a prioritizer first assigns to the URL an integer priority between 1 and based on its fetch history (taking into account the rate at which the web page at this URL has changed between previous crawls). For instance, a document that has exhibited frequent change would be assigned a higher priority. Other heuristics could be application-dependent and explicit - for instance, URLs from news services may always be assigned the highest priority. Now that it has been assigned priority , the URL is now appended to the th of the front queues. Each of the back queues maintains the following invariants: (i) it is non-empty while the crawl is in progress and (ii) it only contains URLs from a single host. An auxiliary table (Figure 20.4 ) is used to maintain the mapping from hosts to back queues. Whenever a back-queue is empty and is being re-filled from a front-queue, table must be updated accordingly. In addition, we maintain a heap with one entry for each back queue, the entry being the earliest time at which the host corresponding to that queue can be contacted again.  Figure 20.4: Example of an auxiliary hosts-to-back queues table. A crawler thread requesting a URL from the frontier extracts the root of this heap and (if necessary) waits until the corresponding time entry . It then takes the URL at the head of the back queue corresponding to the extracted heap root, and proceeds to fetch the URL . After fetching , the calling thread checks whether is empty. If so, it picks a front queue and extracts from its head a URL . The choice of front queue is biased (usually by a random process) towards queues of higher priority, ensuring that URLs of high priority flow more quickly into the back queues. We examine to check whether there is already a back queue holding URLs from its host. If so, is added to that queue and we reach back to the front queues to find another candidate URL for insertion into the now-empty queue . This process continues until is non-empty again. In any case, the thread inserts a heap entry for with a new earliest time based on the properties of the URL in that was last fetched (such as when its host was last contacted as well as the time taken for the last fetch), then continues with its processing. For instance, the new entry could be the current time plus ten times the last fetch time. The number of front queues, together with the policy of assigning priorities and picking queues, determines the priority properties we wish to build into the system. The number of back queues governs the extent to which we can keep all crawl threads busy while respecting politeness. The designers of Mercator recommend a rough rule of three times as many back queues as crawler threads. On a Web-scale crawl, the URL frontier may grow to the point where it demands more memory at a node than is available. The solution is to let most of the URL frontier reside on disk. A portion of each queue is kept in memory, with more brought in from disk as it is drained in memory. Exercises. Why is it better to partition hosts (rather than individual URLs) between the nodes of a distributed crawl system? Why should the host splitter precede the Duplicate URL Eliminator? In the preceding discussion we encountered two recommended ``hard constants'' - the increment on being ten times the last fetch time, and the number of back queues being three times the number of crawl threads. How are these two constants related?
iir_20_3	Distributing indexes In Section 4.4 we described distributed indexing. We now consider the distribution of the index across a large computer cluster that supports querying. Two obvious alternative index implementations suggest themselves: partitioning by terms , also known as global index organization, and partitioning by documents , also know as local index organization. In the former, the dictionary of index terms is partitioned into subsets, each subset residing at a node. Along with the terms at a node, we keep the postings for those terms. A query is routed to the nodes corresponding to its query terms. In principle, this allows greater concurrency since a stream of queries with different query terms would hit different sets of machines. In practice, partitioning indexes by vocabulary terms turns out to be non-trivial. Multi-word queries require the sending of long postings lists between sets of nodes for merging, and the cost of this can outweigh the greater concurrency. Load balancing the partition is governed not by an a priori analysis of relative term frequencies, but rather by the distribution of query terms and their co-occurrences, which can drift with time or exhibit sudden bursts. Achieving good partitions is a function of the co-occurrences of query terms and entails the clustering of terms to optimize objectives that are not easy to quantify. Finally, this strategy makes implementation of dynamic indexing more difficult. A more common implementation is to partition by documents: each node contains the index for a subset of all documents. Each query is distributed to all nodes, with the results from various nodes being merged before presentation to the user. This strategy trades more local disk seeks for less inter-node communication. One difficulty in this approach is that global statistics used in scoring - such as idf - must be computed across the entire document collection even though the index at any single node only contains a subset of the documents. These are computed by distributed ``background'' processes that periodically refresh the node indexes with fresh global statistics. How do we decide the partition of documents to nodes? Based on our development of the crawler architecture in Section 20.2.1 , one simple approach would be to assign all pages from a host to a single node. This partitioning could follow the partitioning of hosts to crawler nodes. A danger of such partitioning is that on many queries, a preponderance of the results would come from documents at a small number of hosts (and hence a small number of index nodes). A hash of each URL into the space of index nodes results in a more uniform distribution of query-time computation across nodes. At query time, the query is broadcast to each of the nodes, with the top results from each node being merged to find the top documents for the query. A common implementation heuristic is to partition the document collection into indexes of documents that are more likely to score highly on most queries (using, for instance, techniques in Chapter 21 ) and low-scoring indexes with the remaining documents. We only search the low-scoring indexes when there are too few matches in the high-scoring indexes, as described in Section 7.2.1 .
iir_20_4	Connectivity servers 21  connectivity server  connectivity queries which URLs link to a given URL? which URLs does a given URL link to? link analysis 21 Suppose that the Web had four billion pages, each with ten links to other pages. In the simplest form, we would require 32 bits or 4 bytes to specify each end (source and destination) of each link, requiring a total of (250)  5 We assume that each web page is represented by a unique integer; the specific scheme used to assign these integers is described below. We build an adjacency table that resembles an inverted index: it has a row for each web page, with the rows ordered by the corresponding integers. The row for any page contains a sorted list of integers, each corresponding to a web page that links to . This table permits us to respond to queries of the form which pages link to ? In similar fashion we build a table whose entries are the pages linked to by . This table representation cuts the space taken by the naive representation (in which we explicitly represent each link by its two end points, each a 32-bit integer) by 50%. Our description below will focus on the table for the links from each page; it should be clear that the techniques apply just as well to the table of links to each page. To further reduce the storage for the table, we exploit several ideas: Similarity between lists: Many rows of the table have many entries in common. Thus, if we explicitly represent a prototype row for several similar rows, the remainder can be succinctly expressed in terms of the prototypical row. Locality: many links from a page go to ``nearby'' pages - pages on the same host, for instance. This suggests that in encoding the destination of a link, we can often use small integers and thereby save space. We use gap encodings in sorted lists: rather than store the destination of each link, we store the offset from the previous entry in the row. In a lexicographic ordering of all URLs, we treat each URL as an alphanumeric string and sort these strings. Figure 20.5 shows a segment of this sorted order. For a true lexicographic sort of web pages, the domain name part of the URL should be inverted, so that www.stanford.edu becomes edu.stanford.www, but this is not necessary here since we are mainly concerned with links local to a single host.  Figure 20.5: A lexicographically ordered set of URLs. To each URL, we assign its position in this ordering as the unique identifying integer. Figure 20.6 shows an example of such a numbering and the resulting table. In this example sequence, www.stanford.edu/biology is assigned the integer 2 since it is second in the sequence. We next exploit a property that stems from the way most websites are structured to get similarity and locality. Most websites have a template with a set of links from each page in the site to a fixed set of pages on the site (such as its copyright notice, terms of use, and so on). In this case, the rows corresponding to pages in a website will have many table entries in common. Moreover, under the lexicographic ordering of URLs, it is very likely that the pages from a website appear as contiguous rows in the table.  Figure 20.6: A four-row segment of the table of links. We adopt the following strategy: we walk down the table, encoding each table row in terms of the seven preceding rows. In the example of Figure 20.6, we could encode the fourth row as ``the same as the row at offset 2 (meaning, two rows earlier in the table), with 9 replaced by 8''. This requires the specification of the offset, the integer(s) dropped (in this case 9) and the integer(s) added (in this case 8). The use of only the seven preceding rows has two advantages: (i) the offset can be expressed with only 3 bits; this choice is optimized empirically (the reason for seven and not eight preceding rows is the subject of Exercise 20.4) and (ii) fixing the maximum offset to a small value like seven avoids having to perform an expensive search among many candidate prototypes in terms of which to express the current row. What if none of the preceding seven rows is a good prototype for expressing the current row? This would happen, for instance, at each boundary between different websites as we walk down the rows of the table. In this case we simply express the row as starting from the empty set and ``adding in'' each integer in that row. By using gap encodings to store the gaps (rather than the actual integers) in each row, and encoding these gaps tightly based on the distribution of their values, we obtain further space reduction. In experiments mentioned in Section 20.5 , the series of techniques outlined here appears to use as few as 3 bits per link, on average - a dramatic reduction from the 64 required in the naive representation. While these ideas give us a representation of sizable web graphs that comfortably fit in memory, we still need to support connectivity queries. What is entailed in retrieving from this representation the set of links from a page? First, we need an index lookup from (a hash of) the URL to its row number in the table. Next, we need to reconstruct these entries, which may be encoded in terms of entries in other rows. This entails following the offsets to reconstruct these other rows - a process that in principle could lead through many levels of indirection. In practice however, this does not happen very often. A heuristic for controlling this can be introduced into the construction of the table: when examining the preceding seven rows as candidates from which to model the current row, we demand a threshold of similarity between the current row and the candidate prototype. This threshold must be chosen with care. If the threshold is set too high, we seldom use prototypes and express many rows afresh. If the threshold is too low, most rows get expressed in terms of prototypes, so that at query time the reconstruction of a row leads to many levels of indirection through preceding prototypes. Exercises. We noted that expressing a row in terms of one of seven preceding rows allowed us to use no more than three bits to specify which of the preceding rows we are using as prototype. Why seven and not eight preceding rows? (Hint: consider the case when none of the preceding seven rows is a good prototype.) We noted that for the scheme in Section 20.4 , decoding the links incident on a URL could result in many levels of indirection. Construct an example in which the number of levels of indirection grows linearly with the number of URLs.
iir_20_5	References and further reading Najork and Heydon, 2002 2001 Burner (1997) Brin and Page (1998) Cho et al. (1998) Hirai et al., 2000 Cho and Garcia-Molina (2002) http://www.robotstxt.org/wc/exclusion.html Boldi et al. (2002) Shkapenyuk and Suel (2002) Our discussion of DNS resolution (Section 20.2.2 ) uses the current convention for internet addresses, known as IPv4 (for Internet Protocol version 4) - each IP address is a sequence of four bytes. In the future, the convention for addresses (collectively known as the internet address space) is likely to use a new standard known as IPv6 (http://www.ipv6.org/). Tomasic and Garcia-Molina (1993) and Jeong and Omiecinski (1995) are key early papers evaluating term partitioning versus document partitioning for distributed indexes. Document partitioning is found to be superior, at least when the distribution of terms is skewed, as it typically is in practice. This result has generally been confirmed in more recent work (MacFarlane et al., 2000). But the outcome depends on the details of the distributed system; at least one thread of work has reached the opposite conclusion (Ribeiro-Neto and Barbosa, 1998, Badue et al., 2001). Sornil (2001) argues for a partitioning scheme that is a hybrid between term and document partitioning. Barroso et al. (2003) describe the distribution methods used at Google. The first implementation of a connectivity server was described by Bharat et al. (1998). The scheme discussed in this chapter, currently believed to be the best published scheme (achieving as few as 3 bits per link for encoding), is described in a series of papers by Boldi and Vigna (2004b;a).
iir_21	Link analysis The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search. In this chapter we focus on the use of hyperlinks for ranking web search results. Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query. We begin by reviewing some basics of the Web as a graph in Section 21.1 , then proceed to the technical development of the elements of link analysis for ranking. Link analysis for web search has intellectual antecedents in the field of citation analysis, aspects of which overlap with an area known as bibliometrics. These disciplines seek to quantify the influence of scholarly articles by analyzing the pattern of citations amongst them. Much as citations represent the conferral of authority from a scholarly article to others, link analysis on the Web treats hyperlinks from a web page to another as a conferral of authority. Clearly, not every citation or hyperlink implies such authority conferral; for this reason, simply measuring the quality of a web page by the number of in-links (citations from other pages) is not robust enough. For instance, one may contrive to set up multiple web pages pointing to a target web page, with the intent of artificially boosting the latter's tally of in-links. This phenomenon is referred to as link spam . Nevertheless, the phenomenon of citation is prevalent and dependable enough that it is feasible for web search engines to derive useful signals for ranking from more sophisticated link analysis. Link analysis also proves to be a useful indicator of what page(s) to crawl next while crawling the web; this is done by using link analysis to guide the priority assignment in the front queues of Chapter 20 . Section 21.1 develops the basic ideas underlying the use of the web graph in link analysis. and 21.3 then develop two distinct methods for link analysis, PageRank and HITS.   Subsections The Web as a graph Anchor text and the web graph PageRank Markov chains Definition: The PageRank computation Topic-specific PageRank Hubs and Authorities Choosing the subset of the Web References and further reading
iir_21_1	The Web as a graph 19.2.1 19.2 The anchor text pointing to page B is a good description of page B. The hyperlink from A to B represents an endorsement of page B, by the creator of page A. This is not always the case; for instance, many links amongst pages within a single website stem from the user of a common template. For instance, most corporate websites have a pointer from every page to a page containing a copyright notice - this is clearly not an endorsement. Accordingly, implementations of link analysis algorithms will typical discount such ``internal'' links.   Subsections Anchor text and the web graph
iir_21_1_1	Anchor text and the web graph Journal of the ACM. http://www.acm.org/jacm/ Journal of the ACM. http://www.acm.org/jacm/ The Web is full of instances where the page B does not provide an accurate description of itself. In many cases this is a matter of how the publishers of page B choose to present themselves; this is especially common with corporate web pages, where a web presence is a marketing statement. For example, at the time of the writing of this book the home page of the IBM corporation (http://www.ibm.com) did not contain the term computer anywhere in its HTML code, despite the fact that IBM is widely viewed as the world's largest computer maker. Similarly, the HTML code for the home page of Yahoo! (http://www.yahoo.com) does not at this time contain the word portal. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page. Consequently, web searchers need not use the terms in a page to query for it. In addition, many web pages are rich in graphics and images, and/or embed their text in these images; in such cases, the HTML parsing performed when crawling will not extract text that is useful for indexing these pages. The ``standard IR'' approach to this would be to use the methods outlined in Chapter 9 and Section 12.4 . The insight behind anchor text is that such methods can be supplanted by anchor text, thereby tapping the power of the community of web page authors. The fact that the anchors of many hyperlinks pointing to http://www.ibm.com include the word computer can be exploited by web search engines. For instance, the anchor text terms can be included as terms under which to index the target web page. Thus, the postings for the term computer would include the document http://www.ibm.com and that for the term portal would include the document http://www.yahoo.com, using a special indicator to show that these terms occur as anchor (rather than in-page) text. As with in-page terms, anchor text terms are generally weighted based on frequency, with a penalty for terms that occur very often (the most common terms in anchor text across the Web are Click and here, using methods very similar to idf). The actual weighting of terms is determined by machine-learned scoring, as in Section 15.4.1 ; current web search engines appear to assign a substantial weighting to anchor text terms. The use of anchor text has some interesting side-effects. Searching for big blue on most web search engines returns the home page of the IBM corporation as the top hit; this is consistent with the popular nickname that many people use to refer to IBM. On the other hand, there have been (and continue to be) many instances where derogatory anchor text such as evil empire leads to somewhat unexpected results on querying for these terms on web search engines. This phenomenon has been exploited in orchestrated campaigns against specific sites. Such orchestrated anchor text may be a form of spamming, since a website can create misleading anchor text pointing to itself, to boost its ranking on selected query terms. Detecting and combating such systematic abuse of anchor text is another form of spam detection that web search engines perform. The window of text surrounding anchor text (sometimes referred to as extended anchor text) is often usable in the same manner as anchor text itself; consider for instance the fragment of web text there is good discussion of vedic scripture here. This has been considered in a number of settings and the useful width of this window has been studied; see Section 21.4 for references. Exercises. Is it always possible to follow directed edges (hyperlinks) in the web graph from any node (web page) to any other? Why or why not? Find an instance of misleading anchor-text on the Web. Given the collection of anchor-text phrases for a web page , suggest a heuristic for choosing one term or phrase from this collection that is most descriptive of . Does your heuristic in the previous exercise take into account a single domain repeating anchor text for from multiple pages in ?
iir_21_2	PageRank  PageRank 6.3 7.2.2 15.4.1 Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of which there are three hyperlinks to nodes B, C and D; the surfer proceeds at the next time step to one of these three nodes, with equal probabilities 1/3.  Figure 21.1: The random surfer at node A proceeds with probability 1/3 to each of B, C and D. As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important. What if the current location of the surfer, the node A, has no out-links? To address this we introduce an additional operation for our random surfer: the teleport operation. In the teleport operation the surfer jumps from a node to any other node in the web graph. This could happen because he types an address into the URL bar of his browser. The destination of a teleport operation is modeled as being chosen uniformly at random from all web pages. In other words, if is the total number of nodes in the web graph, the teleport operation takes the surfer to each node with probability . The surfer would also teleport to his present position with probability . In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: (1) When at a node with no out-links, the surfer invokes the teleport operation. (2) At any node that has outgoing links, the surfer invokes the teleport operation with probability and the standard random walk (follow an out-link chosen uniformly at random as in Figure 21.1 ) with probability , where is a fixed parameter chosen in advance. Typically, might be 0.1. In Section 21.2.1 , we will use the theory of Markov chains to argue that when the surfer follows this combined process (random walk plus teleport) he visits each node of the web graph a fixed fraction of the time that depends on (1) the structure of the web graph and (2) the value of . We call this value the PageRank of and will show how to compute this value in Section 21.2.2 .   Subsections Markov chains Definition: The PageRank computation Topic-specific PageRank
iir_21_2_1	Markov chains discrete-time stochastic process:  states A Markov chain is characterized by an transition probability matrix each of whose entries is in the interval ; the entries in each row of add up to 1. The Markov chain can be in one of the states at any given time-step; then, the entry tells us the probability that the state at the next time-step is , conditioned on the current state being . Each entry is known as a transition probability and depends only on the current state ; this is known as the Markov property. Thus, by the Markov property, (251)   (252)  252  stochastic matrix  principal left eigenvector In a Markov chain, the probability distribution of next states for a Markov chain depends only on the current state, and not on how the Markov chain arrived at the current state. Figure 21.2 shows a simple Markov chain with three states. From the middle state A, we proceed with (equal) probabilities of 0.5 to either B or C. From either B or C, we proceed with probability 1 to A. The transition probability matrix of this Markov chain is then  (253)   Figure 21.2: A simple Markov chain with three states; the numbers on the links indicate the transition probabilities. A Markov chain's probability distribution over its states may be viewed as a probability vector : a vector all of whose entries are in the interval , and the entries add up to 1. An -dimensional probability vector each of whose components corresponds to one of the states of a Markov chain can be viewed as a probability distribution over its states. For our simple Markov chain of Figure 21.2 , the probability vector would have 3 components that sum to 1. We can view a random surfer on the web graph as a Markov chain, with one state for each web page, and each transition probability representing the probability of moving from one web page to another. The teleport operation contributes to these transition probabilities. The adjacency matrix of the web graph is defined as follows: if there is a hyperlink from page to page , then , otherwise . We can readily derive the transition probability matrix for our Markov chain from the matrix : If a row of has no 1's, then replace each element by 1/N. For all other rows proceed as follows. Divide each 1 in by the number of 1's in its row. Thus, if there is a row with three 1's, then each of them is replaced by . Multiply the resulting matrix by . Add to every entry of the resulting matrix, to obtain . We can depict the probability distribution of the surfer's position at any time by a probability vector . At the surfer may begin at a state whose corresponding entry in is 1 while all others are zero. By definition, the surfer's distribution at is given by the probability vector ; at by , and so on. We will detail this process in Section 21.2.2 . We can thus compute the surfer's distribution over the states at any time, given only the initial distribution and the transition probability matrix . If a Markov chain is allowed to run for many time steps, each state is visited at a (different) frequency that depends on the structure of the Markov chain. In our running analogy, the surfer visits certain web pages (say, popular news home pages) more often than other pages. We now make this intuition precise, establishing conditions under which such the visit frequency converges to fixed, steady-state quantity. Following this, we set the PageRank of each node to this steady-state visit frequency and show how it can be computed.   Subsections Definition:
iir_21_2_2	The PageRank computation 214     (255)  The entries in the principal eigenvector are the steady-state probabilities of the random walk with teleporting, and thus the PageRank values for the corresponding web pages. We may interpret Equation 255 as follows: if is the probability distribution of the surfer across the web pages, he remains in the steady-state distribution . Given that is the steady-state distribution, we have that , so 1 is an eigenvalue of P. Thus if we were to compute the principal left eigenvector of the matrix -- the one with eigenvalue 1 -- we would have computed the PageRank values. There are many algorithms available for computing left eigenvectors; the references at the end of Chapter 18 and the present chapter are a guide to these. We give here a rather elementary method, sometimes known as power iteration. If is the initial distribution over the states, then the distribution at time is . As grows large, we would expect that the distribution is very similar to the distribution , since for large we would expect the Markov chain to attain its steady state. By Theorem 21.2.1 this is independent of the initial distribution . The power iteration method simulates the surfer's walk: begin at a state and run the walk for a large number of steps , keeping track of the visit frequencies for each of the states. After a large number of steps , these frequencies ``settle down'' so that the variation in the computed frequencies is below some predetermined threshold. We declare these tabulated frequencies to be the PageRank values. We consider the web graph in Exercise 21.2.3 with . The transition probability matrix of the surfer's walk with teleportation is then  (256)    (257)   (258)  21.3  Figure 21.3: The sequence of probability vectors. Continuing for several steps, we see that the distribution converges to the steady state of . In this simple example, we may directly calculate this steady-state probability distribution by observing the symmetry of the Markov chain: states 1 and 3 are symmetric, as evident from the fact that the first and third rows of the transition probability matrix in Equation 256 are identical. Postulating, then, that they both have the same steady-state probability and denoting this probability by , we know that the steady-state distribution is of the form . Now, using the identity , we solve a simple linear equation to obtain and consequently, . The PageRank values of pages (and the implicit ordering amongst them) are independent of any query a user might pose; PageRank is thus a query-independent measure of the static quality of each web page (recall such static quality measures from Section 7.1.4 ). On the other hand, the relative ordering of pages should, intuitively, depend on the query being served. For this reason, search engines use static quality measures such as PageRank as just one of many factors in scoring a web page on a query. Indeed, the relative contribution of PageRank to the overall score may again be determined by machine-learned scoring as in Section 15.4.1 .   Worked example. Consider the graph in Figure 21.4 . For a teleportation rate of 0.14 its (stochastic) transition probability matrix is: (259)   (260)  21.4      End worked example.
iir_21_2_3	Topic-specific PageRank non-uniformly Suppose our random surfer, endowed with a teleport operation as before, teleports to a random web page on the topic of sports instead of teleporting to a uniformly chosen random web page. We will not focus on how we collect all web pages on the topic of sports; in fact, we only need a non-zero subset of sports-related web pages, so that the teleport operation is feasible. This may be obtained, for instance, from a manually built directory of sports pages such as the open directory project (http://www.dmoz.org/) or that of Yahoo. Provided the set of sports-related pages is non-empty, it follows that there is a non-empty set of web pages over which the random walk has a steady-state distribution; let us denote this sports PageRank distribution by . For web pages not in , we set the PageRank values to zero. We call the topic-specific PageRank for sports.  Topic-specific PageRank.In this example we consider a user whose interests are 60% sports and 40% politics. If the teleportation probability is 10%, this user is modeled as teleporting 6% to sports pages and 4% to politics pages. We do not demand that teleporting takes the random surfer to a uniformly chosen sports page; the distribution over teleporting targets could in fact be arbitrary. In like manner we can envision topic-specific PageRank distributions for each of several topics such as science, religion, politics and so on. Each of these distributions assigns to each web page a PageRank value in the interval . For a user interested in only a single topic from among these topics, we may invoke the corresponding PageRank distribution when scoring and ranking search results. This gives us the potential of considering settings in which the search engine knows what topic a user is interested in. This may happen because users either explicitly register their interests, or because the system learns by observing each user's behavior over time. But what if a user is known to have a mixture of interests from multiple topics? For instance, a user may have an interest mixture (or profile) that is 60% sports and 40% politics; can we compute a personalized PageRank for this user? At first glance, this appears daunting: how could we possibly compute a different PageRank distribution for each user profile (with, potentially, infinitely many possible profiles)? We can in fact address this provided we assume that an individual's interests can be well-approximated as a linear combination of a small number of topic page distributions. A user with this mixture of interests could teleport as follows: determine first whether to teleport to the set of known sports pages, or to the set of known politics pages. This choice is made at random, choosing sports pages 60% of the time and politics pages 40% of the time. Once we choose that a particular teleport step is to (say) a random sports page, we choose a web page in uniformly at random to teleport to. This in turn leads to an ergodic Markov chain with a steady-state distribution that is personalized to this user's preferences over topics (see Exercise 21.2.3 ). While this idea has intuitive appeal, its implementation appears cumbersome: it seems to demand that for each user, we compute a transition probability matrix and compute its steady-state distribution. We are rescued by the fact that the evolution of the probability distribution over the states of a Markov chain can be viewed as a linear system. In Exercise 21.2.3 we will show that it is not necessary to compute a PageRank vector for every distinct combination of user interests over topics; the personalized PageRank vector for any user can be expressed as a linear combination of the underlying topic-specific PageRanks. For instance, the personalized PageRank vector for the user whose interests are 60% sports and 40% politics can be computed as (261)    Exercises. Write down the transition probability matrix for the example in Figure 21.2 . Consider a web graph with three nodes 1, 2 and 3. The links are as follows: . Write down the transition probability matrices for the surfer's walk with teleporting, for the following three values of the teleport probability: (a) ; (b) and (c) . A user of a browser can, in addition to clicking a hyperlink on the page he is currently browsing, use the back button to go back to the page from which he arrived at . Can such a user of back buttons be modeled as a Markov chain? How would we model repeated invocations of the back button? Consider a Markov chain with three states A, B and C, and transition probabilities as follows. From state A, the next state is B with probability 1. From B, the next state is either A with probability , or state C with probability . From C the next state is A with probability 1. For what values of is this Markov chain ergodic? Show that for any directed graph, the Markov chain induced by a random walk with the teleport operation is ergodic. Show that the PageRank of every page is at least . What does this imply about the difference in PageRank values (over the various pages) as becomes close to 1? For the data in Example 21.2.2, write a small routine or use a scientific calculator to compute the PageRank values stated in Equation 260. Suppose that the web graph is stored on disk as an adjacency list, in such a way that you may only query for the out-neighbors of pages in the order in which they are stored. You cannot load the graph in main memory but you may do multiple reads over the full graph. Write the algorithm for computing the PageRank in this setting. Recall the sets and introduced near the beginning of Section 21.2.3 . How does the set relate to ? Is the set always the set of all web pages? Why or why not? Is the sports PageRank of any page in at least as large as its PageRank? Consider a setting where we have two topic-specific PageRank values for each web page: a sports PageRank , and a politics PageRank . Let be the (common) teleportation probability used in computing both sets of topic-specific PageRanks. For , consider a user whose interest profile is divided between a fraction in sports and a fraction in politics. Show that the user's personalized PageRank is the steady-state distribution of a random walk in which - on a teleport step - the walk teleports to a sports page with probability and to a politics page with probability . Show that the Markov chain corresponding to the walk in Exercise 21.2.3 is ergodic and hence the user's personalized PageRank can be obtained by computing the steady-state distribution of this Markov chain. Show that in the steady-state distribution of Exercise 21.2.3, the steady-state probability for any web page equals .
iir_21_3	Hubs and Authorities two  hub score  authority score This approach stems from a particular insight into the creation of web pages, that there are two primary kinds of web pages useful as results for broad-topic searches. By a broad topic search we mean an informational query such as "I wish to learn about leukemia". There are authoritative sources of information on the topic; in this case, the National Cancer Institute's page on leukemia would be such a page. We will call such pages authorities; in the computation we are about to describe, they are the pages that will emerge with high authority scores. On the other hand, there are many pages on the Web that are hand-compiled lists of links to authoritative web pages on a specific topic. These hub pages are not in themselves authoritative sources of topic-specific information, but rather compilations that someone with an interest in the topic has spent time putting together. The approach we will take, then, is to use these hub pages to discover the authority pages. In the computation we now develop, these hub pages are the pages that will emerge with high hub scores. A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages. We thus appear to have a circular definition of hubs and authorities; we will turn this into an iterative computation. Suppose that we have a subset of the web containing good hub and authority pages, together with the hyperlinks amongst them. We will iteratively compute a hub score and an authority score for every web page in this subset, deferring the discussion of how we pick this subset until Section 21.3.1 . For a web page in our subset of the web, we use to denote its hub score and its authority score. Initially, we set for all nodes . We also denote by the existence of a hyperlink from to . The core of the iterative algorithm is a pair of updates to the hub and authority scores of all pages given by Equation 262, which capture the intuitive notions that good hubs point to good authorities and that good authorities are pointed to by good hubs.  (262) (263)   262    What happens as we perform these updates iteratively, recomputing hub scores, then new authority scores based on the recomputed hub scores, and so on? Let us recast the equations Equation 262 into matrix-vector form. Let and denote the vectors of all hub and all authority scores respectively, for the pages in our subset of the web graph. Let denote the adjacency matrix of the subset of the web graph that we are dealing with: is a square matrix with one row and one column for each page in the subset. The entry is 1 if there is a hyperlink from page to page , and 0 otherwise. Then, we may write Equation 262 (264) (265)     264 264 264  (266) (267)   266 18.1   266    (268) (269)       This leads to some key consequences: The iterative updates in Equation 262 (or equivalently, Equation 264), if scaled by the appropriate eigenvalues, are equivalent to the power iteration method for computing the eigenvectors of and . Provided that the principal eigenvalue of is unique, the iteratively computed entries of and settle into unique steady-state values determined by the entries of and hence the link structure of the graph. In computing these eigenvector entries, we are not restricted to using the power iteration method; indeed, we could use any fast method for computing the principal eigenvector of a stochastic matrix. The resulting computation thus takes the following form: Assemble the target subset of web pages, form the graph induced by their hyperlinks and compute and . Compute the principal eigenvectors of and to form the vector of hub scores and authority scores . Output the top-scoring hubs and the top-scoring authorities.  HITS Hyperlink-Induced Topic Search Worked example. Assuming the query jaguar and double-weighting of links whose anchors contain the query word, the matrix for Figure 21.4 is as follows: (270)  The hub and authority vectors are:  (271)   (272)  Here, is the main authority - two hubs ( and ) are pointing to it via highly weighted jaguar links. End worked example. Since the iterative updates captured the intuition of good hubs and good authorities, the high-scoring pages we output would give us good hubs and authorities from the target subset of web pages. In Section 21.3.1 we describe the remaining detail: how do we gather a target subset of web pages around a topic such as leukemia?   Subsections Choosing the subset of the Web
iir_21_3_1	Choosing the subset of the Web In assembling a subset of web pages around a topic such as leukemia, we must cope with the fact that good authority pages may not contain the specific query term leukemia. This is especially true, as we noted in Section 21.1.1 , when an authority page uses its web presence to project a certain marketing image. For instance, many pages on the IBM website are authoritative sources of information on computer hardware, even though these pages may not contain the term computer or hardware. However, a hub compiling computer hardware resources is likely to use these terms and also link to the relevant pages on the IBM website. Building on these observations, the following procedure has been suggested for compiling the subset of the Web for which to compute hub and authority scores. Given a query (say leukemia), use a text index to get all pages containing leukemia. Call this the root set of pages. Build the base set of pages, to include the root set as well as any page that either links to a page in the root set, or is linked to by a page in the root set. We then use the base set for computing hub and authority scores. The base set is constructed in this manner for three reasons: A good authority page may not contain the query text (such as computer hardware). If the text query manages to capture a good hub page in the root set, then the inclusion of all pages linked to by any page in the root set will capture all the good authorities linked to by in the base set. Conversely, if the text query manages to capture a good authority page in the root set, then the inclusion of pages which point to will bring other good hubs into the base set. In other words, the ``expansion'' of the root set into the base set enriches the common pool of good hubs and authorities. Running HITS across a variety of queries reveals some interesting insights about link analysis. Frequently, the documents that emerge as top hubs and authorities include languages other than the language of the query. These pages were presumably drawn into the base set, following the assembly of the root set. Thus, some elements of cross-language retrieval (where a query in one language retrieves documents in another) are evident here; interestingly, this cross-language effect resulted purely from link analysis, with no linguistic translation taking place. We conclude this section with some notes on implementing this algorithm. The root set consists of all pages matching the text query; in fact, implementations (see the references in Section 21.4 ) suggest that it suffices to use 200 or so web pages for the root set, rather than all pages matching the text query. Any algorithm for computing eigenvectors may be used for computing the hub/authority score vector. In fact, we need not compute the exact values of these scores; it suffices to know the relative values of the scores so that we may identify the top hubs and authorities. To this end, it is possible that a small number of iterations of the power iteration method yields the relative ordering of the top hubs and authorities. Experiments have suggested that in practice, about five iterations of Equation 262 yield fairly good results. Moreover, since the link structure of the web graph is fairly sparse (the average web page links to about ten others), we do not perform these as matrix-vector products but rather as additive updates as in Equation 262.  Figure: A sample run of HITS on the query japan elementary schools. Figure 21.6 shows the results of running HITS on the query japan elementary schools. The figure shows the top hubs and authorities; each row lists the title tag from the corresponding HTML page. Because the resulting string is not necessarily in Latin characters, the resulting print is (in many cases) a string of gibberish. Each of these corresponds to a web page that does not use Latin characters, in this case very likely pages in Japanese. There also appear to be pages in other non-English languages, which seems surprising given that the query string is in English. In fact, this result is emblematic of the functioning of HITS - following the assembly of the root set, the (English) query string is ignored. The base set is likely to contain pages in other languages, for instance if an English-language hub page links to the Japanese-language home pages of Japanese elementary schools. Because the subsequent computation of the top hubs and authorities is entirely link-based, some of these non-English pages will appear among the top hubs and authorities. Exercises. If all the hub and authority scores are initialized to 1, what is the hub/authority score of a node after one iteration? How would you interpret the entries of the matrices and ? What is the connection to the co-occurrence matrix in Chapter 18 ? What are the principal eigenvalues of and ? Figure: Web graph for Exercise 21.3.1 . For the web graph in Figure 21.7 , compute PageRank, hub and authority scores for each of the three pages. Also give the relative ordering of the 3 nodes for each of these scores, indicating any ties. PageRank: Assume that at each step of the PageRank random walk, we teleport to a random page with probability 0.1, with a uniform distribution over which particular page we teleport to. Hubs/Authorities: Normalize the hub (authority) scores so that the maximum hub (authority) score is 1. Hint 1: Using symmetries to simplify and solving with linear equations might be easier than using iterative methods. Hint 2: Provide the relative ordering (indicating any ties) of the three nodes for each of the three scoring measures.
iir_21_4	References and further reading Garfield (1955) is seminal in the science of citation analysis. This was built on by Pinski and Narin (1976) to develop a journal influence weight, whose definition is remarkably similar to that of the PageRank measure. The use of anchor text as an aid to searching and ranking stems from the work of McBryan (1994). Extended anchor-text was implicit in his work, with systematic experiments reported in Chakrabarti et al. (1998). Kemeny and Snell (1976) is a classic text on Markov chains. The PageRank measure was developed in Brin and Page (1998) and in Page et al. (1998). A number of methods for the fast computation of PageRank values are surveyed in Berkhin (2005) and in Langville and Meyer (2006); the former also details how the PageRank eigenvector solution may be viewed as solving a linear system, leading to one way of solving Exercise 21.2.3 . The effect of the teleport probability has been studied by Baeza-Yates et al. (2005) and by Boldi et al. (2005). Topic-specific PageRank and variants were developed in Haveliwala (2002), Haveliwala (2003) and in Jeh and Widom (2003). Berkhin (2006a) develops an alternate view of topic-specific PageRank. Ng et al. (2001b) suggests that the PageRank score assignment is more robust than HITS in the sense that scores are less sensitive to small changes in graph topology. However, it has also been noted that the teleport operation contributes significantly to PageRank's robustness in this sense. Both PageRank and HITS can be ``spammed'' by the orchestrated insertion of links into the web graph; indeed, the Web is known to have such link farms that collude to increase the score assigned to certain pages by various link analysis algorithms. The HITS algorithm is due to Kleinberg (1999). Chakrabarti et al. (1998) developed variants that weighted links in the iterative computation based on the presence of query terms in the pages being linked and compared these to results from several web search engines. Bharat and Henzinger (1998) further developed these and other heuristics, showing that certain combinations outperformed the basic HITS algorithm. Borodin et al. (2001) provides a systematic study of several variants of the HITS algorithm. Ng et al. (2001b) introduces a notion of stability for link analysis, arguing that small changes to link topology should not lead to significant changes in the ranked list of results for a query. Numerous other variants of HITS have been developed by a number of authors, the best know of which is perhaps SALSA (Lempel and Moran, 2000).   We use the following abbreviated journal and conference names in the bibliography: CACM Communications of the Association for Computing Machinery. IP M Information Processing and Management. IR Information Retrieval. JACM Journal of the Association for Computing Machinery. JASIS Journal of the American Society for Information Science. JASIST Journal of the American Society for Information Science and Technology. JMLR Journal of Machine Learning Research. TOIS ACM Transactions on Information Systems. Proc. ACL Proceedings of the Annual Meeting of the Association for Computational Linguistics. Available from: http://www.aclweb.org/anthology-index/ Proc. CIKM Proceedings of the ACM CIKM Conference on Information and Knowledge Management. ACM Press. Proc. ECIR Proceedings of the European Conference on Information Retrieval. Proc. ECML Proceedings of the European Conference on Machine Learning. Proc. ICML Proceedings of the International Conference on Machine Learning. Proc. IJCAI Proceedings of the International Joint Conference on Artificial Intelligence. Proc. INEX Proceedings of the Initiative for the Evaluation of XML Retrieval. Proc. KDD Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Proc. NIPS Proceedings of the Neural Information Processing Systems Conference. Proc. PODS Proceedings of the ACM Conference on Principles of Database Systems. Proc. SDAIR Proceedings of the Annual Symposium on Document Analysis and Information Retrieval. Proc. SIGIR Proceedings of the Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval. Available from: http://www.sigir.org/proceedings/Proc-Browse.html Proc. SPIRE Proceedings of the Symposium on String Processing and Information Retrieval. Proc. TREC Proceedings of the Text Retrieval Conference. Proc. UAI Proceedings of the Conference on Uncertainty in Artificial Intelligence. Proc. VLDB Proceedings of the Very Large Data Bases Conference. Proc. WWW Proceedings of the International World Wide Web Conference.
iir_22	Bibliography Aberer, Karl. 2001. P-Grid: A self-organizing access structure for P2P information systems. In Proc. International Conference on Cooperative Information Systems, pp. 179-194. Springer. Aizerman, Mark A., Emmanuel M. Braverman, and Lev I. Rozonoér. 1964. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control 25: 821-837. Akaike, Hirotugu. 1974. A new look at the statistical model identification. IEEE Transactions on automatic control 19 (6): 716-723. Allan, James. 2005. HARD track overview in TREC 2005: High accuracy retrieval from documents. In Proc. TREC. Allan, James, Ron Papka, and Victor Lavrenko. 1998. On-line new event detection and tracking. In Proc. SIGIR, pp. 37-45. ACM Press. DOI: doi.acm.org/10.1145/290941.290954. Allwein, Erin L., Robert E. Schapire, and Yoram Singer. 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. JMLR 1: 113-141. URL: www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf. Alonso, Omar, Sandeepan Banerjee, and Mark Drake. 2006. GIO: A semantic web application using the information grid framework. In Proc. WWW, pp. 857-858. ACM Press. DOI: doi.acm.org/10.1145/1135777.1135913. Altingövde, Ismail Sengör, Engin Demir, Fazli Can, and Özgür Ulusoy. 2008. Incremental cluster-based retrieval using compressed cluster-skipping inverted files. TOIS. To appear. Amer-Yahia, Sihem, Chavdar Botev, Jochen Dörre, and Jayavel Shanmugasundaram. 2006. XQuery full-text extensions explained. IBM Systems Journal 45 (2): 335-352. Amer-Yahia, Sihem, Pat Case, Thomas Rölleke, Jayavel Shanmugasundaram, and Gerhard Weikum. 2005. Report on the DB/IR panel at SIGMOD 2005. SIGMOD Record 34 (4): 71-74. DOI: doi.acm.org/10.1145/1107499.1107514. Amer-Yahia, Sihem, and Mounia Lalmas. 2006. XML search: Languages, INEX and scoring. SIGMOD Record 35 (4): 16-23. DOI: doi.acm.org/10.1145/1228268.1228271. Anagnostopoulos, Aris, Andrei Z. Broder, and Kunal Punera. 2006. Effective and efficient classification on a search-engine model. In Proc. CIKM, pp. 208-217. ACM Press. DOI: doi.acm.org/10.1145/1183614.1183648. Anderberg, Michael R. 1973. Cluster analysis for applications. Academic Press. Andoni, Alexandr, Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab Mirrokni. 2006. Locality-sensitive hashing using stable distributions. In Nearest Neighbor Methods in Learning and Vision: Theory and Practice. MIT Press. Anh, Vo Ngoc, Owen de Kretser, and Alistair Moffat. 2001. Vector-space ranking with effective early termination. In Proc. SIGIR, pp. 35-42. ACM Press. Anh, Vo Ngoc, and Alistair Moffat. 2005. Inverted index compression using word-aligned binary codes. IR 8 (1): 151-166. DOI: dx.doi.org/10.1023/B:INRT.0000048490.99518.5c. Anh, Vo Ngoc, and Alistair Moffat. 2006a. Improved word-aligned binary compression for text indexing. IEEE Transactions on Knowledge and Data Engineering 18 (6): 857-861. Anh, Vo Ngoc, and Alistair Moffat. 2006b. Pruned query evaluation using pre-computed impacts. In Proc. SIGIR, pp. 372-379. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148235. Anh, Vo Ngoc, and Alistair Moffat. 2006c. Structured index organizations for high-throughput text querying. In Proc. SPIRE, pp. 304-315. Springer. Apté, Chidanand, Fred Damerau, and Sholom M. Weiss. 1994. Automated learning of decision rules for text categorization. TOIS 12 (1): 233-251. Arthur, David, and Sergei Vassilvitskii. 2006. How slow is the k-means method? In Proc. ACM Symposium on Computational Geometry, pp. 144-153. Arvola, Paavo, Marko Junkkari, and Jaana Kekäläinen. 2005. Generalized contextualization method for XML information retrieval. In Proc. CIKM, pp. 20-27. Aslam, Javed A., and Emine Yilmaz. 2005. A geometric interpretation and analysis of R-precision. In Proc. CIKM, pp. 664-671. ACM Press. Ault, Thomas Galen, and Yiming Yang. 2002. Information filtering in TREC-9 and TDT-3: A comparative analysis. IR 5 (2-3): 159-187. Badue, Claudine Santos, Ricardo A. Baeza-Yates, Berthier Ribeiro-Neto, and Nivio Ziviani. 2001. Distributed query processing using partitioned inverted files. In Proc. SPIRE, pp. 10-20. Baeza-Yates, Ricardo, Paolo Boldi, and Carlos Castillo. 2005. The choice of a damping function for propagating importance in link-based ranking. Technical report, Dipartimento di Scienze dell'Informazione, Università degli Studi di Milano. Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley. Bahle, Dirk, Hugh E. Williams, and Justin Zobel. 2002. Efficient phrase querying with an auxiliary index. In Proc. SIGIR, pp. 215-221. ACM Press. Baldridge, Jason, and Miles Osborne. 2004. Active learning and the total cost of annotation. In Proc. Empirical Methods in Natural Language Processing, pp. 9-16. Ball, G. H. 1965. Data analysis in the social sciences: What about the details? In Proc. Fall Joint Computer Conference, pp. 533-560. Spartan Books. Banko, Michele, and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proc. ACL. Bar-Ilan, Judit, and Tatyana Gutman. 2005. How do search engines respond to some non-English queries? Journal of Information Science 31 (1): 13-28. Bar-Yossef, Ziv, and Maxim Gurevich. 2006. Random sampling from a search engine's index. In Proc. WWW, pp. 367-376. ACM Press. DOI: doi.acm.org/10.1145/1135777.1135833. Barroso, Luiz André, Jeffrey Dean, and Urs Hölzle. 2003. Web search for a planet: The Google cluster architecture. IEEE Micro 23 (2): 22-28. DOI: dx.doi.org/10.1109/MM.2003.1196112. Bartell, Brian Theodore. 1994. Optimizing ranking functions: A connectionist approach to adaptive information retrieval. PhD thesis, University of California at San Diego, La Jolla, CA. Bartell, Brian T., Garrison W. Cottrell, and Richard K. Belew. 1998. Optimizing similarity using multi-query relevance feedback. JASIS 49 (8): 742-761. Barzilay, Regina, and Michael Elhadad. 1997. Using lexical chains for text summarization. In Workshop on Intelligent Scalable Text Summarization, pp. 10-17. Bast, Holger, and Debapriyo Majumdar. 2005. Why spectral retrieval works. In Proc. SIGIR, pp. 11-18. ACM Press. DOI: doi.acm.org/10.1145/1076034.1076040. Basu, Sugato, Arindam Banerjee, and Raymond J. Mooney. 2004. Active semi-supervision for pairwise constrained clustering. In Proc. SIAM International Conference on Data Mining, pp. 333-344. Beesley, Kenneth R. 1998. Language identifier: A computer program for automatic natural-language identification of on-line text. In Languages at Crossroads: Proc. Annual Conference of the American Translators Association, pp. 47-54. Beesley, Kenneth R., and Lauri Karttunen. 2003. Finite State Morphology. CSLI Publications. Bennett, Paul N. 2000. Assessing the calibration of naive Bayes' posterior estimates. Technical Report CMU-CS-00-155, School of Computer Science, Carnegie Mellon University. Berger, Adam, and John Lafferty. 1999. Information retrieval as statistical translation. In Proc. SIGIR, pp. 222-229. ACM Press. Berkhin, Pavel. 2005. A survey on pagerank computing. Internet Mathematics 2 (1): 73-120. Berkhin, Pavel. 2006a. Bookmark-coloring algorithm for personalized pagerank computing. Internet Mathematics 3 (1): 41-62. Berkhin, Pavel. 2006b. A survey of clustering data mining techniques. In Jacob Kogan, Charles Nicholas, and Marc Teboulleeds.), Grouping Multidimensional Data: Recent Advances in Clustering, pp. 25-71. Springer. Berners-Lee, Tim, Robert Cailliau, Jean-Francois Groff, and Bernd Pollermann. 1992. World-Wide Web: The information universe. Electronic Networking: Research, Applications and Policy 1 (2): 74-82. URL: citeseer.ist.psu.edu/article/berners-lee92worldwide.html. Berry, Michael, and Paul Young. 1995. Using latent semantic indexing for multilanguage information retrieval. Computers and the Humanities 29 (6): 413-429. Berry, Michael W., Susan T. Dumais, and Gavin W. O'Brien. 1995. Using linear algebra for intelligent information retrieval. SIAM Review 37 (4): 573-595. Betsi, Stamatina, Mounia Lalmas, Anastasios Tombros, and Theodora Tsikrika. 2006. User expectations from XML element retrieval. In Proc. SIGIR, pp. 611-612. ACM Press. Bharat, Krishna, and Andrei Broder. 1998. A technique for measuring the relative size and overlap of public web search engines. Computer Networks and ISDN Systems 30 (1-7): 379-388. DOI: dx.doi.org/10.1016/S0169-7552(98)00127-5. Bharat, Krishna, Andrei Broder, Monika Henzinger, Puneet Kumar, and Suresh Venkatasubramanian. 1998. The connectivity server: Fast access to linkage information on the web. In Proc. WWW, pp. 469-477. Bharat, Krishna, Andrei Z. Broder, Jeffrey Dean, and Monika Rauch Henzinger. 2000. A comparison of techniques to find mirrored hosts on the WWW. JASIS 51 (12): 1114-1122. URL: citeseer.ist.psu.edu/bharat99comparison.html. Bharat, Krishna, and Monika R. Henzinger. 1998. Improved algorithms for topic distillation in a hyperlinked environment. In Proc. SIGIR, pp. 104-111. ACM Press. URL: citeseer.ist.psu.edu/bharat98improved.html. Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer. Blair, David C., and M. E. Maron. 1985. An evaluation of retrieval effectiveness for a full-text document-retrieval system. CACM 28 (3): 289-299. Blanco, Roi, and Alvaro Barreiro. 2006. TSP and cluster-based solutions to the reassignment of document identifiers. IR 9 (4): 499-517. Blanco, Roi, and Alvaro Barreiro. 2007. Boosting static pruning of inverted files. In Proc. SIGIR. ACM Press. Blandford, Dan, and Guy Blelloch. 2002. Index compression through document reordering. In Proc. Data Compression Conference, p. 342. IEEE Computer Society. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. JMLR 3: 993-1022. Boldi, Paolo, Bruno Codenotti, Massimo Santini, and Sebastiano Vigna. 2002. Ubicrawler: A scalable fully distributed web crawler. In Proc. Australian World Wide Web Conference. URL: citeseer.ist.psu.edu/article/boldi03ubicrawler.html. Boldi, Paolo, Massimo Santini, and Sebastiano Vigna. 2005. PageRank as a function of the damping factor. In Proc. WWW. URL: citeseer.ist.psu.edu/boldi05pagerank.html. Boldi, Paolo, and Sebastiano Vigna. 2004a. Codes for the World-Wide Web. Internet Mathematics 2 (4): 405-427. Boldi, Paolo, and Sebastiano Vigna. 2004b. The WebGraph framework I: Compression techniques. In Proc. WWW, pp. 595-601. ACM Press. Boldi, Paolo, and Sebastiano Vigna. 2005. Compressed perfect embedded skip lists for quick inverted-index lookups. In Proc. SPIRE. Springer. Boley, Daniel. 1998. Principal direction divisive partitioning. Data Mining and Knowledge Discovery 2 (4): 325-344. DOI: dx.doi.org/10.1023/A:1009740529316. Borodin, Allan, Gareth O. Roberts, Jeffrey S. Rosenthal, and Panayiotis Tsaparas. 2001. Finding authorities and hubs from link structures on the World Wide Web. In Proc. WWW, pp. 415-429. Bourne, Charles P., and Donald F. Ford. 1961. A study of methods for systematically abbreviating English words and names. JACM 8 (4): 538-552. DOI: doi.acm.org/10.1145/321088.321094. Bradley, Paul S., and Usama M. Fayyad. 1998. Refining initial points for K-means clustering. In Proc. ICML, pp. 91-99. Bradley, Paul S., Usama M. Fayyad, and Cory Reina. 1998. Scaling clustering algorithms to large databases. In Proc. KDD, pp. 9-15. Brill, Eric, and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proc. ACL, pp. 286-293. Brin, Sergey, and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In Proc. WWW, pp. 107-117. Brisaboa, Nieves R., Antonio Fariña, Gonzalo Navarro, and José R. Paramá. 2007. Lightweight natural language text compression. IR 10 (1): 1-33. Broder, Andrei. 2002. A taxonomy of web search. SIGIR Forum 36 (2): 3-10. DOI: doi.acm.org/10.1145/792550.792552. Broder, Andrei, S. Ravi Kumar, Farzin Maghoul, Prabhakar Raghavan, Sridhar Rajagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph structure in the web. Computer Networks 33 (1): 309-320. Broder, Andrei Z., Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. 1997. Syntactic clustering of the web. In Proc. WWW, pp. 391-404. Brown, Eric W. 1995. Execution Performance Issues in Full-Text Information Retrieval. PhD thesis, University of Massachusetts, Amherst. Buckley, Chris, James Allan, and Gerard Salton. 1994a. Automatic routing and ad-hoc retrieval using SMART: TREC 2. In Proc. TREC, pp. 45-55. Buckley, Chris, and Gerard Salton. 1995. Optimization of relevance feedback weights. In Proc. SIGIR, pp. 351-357. ACM Press. DOI: doi.acm.org/10.1145/215206.215383. Buckley, Chris, Gerard Salton, and James Allan. 1994b. The effect of adding relevance information in a relevance feedback environment. In Proc. SIGIR, pp. 292-300. ACM Press. Buckley, Chris, Amit Singhal, and Mandar Mitra. 1995. New retrieval approaches using SMART: TREC 4. In Proc. TREC. Buckley, Chris, and Ellen M. Voorhees. 2000. Evaluating evaluation measure stability. In Proc. SIGIR, pp. 33-40. Burges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proc. ICML. Burges, Christopher J. C. 1998. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery 2 (2): 121-167. Burner, Mike. 1997. Crawling towards eternity: Building an archive of the World Wide Web. Web Techniques Magazine 2 (5). Burnham, Kenneth P., and David Anderson. 2002. Model Selection and Multi-Model Inference. Springer. Bush, Vannevar. 1945. As we may think. The Atlantic Monthly. URL: www.theatlantic.com/doc/194507/bush. Büttcher, Stefan, and Charles L. A. Clarke. 2005a. Indexing time vs. query time: Trade-offs in dynamic information retrieval systems. In Proc. CIKM, pp. 317-318. ACM Press. DOI: doi.acm.org/10.1145/1099554.1099645. Büttcher, Stefan, and Charles L. A. Clarke. 2005b. A security model for full-text file system search in multi-user environments. In Proc. FAST. URL: www.usenix.org/events/fast05/tech/buettcher.html. Büttcher, Stefan, and Charles L. A. Clarke. 2006. A document-centric approach to static index pruning in text retrieval systems. In Proc. CIKM, pp. 182-189. DOI: doi.acm.org/10.1145/1183614.1183644. Büttcher, Stefan, Charles L. A. Clarke, and Brad Lushman. 2006. Hybrid index maintenance for growing text collections. In Proc. SIGIR, pp. 356-363. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148233. Cacheda, Fidel, Victor Carneiro, Carmen Guerrero, and Ángel Viña. 2003. Optimization of restricted searches in web directories using hybrid data structures. In Proc. ECIR, pp. 436-451. Callan, Jamie. 2000. Distributed information retrieval. In W. Bruce Crofted.), Advances in information retrieval, pp. 127-150. Kluwer. Can, Fazli, Ismail Sengör Altingövde, and Engin Demir. 2004. Efficiency and effectiveness of query processing in cluster-based retrieval. Information Systems 29 (8): 697-717. DOI: dx.doi.org/10.1016/S0306-4379(03)00062-0. Can, Fazli, and Esen A. Ozkarahan. 1990. Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases. ACM Trans. Database Syst. 15 (4): 483-517. Cao, Guihong, Jian-Yun Nie, and Jing Bai. 2005. Integrating word relationships into language models. In Proc. SIGIR, pp. 298-305. ACM Press. Cao, Yunbo, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsiao-Wuen Hon. 2006. Adapting Ranking SVM to document retrieval. In Proc. SIGIR. ACM Press. Carbonell, Jaime, and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR, pp. 335-336. ACM Press. DOI: doi.acm.org/10.1145/290941.291025. Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics 22: 249-254. Carmel, David, Doron Cohen, Ronald Fagin, Eitan Farchi, Michael Herscovici, Yoelle S. Maarek, and Aya Soffer. 2001. Static index pruning for information retrieval systems. In Proc. SIGIR, pp. 43-50. ACM Press. DOI: doi.acm.org/10.1145/383952.383958. Carmel, David, Yoelle S. Maarek, Matan Mandelbrod, Yosi Mass, and Aya Soffer. 2003. Searching XML documents via XML fragments. In Proc. SIGIR, pp. 151-158. ACM Press. DOI: doi.acm.org/10.1145/860435.860464. Caruana, Rich, and Alexandru Niculescu-Mizil. 2006. An empirical comparison of supervised learning algorithms. In Proc. ICML. Castro, R. M., M. J. Coates, and R. D. Nowak. 2004. Likelihood based hierarchical clustering. IEEE Transactions in Signal Processing 52 (8): 2308-2321. Cavnar, William B., and John M. Trenkle. 1994. N-gram-based text categorization. In Proc. SDAIR, pp. 161-175. Chakrabarti, Soumen. 2002. Mining the Web: Analysis of Hypertext and Semi Structured Data. Morgan Kaufmann. Chakrabarti, Soumen, Byron Dom, David Gibson, Jon Kleinberg, Prabhakar Raghavan, and Sridhar Rajagopalan. 1998. Automatic resource list compilation by analyzing hyperlink structure and associated text. In Proc. WWW. URL: citeseer.ist.psu.edu/chakrabarti98automatic.html. Chapelle, Olivier, Bernhard Schölkopf, and Alexander Zieneds.). 2006. Semi-Supervised Learning. MIT Press. Chaudhuri, Surajit, Gautam Das, Vagelis Hristidis, and Gerhard Weikum. 2006. Probabilistic information retrieval approach for ranking of database query results. ACM Transactions on Database Systems 31 (3): 1134-1168. DOI: doi.acm.org/10.1145/1166074.1166085. Cheeseman, Peter, and John Stutz. 1996. Bayesian classification (AutoClass): Theory and results. In Advances in Knowledge Discovery and Data Mining, pp. 153-180. MIT Press. Chen, Hsin-Hsi, and Chuan-Jie Lin. 2000. A multilingual news summarizer. In Proc. COLING, pp. 159-165. Chen, Pai-Hsuen, Chih-Jen Lin, and Bernhard Schölkopf. 2005. A tutorial on -support vector machines. Applied Stochastic Models in Business and Industry 21: 111-136. Chiaramella, Yves, Philippe Mulhem, and Franck Fourel. 1996. A model for multimedia information retrieval. Technical Report 4-96, University of Glasgow. Chierichetti, Flavio, Alessandro Panconesi, Prabhakar Raghavan, Mauro Sozio, Alessandro Tiberi, and Eli Upfal. 2007. Finding near neighbors through cluster pruning. In Proc. PODS. Cho, Junghoo, and Hector Garcia-Molina. 2002. Parallel crawlers. In Proc. WWW, pp. 124-135. ACM Press. DOI: doi.acm.org/10.1145/511446.511464. Cho, Junghoo, Hector Garcia-Molina, and Lawrence Page. 1998. Efficient crawling through URL ordering. In Proc. WWW, pp. 161-172. Chu-Carroll, Jennifer, John Prager, Krzysztof Czuba, David Ferrucci, and Pablo Duboue. 2006. Semantic search via XML fragments: A high-precision approach to IR. In Proc. SIGIR, pp. 445-452. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148247. Clarke, Charles L.A., Gordon V. Cormack, and Elizabeth A. Tudhope. 2000. Relevance ranking for one to three term queries. IP M 36: 291-311. Cleverdon, Cyril W. 1991. The significance of the Cranfield tests on index languages. In Proc. SIGIR, pp. 3-12. ACM Press. Coden, Anni R., Eric W. Brown, and Savitha Srinivasaneds.). 2002. Information Retrieval Techniques for Speech Applications. Springer. Cohen, Paul R. 1995. Empirical methods for artificial intelligence. MIT Press. Cohen, William W. 1998. Integration of heterogeneous databases without common domains using queries based on textual similarity. In Proc. SIGMOD, pp. 201-212. ACM Press. Cohen, William W., Robert E. Schapire, and Yoram Singer. 1998. Learning to order things. In Proc. NIPS. The MIT Press. URL: citeseer.ist.psu.edu/article/cohen98learning.html. Cohen, William W., and Yoram Singer. 1999. Context-sensitive learning methods for text categorization. TOIS 17 (2): 141-173. Comtet, Louis. 1974. Advanced Combinatorics. Reidel. Cooper, William S., Aitao Chen, and Fredric C. Gey. 1994. Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression. In Proc. TREC, pp. 57-66. Cormen, Thomas H., Charles Eric Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. MIT Press. Cover, Thomas M., and Peter E. Hart. 1967. Nearest neighbor pattern classification. IEEE Transactions on Information Theory 13 (1): 21-27. Cover, Thomas M., and Joy A. Thomas. 1991. Elements of Information Theory. Wiley. Crammer, Koby, and Yoram Singer. 2001. On the algorithmic implementation of multiclass kernel-based machines. JMLR 2: 265-292. Creecy, Robert H., Brij M. Masand, Stephen J. Smith, and David L. Waltz. 1992. Trading MIPS and memory for knowledge engineering. CACM 35 (8): 48-64. DOI: doi.acm.org/10.1145/135226.135228. Crestani, Fabio, Mounia Lalmas, Cornelis J. Van Rijsbergen, and Iain Campbell. 1998. Is this document relevant? ldots probably: A survey of probabilistic models in information retrieval. ACM Computing Surveys 30 (4): 528-552. DOI: doi.acm.org/10.1145/299917.299920. Cristianini, Nello, and John Shawe-Taylor. 2000. Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press. Croft, W. Bruce. 1978. A file organization for cluster-based retrieval. In Proc. SIGIR, pp. 65-82. ACM Press. Croft, W. Bruce, and David J. Harper. 1979. Using probabilistic models of document retrieval without relevance information. Journal of Documentation 35 (4): 285-295. Croft, W. Bruce, and John Laffertyeds.). 2003. Language Modeling for Information Retrieval. Springer. Crouch, Carolyn J. 1988. A cluster-based approach to thesaurus construction. In Proc. SIGIR, pp. 309-320. ACM Press. DOI: doi.acm.org/10.1145/62437.62467. Cucerzan, Silviu, and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proc. Empirical Methods in Natural Language Processing. Cutting, Douglas R., David R. Karger, and Jan O. Pedersen. 1993. Constant interaction-time Scatter/Gather browsing of very large document collections. In Proc. SIGIR, pp. 126-134. ACM Press. Cutting, Douglas R., Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scatter/Gather: A cluster-based approach to browsing large document collections. In Proc. SIGIR, pp. 318-329. ACM Press. Damerau, Fred J. 1964. A technique for computer detection and correction of spelling errors. CACM 7 (3): 171-176. DOI: doi.acm.org/10.1145/363958.363994. Davidson, Ian, and Ashwin Satyanarayana. 2003. Speeding up k-means clustering by bootstrap averaging. In ICDM 2003 Workshop on Clustering Large Data Sets. Day, William H., and Herbert Edelsbrunner. 1984. Efficient algorithms for agglomerative hierarchical clustering methods. Journal of Classification 1: 1-24. de Moura, Edleno Silva, Gonzalo Navarro, Nivio Ziviani, and Ricardo Baeza-Yates. 2000. Fast and flexible word searching on compressed text. TOIS 18 (2): 113-139. DOI: doi.acm.org/10.1145/348751.348754. Dean, Jeffrey, and Sanjay Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Proc. Symposium on Operating System Design and Implementation. Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. JASIS 41 (6): 391-407. del Bimbo, Alberto. 1999. Visual Information Retrieval. Morgan Kaufmann. Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society Series B 39: 1-38. Dhillon, Inderjit S. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In Proc. KDD, pp. 269-274. Dhillon, Inderjit S., and Dharmendra S. Modha. 2001. Concept decompositions for large sparse text data using clustering. Machine Learning 42 (1/2): 143-175. DOI: dx.doi.org/10.1023/A:1007612920971. Di Eugenio, Barbara, and Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics 30 (1): 95-101. DOI: dx.doi.org/10.1162/089120104773633402. Dietterich, Thomas G. 2002. Ensemble learning. In Michael A. Arbibed.), The Handbook of Brain Theory and Neural Networks, 2nd edition. MIT Press. Dietterich, Thomas G., and Ghulum Bakiri. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research 2: 263-286. Dom, Byron E. 2002. An information-theoretic external cluster-validity measure. In Proc. UAI. Domingos, Pedro. 2000. A unified bias-variance decomposition for zero-one and squared loss. In Proc. National Conference on Artificial Intelligence and Proc. Conference Innovative Applications of Artificial Intelligence, pp. 564-569. AAAI Press / The MIT Press. Domingos, Pedro, and Michael J. Pazzani. 1997. On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning 29 (2-3): 103-130. URL: citeseer.ist.psu.edu/domingos97optimality.html. Downie, J. Stephen. 2006. The Music Information Retrieval Evaluation eXchange (MIREX). D-Lib Magazine 12 (12). Duda, Richard O., Peter E. Hart, and David G. Stork. 2000. Pattern Classification, 2nd edition. Wiley-Interscience. Dumais, Susan, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representations for text categorization. In Proc. CIKM, pp. 148-155. ACM Press. DOI: doi.acm.org/10.1145/288627.288651. Dumais, Susan T. 1993. Latent semantic indexing (LSI) and TREC-2. In Proc. TREC, pp. 105-115. Dumais, Susan T. 1995. Latent semantic indexing (LSI): TREC-3 report. In Proc. TREC, pp. 219-230. Dumais, Susan T., and Hao Chen. 2000. Hierarchical classification of Web content. In Proc. SIGIR, pp. 256-263. ACM Press. Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19 (1): 61-74. Dunning, Ted. 1994. Statistical identification of language. Technical Report 94-273, Computing Research Laboratory, New Mexico State University. Eckart, Carl, and Gale Young. 1936. The approximation of a matrix by another of lower rank. Psychometrika 1: 211-218. El-Hamdouchi, Abdelmoula, and Peter Willett. 1986. Hierarchic document classification using Ward's clustering method. In Proc. SIGIR, pp. 149-156. ACM Press. DOI: doi.acm.org/10.1145/253168.253200. Elias, Peter. 1975. Universal code word sets and representations of the integers. IEEE Transactions on Information Theory 21 (2): 194-203. Eyheramendy, Susana, David Lewis, and David Madigan. 2003. On the Naive Bayes model for text categorization. In International Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics. Fallows, Deborah, 2004. The internet and daily life. URL: www.pewinternet.org/pdfs/PIP_Internet_and_Daily_Life.pdf. Pew/Internet and American Life Project. Fayyad, Usama M., Cory Reina, and Paul S. Bradley. 1998. Initialization of iterative refinement clustering algorithms. In Proc. KDD, pp. 194-198. Fellbaum, Christiane D. 1998. WordNet - An Electronic Lexical Database. MIT Press. Ferragina, Paolo, and Rossano Venturini. 2007. Compressed permuterm indexes. In Proc. SIGIR. ACM Press. Forman, George. 2004. A pitfall and solution in multi-class feature selection for text classification. In Proc. ICML. Forman, George. 2006. Tackling concept drift by temporal inductive transfer. In Proc. SIGIR, pp. 252-259. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148216. Forman, George, and Ira Cohen. 2004. Learning from little: Comparison of classifiers given little training. In Proc. PKDD, pp. 161-172. Fowlkes, Edward B., and Colin L. Mallows. 1983. A method for comparing two hierarchical clusterings. Journal of the American Statistical Association 78 (383): 553-569. URL: www.jstor.org/view/01621459/di985957/98p0926l/0. Fox, Edward A., and Whay C. Lee. 1991. FAST-INV: A fast algorithm for building large inverted files. Technical report, Virginia Polytechnic Institute   State University, Blacksburg, VA, USA. Fraenkel, Aviezri S., and Shmuel T. Klein. 1985. Novel compression of sparse bit-strings - preliminary report. In Combinatorial Algorithms on Words, NATO ASI Series Vol F12, pp. 169-183. Springer. Frakes, William B., and Ricardo Baeza-Yateseds.). 1992. Information Retrieval: Data Structures and Algorithms. Prentice Hall. Fraley, Chris, and Adrian E. Raftery. 1998. How many clusters? Which clustering method? Answers via model-based cluster analysis. Computer Journal 41 (8): 578-588. Friedl, Jeffrey E. F. 2006. Mastering Regular Expressions, 3rd edition. O'Reilly. Friedman, Jerome H. 1997. On bias, variance, 0/1-loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery 1 (1): 55-77. Friedman, Nir, and Moises Goldszmidt. 1996. Building classifiers using Bayesian networks. In Proc. National Conference on Artificial Intelligence, pp. 1277-1284. Fuhr, Norbert. 1989. Optimum polynomial retrieval functions based on the probability ranking principle. TOIS 7 (3): 183-204. Fuhr, Norbert. 1992. Probabilistic models in information retrieval. Computer Journal 35 (3): 243-255. Fuhr, Norbert, Norbert Gövert, Gabriella Kazai, and Mounia Lalmaseds.). 2003a. INitiative for the Evaluation of XML Retrieval (INEX). Proc. First INEX Workshop. ERCIM. Fuhr, Norbert, and Kai Großjohann. 2004. XIRQL: An XML query language based on information retrieval concepts. TOIS 22 (2): 313-356. URL: doi.acm.org/10.1145/984321.984326. Fuhr, Norbert, and Mounia Lalmas. 2007. Advances in XML retrieval: The INEX initiative. In International Workshop on Research Issues in Digital Libraries. Fuhr, Norbert, Mounia Lalmas, Saadia Malik, and Gabriella Kazaieds.). 2006. Advances in XML Information Retrieval and Evaluation, 4th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2005. Springer. Fuhr, Norbert, Mounia Lalmas, Saadia Malik, and Zoltán Szlávikeds.). 2005. Advances in XML Information Retrieval, Third International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2004. Springer. Fuhr, Norbert, Mounia Lalmas, and Andrew Trotmaneds.). 2007. Comparative Evaluation of XML Information Retrieval Systems, 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2006. Springer. Fuhr, Norbert, Saadia Malik, and Mounia Lalmaseds.). 2003b. INEX 2003 Workshop. URL: inex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. Fuhr, Norbert, and Ulrich Pfeifer. 1994. Probabilistic information retrieval as a combination of abstraction, inductive learning, and probabilistic assumptions. TOIS 12 (1): 92-115. DOI: doi.acm.org/10.1145/174608.174612. Fuhr, Norbert, and Thomas Rölleke. 1997. A probabilistic relational algebra for the integration of information retrieval and database systems. TOIS 15 (1): 32-66. DOI: doi.acm.org/10.1145/239041.239045. Gaertner, Thomas, John W. Lloyd, and Peter A. Flach. 2002. Kernels for structured data. In Proc. International Conference on Inductive Logic Programming, pp. 66-83. Gao, Jianfeng, Mu Li, Chang-Ning Huang, and Andi Wu. 2005. Chinese word segmentation and named entity recognition: A pragmatic approach. Computational Linguistics 31 (4): 531-574. Gao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. 2004. Dependence language model for information retrieval. In Proc. SIGIR, pp. 170-177. ACM Press. Garcia, Steven, Hugh E. Williams, and Adam Cannane. 2004. Access-ordered indexes. In Proc. Australasian Conference on Computer Science, pp. 7-14. Garcia-Molina, Hector, Jennifer Widom, and Jeffrey D. Ullman. 1999. Database System Implementation. Prentice Hall. Garfield, Eugene. 1955. Citation indexes to science: A new dimension in documentation through association of ideas. Science 122: 108-111. Garfield, Eugene. 1976. The permuterm subject index: An autobiographic review. JASIS 27 (5-6): 288-291. Geman, Stuart, Elie Bienenstock, and René Doursat. 1992. Neural networks and the bias/variance dilemma. Neural Computation 4 (1): 1-58. Geng, Xiubo, Tie-Yan Liu, Tao Qin, and Hang Li. 2007. Feature selection for ranking. In Proc. SIGIR, pp. 407-414. ACM Press. Gerrand, Peter. 2007. Estimating linguistic diversity on the internet: A taxonomy to avoid pitfalls and paradoxes. Journal of Computer-Mediated Communication 12 (4). URL: jcmc.indiana.edu/vol12/issue4/gerrand.html. article 8. Gey, Fredric C. 1994. Inferring probability of relevance using the method of logistic regression. In Proc. SIGIR, pp. 222-231. ACM Press. Ghamrawi, Nadia, and Andrew McCallum. 2005. Collective multi-label classification. In Proc. CIKM, pp. 195-200. ACM Press. DOI: doi.acm.org/10.1145/1099554.1099591. Glover, Eric, David M. Pennock, Steve Lawrence, and Robert Krovetz. 2002a. Inferring hierarchical descriptions. In Proc. CIKM, pp. 507-514. ACM Press. DOI: doi.acm.org/10.1145/584792.584876. Glover, Eric J., Kostas Tsioutsiouliklis, Steve Lawrence, David M. Pennock, and Gary W. Flake. 2002b. Using web structure for classifying and describing web pages. In Proc. WWW, pp. 562-569. ACM Press. DOI: doi.acm.org/10.1145/511446.511520. Gövert, Norbert, and Gabriella Kazai. 2003. Overview of the INitiative for the Evaluation of XML retrieval (INEX) 2002. In Fuhr et al. (2003b), pp. 1-17. URL: inex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. Grabs, Torsten, and Hans-Jörg Schek. 2002. Generating vector spaces on-the-fly for flexible XML retrieval. In XML and Information Retrieval Workshop at SIGIR 2002. Greiff, Warren R. 1998. A theory of term weighting based on exploratory data analysis. In Proc. SIGIR, pp. 11-19. ACM Press. Grinstead, Charles M., and J. Laurie Snell. 1997. Introduction to Probability, 2nd edition. American Mathematical Society. URL: www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf. Grossman, David A., and Ophir Frieder. 2004. Information Retrieval: Algorithms and Heuristics, 2nd edition. Springer. Gusfield, Dan. 1997. Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology. Cambridge University Press. Hamerly, Greg, and Charles Elkan. 2003. Learning the in -means. In Proc. NIPS. URL: books.nips.cc/papers/files/nips16/NIPS2003_AA36.pdf. Han, Eui-Hong, and George Karypis. 2000. Centroid-based document classification: Analysis and experimental results. In Proc. PKDD, pp. 424-431. Hand, David J. 2006. Classifier technology and the illusion of progress. Statistical Science 21: 1-14. Hand, David J., and Keming Yu. 2001. Idiot's Bayes: Not so stupid after all. International Statistical Review 69 (3): 385-398. Harman, Donna. 1991. How effective is suffixing? JASIS 42: 7-15. Harman, Donna. 1992. Relevance feedback revisited. In Proc. SIGIR, pp. 1-10. ACM Press. Harman, Donna, Ricardo Baeza-Yates, Edward Fox, and W. Lee. 1992. Inverted files. In Frakes and Baeza-Yates (1992), pp. 28-43. Harman, Donna, and Gerald Candela. 1990. Retrieving records from a gigabyte of text on a minicomputer using statistical ranking. JASIS 41 (8): 581-589. Harold, Elliotte Rusty, and Scott W. Means. 2004. XML in a Nutshell, 3rd edition. O'Reilly. Harter, Stephen P. 1998. Variations in relevance assessments and the measurement of retrieval effectiveness. JASIS 47: 37-49. Hartigan, J. A., and M. A. Wong. 1979. A K-means clustering algorithm. Applied Statistics 28: 100-108. Hastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. Hatzivassiloglou, Vasileios, Luis Gravano, and Ankineedu Maganti. 2000. An investigation of linguistic features and clustering algorithms for topical document clustering. In Proc. SIGIR, pp. 224-231. ACM Press. DOI: doi.acm.org/10.1145/345508.345582. Haveliwala, Taher. 2003. Topic-sensitive PageRank: A context-sensitive ranking algorithm for web search. IEEE Transactions on Knowledge and Data Engineering 15 (4): 784-796. URL: citeseer.ist.psu.edu/article/haveliwala03topicsensitive.html. Haveliwala, Taher H. 2002. Topic-sensitive PageRank. In Proc. WWW. URL: citeseer.ist.psu.edu/haveliwala02topicsensitive.html. Hayes, Philip J., and Steven P. Weinstein. 1990. CONSTRUE/TIS: A system for content-based indexing of a database of news stories. In Proc. Conference on Innovative Applications of Artificial Intelligence, pp. 49-66. Heaps, Harold S. 1978. Information Retrieval: Computational and Theoretical Aspects. Academic Press. Hearst, Marti A. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics 23 (1): 33-64. Hearst, Marti A. 2006. Clustering versus faceted categories for information exploration. CACM 49 (4): 59-61. DOI: doi.acm.org/10.1145/1121949.1121983. Hearst, Marti A., and Jan O. Pedersen. 1996. Reexamining the cluster hypothesis. In Proc. SIGIR, pp. 76-84. ACM Press. Hearst, Marti A., and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proc. SIGIR, pp. 59-68. ACM Press. DOI: doi.acm.org/10.1145/160688.160695. Heinz, Steffen, and Justin Zobel. 2003. Efficient single-pass index construction for text databases. JASIST 54 (8): 713-729. DOI: dx.doi.org/10.1002/asi.10268. Heinz, Steffen, Justin Zobel, and Hugh E. Williams. 2002. Burst tries: A fast, efficient data structure for string keys. TOIS 20 (2): 192-223. DOI: doi.acm.org/10.1145/506309.506312. Henzinger, Monika R., Allan Heydon, Michael Mitzenmacher, and Marc Najork. 2000. On near-uniform URL sampling. In Proc. WWW, pp. 295-308. North-Holland. DOI: dx.doi.org/10.1016/S1389-1286(00)00055-4. Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers, pp. 115-132. MIT Press. Hersh, William, Chris Buckley, T. J. Leone, and David Hickam. 1994. OHSUMED: An interactive retrieval evaluation and new large test collection for research. In Proc. SIGIR, pp. 192-201. ACM Press. Hersh, William R., Andrew Turpin, Susan Price, Benjamin Chan, Dale Kraemer, Lynetta Sacherek, and Daniel Olson. 2000a. Do batch and user evaluation give the same results? In Proc. SIGIR, pp. 17-24. Hersh, William R., Andrew Turpin, Susan Price, Dale Kraemer, Daniel Olson, Benjamin Chan, and Lynetta Sacherek. 2001. Challenging conventional assumptions of automated information retrieval with real users: Boolean searching and batch retrieval evaluations. IP M 37 (3): 383-402. Hersh, William R., Andrew Turpin, Lynetta Sacherek, Daniel Olson, Susan Price, Benjamin Chan, and Dale Kraemer. 2000b. Further analysis of whether batch and user evaluations give the same results with a question-answering task. In Proc. TREC. Hiemstra, Djoerd. 1998. A linguistically motivated probabilistic model of information retrieval. In Proc. ECDL, volume 1513 of LNCS, pp. 569-584. Hiemstra, Djoerd. 2000. A probabilistic justification for using tf.idf term weighting in information retrieval. International Journal on Digital Libraries 3 (2): 131-139. Hiemstra, Djoerd, and Wessel Kraaij. 2005. A language-modeling approach to TREC. In Voorhees and Harman (2005), pp. 373-395. Hirai, Jun, Sriram Raghavan, Hector Garcia-Molina, and Andreas Paepcke. 2000. WebBase: A repository of web pages. In Proc. WWW, pp. 277-293. Hofmann, Thomas. 1999a. Probabilistic Latent Semantic Indexing. In Proc. UAI. URL: citeseer.ist.psu.edu/hofmann99probabilistic.html. Hofmann, Thomas. 1999b. Probabilistic Latent Semantic Indexing. In Proc. SIGIR, pp. 50-57. ACM Press. URL: citeseer.ist.psu.edu/article/hofmann99probabilistic.html. Hollink, Vera, Jaap Kamps, Christof Monz, and Maarten de Rijke. 2004. Monolingual document retrieval for European languages. IR 7 (1): 33-52. Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman. 2000. Introduction to Automata Theory, Languages, and Computation, 2nd edition. Addison Wesley. Huang, Yifen, and Tom M. Mitchell. 2006. Text clustering with extended user feedback. In Proc. SIGIR, pp. 413-420. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148242. Hubert, Lawrence, and Phipps Arabie. 1985. Comparing partitions. Journal of Classification 2: 193-218. Hughes, Baden, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006. Reconsidering language identification for written language resources. In Proc. International Conference on Language Resources and Evaluation, pp. 485-488. Hull, David. 1993. Using statistical testing in the evaluation of retrieval performance. In Proc. SIGIR, pp. 329-338. ACM Press. Hull, David. 1996. Stemming algorithms - A case study for detailed evaluation. JASIS 47 (1): 70-84. Ide, E. 1971. New experiments in relevance feedback. In Salton (1971b), pp. 337-354. Indyk, Piotr. 2004. Nearest neighbors in high-dimensional spaces. In J. E. Goodman and J. O'Rourkeeds.), Handbook of Discrete and Computational Geometry, 2nd edition, pp. 877-892. Chapman and Hall/CRC Press. Ingwersen, Peter, and Kalervo Järvelin. 2005. The Turn: Integration of Information Seeking and Retrieval in Context. Springer. Ittner, David J., David D. Lewis, and David D. Ahn. 1995. Text categorization of low quality images. In Proc. SDAIR, pp. 301-315. Iwayama, Makoto, and Takenobu Tokunaga. 1995. Cluster-based text categorization: A comparison of category search strategies. In Proc. SIGIR, pp. 273-280. ACM Press. Jackson, Peter, and Isabelle Moulinier. 2002. Natural Language Processing for Online Applications: Text Retrieval, Extraction and Categorization. John Benjamins. Jacobs, Paul S., and Lisa F. Rau. 1990. SCISOR: Extracting information from on-line news. CACM 33: 88-97. Jain, Anil, M. Narasimha Murty, and Patrick Flynn. 1999. Data clustering: A review. ACM Computing Surveys 31 (3): 264-323. Jain, Anil K., and Richard C. Dubes. 1988. Algorithms for Clustering Data. Prentice Hall. Jardine, N., and Cornelis Joost van Rijsbergen. 1971. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval 7: 217-240. Järvelin, Kalervo, and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. TOIS 20 (4): 422-446. Jeh, Glen, and Jennifer Widom. 2003. Scaling personalized web search. In Proc. WWW, pp. 271-279. ACM Press. Jensen, Finn V., and Finn B. Jensen. 2001. Bayesian Networks and Decision Graphs. Springer. Jeong, Byeong-Soo, and Edward Omiecinski. 1995. Inverted file partitioning schemes in multiple disk systems. IEEE Transactions on Parallel and Distributed Systems 6 (2): 142-153. Ji, Xiang, and Wei Xu. 2006. Document clustering with prior knowledge. In Proc. SIGIR, pp. 405-412. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148241. Jing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proc. Conference on Applied Natural Language Processing, pp. 310-315. Joachims, Thorsten. 1997. A probabilistic analysis of the Rocchio algorithm with tfidf for text categorization. In Proc. ICML, pp. 143-151. Morgan Kaufmann. Joachims, Thorsten. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proc. ECML, pp. 137-142. Springer. Joachims, Thorsten. 1999. Making large-scale SVM learning practical. In B. Schölkopf, C. Burges, and A. Smolaeds.), Advances in Kernel Methods - Support Vector Learning. MIT Press. Joachims, Thorsten. 2002a. Learning to Classify Text Using Support Vector Machines. Kluwer. Joachims, Thorsten. 2002b. Optimizing search engines using clickthrough data. In Proc. KDD, pp. 133-142. Joachims, Thorsten. 2006a. Training linear SVMs in linear time. In Proc. KDD, pp. 217-226. ACM Press. DOI: doi.acm.org/10.1145/1150402.1150429. Joachims, Thorsten. 2006b. Transductive support vector machines. In Chapelle et al. (2006), pp. 105-118. Joachims, Thorsten, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2005. Accurately interpreting clickthrough data as implicit feedback. In Proc. SIGIR, pp. 154-161. ACM Press. Johnson, David, Vishv Malhotra, and Peter Vamplew. 2006. More effective web search using bigrams and trigrams. Webology 3 (4). URL: www.webology.ir/2006/v3n4/a35.html. Article 35. Jurafsky, Dan, and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition, 2nd edition. Prentice Hall. Käki, Mika. 2005. Findex: Search result categories help users when document ranking fails. In Proc. SIGCHI, pp. 131-140. ACM Press. DOI: doi.acm.org/10.1145/1054972.1054991. Kammenhuber, Nils, Julia Luxenburger, Anja Feldmann, and Gerhard Weikum. 2006. Web search clickstreams. In Proc. ACM SIGCOMM on Internet Measurement, pp. 245-250. ACM Press. Kamps, Jaap, Maarten de Rijke, and Börkur Sigurbjörnsson. 2004. Length normalization in XML retrieval. In Proc. SIGIR, pp. 80-87. ACM Press. DOI: doi.acm.org/10.1145/1008992.1009009. Kamps, Jaap, Maarten Marx, Maarten de Rijke, and Börkur Sigurbjörnsson. 2006. Articulating information needs in XML query languages. TOIS 24 (4): 407-436. DOI: doi.acm.org/10.1145/1185877.1185879. Kamvar, Sepandar D., Dan Klein, and Christopher D. Manning. 2002. Interpreting and extending classical agglomerative clustering algorithms using a model-based approach. In Proc. ICML, pp. 283-290. Morgan Kaufmann. Kannan, Ravi, Santosh Vempala, and Adrian Vetta. 2000. On clusterings - Good, bad and spectral. In Proc. Symposium on Foundations of Computer Science, pp. 367-377. IEEE Computer Society. Kaszkiel, Marcin, and Justin Zobel. 1997. Passage retrieval revisited. In Proc. SIGIR, pp. 178-185. ACM Press. DOI: doi.acm.org/10.1145/258525.258561. Kaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding groups in data. Wiley. Kazai, Gabriella, and Mounia Lalmas. 2006. eXtended cumulated gain measures for the evaluation of content-oriented XML retrieval. TOIS 24 (4): 503-542. DOI: doi.acm.org/10.1145/1185883. Kekäläinen, Jaana. 2005. Binary and graded relevance in IR evaluations - Comparison of the effects on ranking of IR systems. IP M 41: 1019-1033. Kekäläinen, Jaana, and Kalervo Järvelin. 2002. Using graded relevance assessments in IR evaluation. JASIST 53 (13): 1120-1129. Kemeny, John G., and J. Laurie Snell. 1976. Finite Markov Chains. Springer. Kent, Allen, Madeline M. Berry, Fred U. Luehrs, Jr., and J. W. Perry. 1955. Machine literature searching VIII. Operational criteria for designing information retrieval systems. American Documentation 6 (2): 93-101. Kernighan, Mark D., Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proc. ACL, pp. 205-210. King, Benjamin. 1967. Step-wise clustering procedures. Journal of the American Statistical Association 69: 86-101. Kishida, Kazuaki, Kuang-Hua Chen, Sukhoon Lee, Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen, and Sung Hyon Myaeng. 2005. Overview of CLIR task at the fifth NTCIR workshop. In NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access. National Institute of Informatics. Klein, Dan, and Christopher D. Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proc. Empirical Methods in Natural Language Processing, pp. 9-16. Kleinberg, Jon M. 1997. Two algorithms for nearest-neighbor search in high dimensions. In Proc. ACM Symposium on Theory of Computing, pp. 599-608. ACM Press. DOI: doi.acm.org/10.1145/258533.258653. Kleinberg, Jon M. 1999. Authoritative sources in a hyperlinked environment. JACM 46 (5): 604-632. URL: citeseer.ist.psu.edu/article/kleinberg98authoritative.html. Kleinberg, Jon M. 2002. An impossibility theorem for clustering. In Proc. NIPS. Knuth, Donald E. 1997. The Art of Computer Programming, Volume 3: Sorting and Searching, 3rd edition. Addison Wesley. Ko, Youngjoong, Jinwoo Park, and Jungyun Seo. 2004. Improving text categorization using the importance of sentences. IP M 40 (1): 65-79. Koenemann, Jürgen, and Nicholas J. Belkin. 1996. A case for interaction: A study of interactive information retrieval behavior and effectiveness. In Proc. SIGCHI, pp. 205-212. ACM Press. DOI: doi.acm.org/10.1145/238386.238487. Kocz, Aleksander, Vidya Prabakarmurthi, and Jugal Kalita. 2000. Summarization as feature selection for text categorization. In Proc. CIKM, pp. 365-370. ACM Press. Kocz, Aleksander, and Wen-Tau Yih. 2007. Raising the baseline for high-precision text classifiers. In Proc. KDD. Koller, Daphne, and Mehran Sahami. 1997. Hierarchically classifying documents using very few words. In Proc. ICML, pp. 170-178. Konheim, Alan G. 1981. Cryptography: A Primer. John Wiley   Sons. Korfhage, Robert R. 1997. Information Storage and Retrieval. Wiley. Kozlov, M. K., S. P. Tarasov, and L. G. Khachiyan. 1979. Polynomial solvability of convex quadratic programming. Soviet Mathematics Doklady 20: 1108-1111. Translated from original in Doklady Akademiia Nauk SSR, 228 (1979). Kraaij, Wessel, and Martijn Spitters. 2003. Language models for topic tracking. In W. B. Croft and J. Laffertyeds.), Language Modeling for Information Retrieval, pp. 95-124. Kluwer. Kraaij, Wessel, Thijs Westerveld, and Djoerd Hiemstra. 2002. The importance of prior probabilities for entry page search. In Proc. SIGIR, pp. 27-34. ACM Press. Krippendorff, Klaus. 2003. Content Analysis: An Introduction to its Methodology. Sage. Krovetz, Bob. 1995. Word sense disambiguation for large text databases. PhD thesis, University of Massachusetts Amherst. Kukich, Karen. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys 24 (4): 377-439. DOI: doi.acm.org/10.1145/146370.146380. Kumar, Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins. 1999. Trawling the Web for emerging cyber-communities. Computer Networks 31 (11-16): 1481-1493. URL: citeseer.ist.psu.edu/kumar99trawling.html. Kumar, S. Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, Dandapani Sivakumar, Andrew Tomkins, and Eli Upfal. 2000. The Web as a graph. In Proc. PODS, pp. 1-10. ACM Press. URL: citeseer.ist.psu.edu/article/kumar00web.html. Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proc. SIGIR, pp. 68-73. ACM Press. Kurland, Oren, and Lillian Lee. 2004. Corpus structure, language models, and ad hoc information retrieval. In Proc. SIGIR, pp. 194-201. ACM Press. DOI: doi.acm.org/10.1145/1008992.1009027. Lafferty, John, and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. SIGIR, pp. 111-119. ACM Press. Lafferty, John, and Chengxiang Zhai. 2003. Probabilistic relevance models based on document and query generation. In W. Bruce Croft and John Laffertyeds.), Language Modeling for Information Retrieval. Kluwer. Lalmas, Mounia, Gabriella Kazai, Jaap Kamps, Jovan Pehcevski, Benjamin Piwowarski, and Stephen E. Robertson. 2007. INEX 2006 evaluation measures. In Fuhr et al. (2007), pp. 20-34. Lalmas, Mounia, and Anastasios Tombros. 2007. Evaluating XML retrieval effectiveness at INEX. SIGIR Forum 41 (1): 40-57. DOI: doi.acm.org/10.1145/1273221.1273225. Lance, G. N., and W. T. Williams. 1967. A general theory of classificatory sorting strategies 1. Hierarchical systems. Computer Journal 9 (4): 373-380. Langville, Amy, and Carl Meyer. 2006. Google's PageRank and Beyond: The Science of Search Engine Rankings. Princeton University Press. Larsen, Bjornar, and Chinatsu Aone. 1999. Fast and effective text mining using linear-time document clustering. In Proc. KDD, pp. 16-22. ACM Press. DOI: doi.acm.org/10.1145/312129.312186. Larson, Ray R. 2005. A fusion approach to XML structured document retrieval. IR 8 (4): 601-629. DOI: dx.doi.org/10.1007/s10791-005-0749-0. Lavrenko, Victor, and W. Bruce Croft. 2001. Relevance-based language models. In Proc. SIGIR, pp. 120-127. ACM Press. Lawrence, Steve, and C. Lee Giles. 1998. Searching the World Wide Web. Science 280 (5360): 98-100. URL: citeseer.ist.psu.edu/lawrence98searching.html. Lawrence, Steve, and C. Lee Giles. 1999. Accessibility of information on the web. Nature 500: 107-109. Lee, Whay C., and Edward A. Fox. 1988. Experimental comparison of schemes for interpreting Boolean queries. Technical Report TR-88-27, Computer Science, Virginia Polytechnic Institute and State University. Lempel, Ronny, and Shlomo Moran. 2000. The stochastic approach for link-structure analysis (SALSA) and the TKC effect. Computer Networks 33 (1-6): 387-401. URL: citeseer.ist.psu.edu/lempel00stochastic.html. Lesk, Michael. 1988. Grab - Inverted indexes with low storage overhead. Computing Systems 1: 207-220. Lesk, Michael. 2004. Understanding Digital Libraries, 2nd edition. Morgan Kaufmann. Lester, Nicholas, Alistair Moffat, and Justin Zobel. 2005. Fast on-line index construction by geometric partitioning. In Proc. CIKM, pp. 776-783. ACM Press. DOI: doi.acm.org/10.1145/1099554.1099739. Lester, Nicholas, Justin Zobel, and Hugh E. Williams. 2006. Efficient online index maintenance for contiguous inverted lists. IP M 42 (4): 916-933. DOI: dx.doi.org/10.1016/j.ipm.2005.09.005. Levenshtein, Vladimir I. 1965. Binary codes capable of correcting spurious insertions and deletions of ones. Problems of Information Transmission 1: 8-17. Lew, Michael S. 2001. Principles of Visual Information Retrieval. Springer. Lewis, David D. 1995. Evaluating and optimizing autonomous text classification systems. In Proc. SIGIR. ACM Press. Lewis, David D. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proc. ECML, pp. 4-15. Springer. Lewis, David D., and Karen Spärck Jones. 1996. Natural language processing for information retrieval. CACM 39 (1): 92-101. DOI: doi.acm.org/10.1145/234173.234210. Lewis, David D., and Marc Ringuette. 1994. A comparison of two learning algorithms for text categorization. In Proc. SDAIR, pp. 81-93. Lewis, David D., Robert E. Schapire, James P. Callan, and Ron Papka. 1996. Training algorithms for linear text classifiers. In Proc. SIGIR, pp. 298-306. ACM Press. DOI: doi.acm.org/10.1145/243199.243277. Lewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. JMLR 5: 361-397. Li, Fan, and Yiming Yang. 2003. A loss function analysis for classification methods in text categorization. In Proc. ICML, pp. 472-479. Liddy, Elizabeth D. 2005. Automatic document retrieval. In Encyclopedia of Language and Linguistics, 2nd edition. Elsevier. List, Johan, Vojkan Mihajlovic, Georgina Ramírez, Arjen P. Vries, Djoerd Hiemstra, and Henk Ernst Blok. 2005. TIJAH: Embracing IR methods in XML databases. IR 8 (4): 547-570. DOI: dx.doi.org/10.1007/s10791-005-0747-2. Lita, Lucian Vlad, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuEcasIng. In Proc. ACL, pp. 152-159. Littman, Michael L., Susan T. Dumais, and Thomas K. Landauer. 1998. Automatic cross-language information retrieval using latent semantic indexing. In Gregory Grefenstetteed.), Proc. Cross-Language Information Retrieval. Kluwer. URL: citeseer.ist.psu.edu/littman98automatic.html. Liu, Tie-Yan, Yiming Yang, Hao Wan, Hua-Jun Zeng, Zheng Chen, and Wei-Ying Ma. 2005. Support vector machines classification with very large scale taxonomy. ACM SIGKDD Explorations 7 (1): 36-43. Liu, Xiaoyong, and W. Bruce Croft. 2004. Cluster-based retrieval using language models. In Proc. SIGIR, pp. 186-193. ACM Press. DOI: doi.acm.org/10.1145/1008992.1009026. Lloyd, Stuart P. 1982. Least squares quantization in PCM. IEEE Transactions on Information Theory 28 (2): 129-136. Lodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. JMLR 2: 419-444. Lombard, Matthew, Cheryl C. Bracken, and Jennifer Snyder-Duch. 2002. Content analysis in mass communication: Assessment and reporting of intercoder reliability. Human Communication Research 28: 587-604. Long, Xiaohui, and Torsten Suel. 2003. Optimized query execution in large search engines with global page ordering. In Proc. VLDB. URL: citeseer.ist.psu.edu/long03optimized.html. Lovins, Julie Beth. 1968. Development of a stemming algorithm. Translation and Computational Linguistics 11 (1): 22-31. Lu, Wei, Stephen E. Robertson, and Andrew MacFarlane. 2007. CISR at INEX 2006. In Fuhr et al. (2007), pp. 57-63. Luhn, Hans Peter. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development 1 (4): 309-317. Luhn, Hans Peter. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development 2 (2): 159-165, 317. Luk, Robert W. P., and Kui-Lam Kwok. 2002. A comparison of Chinese document indexing strategies and retrieval models. ACM Transactions on Asian Language Information Processing 1 (3): 225-268. Lunde, Ken. 1998. CJKV Information Processing. O'Reilly. MacFarlane, A., J.A. McCann, and S.E. Robertson. 2000. Parallel search using partitioned inverted files. In Proc. SPIRE, pp. 209-220. MacQueen, James B. 1967. Some methods for classification and analysis of multivariate observations. In Proc. Berkeley Symposium on Mathematics, Statistics and Probability, pp. 281-297. University of California Press. Manning, Christopher D., and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press. Maron, M. E., and J. L. Kuhns. 1960. On relevance, probabilistic indexing, and information retrieval. JACM 7 (3): 216-244. Mass, Yosi, Matan Mandelbrod, Einat Amitay, David Carmel, Yoëlle S. Maarek, and Aya Soffer. 2003. JuruXML - An XML retrieval system at INEX'02. In Fuhr et al. (2003b), pp. 73-80. URL: inex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. McBryan, Oliver A. 1994. GENVL and WWWW: Tools for Taming the Web. In Proc. WWW. URL: citeseer.ist.psu.edu/mcbryan94genvl.html. McCallum, Andrew, and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In AAAI/ICML Workshop on Learning for Text Categorization, pp. 41-48. McCallum, Andrew, Ronald Rosenfeld, Tom M. Mitchell, and Andrew Y. Ng. 1998. Improving text classification by shrinkage in a hierarchy of classes. In Proc. ICML, pp. 359-367. Morgan Kaufmann. McCallum, Andrew Kachites. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. www.cs.cmu.edu/~mccallum/bow. McKeown, Kathleen, and Dragomir R. Radev. 1995. Generating summaries of multiple news articles. In Proc. SIGIR, pp. 74-82. ACM Press. DOI: doi.acm.org/10.1145/215206.215334. McKeown, Kathleen R., Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia's Newsblaster. In Proc. Human Language Technology Conference. McLachlan, Geoffrey J., and Thiriyambakam Krishnan. 1996. The EM Algorithm and Extensions. John Wiley   Sons. Meadow, Charles T., Donald H. Kraft, and Bert R. Boyce. 1999. Text Information Retrieval Systems. Academic Press. Meila, Marina. 2005. Comparing clusterings - An axiomatic view. In Proc. ICML. Melnik, Sergey, Sriram Raghavan, Beverly Yang, and Hector Garcia-Molina. 2001. Building a distributed full-text index for the web. In Proc. WWW, pp. 396-406. ACM Press. DOI: doi.acm.org/10.1145/371920.372095. Mihajlovic, Vojkan, Henk Ernst Blok, Djoerd Hiemstra, and Peter M. G. Apers. 2005. Score region algebra: Building a transparent XML-R database. In Proc. CIKM, pp. 12-19. DOI: doi.acm.org/10.1145/1099554.1099560. Miller, David R. H., Tim Leek, and Richard M. Schwartz. 1999. A hidden Markov model information retrieval system. In Proc. SIGIR, pp. 214-221. ACM Press. Minsky, Marvin Lee, and Seymour Paperteds.). 1988. Perceptrons: An introduction to computational geometry. MIT Press. Expanded edition. Mitchell, Tom M. 1997. Machine Learning. McGraw Hill. Moffat, Alistair, and Timothy A. H. Bell. 1995. In situ generation of compressed inverted files. JASIS 46 (7): 537-550. Moffat, Alistair, and Lang Stuiver. 1996. Exploiting clustering in inverted file compression. In Proc. Conference on Data Compression, pp. 82-91. IEEE Computer Society. Moffat, Alistair, and Justin Zobel. 1992. Parameterised compression for sparse bitmaps. In Proc. SIGIR, pp. 274-285. ACM Press. DOI: doi.acm.org/10.1145/133160.133210. Moffat, Alistair, and Justin Zobel. 1996. Self-indexing inverted files for fast text retrieval. TOIS 14 (4): 349-379. Moffat, Alistair, and Justin Zobel. 1998. Exploring the similarity space. SIGIR Forum 32 (1). Mooers, Calvin. 1961. From a point of view of mathematical etc. techniques. In R. A. Fairthorneed.), Towards information retrieval, pp. xvii-xxiii. Butterworths. Mooers, Calvin E. 1950. Coding, information retrieval, and the rapid selector. American Documentation 1 (4): 225-229. Moschitti, Alessandro. 2003. A study on optimal parameter tuning for Rocchio text classifier. In Proc. ECIR, pp. 420-435. Moschitti, Alessandro, and Roberto Basili. 2004. Complex linguistic features for text classification: A comprehensive study. In Proc. ECIR, pp. 181-196. Murata, Masaki, Qing Ma, Kiyotaka Uchimoto, Hiromi Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000. Japanese probabilistic information retrieval using location and category information. In International Workshop on Information Retrieval With Asian Languages, pp. 81-88. URL: portal.acm.org/citation.cfm?doid=355214.355226. Muresan, Gheorghe, and David J. Harper. 2004. Topic modeling for mediated access to very large document collections. JASIST 55 (10): 892-910. DOI: dx.doi.org/10.1002/asi.20034. Murtagh, Fionn. 1983. A survey of recent advances in hierarchical clustering algorithms. Computer Journal 26 (4): 354-359. Najork, Marc, and Allan Heydon. 2001. High-performance web crawling. Technical Report 173, Compaq Systems Research Center. Najork, Marc, and Allan Heydon. 2002. High-performance web crawling. In James Abello, Panos Pardalos, and Mauricio Resendeeds.), Handbook of Massive Data Sets, chapter 2. Kluwer. Navarro, Gonzalo, and Ricardo Baeza-Yates. 1997. Proximal nodes: A model to query document databases by content and structure. TOIS 15 (4): 400-435. DOI: doi.acm.org/10.1145/263479.263482. Newsam, Shawn, Sitaram Bhagavathy, and B. S. Manjunath. 2001. Category-based image retrieval. In Proc. IEEE International Conference on Image Processing, Special Session on Multimedia Indexing, Browsing and Retrieval, pp. 596-599. Ng, Andrew Y., and Michael I. Jordan. 2001. On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. In Proc. NIPS, pp. 841-848. URL: www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA28.ps.gz. Ng, Andrew Y., Michael I. Jordan, and Yair Weiss. 2001a. On spectral clustering: Analysis and an algorithm. In Proc. NIPS, pp. 849-856. Ng, Andrew Y., Alice X. Zheng, and Michael I. Jordan. 2001b. Link analysis, eigenvectors and stability. In Proc. IJCAI, pp. 903-910. URL: citeseer.ist.psu.edu/ng01link.html. Nigam, Kamal, Andrew McCallum, and Tom Mitchell. 2006. Semi-supervised text classification using EM. In Chapelle et al. (2006), pp. 33-56. Ntoulas, Alexandros, and Junghoo Cho. 2007. Pruning policies for two-tiered inverted index with correctness guarantee. In Proc. SIGIR, pp. 191-198. ACM Press. Oard, Douglas W., and Bonnie J. Dorr. 1996. A survey of multilingual text retrieval. Technical Report UMIACS-TR-96-19, Institute for Advanced Computer Studies, University of Maryland, College Park, MD, USA. Ogilvie, Paul, and Jamie Callan. 2005. Parameter estimation for a simple hierarchical generative model for XML retrieval. In Proc. INEX, pp. 211-224. DOI: dx.doi.org/10.1007/11766278_16. O'Keefe, Richard A., and Andrew Trotman. 2004. The simplest query language that could possibly work. In Fuhr et al. (2005), pp. 167-174. Osinski, Stanisaw, and Dawid Weiss. 2005. A concept-driven algorithm for clustering search results. IEEE Intelligent Systems 20 (3): 48-54. Page, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The PageRank citation ranking: Bringing order to the web. Technical report, Stanford Digital Library Technologies Project. URL: citeseer.ist.psu.edu/page98pagerank.html. Paice, Chris D. 1990. Another stemmer. SIGIR Forum 24 (3): 56-61. Papineni, Kishore. 2001. Why inverse document frequency? In Proc. North American Chapter of the Association for Computational Linguistics, pp. 1-8. Pavlov, Dmitry, Ramnath Balasubramanyan, Byron Dom, Shyam Kapur, and Jignashu Parikh. 2004. Document preprocessing for naive Bayes classification and clustering with mixture of multinomials. In Proc. KDD, pp. 829-834. Pelleg, Dan, and Andrew Moore. 1999. Accelerating exact k-means algorithms with geometric reasoning. In Proc. KDD, pp. 277-281. ACM Press. DOI: doi.acm.org/10.1145/312129.312248. Pelleg, Dan, and Andrew Moore. 2000. X-means: Extending k-means with efficient estimation of the number of clusters. In Proc. ICML, pp. 727-734. Morgan Kaufmann. Perkins, Simon, Kevin Lacker, and James Theiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. JMLR 3: 1333-1356. Persin, Michael. 1994. Document filtering for fast ranking. In Proc. SIGIR, pp. 339-348. ACM Press. Persin, Michael, Justin Zobel, and Ron Sacks-Davis. 1996. Filtered document retrieval with frequency-sorted indexes. JASIS 47 (10): 749-764. Peterson, James L. 1980. Computer programs for detecting and correcting spelling errors. CACM 23 (12): 676-687. DOI: doi.acm.org/10.1145/359038.359041. Picca, Davide, Benoît Curdy, and François Bavaud. 2006. Non-linear correspondence analysis in text retrieval: A kernel view. In Proc. JADT. Pinski, Gabriel, and Francis Narin. 1976. Citation influence for journal aggregates of scientific publications: Theory, with application to the literature of Physics. IP M 12: 297-326. Pirolli, Peter L. T. 2007. Information Foraging Theory: Adaptive Interaction With Information. Oxford University Press. Platt, John. 2000. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In A.J. Smola, P.L. Bartlett, B. Schölkopf, and D. Schuurmans (eds.), Advances in Large Margin Classifiers, pp. 61-74. MIT Press. Ponte, Jay M., and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. SIGIR, pp. 275-281. ACM Press. Popescul, Alexandrin, and Lyle H. Ungar. 2000. Automatic labeling of document clusters. Unpublished MS, U. Pennsylvania. URL: http://www.cis.upenn.edu/ popescul/Publications/popescul00labeling.pdf. Porter, Martin F. 1980. An algorithm for suffix stripping. Program 14 (3): 130-137. Pugh, William. 1990. Skip lists: A probabilistic alternative to balanced trees. CACM 33 (6): 668-676. Qin, Tao, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-Sheng Wang, and Hang Li. 2007. Ranking with multiple hyperplanes. In Proc. SIGIR. ACM Press. Qiu, Yonggang, and H.P. Frei. 1993. Concept based query expansion. In Proc. SIGIR, pp. 160-169. ACM Press. R Development Core Team. 2005. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna. URL: www.R-project.org. ISBN 3-900051-07-0. Radev, Dragomir R., Sasha Blair-Goldensohn, Zhu Zhang, and Revathi Sundara Raghavan. 2001. Interactive, domain-independent identification and summarization of topically related news articles. In Proc. European Conference on Research and Advanced Technology for Digital Libraries, pp. 225-238. Rahm, Erhard, and Philip A. Bernstein. 2001. A survey of approaches to automatic schema matching. VLDB Journal 10 (4): 334-350. URL: citeseer.ist.psu.edu/rahm01survey.html. Rand, William M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association 66 (336): 846-850. Rasmussen, Edie. 1992. Clustering algorithms. In Frakes and Baeza-Yates (1992), pp. 419-442. Rennie, Jason D., Lawrence Shih, Jaime Teevan, and David R. Karger. 2003. Tackling the poor assumptions of naive Bayes text classifiers. In Proc. ICML, pp. 616-623. Ribeiro-Neto, Berthier, Edleno S. Moura, Marden S. Neubert, and Nivio Ziviani. 1999. Efficient distributed algorithms to build inverted files. In Proc. SIGIR, pp. 105-112. ACM Press. DOI: doi.acm.org/10.1145/312624.312663. Ribeiro-Neto, Berthier A., and Ramurti A. Barbosa. 1998. Query performance for tightly coupled distributed digital libraries. In Proc. ACM Conference on Digital Libraries, pp. 182-190. Rice, John A. 2006. Mathematical Statistics and Data Analysis. Duxbury Press. Richardson, M., A. Prakash, and E. Brill. 2006. Beyond PageRank: machine learning for static ranking. In Proc. WWW, pp. 707-715. Riezler, Stefan, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proc. ACL, pp. 464-471. Association for Computational Linguistics. URL: www.aclweb.org/anthology/P/P07/P07-1059. Ripley, B. D. 1996. Pattern Recognition and Neural Networks. Cambridge University Press. Robertson, Stephen. 2005. How Okapi came to TREC. In Voorhees and Harman (2005), pp. 287-299. Robertson, Stephen, Hugo Zaragoza, and Michael Taylor. 2004. Simple BM25 extension to multiple weighted fields. In Proc. CIKM, pp. 42-49. DOI: doi.acm.org/10.1145/1031171.1031181. Robertson, Stephen E., and Karen Spärck Jones. 1976. Relevance weighting of search terms. JASIS 27: 129-146. Rocchio, J. J. 1971. Relevance feedback in information retrieval. In Salton (1971b), pp. 313-323. Roget, P. M. 1946. Roget's International Thesaurus. Thomas Y. Crowell. Rosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In Proc. UAI, pp. 487-494. Ross, Sheldon. 2006. A First Course in Probability. Pearson Prentice Hall. Rusmevichientong, Paat, David M. Pennock, Steve Lawrence, and C. Lee Giles. 2001. Methods for sampling pages uniformly from the world wide web. In Proc. AAAI Fall Symposium on Using Uncertainty Within Computation, pp. 121-128. URL: citeseer.ist.psu.edu/rusmevichientong01methods.html. Ruthven, Ian, and Mounia Lalmas. 2003. A survey on the use of relevance feedback for information access systems. Knowledge Engineering Review 18 (1). Sahoo, Nachiketa, Jamie Callan, Ramayya Krishnan, George Duncan, and Rema Padman. 2006. Incremental hierarchical clustering of text documents. In Proc. CIKM, pp. 357-366. DOI: doi.acm.org/10.1145/1183614.1183667. Sakai, Tetsuya. 2007. On the reliability of information retrieval metrics based on graded relevance. IP M 43 (2): 531-548. Salton, Gerard. 1971a. Cluster search strategies and the optimization of retrieval effectiveness. In The SMART Retrieval System - Experiments in Automatic Document Processing Salton (1971b), pp. 223-242. Salton, Gerarded.). 1971b. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall. Salton, Gerard. 1975. Dynamic information and library processing. Prentice Hall. Salton, Gerard. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison Wesley. Salton, Gerard. 1991. The Smart project in automatic document retrieval. In Proc. SIGIR, pp. 356-358. ACM Press. Salton, Gerard, James Allan, and Chris Buckley. 1993. Approaches to passage retrieval in full text information systems. In Proc. SIGIR, pp. 49-58. ACM Press. DOI: doi.acm.org/10.1145/160688.160693. Salton, Gerard, and Chris Buckley. 1987. Term weighting approaches in automatic text retrieval. Technical report, Cornell University, Ithaca, NY, USA. Salton, Gerard, and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. IP M 24 (5): 513-523. Salton, Gerard, and Chris Buckley. 1990. Improving retrieval performance by relevance feedback. JASIS 41 (4): 288-297. Saracevic, Tefko, and Paul Kantor. 1988. A study of information seeking and retrieving. II: Users, questions and effectiveness. JASIS 39: 177-196. Saracevic, Tefko, and Paul Kantor. 1996. A study of information seeking and retrieving. III: Searchers, searches, overlap. JASIS 39 (3): 197-216. Savaresi, Sergio M., and Daniel Boley. 2004. A comparative analysis on the bisecting K-means and the PDDP clustering algorithms. Intelligent Data Analysis 8 (4): 345-362. Schamber, Linda, Michael Eisenberg, and Michael S. Nilan. 1990. A re-examination of relevance: toward a dynamic, situational definition. IP M 26 (6): 755-776. Schapire, Robert E. 2003. The boosting approach to machine learning: An overview. In D. D. Denison, M. H. Hansen, C. Holmes, B. Mallick, and B. Yu (eds.), Nonlinear Estimation and Classification. Springer. Schapire, Robert E., and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning 39 (2/3): 135-168. Schapire, Robert E., Yoram Singer, and Amit Singhal. 1998. Boosting and Rocchio applied to text filtering. In Proc. SIGIR, pp. 215-223. ACM Press. Schlieder, Torsten, and Holger Meuss. 2002. Querying and ranking XML documents. JASIST 53 (6): 489-503. DOI: dx.doi.org/10.1002/asi.10060. Scholer, Falk, Hugh E. Williams, John Yiannis, and Justin Zobel. 2002. Compression of inverted indexes for fast query evaluation. In Proc. SIGIR, pp. 222-229. ACM Press. DOI: doi.acm.org/10.1145/564376.564416. Schölkopf, Bernhard, and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press. Schütze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics 24 (1): 97-124. Schütze, Hinrich, David A. Hull, and Jan O. Pedersen. 1995. A comparison of classifiers and document representations for the routing problem. In Proc. SIGIR, pp. 229-237. ACM Press. Schütze, Hinrich, and Jan O. Pedersen. 1995. Information retrieval based on word senses. In Proc. SDAIR, pp. 161-175. Schütze, Hinrich, and Craig Silverstein. 1997. Projections for efficient document clustering. In Proc. SIGIR, pp. 74-81. ACM Press. Schwarz, Gideon. 1978. Estimating the dimension of a model. Annals of Statistics 6 (2): 461-464. Sebastiani, Fabrizio. 2002. Machine learning in automated text categorization. ACM Computing Surveys 34 (1): 1-47. Shawe-Taylor, John, and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press. Shkapenyuk, Vladislav, and Torsten Suel. 2002. Design and implementation of a high-performance distributed web crawler. In Proc. International Conference on Data Engineering. URL: citeseer.ist.psu.edu/shkapenyuk02design.html. Siegel, Sidney, and N. John Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral Sciences, 2nd edition. McGraw Hill. Sifry, Dave, 2007. The state of the Live Web, April 2007. URL: technorati.com/weblog/2007/04/328.html. Sigurbjörnsson, Börkur, Jaap Kamps, and Maarten de Rijke. 2004. Mixture models, overlap, and structural hints in XML element retrieval. In Proc. INEX, pp. 196-210. Silverstein, Craig, Monika Rauch Henzinger, Hannes Marais, and Michael Moricz. 1999. Analysis of a very large web search engine query log. SIGIR Forum 33 (1): 6-12. Silvestri, Fabrizio. 2007. Sorting out the document identifier assignment problem. In Proc. ECIR, pp. 101-112. Silvestri, Fabrizio, Raffaele Perego, and Salvatore Orlando. 2004. Assigning document identifiers to enhance compressibility of web search engines indexes. In Proc. ACM Symposium on Applied Computing, pp. 600-605. Sindhwani, V., and S. S. Keerthi. 2006. Large scale semi-supervised linear SVMs. In Proc. SIGIR, pp. 477-484. Singhal, Amit, Chris Buckley, and Mandar Mitra. 1996a. Pivoted document length normalization. In Proc. SIGIR, pp. 21-29. ACM Press. URL: citeseer.ist.psu.edu/singhal96pivoted.html. Singhal, Amit, Mandar Mitra, and Chris Buckley. 1997. Learning routing queries in a query zone. In Proc. SIGIR, pp. 25-32. ACM Press. Singhal, Amit, Gerard Salton, and Chris Buckley. 1995. Length normalization in degraded text collections. Technical report, Cornell University, Ithaca, NY. Singhal, Amit, Gerard Salton, and Chris Buckley. 1996b. Length normalization in degraded text collections. In Proc. SDAIR, pp. 149-162. Singitham, Pavan Kumar C., Mahathi S. Mahabhashyam, and Prabhakar Raghavan. 2004. Efficiency-quality tradeoffs for vector score aggregation. In Proc. VLDB, pp. 624-635. URL: www.vldb.org/conf/2004/RS17P1.PDF. Smeulders, Arnold W. M., Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Trans. Pattern Anal. Mach. Intell. 22 (12): 1349-1380. DOI: dx.doi.org/10.1109/34.895972. Sneath, Peter H.A., and Robert R. Sokal. 1973. Numerical Taxonomy: The Principles and Practice of Numerical Classification. W.H. Freeman. Snedecor, George Waddel, and William G. Cochran. 1989. Statistical methods. Iowa State University Press. Somogyi, Zoltan. 1990. The Melbourne University bibliography system. Technical Report 90/3, Melbourne University, Parkville, Victoria, Australia. Song, Ruihua, Ji-Rong Wen, and Wei-Ying Ma. 2005. Viewing term proximity from a different perspective. Technical Report MSR-TR-2005-69, Microsoft Research. Sornil, Ohm. 2001. Parallel Inverted Index for Large-Scale, Dynamic Digital Libraries. PhD thesis, Virginia Tech. URL: scholar.lib.vt.edu/theses/available/etd-02062001-114915/. Spärck Jones, Karen. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation 28 (1): 11-21. Spärck Jones, Karen. 2004. Language modelling's generative model: Is it rational? MS, Computer Laboratory, University of Cambridge. URL: www.cl.cam.ac.uk/~ksj21/langmodnote4.pdf. Spärck Jones, Karen, S. Walker, and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: Development and comparative experiments. IP M 36 (6): 779-808, 809-840. Spink, Amanda, and Charles Coleeds.). 2005. New Directions in Cognitive Information Retrieval. Springer. Spink, Amanda, Bernard J. Jansen, and H. Cenk Ozmultu. 2000. Use of query reformulation and relevance feedback by Excite users. Internet Research: Electronic Networking Applications and Policy 10 (4): 317-328. URL: ist.psu.edu/faculty_pages/jjansen/academic/pubs/internetresearch2000.pdf. Sproat, Richard, and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In SIGHAN Workshop on Chinese Language Processing. Sproat, Richard, William Gale, Chilin Shih, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics 22 (3): 377-404. Sproat, Richard William. 1992. Morphology and computation. MIT Press. Stein, Benno, and Sven Meyer zu Eissen. 2004. Topic identification: Framework and application. In Proc. International Conference on Knowledge Management. Stein, Benno, Sven Meyer zu Eissen, and Frank Wißbrock. 2003. On cluster validity and the information need of users. In Proc. Artificial Intelligence and Applications. Steinbach, Michael, George Karypis, and Vipin Kumar. 2000. A comparison of document clustering techniques. In KDD Workshop on Text Mining. Strang, Gilberted.). 1986. Introduction to Applied Mathematics. Wellesley-Cambridge Press. Strehl, Alexander. 2002. Relationship-based Clustering and Cluster Ensembles for High-dimensional Data Mining. PhD thesis, The University of Texas at Austin. Strohman, Trevor, and W. Bruce Croft. 2007. Efficient document retrieval in main memory. In Proc. SIGIR, pp. 175-182. ACM Press. Swanson, Don R. 1988. Historical note: Information retrieval and the future of an illusion. JASIS 39 (2): 92-98. Tague-Sutcliffe, Jean, and James Blustein. 1995. A statistical analysis of the TREC-3 data. In Proc. TREC, pp. 385-398. Tan, Songbo, and Xueqi Cheng. 2007. Using hypothesis margin to boost centroid text classifier. In Proc. ACM Symposium on Applied Computing, pp. 398-403. ACM Press. DOI: doi.acm.org/10.1145/1244002.1244096. Tannier, Xavier, and Shlomo Geva. 2005. XML retrieval with a natural language interface. In Proc. SPIRE, pp. 29-40. Tao, Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language model information retrieval with document expansion. In Proc. Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics, pp. 407-414. Taube, Mortimer, and Harold Woostereds.). 1958. Information storage and retrieval: Theory, systems, and devices. Columbia University Press. Taylor, Michael, Hugo Zaragoza, Nick Craswell, Stephen Robertson, and Chris Burges. 2006. Optimisation methods for ranking functions with multiple parameters. In Proc. CIKM. ACM Press. Teh, Yee Whye, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association 101 (476): 1566-1581. Theobald, Martin, Holger Bast, Debapriyo Majumdar, Ralf Schenkel, and Gerhard Weikum. 2008. TopX: Efficient and versatile top- k query processing for semistructured data. VLDB Journal 17 (1): 81-115. Theobald, Martin, Ralf Schenkel, and Gerhard Weikum. 2005. An efficient and versatile query engine for TopX search. In Proc. VLDB, pp. 625-636. VLDB Endowment. Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society Series B 63: 411-423. Tishby, Naftali, and Noam Slonim. 2000. Data clustering by Markovian relaxation and the information bottleneck method. In Proc. NIPS, pp. 640-646. Toda, Hiroyuki, and Ryoji Kataoka. 2005. A search result clustering method using informatively named entities. In International Workshop on Web Information and Data Management, pp. 81-86. ACM Press. DOI: doi.acm.org/10.1145/1097047.1097063. Tomasic, Anthony, and Hector Garcia-Molina. 1993. Query processing and inverted indices in shared-nothing document information retrieval systems. VLDB Journal 2 (3): 243-275. Tombros, Anastasios, and Mark Sanderson. 1998. Advantages of query biased summaries in information retrieval. In Proc. SIGIR, pp. 2-10. ACM Press. DOI: doi.acm.org/10.1145/290941.290947. Tombros, Anastasios, Robert Villa, and Cornelis Joost van Rijsbergen. 2002. The effectiveness of query-specific hierarchic clustering in information retrieval. IP M 38 (4): 559-582. DOI: dx.doi.org/10.1016/S0306-4573(01)00048-6. Tomlinson, Stephen. 2003. Lexical and algorithmic stemming compared for 9 European languages with Hummingbird Searchserver at CLEF 2003. In Proc. Cross-Language Evaluation Forum, pp. 286-300. Tong, Simon, and Daphne Koller. 2001. Support vector machine active learning with applications to text classification. JMLR 2: 45-66. Toutanova, Kristina, and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proc. ACL, pp. 144-151. Treeratpituk, Pucktada, and Jamie Callan. 2006. An experimental study on automatically labeling hierarchical clusters using statistical features. In Proc. SIGIR, pp. 707-708. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148328. Trotman, Andrew. 2003. Compressing inverted files. IR 6 (1): 5-19. DOI: dx.doi.org/10.1023/A:1022949613039. Trotman, Andrew, and Shlomo Geva. 2006. Passage retrieval and other XML-retrieval tasks. In SIGIR 2006 Workshop on XML Element Retrieval Methodology, pp. 43-50. Trotman, Andrew, Shlomo Geva, and Jaap Kampseds.). 2007. SIGIR Workshop on Focused Retrieval. University of Otago. Trotman, Andrew, Nils Pharo, and Miro Lehtonen. 2006. XML-IR users and use cases. In Proc. INEX, pp. 400-412. Trotman, Andrew, and Börkur Sigurbjörnsson. 2004. Narrowed Extended XPath I (NEXI). In Fuhr et al. (2005), pp. 16-40. DOI: dx.doi.org/10.1007/11424550_2. Tseng, Huihsin, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In SIGHAN Workshop on Chinese Language Processing. Tsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. JMLR 6: 1453-1484. Turpin, Andrew, and William R. Hersh. 2001. Why batch and user evaluations do not give the same results. In Proc. SIGIR, pp. 225-231. Turpin, Andrew, and William R. Hersh. 2002. User interface effects in past batch versus user experiments. In Proc. SIGIR, pp. 431-432. Turpin, Andrew, Yohannes Tsegay, David Hawking, and Hugh E. Williams. 2007. Fast generation of result snippets in web search. In Proc. SIGIR, pp. 127-134. ACM Press. Turtle, Howard. 1994. Natural language vs. Boolean query evaluation: A comparison of retrieval performance. In Proc. SIGIR, pp. 212-220. ACM Press. Turtle, Howard, and W. Bruce Croft. 1989. Inference networks for document retrieval. In Proc. SIGIR, pp. 1-24. ACM Press. Turtle, Howard, and W. Bruce Croft. 1991. Evaluation of an inference network-based retrieval model. TOIS 9 (3): 187-222. Turtle, Howard, and James Flood. 1995. Query evaluation: strategies and optimizations. IP M 31 (6): 831-850. DOI: dx.doi.org/10.1016/0306-4573(95)00020-H. Vaithyanathan, Shivakumar, and Byron Dom. 2000. Model-based hierarchical clustering. In Proc. UAI, pp. 599-608. Morgan Kaufmann. van Rijsbergen, Cornelis Joost. 1979. Information Retrieval, 2nd edition. Butterworths. van Rijsbergen, Cornelis Joost. 1989. Towards an information logic. In Proc. SIGIR, pp. 77-86. ACM Press. DOI: doi.acm.org/10.1145/75334.75344. van Zwol, Roelof, Jeroen Baas, Herre van Oostendorp, and Frans Wiering. 2006. Bricks: The building blocks to tackle query formulation in structured document retrieval. In Proc. ECIR, pp. 314-325. Vapnik, Vladimir N. 1998. Statistical Learning Theory. Wiley-Interscience. Vittaut, Jean-Noël, and Patrick Gallinari. 2006. Machine learning ranking for structured information retrieval. In Proc. ECIR, pp. 338-349. Voorhees, Ellen M. 1985a. The cluster hypothesis revisited. In Proc. SIGIR, pp. 188-196. ACM Press. Voorhees, Ellen M. 1985b. The effectiveness and efficiency of agglomerative hierarchic clustering in document retrieval. Technical Report TR 85-705, Cornell. Voorhees, Ellen M. 2000. Variations in relevance judgments and the measurement of retrieval effectiveness. IP M 36: 697-716. Voorhees, Ellen M., and Donna Harmaneds.). 2005. TREC: Experiment and Evaluation in Information Retrieval. MIT Press. Wagner, Robert A., and Michael J. Fischer. 1974. The string-to-string correction problem. JACM 21 (1): 168-173. DOI: doi.acm.org/10.1145/321796.321811. Ward Jr., J. H. 1963. Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association 58: 236-244. Wei, Xing, and W. Bruce Croft. 2006. LDA-based document models for ad-hoc retrieval. In Proc. SIGIR, pp. 178-185. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148204. Weigend, Andreas S., Erik D. Wiener, and Jan O. Pedersen. 1999. Exploiting hierarchy in text categorization. IR 1 (3): 193-216. Weston, Jason, and Chris Watkins. 1999. Support vector machines for multi-class pattern recognition. In Proc. European Symposium on Artificial Neural Networks, pp. 219-224. Williams, Hugh E., and Justin Zobel. 2005. Searchable words on the web. International Journal on Digital Libraries 5 (2): 99-105. DOI: dx.doi.org/10.1007/s00799-003-0050-z. Williams, Hugh E., Justin Zobel, and Dirk Bahle. 2004. Fast phrase querying with combined indexes. TOIS 22 (4): 573-594. Witten, Ian H., and Timothy C. Bell. 1990. Source models for natural language text. International Journal Man-Machine Studies 32 (5): 545-579. Witten, Ian H., and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition. Morgan Kaufmann. Witten, Ian H., Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Compressing and Indexing Documents and Images, 2nd edition. Morgan Kaufmann. Wong, S. K. Michael, Yiyu Yao, and Peter Bollmann. 1988. Linear structure in information retrieval. In Proc. SIGIR, pp. 219-232. ACM Press. Woodley, Alan, and Shlomo Geva. 2006. NLPX at INEX 2006. In Proc. INEX, pp. 302-311. Xu, Jinxi, and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In Proc. SIGIR, pp. 4-11. ACM Press. Xu, Jinxi, and W. Bruce Croft. 1999. Cluster-based language models for distributed retrieval. In Proc. SIGIR, pp. 254-261. ACM Press. DOI: doi.acm.org/10.1145/312624.312687. Yang, Hui, and Jamie Callan. 2006. Near-duplicate detection by instance-level constrained clustering. In Proc. SIGIR, pp. 421-428. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148243. Yang, Yiming. 1994. Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. In Proc. SIGIR, pp. 13-22. ACM Press. Yang, Yiming. 1999. An evaluation of statistical approaches to text categorization. IR 1: 69-90. Yang, Yiming. 2001. A study of thresholding strategies for text categorization. In Proc. SIGIR, pp. 137-145. ACM Press. DOI: doi.acm.org/10.1145/383952.383975. Yang, Yiming, and Bryan Kisiel. 2003. Margin-based local regression for adaptive filtering. In Proc. CIKM, pp. 191-198. DOI: doi.acm.org/10.1145/956863.956902. Yang, Yiming, and Xin Liu. 1999. A re-examination of text categorization methods. In Proc. SIGIR, pp. 42-49. ACM Press. Yang, Yiming, and Jan Pedersen. 1997. Feature selection in statistical learning of text categorization. In Proc. ICML. Yue, Yisong, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A support vector method for optimizing average precision. In Proc. SIGIR. ACM Press. Zamir, Oren, and Oren Etzioni. 1999. Grouper: A dynamic clustering interface to web search results. In Proc. WWW, pp. 1361-1374. Elsevier North-Holland. DOI: dx.doi.org/10.1016/S1389-1286(99)00054-7. Zaragoza, Hugo, Djoerd Hiemstra, Michael Tipping, and Stephen Robertson. 2003. Bayesian extension to the language model for ad hoc information retrieval. In Proc. SIGIR, pp. 4-9. ACM Press. Zavrel, Jakub, Peter Berck, and Willem Lavrijssen. 2000. Information extraction by text classification: Corpus mining for features. In Workshop Information Extraction Meets Corpus Linguistics. URL: www.cnts.ua.ac.be/Publications/2000/ZBL00. Held in conjunction with LREC-2000. Zha, Hongyuan, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and Horst D. Simon. 2001. Bipartite graph partitioning and data clustering. In Proc. CIKM, pp. 25-32. Zhai, Chengxiang, and John Lafferty. 2001a. Model-based feedback in the language modeling approach to information retrieval. In Proc. CIKM. ACM Press. Zhai, Chengxiang, and John Lafferty. 2001b. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. SIGIR, pp. 334-342. ACM Press. Zhai, ChengXiang, and John Lafferty. 2002. Two-stage language models for information retrieval. In Proc. SIGIR, pp. 49-56. ACM Press. DOI: doi.acm.org/10.1145/564376.564387. Zhang, Jiangong, Xiaohui Long, and Torsten Suel. 2007. Performance of compressed inverted list caching in search engines. In Proc. CIKM. Zhang, Tong, and Frank J. Oles. 2001. Text categorization based on regularized linear classification methods. IR 4 (1): 5-31. URL: citeseer.ist.psu.edu/zhang00text.html. Zhao, Ying, and George Karypis. 2002. Evaluation of hierarchical clustering algorithms for document datasets. In Proc. CIKM, pp. 515-524. ACM Press. DOI: doi.acm.org/10.1145/584792.584877. Zipf, George Kingsley. 1949. Human Behavior and the Principle of Least Effort. Addison Wesley. Zobel, Justin. 1998. How reliable are the results of large-scale information retrieval experiments? In Proc. SIGIR, pp. 307-314. Zobel, Justin, and Philip Dart. 1995. Finding approximate matches in large lexicons. Software Practice and Experience 25 (3): 331-345. URL: citeseer.ifi.unizh.ch/zobel95finding.html. Zobel, Justin, and Philip Dart. 1996. Phonetic string matching: Lessons from information retrieval. In Proc. SIGIR, pp. 166-173. ACM Press. Zobel, Justin, and Alistair Moffat. 2006. Inverted files for text search engines. ACM Computing Surveys 38 (2). Zobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron Sacks-Davis. 1995. Efficient retrieval of partial documents. IP M 31 (3): 361-377. DOI: dx.doi.org/10.1016/0306-4573(94)00052-5. Zukowski, Marcin, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar RAM-CPU cache compression. In Proc. International Conference on Data Engineering, p. 59. IEEE Computer Society. DOI: dx.doi.org/10.1109/ICDE.2006.150.
iir_23	Index 1/0 loss The 1/0 loss case 11-point interpolated average precision Evaluation of ranked retrieval 20 Newsgroups Standard test collections feature selection Feature selectionChi2 Feature nearest neighbor classification k nearest neighbor -gram index k-gram indexes for wildcard -gram index k-gram indexes for spelling encoding Variable byte codes encoding Gamma codes encoding Gamma codes - codes Gamma codes codes Gamma codes - codes References and further reading distance Pivoted normalized document length A/B test Refining a deployed system Access control lists Other types of indexes | Other types of indexes | Other types of indexes | Other types of indexes accumulator Weighted zone scoring | Computing vector scores accuracy Evaluation of unranked retrieval active learning Choosing what kind of ad hoc retrieval An example information retrieval | Text classification and Naive Add-one smoothing Naive Bayes text classification | Naive Bayes text classification adjacency table Connectivity servers adversarial information retrieval Spam Akaike Information Criterion Cluster cardinality in K-means algorithmic search Advertising as the economic anchor text The web graph any-of classification The text classification problem | Classification with more than authority score Hubs and Authorities Auxiliary index Dynamic indexing | Dynamic indexing average-link clustering Group-average agglomerative clustering B-tree Search structures for dictionaries bag of words Term frequency and weighting | Properties of Naive Bayes bag-of-words Properties of Naive Bayes balanced F measure Evaluation of unranked retrieval Bayes error rate Time complexity and optimality Bayes Optimal Decision Rule The 1/0 loss case Bayes risk The 1/0 loss case Bayes' Rule Review of basic probability Bayesian networks Bayesian network approaches to Bayesian prior Probability estimates in theory Bernoulli model The Bernoulli model | The Bernoulli model | The Bernoulli model | Properties of Naive Bayes | A variant of the best-merge persistence Time complexity of HAC bias The bias-variance tradeoff bias-variance tradeoff Types of language models | Feature selection | The bias-variance tradeoff | Support vector machines: The biclustering References and further reading bigram language model Types of language models Binary Independence Model The Binary Independence Model binary tree Search structures for dictionaries | Hierarchical clustering biword index Biword indexes | Combination schemes blind relevance feedback see pseudo relevance feedback blocked sort-based indexing algorithm Blocked sort-based indexing Blocked sort-based indexing algorithm (BSBI) Blocked sort-based indexing | Blocked sort-based indexing | Other types of indexes blocked storage Blocked storage Blocked storage described Dictionary as a string | Blocked storage blog XML retrieval BM25 weights Okapi BM25: a non-binary boosting References and further reading bottom-up clustering see hierarchical agglomerative clustering bowtie The web graph Break-even Evaluation of text classification | Experimental results break-even point Evaluation of ranked retrieval BSBI Blocked sort-based indexing Buckshot algorithm Implementation notes Buffer Hardware basics | Hardware basics caching A first take at | Hardware basics | Putting it all together | Crawler architecture | DNS resolution compression and Index compression | Index compression defined Hardware basics capture-recapture method Index size and estimation cardinality in clustering Cardinality - the number CAS topics Evaluation of XML retrieval case-folding Capitalization/case-folding. Category The text classification problem | The text classification problem centroid Rocchio classification | K-means in relevance feedback The underlying theory. centroid-based classification References and further reading chain rule Review of basic probability chaining in clustering Single-link and complete-link clustering champion lists Tiered indexes class boundary Linear versus nonlinear classifiers Classes, defined The text classification problem | The text classification problem Classes, maximum a posteriori Naive Bayes text classification classification Text classification and Naive | Result ranking by machine Classification function The text classification problem | The text classification problem Classification, defined Text classification and Naive | Text classification and Naive classifier Probabilistic relevance feedback Classifiers, defined The text classification problem Classifiers, two-class Evaluation of text classification CLEF Standard test collections click spam Advertising as the economic clickstream mining Refining a deployed system | Indirect relevance feedback clickthrough log analysis Refining a deployed system clique Single-link and complete-link clustering cluster Distributed indexing | Flat clustering in relevance feedback When does relevance feedback cluster hypothesis Clustering in information retrieval cluster-based classification References and further reading cluster-internal labeling Cluster labeling Clusters defined Distributed indexing CO topics Evaluation of XML retrieval co-clustering References and further reading collection An example information retrieval collection frequency Dropping common terms: stop | Frequency-based feature selection Collections statistics, large Other types of indexes combination similarity Hierarchical agglomerative clustering | Single-link and complete-link clustering | Optimality of HAC | Optimality of HAC complete-link clustering Single-link and complete-link clustering complete-linkage clustering see complete-link clustering component coverage Evaluation of XML retrieval compound-splitter Tokenization compounds Tokenization Compression lossless / lossy Statistical properties of terms of dictionaries Zipf's law: Modeling the | Blocked storage of postings list Blocked storage | Gamma codes parameterized References and further reading Compression / indexes Heaps' law Statistical properties of terms | Heaps' law: Estimating the Zipf's law Heaps' law: Estimating the | Zipf's law: Modeling the | Zipf's law: Modeling the Concept drift Properties of Naive Bayes | Properties of Naive Bayes | Evaluation of text classification | References and further reading | Choosing what kind of conditional independence assumption Deriving a ranking function | Properties of Naive Bayes | Properties of Naive Bayes confusion matrix Classification with more than connected component Single-link and complete-link clustering connectivity queries Connectivity servers connectivity server Connectivity servers content management system References and further reading Content management systems References and further reading context XML Basic XML concepts context resemblance A vector space model contiguity hypothesis Vector space classification continuation bit Variable byte codes | Variable byte codes corpus An example information retrieval cosine similarity Dot products | References and further reading CPC Advertising as the economic CPM Advertising as the economic Cranfield Standard test collections cross-entropy Extended language modeling approaches cross-language information retrieval Standard test collections | References and further reading cumulative gain Evaluation of ranked retrieval data-centric XML XML retrieval | Text-centric vs. data-centric XML database relational Boolean retrieval | XML retrieval | Text-centric vs. data-centric XML Databases communication with References and further reading decision boundary Rocchio classification | Linear versus nonlinear classifiers decision hyperplane Vector space classification | Linear versus nonlinear classifiers Decision trees Evaluation of text classification | Evaluation of text classification | References and further reading dendrogram Hierarchical agglomerative clustering development set Evaluation of text classification Development sets Evaluation of text classification development test collection Information retrieval system evaluation Dice coefficient Evaluation of ranked retrieval dictionary An example information retrieval | A first take at differential cluster labeling Cluster labeling digital libraries XML retrieval Disk seek Hardware basics distortion Cluster cardinality in K-means distributed index Distributed indexing | Distributed indexing | References and further reading Distributed indexing Single-pass in-memory indexing | Distributed indexing | Distributed indexing distributed information retrieval see distributed crawling | References and further reading divisive clustering Divisive clustering DNS resolution DNS resolution DNS server DNS resolution docID A first take at document An example information retrieval | Choosing a document unit document collection see collection document frequency A first take at | Inverse document frequency | Frequency-based feature selection document likelihood model Extended language modeling approaches document partitioning Distributing indexes Document space The text classification problem | The text classification problem document vector Tf-idf weighting | The vector space model document-at-a-time Computing vector scores | Impact ordering document-partitioned index Distributed indexing dot product Dot products Dynamic indexing Distributed indexing East Asian languages References and further reading edit distance Edit distance effectiveness An example information retrieval | Evaluation of text classification Effectiveness, text classification Evaluation of text classification | Evaluation of text classification | Evaluation of text classification Efficiency Evaluation of text classification eigen decomposition Matrix decompositions eigenvalue Linear algebra review EM algorithm Model-based clustering email sorting Text classification and Naive Email, sorting Text classification and Naive enterprise resource planning References and further reading Enterprise search Index construction | Index construction Entropy Gamma codes | Gamma codes | References and further reading | Evaluation of clustering equivalence classes Normalization (equivalence classing of Ergodic Markov Chain Definition: Euclidean distance Pivoted normalized document length | References and further reading Euclidean length Dot products Evalution of retrieval systems, text classification Evaluation of text classification | Evaluation of text classification Evalution of retrieval systems, x Assessing as a evidence accumulation Designing parsing and scoring exclusive clustering A note on terminology. exhaustive clustering A note on terminology. expectation step Model-based clustering Expectation-Maximization algorithm Choosing what kind of | Model-based clustering expected edge density References and further reading extended query Challenges in XML retrieval Extensible Markup Language XML retrieval external criterion of quality Evaluation of clustering External sorting algorithm Blocked sort-based indexing | Blocked sort-based indexing false negative Evaluation of clustering false positive Evaluation of clustering feature engineering Features for text feature selection Feature selection Feature selection / text classification, greedy Comparison of feature selection Feature selection / text classification, method comparison Comparison of feature selection Feature selection / text classification, multiple classifiers Feature selection for multiple | Feature selection for multiple Feature selection / text classification, mutual information Mutual information Feature selection / text classification, noise feature Feature selection Feature selection / text classification, overfitting Feature selection Feature selection / text classification, overview Feature selection Feature selection / text classification, statistical significance Feature selectionChi2 Feature Feature selection / text classification, x Feature selectionChi2 Feature Feature selection/text classification, frequency-based Frequency-based feature selection | Frequency-based feature selection Feature selection/text classification, method comparison Comparison of feature selection | Comparison of feature selection Feature selection/text classification, mutual information Mutual information Feature selection/text classification, overview Feature selection Feature selection/text classification, x Feature selectionChi2 Feature field Parametric and zone indexes filtering Text classification and Naive | Text classification and Naive | References and further reading first story detection Optimality of HAC | References and further reading flat clustering Flat clustering focused retrieval References and further reading free text Scoring, term weighting and | Vector space scoring and free text query see query, free text | Computing vector scores | Designing parsing and scoring | XML retrieval frequency-based feature selection Frequency-based feature selection Frobenius norm Low-rank approximations Front coding Blocked storage | Blocked storage functional margin Support vector machines: The F measure Evaluation of unranked retrieval | References and further reading as an evaluation measure in clustering Evaluation of clustering GAAC Group-average agglomerative clustering generative model Finite automata and language | The bias-variance tradeoff | The bias-variance tradeoff geometric margin Support vector machines: The gold standard Information retrieval system evaluation Golomb codes References and further reading | References and further reading GOV2 Standard test collections greedy feature selection Comparison of feature selection grep An example information retrieval ground truth Information retrieval system evaluation group-average agglomerative clustering Group-average agglomerative clustering group-average clustering Group-average agglomerative clustering HAC Hierarchical agglomerative clustering hard assignment Flat clustering hard clustering Flat clustering | A note on terminology. harmonic number Gamma codes Harmonic numbers Gamma codes Hashing Blocked storage | Blocked storage Heaps' law Heaps' law: Estimating the held-out k nearest neighbor Held-out data Evaluation of text classification | Evaluation of text classification hierarchic clustering Hierarchical clustering hierarchical agglomerative clustering Hierarchical agglomerative clustering hierarchical classification Large and difficult category | References and further reading hierarchical clustering Flat clustering | Hierarchical clustering Hierarchical Dirichlet Processes References and further reading hierarchy in clustering Hierarchical clustering highlighting Challenges in XML retrieval HITS Hubs and Authorities HTML Background and history http Background and history hub score Hubs and Authorities hyphens Tokenization i.i.d. Evaluation of text classification | see independent and identically distributed Ide dec-hi The Rocchio (1971) algorithm. idf Other types of indexes | Challenges in XML retrieval | Probability estimates in practice | Okapi BM25: a non-binary iid see independent and identically distributed impact Other types of indexes implicit relevance feedback Indirect relevance feedback in-links The web graph | Link analysis incidence matrix An example information retrieval | Term-document matrices and singular Independence Feature selectionChi2 Feature | Feature selectionChi2 Feature independent and identically distributed Evaluation of text classification in clustering Cluster cardinality in K-means Independent and identically distributed ( IID ) Evaluation of text classification index An example information retrieval | see permuterm index | see alsoparametric index, zone index index construction Index construction resources References and further reading Indexer Index construction | Index construction indexing Index construction defined Index construction sort-based A first take at indexing granularity Choosing a document unit indexing unit Challenges in XML retrieval INEX Evaluation of XML retrieval Information gain Evaluation of text classification | Evaluation of text classification information need An example information retrieval | Information retrieval system evaluation information retrieval Boolean retrieval hardware issues Index construction | Hardware basics terms , statistical properties of Index compression | Zipf's law: Modeling the informational queries User query needs inner product Dot products instance-based learning Time complexity and optimality internal criterion of quality Evaluation of clustering interpolated precision Evaluation of ranked retrieval intersection postings list Processing Boolean queries inverse document frequency Inverse document frequency | Computing vector scores inversion Blocked sort-based indexing | Hierarchical agglomerative clustering | Centroid clustering Inversions defined Blocked sort-based indexing inverted file see inverted index inverted index An example information retrieval inverted list see postings list Inverter Distributed indexing | Distributed indexing | Distributed indexing IP address DNS resolution Jaccard coefficient k-gram indexes for spelling | Near-duplicates and shingling k nearest neighbor classification (kNN), multinomial Naive Bayes vs., 249.57 k nearest neighbor classification (kNN), as nonlinear classification Properties of Naive Bayes K-medoids K-means kappa statistic Assessing relevance | References and further reading | References and further reading kernel Nonlinear SVMs kernel function Nonlinear SVMs kernel trick Nonlinear SVMs key-value pairs Distributed indexing keyword-in-context Results snippets kNN classification k nearest neighbor Kruskal's algorithm References and further reading Kullback-Leibler divergence Extended language modeling approaches | Exercises | References and further reading KWIC see keyword-in-context label The text classification problem labeling Text classification and Naive Labeling, defined Text classification and Naive language Finite automata and language language identification Tokenization | References and further reading language model Finite automata and language Laplace smoothing Naive Bayes text classification Latent Dirichlet Allocation References and further reading latent semantic indexing Latent semantic indexing LDA References and further reading learning algorithm The text classification problem learning error The bias-variance tradeoff learning method The text classification problem lemma Stemming and lemmatization lemmatization Stemming and lemmatization lemmatizer Stemming and lemmatization length-normalization Dot products Levenshtein distance Edit distance lexicalized subtree A vector space model lexicon An example information retrieval likelihood Review of basic probability likelihood ratio Finite automata and language linear classifier Linear versus nonlinear classifiers | A simple example of linear problem Linear versus nonlinear classifiers linear separability Linear versus nonlinear classifiers link farms References and further reading link spam Spam | Link analysis LLRUN References and further reading LM Using query likelihood language Logarithmic merging Dynamic indexing | Dynamic indexing | Dynamic indexing lossless Statistical properties of terms lossy compression Statistical properties of terms low-rank approximation Low-rank approximations LSA Latent semantic indexing LSI as soft clustering Latent semantic indexing machine translation Types of language models | Using query likelihood language | Extended language modeling approaches machine-learned relevance Learning weights | A simple example of Macroaveraging Evaluation of text classification | Evaluation of text classification | Evaluation of text classification MAP Evaluation of ranked retrieval | Probability estimates in theory | Naive Bayes text classification Map phase Distributed indexing | Distributed indexing MapReduce Distributed indexing | Distributed indexing | Distributed indexing | Distributed indexing | References and further reading margin Support vector machines: The marginal relevance Critiques and justifications of marginal statistic Assessing relevance Master node Distributed indexing | Distributed indexing matrix decomposition Matrix decompositions maximization step Model-based clustering maximum a posteriori Probability estimates in theory | Properties of Naive Bayes maximum a posteriori class Naive Bayes text classification maximum likelihood estimate Probability estimates in theory | Naive Bayes text classification Maximum likelihood estimate ( MLE ) Naive Bayes text classification Maximum likelihood estimate (MLE) Mutual information maximum likelihood estimation Estimating the query generation Mean Average Precision see MAP medoid K-means memory capacity The bias-variance tradeoff memory-based learning Time complexity and optimality Mercator Crawling Mercer kernel Nonlinear SVMs merge postings Processing Boolean queries merge algorithm Processing Boolean queries metadata Tokenization | Parametric and zone indexes | Results snippets | Basic XML concepts | References and further reading | Spam microaveraging Evaluation of text classification minimum spanning tree References and further reading | Exercises minimum variance clustering References and further reading MLE see maximum likelihood estimate ModApte split Evaluation of text classification | Evaluation of text classification | References and further reading model complexity The bias-variance tradeoff | Cluster cardinality in K-means model-based clustering Model-based clustering monotonicity Hierarchical agglomerative clustering multiclass classification Classification with more than multiclass SVM References and further reading multilabel classification Classification with more than multimodal class Rocchio classification Multinomial Naive Bayes, random variable X / U Properties of Naive Bayes multinomial classification Classification with more than multinomial distribution Multinomial distributions over words Multinomial model Relation to multinomial unigram | Relation to multinomial unigram | The Bernoulli model | A variant of the multinomial Naive Bayes Naive Bayes text classification Multinomial Naive Bayes, in text classification Naive Bayes text classification Multinomial Naive Bayes, in text classification Relation to multinomial unigram Multinomial Naive Bayes, optimal classifier Properties of Naive Bayes Multinomial Naive Bayes, positional independence assumption Naive Bayes text classification | Properties of Naive Bayes Multinomial Naive Bayes, sparseness Naive Bayes text classification multinomial NB see multinomial Naive Bayes multivalue classification Classification with more than multivariate Bernoulli model The Bernoulli model mutual information Mutual information | Evaluation of clustering Naive Bayes assumption Deriving a ranking function named entity tagging XML retrieval | Features for text National Institute of Standards and Technology Standard test collections natural language processing Book organization and course | Stemming and lemmatization | Results snippets | References and further reading | Language modeling versus other | Model-based clustering navigational queries User query needs NDCG Evaluation of ranked retrieval nested elements Challenges in XML retrieval NEXI Basic XML concepts next word index Combination schemes Nibble Variable byte codes | Variable byte codes NLP see natural language processing NMI Evaluation of clustering noise document Linear versus nonlinear classifiers noise feature Properties of Naive Bayes | Feature selection nonlinear classifier Linear versus nonlinear classifiers nonlinear problem Linear versus nonlinear classifiers normal vector Rocchio classification normalized discounted cumulative gain Evaluation of ranked retrieval normalized mutual information Evaluation of clustering novelty detection Optimality of HAC NTCIR Standard test collections | References and further reading objective function Problem statement | K-means odds Review of basic probability odds ratio Deriving a ranking function Okapi weighting Okapi BM25: a non-binary one-of classification The text classification problem | Evaluation of text classification | Evaluation of text classification | Classification with more than optimal classifier Properties of Naive Bayes | The bias-variance tradeoff optimal clustering Optimality of HAC optimal learning method The bias-variance tradeoff ordinal regression Result ranking by machine out-links The web graph outlier K-means overfitting Feature selection | The bias-variance tradeoff Oxford English Dictionary Statistical properties of terms PageRank PageRank paid inclusion Spam parameter tuning Information retrieval system evaluation | References and further reading | References and further reading | References and further reading parameter tying Separate feature spaces for parameter-free compression Gamma codes parameterized compression References and further reading parametric index Parametric and zone indexes parametric search XML retrieval Parser Distributed indexing | Distributed indexing partition rule Review of basic probability partitional clustering A note on terminology. passage retrieval References and further reading patent databases XML retrieval perceptron algorithm References and further reading | References and further reading performance Evaluation of text classification permuterm index Permuterm indexes personalized PageRank Topic-specific PageRank phrase index Biword indexes phrase queries Positional postings and phrase | References and further reading phrase search The extended Boolean model pivoted document length normalization Pivoted normalized document length Pointwise mutual information Mutual information | References and further reading | References and further reading polychotomous Classification with more than polytomous classification Classification with more than polytope k nearest neighbor pooling Assessing relevance | References and further reading pornography filtering Text classification and Naive | Features for text Porter stemmer Stemming and lemmatization positional independence Properties of Naive Bayes positional index Positional indexes posterior probability Review of basic probability posting An example information retrieval | An example information retrieval | A first take at | Blocked sort-based indexing | Index compression Postings compression and Index compression in block sort-based indexing Blocked sort-based indexing postings list An example information retrieval power law Zipf's law: Modeling the | The web graph precision An example information retrieval | Evaluation of unranked retrieval precision at Evaluation of ranked retrieval precision-recall curve Evaluation of ranked retrieval prefix-free code Gamma codes Preprocessing, effects of Statistical properties of terms principal direction divisive partitioning References and further reading principal left eigenvector Markov chains prior probability Review of basic probability Probability Ranking Principle The 1/0 loss case probability vector Markov chains prototype Vector space classification proximity operator The extended Boolean model proximity weighting Query-term proximity pseudo relevance feedback Pseudo relevance feedback pseudocounts Probability estimates in theory pull model References and further reading purity Evaluation of clustering push model References and further reading Quadratic Programming Support vector machines: The query An example information retrieval free text The extended Boolean model | The extended Boolean model | Term frequency and weighting simple conjunctive Processing Boolean queries query expansion Query expansion query likelihood model Using query likelihood language query optimization Processing Boolean queries query-by-example Basic XML concepts | Language modeling versus other R-precision Evaluation of ranked retrieval | References and further reading Rand index Evaluation of clustering adjusted References and further reading random variable Review of basic probability random variable Properties of Naive Bayes random variable Properties of Naive Bayes random variable Properties of Naive Bayes Random variables, C Properties of Naive Bayes rank Linear algebra review Ranked Boolean retrieval Weighted zone scoring ranked retrieval Other types of indexes | References and further reading model The extended Boolean model Ranked retrieval models described Other types of indexes ranking SVM Result ranking by machine recall An example information retrieval | Evaluation of unranked retrieval Reduce phase Distributed indexing | Distributed indexing reduced SVD Term-document matrices and singular | Low-rank approximations regression Result ranking by machine regular expressions An example information retrieval | References and further reading regularization Soft margin classification relational database XML retrieval | Text-centric vs. data-centric XML relative frequency Probability estimates in theory relevance An example information retrieval | Information retrieval system evaluation relevance feedback Relevance feedback and pseudo residual sum of squares K-means results snippets Putting it all together retrieval model Boolean An example information retrieval Retrieval Status Value Deriving a ranking function retrieval systems Other types of indexes Reuters-21578 Standard test collections Reuters-21578 collection, text classification in Evaluation of text classification | Evaluation of text classification | Evaluation of text classification | Evaluation of text classification Reuters-RCV1 Blocked sort-based indexing | Standard test collections Reuters-RCV1 collection described Blocked sort-based indexing | Blocked sort-based indexing | References and further reading dictionary-as-a-string storage Dictionary compression | Dictionary as a string RF Relevance feedback and pseudo Robots Exclusion Protocol Crawler architecture ROC curve Evaluation of ranked retrieval Rocchio algorithm The Rocchio (1971) algorithm. Rocchio classification Rocchio classification Routing Text classification and Naive | Text classification and Naive | References and further reading RSS K-means rule of 30 Statistical properties of terms Rules in text classification Text classification and Naive | Text classification and Naive Scatter-Gather Clustering in information retrieval schema Basic XML concepts schema diversity Challenges in XML retrieval schema heterogeneity Challenges in XML retrieval search advertising Advertising as the economic search engine marketing Advertising as the economic Search Engine Optimizers Spam search result clustering Clustering in information retrieval search results Clustering in information retrieval security Other types of indexes | Other types of indexes seed K-means seek time Hardware basics Segment file Distributed indexing | Distributed indexing semi-supervised learning Choosing what kind of semistructured query XML retrieval semistructured retrieval Boolean retrieval | XML retrieval sensitivity Evaluation of ranked retrieval sentiment detection Text classification and Naive | Text classification and Naive Sequence model Properties of Naive Bayes | Properties of Naive Bayes shingling Near-duplicates and shingling single-label classification Classification with more than single-link clustering Single-link and complete-link clustering single-linkage clustering see single-link clustering single-pass in-memory indexing Single-pass in-memory indexing Single-pass in-memory indexing (SPIMI) Blocked sort-based indexing | Single-pass in-memory indexing | References and further reading singleton Hierarchical agglomerative clustering singleton cluster K-means singular value decomposition Term-document matrices and singular skip list Faster postings list intersection | References and further reading slack variables Soft margin classification SMART The Rocchio (1971) algorithm. smoothing Maximum tf normalization | Probability estimates in theory add Probability estimates in theory add Probability estimates in theory add Probability estimates in theory add Probabilistic approaches to relevance add Probabilistic approaches to relevance add Okapi BM25: a non-binary add Relation to multinomial unigram Bayesian prior Probability estimates in theory | Probabilistic approaches to relevance | Estimating the query generation linear interpolation Estimating the query generation snippet Results snippets soft assignment Flat clustering soft clustering Flat clustering | A note on terminology. | Hierarchical clustering Sort-based multiway merge References and further reading sorting in index construction A first take at soundex Phonetic correction spam Features for text | Spam email Text classification and Naive web Text classification and Naive sparseness Types of language models | Estimating the query generation | Naive Bayes text classification specificity Evaluation of ranked retrieval spectral clustering References and further reading speech recognition Types of language models spelling correction Putting it all together | Types of language models | Multinomial distributions over words spider Overview spider traps Index size and estimation SPIMI Single-pass in-memory indexing splits Distributed indexing sponsored search Advertising as the economic Standing query Text classification and Naive | Text classification and Naive static quality scores Static quality scores and static web pages Web characteristics statistical significance Feature selectionChi2 Feature Statistical text classification Text classification and Naive | Text classification and Naive steady-state Definition: | The PageRank computation stemming Stemming and lemmatization | References and further reading stochastic matrix Markov chains stop list Dropping common terms: stop stop words Term frequency and weighting stop words Tokenization | Dropping common terms: stop | Combination schemes | Term frequency and weighting | Maximum tf normalization structural SVM Result ranking by machine structural SVMs Multiclass SVMs structural term A vector space model structured document retrieval principle Challenges in XML retrieval structured query XML retrieval structured retrieval XML retrieval | XML retrieval summarization References and further reading summary dynamic Results snippets static Results snippets Supervised learning The text classification problem | The text classification problem support vector Support vector machines: The support vector machine Support vector machines and | References and further reading multiclass Multiclass SVMs Support vector machines ( SVMs ) , effectiveness Evaluation of text classification SVD References and further reading | References and further reading | Term-document matrices and singular SVM see support vector machine symmetric diagonal decomposition Matrix decompositions | Term-document matrices and singular | Term-document matrices and singular synonymy Relevance feedback and query teleport PageRank term An example information retrieval | The term vocabulary and | Tokenization term frequency The extended Boolean model | Term frequency and weighting term normalization Normalization (equivalence classing of term partitioning Distributing indexes term-at-a-time Computing vector scores | Impact ordering term-document matrix Dot products term-partitioned index Distributed indexing termID Blocked sort-based indexing Test data The text classification problem | The text classification problem test set The text classification problem | Evaluation of text classification text categorization Text classification and Naive text classification Text classification and Naive Text classification, defined Text classification and Naive Text classification, feature selection Feature selection | Comparison of feature selection Text classification, overview The text classification problem | The text classification problem Text classification, vertical search engines Text classification and Naive text summarization Results snippets text-centric XML Text-centric vs. data-centric XML tf see term frequency tf-idf Tf-idf weighting tiered indexes Tiered indexes token The term vocabulary and | Tokenization token normalization Normalization (equivalence classing of top docs References and further reading top-down clustering Divisive clustering topic Standard test collections | Text classification and Naive in XML retrieval Evaluation of XML retrieval topic classification Text classification and Naive topic spotting Text classification and Naive topic-specific PageRank Topic-specific PageRank topical relevance Evaluation of XML retrieval training set The text classification problem | Evaluation of text classification transactional query User query needs transductive SVMs Choosing what kind of translation model Extended language modeling approaches TREC Standard test collections | References and further reading trec_eval References and further reading truecasing Capitalization/case-folding. | References and further reading truncated SVD Term-document matrices and singular | Low-rank approximations | Latent semantic indexing two-class classifier Evaluation of text classification type Tokenization unary code Gamma codes unigram language model Types of language models union-find algorithm Optimality of HAC | Near-duplicates and shingling universal code Gamma codes unsupervised learning Flat clustering URL Background and history URL normalization Crawler architecture Utility measure References and further reading | References and further reading Variable byte encoding Postings file compression | Variable byte codes | Variable byte codes variance The bias-variance tradeoff vector space model The vector space model vertical search engine Text classification and Naive vocabulary An example information retrieval Voronoi tessellation k nearest neighbor Ward's method References and further reading web crawler Overview weight vector Support vector machines: The weighted zone scoring Parametric and zone indexes Wikipedia Evaluation of XML retrieval wildcard query An example information retrieval | Dictionaries and tolerant retrieval | Wildcard queries within-point scatter Exercises word segmentation Tokenization XML Obtaining the character sequence | XML retrieval XML attribute Basic XML concepts XML DOM Basic XML concepts XML DTD Basic XML concepts XML element Basic XML concepts XML fragment References and further reading XML Schema Basic XML concepts XML tag Basic XML concepts XPath Basic XML concepts Zipf's law Zipf's law: Modeling the zone Parametric and zone indexes | Improving classifier performance | Document zones in text | Connections to text summarization. zone index Parametric and zone indexes zone search XML retrieval
iir_2_1_1	Obtaining the character sequence in a document Digital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more complex. The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards. We need to determine the correct encoding. This can be regarded as a machine learning classification problem, as discussed in Chapter 13 ,but is often handled by heuristic methods, user selection, or by using provided document metadata. Once the encoding is determined, we decode the byte sequence to a character sequence. We might save the choice of encoding because it gives some evidence about what language the document is written in. The characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files. Again, we must determine the document format, and then an appropriate decoder has to be used. Even for plain text documents, additional decoding may need to be done. In XML documents xmlbasic, character entities, such as  amp;, need to be decoded to give the correct character, namely   for  amp;. Finally, the textual part of the document may need to be extracted out of other material that will not be processed. This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files. We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters. Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is. Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk. This problem is usually solved by licensing a software library that handles decoding document formats and character encodings. The idea that text is a linear sequence of characters is also called into question by some writing systems, such as Arabic, where text takes on some two dimensional and mixed order characteristics, as shown in and 2.2 . But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1 .  An example of a vocalized Modern Standard Arabic word.The writing is from right to left and letters undergo complex mutations as they are combined. The representation of short vowels (here, /i/ and /u/) and the final /n/ (nunation) departs from strict linearity by being represented as diacritics above and below letters. Nevertheless, the represented text is still clearly a linear ordering of characters representing sounds. Full vocalization, as here, normally appears only in the Koran and children's books. Day-to-day text is unvocalized (short vowels are not represented but the letter for a would still appear) or partially vocalized, with short vowels inserted in places where the writer perceives ambiguities. These choices add further complexities to indexing.  The conceptual linear order of characters is not necessarily the order that you see on the page. In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts. With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings.
iir_2_1_2	Choosing a document unit The next phase is to determine what the document unit for indexing is. Thus far we have assumed that documents are fixed units for the purposes of indexing. For example, we take each file in a folder as a document. But there are many cases in which you might want to do something different. A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document. Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents. If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document. Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a LATEX document) and split them into separate HTML pages for each slide or subsection, stored as separate files. In these cases, you might want to combine multiple files into a single document. More generally, for very long documents, the issue of indexing granularity arises. For a collection of books, it would usually be a bad idea to index an entire book as a document. A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query. Instead, we may well wish to index each chapter or paragraph as a mini-document. Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document. But why stop there? We could treat individual sentences as mini-documents. It becomes clear that there is a precisionrecall tradeoff here. If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find. The problems with large document units can be alleviated by use of explicit or implicit proximity search ( and 7.2.2 ), and the tradeoffs in resulting system performance that we are hinting at are discussed in Chapter 8 . The issue of index granularity, and in particular a need to simultaneously index documents at multiple levels of granularity, appears prominently in XML retrieval, and is taken up again in Chapter 10 . An IR system should be designed to offer choices of granularity. For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns. For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed.
iir_2_2	Determining the vocabulary of terms   Subsections Tokenization Dropping common terms: stop words Normalization (equivalence classing of terms) Accents and diacritics. Capitalization/case-folding. Other issues in English. Other languages. Stemming and lemmatization
iir_2_2_1	Tokenization Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization: Input: Friends, Romans, Countrymen, lend me your ears; Output:   token   type  term 2.2.3  2.2.2  sleep perchance dream The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions? Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing. O'Neill ? aren't ?      These issues of tokenization are language-specific. It thus requires the language of the document to be known. Language identification based on classifiers that use short character subsequences as features is highly effective; most languages have distinctive signature patterns (see page 2.5 for references). For most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms, such as the programming languages C++ and C#, aircraft names like B-52, or a T.V. show name such as M*A*S*H - which is sufficiently integrated into popular culture that you find usages such as M*A*S*H-style hospitals. Computer technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token, including email addresses (jblack@mail.yahoo.com), web URLs (http://stuff.big.com/new/specials.html), numeric IP addresses (142.32.48.231), package tracking numbers (1Z9999W99845399981), and more. One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary. However, this comes at a large cost in restricting what people can search for. For instance, people might want to search in a bug database for the line number where an error occurs. Items such as the date of an email, which have a clear semantic type, are often indexed separately as document metadata parametricsection. In English, hyphenation is used for various purposes ranging from splitting up vowels in words (co-education) to joining nouns as names (Hewlett-Packard) to a copyediting device to show word grouping (the hold-him-back-and-drag-him-away maneuver). It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation), the last should be separated into words, and that the middle case is unclear. Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms. Conceptually, splitting on white space can also split what should be regarded as a single token. This occurs most commonly with names (San Francisco, Los Angeles) but also with borrowed foreign phrases (au fait) and compounds that are sometimes written as a single word and sometimes space separated (such as white space vs. whitespace). Other cases with internal spaces that we might wish to regard as a single token include phone numbers ((800) 234-2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing New York University. The problems of hyphens and non-separating whitespace can even interact. Advertisements for air fares frequently contain items like San Francisco-Los Angeles, where simply doing whitespace splitting would give unfortunate results. In such cases, issues of tokenization interact with handling phrase queries (which we discuss in Section 2.4 (page )), particularly if we would like queries for all of lowercase, lower-case and lower case to return the same results. The last two can be handled by splitting on hyphens and using a phrase index. Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way. One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (westlaw), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR ``over eager'' OR overeager. However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization. Each new language presents some new issues. For instance, French has a variant use of the apostrophe for a reduced definite article the before a word beginning with a vowel (e.g., l'ensemble) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g., donne-moi give me). Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both l'ensemble and un ensemble to be indexed under ensemble. Other languages make the problem harder in new ways. German writes compound nouns without spaces (e.g., Computerlinguistik `computational linguistics'; Lebensversicherungsgesellschaftsangestellter `life insurance company employee'). Retrieval systems for German greatly benefit from the use of a compound-splitter module, which is usually implemented by seeing if a word can be subdivided into multiple words that appear in a vocabulary. This phenomenon reaches its limit case with major East Asian Languages (e.g., Chinese, Japanese, Korean, and Thai), where text is written without any spaces between words. An example is shown in Figure 2.3 . One approach here is to perform word segmentation as prior linguistic processing. Methods of word segmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine learning sequence models, such as hidden Markov models or conditional random fields, trained over hand-segmented words (see the references in Section 2.5 ). Since there are multiple possible segmentations of character sequences (see Figure 2.4 ), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization. The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character -grams), regardless of whether particular sequences cross word boundaries or not. Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway. Even in English, some cases of where to put word boundaries are just orthographic conventions - think of notwithstanding vs. not to mention or into vs. on to - but people are educated to write the words with consistent use of spaces.  The standard unsegmented form of Chinese text using the simplified characters of mainland China.There is no whitespace between words, not even between sentences - the apparent space after the Chinese period ( ) is just a typographical illusion caused by placing the character on the left side of its square box. The first sentence is just words in Chinese characters with no spaces between them. The second and third sentences include Arabic numerals and punctuation breaking up the Chinese characters.  Ambiguities in Chinese word segmentation.The two characters can be treated as one word meaning `monk' or as a sequence of two words meaning `and' and `still'.
iir_2_2_2	Dropping common terms: stop words  Figure 2.5: A stop list of 25 semantically non-selective words which are common in Reuters-RCV1. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . The general strategy for determining a stop list is to sort the terms by collection frequency (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a stop list , the members of which are then discarded during indexing. An example of a stop list is shown in Figure 2.5 . Using a stop list significantly reduces the number of postings that a system has to store; we will present some statistics on this in Chapter 5 (see Table 5.1 , page 5.1 ). And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don't seem very useful. However, this is not true for phrase searches. The phrase query ``President of the United States'', which contains two stop words, is more precise than President AND ``United States''. The meaning of flights to London is likely to be lost if the word to is stopped out. A search for Vannevar Bush's article As we may think will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think. Some special query types are disproportionately affected. Some song titles and well known pieces of verse consist entirely of words that are commonly on stop lists (To be or not to be, Let It Be, I don't want to be, ...). The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists. Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways. We will show in Section 5.3 (page ) how good compression techniques greatly reduce the cost of storing the postings for common words. idf then discusses how standard term weighting leads to very common words having little impact on document rankings. Finally, Section 7.1.5 (page ) shows how an IR system with impact-sorted indexes can terminate scanning a postings list early when weights get small, and hence common words do not cause a large additional processing cost for the average query, even though postings lists for stop words are very long. So for most modern IR systems, the additional cost of including stop words is not that big - neither in terms of index size nor in terms of query processing time.
iir_2_2_3	Normalization (equivalence classing of terms) Having broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document. However, there are many cases when two character sequences are not quite the same but you would like a match to occur. For instance, if you search for USA, you might hope to also match documents containing U.S.A. Token normalization is the process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens. The most standard way to normalize is to implicitly create equivalence classes , which are normally named after one member of the set. For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either. The advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes. It is only easy to write rules of this sort that remove characters. Since the equivalence classes are implicit, it is not obvious when you might want to add characters. For instance, it would be hard to know to turn antidiscriminatory into anti-discriminatory.  Figure 2.6: An example of how asymmetric expansion of query terms can usefully model users' expectations. An alternative to creating equivalence classes is to maintain relations between unnormalized tokens. This method can be extended to hand-constructed lists of synonyms such as car and automobile, a topic we discuss further in Chapter 9 . These term relationships can be achieved in two ways. The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term. A query term is then effectively a disjunction of several postings lists. The alternative is to perform the expansion during index construction. When the document contains automobile, we index it under car as well (and, usually, also vice-versa). Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge. The first method adds a query expansion dictionary and requires more processing at query time, while the second method requires more space for storing postings. Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing. These approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical. This means there can be an asymmetry in expansion. An example of how such an asymmetry can be exploited is shown in Figure 2.6 : if the user enters windows, we wish to allow matches with the capitalized Windows operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase windows. The best amount of equivalence classing or query expansion to do is a fairly open question. Doing some definitely seems a good idea. But doing a lot can easily have unexpected consequences of broadening queries in unintended ways. For instance, equivalence-classing U.S.A. and USA to the latter by deleting periods from tokens might at first seem very reasonable, given the prevalent pattern of optional use of periods in acronyms. However, if I put in as my query term C.A.T., I might be rather upset if it matches every appearance of the word cat in documents. Below we present some of the forms of normalization that are commonly employed and how they are implemented. In many cases they seem helpful, but they can also do harm. In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance.   Subsections Accents and diacritics. Capitalization/case-folding. Other issues in English. Other languages.
iir_2_2_4	Stemming and lemmatization For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: am, are, is be car, cars, car's, cars' car the boy's cars are different colors the boy car be differ color  Stemming  Lemmatization  lemma saw s see saw The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter's algorithm (Porter, 1980). The entire algorithm is too long and intricate to present here, but we will indicate its general nature. Porter's algorithm consists of 5 phases of word reductions, applied sequentially. Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix. In the first phase, this convention is used with the following rule group: Many of the later rules use a concept of the measure of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word. For example, the rule: ( )    EMENT     replacement replac cement c http://www.tartarus.org/~martin/PorterStemmer/  Figure 2.8: A comparison of three stemming algorithms on a sample text. Other stemmers exist, including the older, one-pass Lovins stemmer (Lovins, 1968), and newer entrants like the Paice/Husk stemmer (Paice, 1990); see: http://www.cs.waikato.ac.nz/~eibe/stemmers/ http://www.comp.lancs.ac.uk/computing/research/stemming/ 2.8 Rather than using a stemmer, you can use a lemmatizer , a tool from Natural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval. It is hard to say more, because either form of normalization tends not to improve English information retrieval performance in aggregate - at least not by very much. While it helps a lot for some queries, it equally hurts performance a lot for others. Stemming increases recall while harming precision. As an example of what can go wrong, note that the Porter stemmer stems all of the following words: operate operating operates operation operative operatives operational operate operational and research operating and system operative and dentistry operate system The situation is different for languages with much more morphology (such as Spanish, German, and Finnish). Results in the European CLEF evaluations have repeatedly shown quite large gains from the use of stemmers (and compound splitting for languages like German); see the references in Section 2.5 . Exercises. Are the following statements true or false? In a Boolean retrieval system, stemming never lowers precision. In a Boolean retrieval system, stemming never lowers recall. Stemming increases the size of the vocabulary. Stemming should be invoked at indexing time but not while processing a query. Suggest what normalized form should be used for these words (including the word itself as a possibility): 'Cos Shi'ite cont'd Hawai'i O'Rourke The following pairs of words are stemmed to the same form by the Porter stemmer. Which pairs would you argue shouldn't be conflated. Give your reasoning. abandon/abandonment absorbency/absorbent marketing/markets university/universe volume/volumes For the Porter stemmer rule group shown in porter-rule-group: What is the purpose of including an identity rule such as SS  SS? Applying just this rule group, what will the following words be stemmed to? circus canaries boss What rule should be added to correctly stem pony? The stemming for ponies and pony might seem strange. Does it have a deleterious effect on retrieval? Why or why not?
iir_2_3	Faster postings list intersection via skip pointers In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3 (page ): we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are and , the intersection takes operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn't changing too fast. One way to do this is to use a skip list by augmenting postings lists with skip pointers (at indexing time), as shown in Figure 2.9 . Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers.  Postings lists with skip pointers.The postings intersection can use a skip pointer when the end point is still less than the item on the other list.  Figure 2.10: Postings lists intersection with skip pointers. Consider first efficient merging, with Figure 2.9 as an example. Suppose we've stepped through the lists in the figure until we have matched on each list and moved it to the results list. We advance both pointers, giving us on the upper list and on the lower list. The smallest item is then the element on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to . We thus avoid stepping to and on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown in Figure 2.10 . Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries. Where do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length , use evenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms. Building effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective. Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you're running a search engine with everything in memory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 (page ) and the impact of index size on system speed in Chapter 5 . Exercises. Why are skip pointers not useful for queries of the form OR ? We have a two-word query. For one term the postings list consists of the following 16 entries: [4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180] and for the other it is the one entry postings list: [47]. Work out how many comparisons would be done to intersect the two postings lists with the following two strategies. Briefly justify your answers: Using standard postings lists Using postings lists stored with skip pointers, with a skip length of , as suggested in Section 2.3 . Consider a postings intersection between this postings list, with skip pointers: xunit=0.6cm,arcangle=30 and the following intermediate result postings list (which hence has no skip pointers): 3    5    89    95    97    99    100    101 Trace through the postings intersection algorithm in Figure 2.10 (page ). How often is a skip pointer followed (i.e., is advanced to )? How many postings comparisons will be made by this algorithm while intersecting the two lists? How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers?
iir_2_4	Positional postings and phrase queries Many complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university. is not a match. Most recent search engines support a double quotes syntax (``stanford university'') for phrase queries , which has proven to be very easily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 (page ) in the context of ranked retrieval.   Subsections Biword indexes Positional indexes Positional index size. Combination schemes
iir_2_4_1	Biword indexes One approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase. For example, the text Friends, Romans, Countrymen would generate the biwords : friends romans romans countrymen ``stanford university'' AND ``university palo'' AND ``palo alto'' Among possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for. But related nouns can often be divided from each other by various function words, in phrases such as the abolition of slavery or renegotiation of the constitution. These needs can be incorporated into the biword indexing model in the following way. First, we tokenize the text and perform part-of-speech-tagging.We can then group terms into nouns, including proper nouns, (N) and function words, including articles and prepositions, (X), among other classes. Now deem any string of terms of the form NX*N to be an extended biword. Each such extended biword is made a term in the vocabulary. For example: renegotiation of the constitution N X X N This algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries. Using the above algorithm, the query cost overruns on a power plant ``cost overruns'' AND ``overruns power'' AND ``power plant'' The concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a phrase index . Indeed, searches for a single term are not naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms. While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed. But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size. Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary. However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme.
iir_2_4_2	Positional indexes For the reasons given, a biword index is not the standard solution. Rather, a positional index is most commonly employed. Here, for each term in the vocabulary, we store postings of the form docID: position1, position2, ..., as shown in Figure 2.11 , where each position is a token index in the document. Each posting will also usually record the term frequency, for reasons discussed in Chapter 6 .   To process a phrase query, you still need to access the inverted index entries for each distinct term. As before, you would start with the least frequent term and then work to further restrict the list of possible candidates. In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated. This requires working out offsets between the words. Worked example. Satisfying phrase queries.phrasequery Suppose the postings lists for to and be are as in Figure 2.11 , and the query is ``to be or not to be''. The postings lists to access are: to, be, or, not. We will examine intersecting the postings lists for to and be. We first look for documents that contain both terms. Then, we look for places in the lists where there is an occurrence of be with a token index one higher than a position of to, and then we look for another occurrence of each word with token index 4 higher than the first occurrence. In the above lists, the pattern of occurrences that is a possible match is: to: ...; 4: ...,429,433 ; ... be: ...; 4: ...,430,434 ; ... End worked example.   The same general method is applied for within word proximity searches, of the sort we saw in westlaw: employment /3 place   2.12  2.4.3   Subsections Positional index size.
iir_2_4_3	Combination schemes The strategies of biword indexes and positional indexes can be fruitfully combined. If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists. A combination strategy uses a phrase index, or just a biword index , for certain queries and uses a positional index for other phrase queries. Good queries to include in the phrase index are ones known to be common based on recent querying behavior. But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare. Adding Britney Spears as a phrase index entry may only give a speedup factor to that query of about 3, since most documents that mention either word are valid results, whereas adding The Who as a phrase index entry may speed up that query by a factor of 1000. Hence, having the latter is more desirable, even if it is a relatively less common query. Williams et al. (2004) evaluate an even more sophisticated scheme which employs indexes of both these sorts and additionally a partial next word index as a halfway house between the first two strategies. For each term, a next word index records terms that follow it in a document. They conclude that such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone. Exercises. Assume a biword index. Give an example of a document which will be returned for a query of New York University but is actually a false positive which should not be returned. Shown below is a portion of a positional index in the format: term: doc1: position1, position2, ...; doc2: position1, position2, ...; etc. angels: 2: 36,174,252,651 ; 4: 12,22,102,432 ; 7: 17 ; fools: 2: 1,17,74,222 ; 4: 8,78,108,458 ; 7: 3,13,23,193 ; fear: 2: 87,704,722,901 ; 4: 13,43,113,433 ; 7: 18,328,528 ; in: 2: 3,37,76,444,851 ; 4: 10,20,110,470,500 ; 7: 5,15,25,195 ; rush: 2: 2,66,194,321,702 ; 4: 9,69,149,429,569 ; 7: 4,14,404 ; to: 2: 47,86,234,999 ; 4: 14,24,774,944 ; 7: 199,319,599,709 ; tread: 2: 57,94,333 ; 4: 15,35,155 ; 7: 20,320 ; where: 2: 67,124,393,1001 ; 4: 11,41,101,421,431 ; 7: 16,36,736 ; Which document(s) if any match each of the following queries, where each expression within quotes is a phrase query? ``fools rush in'' ``fools rush in'' AND ``angels fear to tread'' Consider the following fragment of a positional index with the format: word: document: position, position, ; document: position, ... Gates: 1: 3 ; 2: 6 ; 3: 2,17 ; 4: 1 ; IBM: 4: 3 ; 7: 14 ; Microsoft: 1: 1 ; 2: 1,21 ; 3: 3 ; 5: 16,22,51 ; The / operator, word1 / word2 finds occurrences of word1 within words of word2 (on either side), where is a positive integer argument. Thus demands that word1 be adjacent to word2. Describe the set of documents that satisfy the query Gates /2 Microsoft. Describe each set of values for for which the query Gates / Microsoft returns a different set of documents as the answer. Consider the general procedure for merging two positional postings lists for a given document, to determine the document positions where a document satisfies a / clause (in general there can be multiple positions at which each term occurs in a single document). We begin with a pointer to the position of occurrence of each term and move each pointer along the list of occurrences in the document, checking as we do so whether we have a hit for /. Each move of either pointer counts as a step. Let denote the total number of occurrences of the two terms in the document. What is the big-O complexity of the merge procedure, if we wish to have postings including positions in the result? Consider the adaptation of the basic algorithm for intersection of two postings lists postings-merge-algorithm to the one in Figure 2.12 (page ), which handles proximity queries. A naive algorithm for this operation could be , where is the sum of the lengths of the postings lists (i.e., the sum of document frequencies) and is the maximum length of a document (in tokens). Go through this algorithm carefully and explain how it works. What is the complexity of this algorithm? Justify your answer carefully. For certain queries and data distributions, would another algorithm be more efficient? What complexity does it have? Suppose we wish to use a postings intersection procedure to determine simply the list of documents that satisfy a / clause, rather than returning the list of positions, as in Figure 2.12 (page ). For simplicity, assume . Let denote the total number of occurrences of the two terms in the document collection (i.e., the sum of their collection frequencies). Which of the following is true? Justify your answer. The merge can be accomplished in a number of steps linear in and independent of , and we can ensure that each pointer moves only to the right. The merge can be accomplished in a number of steps linear in and independent of , but a pointer may be forced to move non-monotonically (i.e., to sometimes back up) The merge can require steps in some cases. How could an IR system combine use of a positional index and use of stop words? What is the potential problem, and how could it be handled?
iir_2_5	References and further reading Exhaustive discussion of the character-level processing of can be found in Lunde (1998). Character bigram indexes are perhaps the most standard approach to indexing Chinese, although some systems use word segmentation. Due to differences in the language and writing system, word segmentation is most usual for Japanese (Luk and Kwok, 2002, Kishida et al., 2005). The structure of a character -gram index over unsegmented text differs from that in Section 3.2.2 (page ): there the -gram dictionary points to postings lists of entries in the regular dictionary, whereas here it points directly to document postings lists. For further discussion of Chinese word segmentation, see Tseng et al. (2005), Sproat and Emerson (2003), Sproat et al. (1996), and Gao et al. (2005). Lita et al. (2003) present a method for truecasing . Natural language processing work on computational morphology is presented in (Sproat, 1992, Beesley and Karttunen, 2003). Language identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level -gram language identification algorithm. While other methods such as looking for particular distinctive function words and letter combinations have been used, with the advent of widespread digital text, many people have explored the character -gram technique, and found it to be highly successful (Beesley, 1998, Dunning, 1994, Cavnar and Trenkle, 1994). Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al. (2006) for a recent survey. Experiments on and discussion of the positive and negative impact of stemming in English can be found in the following works: Salton (1989), Krovetz (1995), Hull (1996), Harman (1991). Hollink et al. (2004) provide detailed results for the effectiveness of language-specific methods on 8 European languages. In terms of percent change in mean average precision (see page 8.4 ) over a baseline system, diacritic removal gains up to 23% (being especially helpful for Finnish, French, and Swedish). Stemming helped markedly for Finnish (30% improvement) and Spanish (10% improvement), but for most languages, including English, the gain from stemming was in the range 0-5%, and results from a lemmatizer were poorer still. Compound splitting gained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather than language-particular methods, indexing character -grams (as we suggested for Chinese) could often give as good or better results: using within-word character 4-grams rather than words gave gains of 37% in Finnish, 27% in Swedish, and 20% in German, while even being slightly positive for other languages, such as Dutch, Spanish, and English. Tomlinson (2003) presents broadly similar results. Bar-Ilan and Gutman (2005) suggest that, at the time of their study (2003), the major commercial web search engines suffered from lacking decent language-particular processing; for example, a query on www.google.fr for l'électricité did not separate off the article l' but only matched pages with precisely this string of article+noun. The classic presentation of for IR can be found in Moffat and Zobel (1996). Extended techniques are discussed in Boldi and Vigna (2005). The main paper in the algorithms literature is Pugh (1990), which uses multilevel skip pointers to give expected list access (the same expected efficiency as using a tree data structure) with less implementational complexity. In practice, the effectiveness of using skip pointers depends on various system parameters. Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al. (2002, p. 217) report that, with modern CPUs, using skip lists instead slows down search because it expands the size of the postings list (i.e., disk I/O dominates performance). In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs. Johnson et al. (2006) report that 11.7% of all queries in two 2002 web query logs contained phrase queries , though Kammenhuber et al. (2006) report only 3% phrase queries for a different data set. Silverstein et al. (1999) note that many queries without explicit phrase operators are actually implicit phrase searches.
iir_3	Dictionaries and tolerant retrieval In Chapters 1 2 we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a wildcard query : a query such as *a*e*i*o*u*, which seeks documents containing any term that includes all the five vowels in sequence. The * symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated. We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3 . Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection. Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1 2 , in which each vocabulary term has a postings list with the documents in the collection.   Subsections Search structures for dictionaries Wildcard queries General wildcard queries Permuterm indexes k-gram indexes for wildcard queries Spelling correction Implementing spelling correction Forms of spelling correction Edit distance k-gram indexes for spelling correction Context sensitive spelling correction Phonetic correction References and further reading
iir_3_1	Search structures for dictionaries Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the corresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot - and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain. At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2 . Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years' time.  A binary search tree.In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest. Search trees overcome many of these issues - for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the binary tree , in which each internal node has two children. The search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is ) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained. To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the B-tree - a search tree in which every internal node has a number of children in the interval , where and are appropriate positive integers; Figure 3.2 shows an example with and . Each branch under an internal node again represents a test for a range of character sequences, as in the binary tree example of Figure 3.1 . A B-tree may be viewed as ``collapsing'' multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers and are determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees.  A B-tree.In this example every internal node has between 2 and 4 children. It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets.
iir_3_2	Wildcard queries Wildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which leads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart). A query such as mon* is known as a trailing wildcard query , because the * symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set of terms in the dictionary with the prefix mon. Finally, we use lookups on the standard inverted index to retrieve all documents containing any term in . But what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider leading wildcard queries, or queries of the form *mon. Consider a reverse B-tree on the dictionary - one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms in the vocabulary with a given prefix. In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set of dictionary terms beginning with the prefix se, then the reverse B-tree to enumerate the set of terms ending with the suffix mon. Next, we take the intersection of these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single * symbol using two B-trees, the normal B-tree and a reverse B-tree.   Subsections General wildcard queries Permuterm indexes k-gram indexes for wildcard queries
iir_3_2_2	k-gram indexes for wildcard queries Whereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase. We now present a second technique, known as the -gram index, for processing wildcard queries. We will also use -gram indexes in Section 3.3.4 . A -gram is a sequence of characters. Thus cas, ast and stl are all 3-grams occurring in the term castle. We use a special character $ to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$. In a -gram index , the dictionary contains all -grams that occur in any term in the vocabulary. Each postings list points from a -gram to all vocabulary terms containing that -gram. For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval. An example is given in Figure 3.4 .   How does such an index help us with wildcard queries? Consider the wildcard query re*ve. We are seeking documents containing any term that begins with re and ends with ve. Accordingly, we run the Boolean query $re AND ve$. This is looked up in the 3-gram index and yields a list of matching terms such as relive, remove and retrieve. Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query. There is however a difficulty with the use of -gram indexes, that demands one further step of processing. Consider using the 3-gram index described above for the query red*. Following the process described above, we first issue the Boolean query $re AND red to the 3-gram index. This leads to a match on terms such as retired, which contain the conjunction of the two 3-grams $re and red, yet do not match the original wildcard query red*. To cope with this, we introduce a post-filtering step, in which the terms enumerated by the Boolean query on the 3-gram index are checked individually against the original query red*. This is a simple string-matching operation and weeds out terms such as retired that do not match the original query. Terms that survive are then searched in the standard inverted index as usual. We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index. Search engines do allow the combination of wildcard queries using Boolean operators, for example, re*d AND fe*ri. What is the appropriate semantics for such a query? Since each wildcard query turns into a disjunction of single-term queries, the appropriate interpretation of this example is that we have a conjunction of disjunctions: we seek all documents that contain any term matching re*d and any term matching fe*ri. Even without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index. A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an ``Advanced Query'' interface) that most users never use. Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine. Exercises. In the permuterm index, each permuterm vocabulary term points to the original vocabulary term(s) from which it was derived. How many original vocabulary terms can there be in the postings list of a permuterm vocabulary term? Write down the entries in the permuterm index dictionary that are generated by the term mama. If you wanted to search for s*ng in a permuterm wildcard index, what key(s) would one do the lookup on? Refer to Figure 3.4 ; it is pointed out in the caption that the vocabulary terms in the postings are lexicographically ordered. Why is this ordering useful? Consider again the query fi*mo*er from Section 3.2.1 . What Boolean query on a bigram index would be generated for this query? Can you think of a term that matches the permuterm query in Section 3.2.1 , but does not satisfy this Boolean query? Give an example of a sentence that falsely matches the wildcard query mon*h if the search were to simply use a conjunction of bigrams.
iir_3_3	Spelling correction We next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney's spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on -gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.   Subsections Implementing spelling correction Forms of spelling correction Edit distance k-gram indexes for spelling correction Context sensitive spelling correction
iir_3_3_1	Implementing spelling correction Of various alternative correct spellings for a mis-spelled query, choose the ``nearest'' one. This demands that we have a notion of nearness or proximity between a pair of queries. We will develop these proximity measures in Section 3.3.3 . When two correctly spelled queries are tied (or nearly tied), select the one that is more common. For instance, grunt and grant both seem equally plausible as corrections for grnt. Then, the algorithm should choose the more common of grunt and grant as the correction. The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction. A different notion of more common is employed in many search engines, especially on the web. The idea is to use the correction that is most common among queries typed in by other users. The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt. Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation. Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways: On the query carot always retrieve documents containing carot as well as any ``spell-corrected'' version of carot, including carrot and tarot. As in (1) above, but only when the query term carot is not in the dictionary. As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents). When the original query returns fewer than a preset number of documents, the search interface presents a spelling suggestion to the end user: this suggestion consists of the spell-corrected query term(s). Thus, the search engine might respond to the user: ``Did you mean carrot?''
iir_3_3_2	Forms of spelling correction isolated-term context-sensitive We begin by examining two techniques for addressing isolated-term correction: edit distance, and -gram overlap. We then proceed to context-sensitive correction.
iir_3_3_3	Edit distance    edit distance edit operations    Levenshtein distance 3.4 It is well-known how to compute the (weighted) edit distance between two strings in time , where denotes the length of a string . The idea is to use the dynamic programming algorithm in Figure 3.5 , where the characters in and are given in array form. The algorithm fills the (integer) entries in a matrix whose two dimensions equal the lengths of the two strings whose edit distances is being computed; the entry of the matrix will hold (after the algorithm is executed) the edit distance between the strings consisting of the first characters of and the first characters of . The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5 , where the three quantities whose minimum is taken correspond to substituting a character in , inserting a character in and inserting a character in .  Figure 3.5: Dynamic programming algorithm for computing the edit distance between strings and . Figure 3.6 shows an example Levenshtein distance computation of Figure 3.5 . The typical cell has four entries formatted as a cell. The lower right entry in each cell is the of the other three, corresponding to the main dynamic programming step in Figure 3.5 . The other three entries are the three entries or 1 depending on whether and . The cells with numbers in italics depict the path by which we determine the Levenshtein distance.   The spelling correction problem however demands more than computing edit distance: given a set of strings (corresponding to terms in the vocabulary) and a query string , we seek the string(s) in of least edit distance from . We may view this as a decoding problem, in which the codewords (the strings in ) are prescribed in advance. The obvious way of doing this is to compute the edit distance from to each string in , before selecting the string(s) of minimum edit distance. This exhaustive search is inordinately expensive. Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s). The simplest such heuristic is to restrict the search to dictionary terms beginning with the same letter as the query string; the hope would be that spelling errors do not occur in the first character of the query. A more sophisticated variant of this heuristic is to use a version of the permuterm index, in which we omit the end-of-word symbol $. Consider the set of all rotations of the query string . For each rotation from this set, we traverse the B-tree into the permuterm index, thereby retrieving all dictionary terms that have a rotation beginning with . For instance, if is mase and we consider the rotation , we would retrieve dictionary terms such as semantic and semaphore that do not have a small edit distance to . Unfortunately, we would miss more pertinent dictionary terms such as mare and mane. To address this, we refine this rotation scheme: for each rotation, we omit a suffix of characters before performing the B-tree traversal. This ensures that each term in the set of terms retrieved from the dictionary includes a ``long'' substring in common with . The value of could depend on the length of . Alternatively, we may set it to a fixed constant such as .
iir_3_3_4	k-gram indexes for spelling correction To further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the -gram index of Section 3.2.2 (page ) to assist with retrieving vocabulary terms with low edit distance to the query . Once we retrieve such terms, we can then find the ones of least edit distance from . In fact, we will use the -gram index to retrieve vocabulary terms that have many -grams in common with the query. We will argue that for reasonable definitions of ``many -grams in common,'' the retrieval process is essentially that of a single scan through the postings for the -grams in the query string .  Figure: Matching at least two of the three 2-grams in the query bord. The 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the postings for the three bigrams in the query bord. Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams. A single scan of the postings (much as in Chapter 1 ) would let us enumerate all such terms; in the example of Figure 3.7 we would enumerate aboard, boardroom and border. This straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of -grams from the query : terms like boardroom, an implausible ``correction'' of bord, get enumerated. Consequently, we require more nuanced measures of the overlap in -grams between a vocabulary term and . The linear scan intersection can be adapted when the measure of overlap is the Jaccard coefficient for measuring the overlap between two sets and , defined to be . The two sets we consider are the set of -grams in the query , and the set of -grams in a vocabulary term. As the scan proceeds, we proceed from one vocabulary term to the next, computing on the fly the Jaccard coefficient between and . If the coefficient exceeds a preset threshold, we add to the output; if not, we move on to the next term in the postings. To compute the Jaccard coefficient, we need the set of -grams in and . Since we are scanning the postings for all -grams in , we immediately have these -grams on hand. What about the -grams of ? In principle, we could enumerate these on the fly from ; in practice this is not only slow but potentially infeasible since, in all likelihood, the postings entries themselves do not contain the complete string but rather some encoding of . The crucial observation is that to compute the Jaccard coefficient, we only need the length of the string . To see this, recall the example of Figure 3.7 and consider the point when the postings scan for query bord reaches term boardroom. We know that two bigrams match. If the postings stored the (pre-computed) number of bigrams in boardroom (namely, 8), we have all the information we require to compute the Jaccard coefficient to be ; the numerator is obtained from the number of postings hits (2, from bo and rd) while the denominator is the sum of the number of bigrams in bord and boardroom, less the number of postings hits. We could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans. How do we use these for spelling correction? One method that has some empirical support is to first use the -gram index to enumerate a set of candidate vocabulary terms that are potential corrections of . We then compute the edit distance from to each term in this set, selecting terms from the set with small edit distance to .
iir_3_3_5	Context sensitive spelling correction 3.3.4 This enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives. Several heuristics are used to trim this space. In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users. For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form. In this example, the biword fled fore is likely to be rare compared to the biword flew from. Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow. As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors. Exercises. If denotes the length of string , show that the edit distance between and is never more than Compute the edit distance between paris and alice. Write down the array of distances between all prefixes as computed by the algorithm in Figure 3.5 . Write pseudocode showing the details of computing on the fly the Jaccard coefficient while scanning the postings of the -gram index, as mentioned on page 3.3.4 . Compute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or. Consider the four-term query catched in the rye and suppose that each of the query terms has five alternative terms suggested by isolated-term correction. How many possible corrected phrases must we consider if we do not trim the space of corrected phrases, but instead try all six variants for each of the terms? For each of the prefixes of the query -- catched, catched in and catched in the -- we have a number of substitute prefixes arising from each term and its alternatives. Suppose that we were to retain only the top 10 of these substitute prefixes, as measured by its number of occurrences in the collection. We eliminate the rest from consideration for extension to longer prefixes: thus, if batched in is not one of the 10 most common 2-term queries in the collection, we do not consider any extension of batched in as possibly leading to a correction of catched in the rye. How many of the possible substitute prefixes are we eliminating at each phase? Are we guaranteed that retaining and extending only the 10 commonest substitute prefixes of catched in will lead to one of the 10 commonest substitute prefixes of catched in the?
iir_3_4	Phonetic correction phonetic Algorithms for such phonetic hashing are commonly collectively known as soundex algorithms. However, there is an original soundex algorithm, with various variants, built on the following scheme: Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index. Do the same with query terms. When the query calls for a soundex match, search this soundex index. Retain the first letter of the term. Change all occurrences of the following letters to '0' (zero): 'A', E', 'I', 'O', 'U', 'H', 'W', 'Y'. Change letters to digits as follows: B, F, P, V to 1. C, G, J, K, Q, S, X, Z to 2. D,T to 3. L to 4. M, N to 5. R to 6. Repeatedly remove one out of each pair of consecutive identical digits. Remove all zeros from the resulting string. Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits. For an example of a soundex map, Hermann maps to H655. Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index. This algorithm rests on a few observations: (1) vowels are viewed as interchangeable, in transcribing names; (2) consonants with similar sounds (e.g., D and T) are put in equivalence classes. This leads to related names often having the same soundex codes. While these rules work for many cases, especially European languages, such rules tend to be writing system dependent. For example, Chinese names can be written in Wade-Giles or Pinyin transcription. While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently. Exercises. Find two differently spelled proper nouns whose soundex codes are the same. Find two phonetically similar proper nouns whose soundex codes are different.
iir_3_5	References and further reading Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries. Garfield (1976) gives one of the first complete descriptions of the permuterm index. Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes. One of the earliest formal treatments of spelling correction was due to Damerau (1964). The notion of edit distance that we have used is due to Levenshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974). Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that -gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings. Gusfield (1997) is a standard reference on string algorithms such as edit distance. Probabilistic models (``noisy channel'' models) for spelling correction were pioneered by Kernighan et al. (1990) and further developed by Brill and Moore (2000) and Toutanova and Moore (2002). In these models, the mis-spelled query is viewed as a probabilistic corruption of a correct query. They have a similar mathematical basis to the language model methods presented in Chapter 12 , and also provide ways of incorporating phonetic similarity, closeness on the keyboard, and data from the actual spelling mistakes of users. Many would regard them as the state-of-the-art approach. Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs. The soundex algorithm is attributed to Margaret K. Odell and Robert C. Russelli (from U.S. patents granted in 1918 and 1922); the version described here draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate various phonetic matching algorithms, finding that a variant of the soundex algorithm performs poorly for general spelling correction, but that other algorithms based on the phonetic similarity of term pronunciations perform well.
iir_4	Index construction In this chapter, we look at how to construct an inverted index. We call this process index construction or indexing ; the process or machine that performs it the indexer . The design of indexing algorithms is governed by hardware constraints. We therefore begin this chapter with a review of the basics of computer hardware that are relevant for indexing. We then introduce blocked sort-based indexing (Section 4.2 ), an efficient single-machine algorithm designed for static collections that can be viewed as a more scalable version of the basic sort-based indexing algorithm we introduced in Chapter 1 . Section 4.3 describes single-pass in-memory indexing, an algorithm that has even better scaling properties because it does not hold the vocabulary in memory. For very large collections like the web, indexing has to be distributed over computer clusters with hundreds or thousands of machines. We discuss this in Section 4.4 . Collections with frequent changes require dynamic indexing introduced in Section 4.5 so that changes in the collection are immediately reflected in the index. Finally, we cover some complicating issues that can arise in indexing - such as security and indexes for ranked retrieval - in Section 4.6 . Index construction interacts with several topics covered in other chapters. The indexer needs raw text, but documents are encoded in many ways (see Chapter 2 ). Indexers compress and decompress intermediate files and the final index (see Chapter 5 ). In web search, documents are not on a local file system, but have to be spidered or crawled (see Chapter 20 ). In enterprise search , most documents are encapsulated in varied content management systems, email applications, and databases. We give some examples in Section 4.7 . Although most of these applications can be accessed via http, native Application Programming Interfaces (APIs) are usually more efficient. The reader should be aware that building the subsystem that feeds raw text to the indexing process can in itself be a challenging problem.   Subsections Hardware basics Blocked sort-based indexing Single-pass in-memory indexing Distributed indexing Dynamic indexing Other types of indexes References and further reading
iir_4_1	Hardware basics   Table 4.1: Typical system parameters in 2007. The seek time is the time needed to position the disk head in a new position. The transfer time per byte is the rate of transfer from disk to memory when the head is in the right position.   Symbol Statistic Value     average seek time 5 ms s     transfer time per byte 0.02 s s       processor's clock rate     lowlevel operation             (e.g., compare   swap a word) 0.01 s s       size of main memory several GB       size of disk space 1 TB or more    When building an information retrieval (IR) system, many decisions are based on the characteristics of the computer hardware on which the system runs. We therefore begin this chapter with a brief review of computer hardware. Performance characteristics typical of systems in 2007 are shown in Table 4.1 . A list of hardware basics that we need in this book to motivate IR system design follows. Access to data in memory is much faster than access to data on disk. It takes a few clock cycles (perhaps seconds) to access a byte in memory, but much longer to transfer it from disk (about seconds). Consequently, we want to keep as much data as possible in memory, especially those data that we need to access frequently. We call the technique of keeping frequently used disk data in main memory caching . When doing a disk read or write, it takes a while for the disk head to move to the part of the disk where the data are located. This time is called the seek time and it averages 5 ms for typical disks. No data are being transferred during the seek. To maximize data transfer rates, chunks of data that will be read together should therefore be stored contiguously on disk. For example, using the numbers in Table 4.1 it may take as little as 0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is stored as one chunk, but up to seconds if it is stored in 100 noncontiguous chunks because we need to move the disk head up to 100 times. Operating systems generally read and write entire blocks. Thus, reading a single byte from disk can take as much time as reading the entire block. Block sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part of main memory where a block being read or written is stored a buffer . Data transfers from disk to memory are handled by the system bus, not by the processor. This means that the processor is available to process data during disk I/O. We can exploit this fact to speed up data transfers by storing compressed data on disk. Assuming an efficient decompression algorithm, the total time of reading and then decompressing compressed data is usually less than reading uncompressed data. Servers used in IR systems typically have several gigabytes (GB) of main memory, sometimes tens of GB. Available disk space is several orders of magnitude larger.
iir_4_2	Blocked sort-based indexing The basic steps in constructing a nonpositional index are depicted in Figure 1.4 (page ). We first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. In this chapter, we describe methods for large collections that require the use of secondary storage. To make index construction more efficient, we represent terms as termIDs (instead of strings as we did in Figure 1.4 ), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection; or, in a two-pass approach, we compile the vocabulary in the first pass and construct the inverted index in the second pass. The index construction algorithms described in this chapter all do a single pass through the data. Section 4.7 gives references to multipass algorithms that are preferable in certain applications, for example, when disk space is scarce. We work with the Reuters-RCV1 collection as our model collection in this chapter, a collection with roughly 1 GB of text. It consists of about 800,000 documents that were sent over the Reuters newswire during a 1-year period between August 20, 1996, and August 19, 1997. A typical document is shown in Figure 4.1 , but note that we ignore multimedia information like images in this book and are only concerned with text. Reuters-RCV1 covers a wide range of international topics, including politics, business, sports, and (as in this example) science. Some key statistics of the collection are shown in Table 4.2 . Reuters-RCV1 has 100 million tokens. Collecting all termID-docID pairs of the collection using 4 bytes each for termID and docID therefore requires 0.8 GB of storage. Typical collections today are often one or two orders of magnitude larger than Reuters-RCV1. You can easily see how such collections overwhelm even large computers if we try to sort their termID-docID pairs in memory. If the size of the intermediate files during index construction is within a small factor of available memory, then the compression techniques introduced in Chapter 5 can help; however, the postings file of many large collections cannot fit into memory even after compression.   Table: Collection statistics for Reuters-RCV1. Values are rounded for the computations in this book. The unrounded values are: 806,791 documents, 222 tokens per document, 391,523 (distinct) terms, 6.04 bytes per token with spaces and punctuation, 4.5 bytes per token without spaces and punctuation, 7.5 bytes per term, and 96,969,056 tokens. The numbers in this table correspond to the third line (``case folding'') in icompresstb5.   Symbol Statistic Value     documents 800,000     avg. # tokens per document 200     terms 400,000       avg. # bytes per token (incl. spaces/punct.) 6       avg. # bytes per token (without spaces/punct.) 4.5       avg. # bytes per term 7.5     tokens 100,000,000     Figure 4.1: Document from the Reuters newswire. With main memory insufficient, we need to use an external sorting algorithm , that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks as we explained in Section 4.1 . One solution is the blocked sort-based indexing algorithm or BSBI in Figure 4.2 . BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. The algorithm parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full (PARSENEXTBLOCK in Figure 4.2 ). We choose the block size to fit comfortably into memory to permit a fast in-memory sort. The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk. Applying this to Reuters-RCV1 and assuming we can fit 10 million termID-docID pairs into memory, we end up with ten blocks, each an inverted index of one part of the collection.    Merging in blocked sort-based indexing.Two blocks (``postings lists to be merged'') are loaded from disk into memory, merged in memory (``merged postings lists'') and written back to disk. We show terms instead of termIDs for better readability. In the final step, the algorithm simultaneously merges the ten blocks into one large merged index. An example with two blocks is shown in Figure 4.3 , where we use to denote the document of the collection. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. In each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary. How expensive is BSBI? Its time complexity is because the step with the highest time complexity is sorting and is an upper bound for the number of items we must sort (i.e., the number of termID-docID pairs). But the actual indexing time is usually dominated by the time it takes to parse the documents (PARSENEXTBLOCK) and to do the final merge (MERGEBLOCKS). Exercise 4.6 asks you to compute the total index construction time for RCV1 that includes these steps as well as inverting the blocks and writing them to disk. Notice that Reuters-RCV1 is not particularly large in an age when one or more GB of memory are standard on personal computers. With appropriate compression (Chapter 5 ), we could have created an inverted index for RCV1 in memory on a not overly beefy server. The techniques we have described are needed, however, for collections that are several orders of magnitude larger. Exercises. If we need comparisons (where is the number of termID-docID pairs) and two disk seeks for each comparison, how much time would index construction for Reuters-RCV1 take if we used disk instead of memory for storage and an unoptimized sorting algorithm (i.e., not an external sorting algorithm)? Use the system parameters in Table 4.1 . How would you create the dictionary in blocked sort-based indexing on the fly to avoid an extra pass through the data?
iir_4_3	Single-pass in-memory indexing  single-pass in-memory indexing  SPIMI  Figure 4.4: Inversion of a block in single-pass in-memory indexing The SPIMI algorithm is shown in Figure 4.4 . The part of the algorithm that parses documents and turns them into a stream of term-docID pairs, which we call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on the token stream until the entire collection has been processed. Tokens are processed one by one (line 4) during each successive call of SPIMI-INVERT. When a term occurs for the first time, it is added to the dictionary (best implemented as a hash), and a new postings list is created (line 6). The call in line 7 returns this postings list for subsequent occurrences of the term. A difference between BSBI and SPIMI is that SPIMI adds a posting directly to its postings list (line 10). Instead of first collecting all termID-docID pairs and then sorting them (as we did in BSBI), each postings list is dynamic (i.e., its size is adjusted as it grows) and it is immediately available to collect postings. This has two advantages: It is faster because there is no sorting required, and it saves memory because we keep track of the term a postings list belongs to, so the termIDs of postings need not be stored. As a result, the blocks that individual calls of SPIMI-INVERT can process are much larger and the index construction process as a whole is more efficient. Because we do not know how large the postings list of a term will be when we first encounter it, we allocate space for a short postings list initially and double the space each time it is full (lines 8-9). This means that some memory is wasted, which counteracts the memory savings from the omission of termIDs in intermediate data structures. However, the overall memory requirements for the dynamically constructed index of a block in SPIMI are still lower than in BSBI. When memory has been exhausted, we write the index of the block (which consists of the dictionary and the postings lists) to disk (line 12). We have to sort the terms (line 11) before doing this because we want to write postings lists in lexicographic order to facilitate the final merging step. If each block's postings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each block. Each call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last step of SPIMI (corresponding to line 7 in Figure 4.2 ; not shown in Figure 4.4 ) is then to merge the blocks into the final inverted index. In addition to constructing a new dictionary structure for each block and eliminating the expensive sorting step, SPIMI has a third important component: compression. Both the postings and the dictionary terms can be stored compactly on disk if we employ compression. Compression increases the efficiency of the algorithm further because we can process even larger blocks, and because the individual blocks require less space on disk. We refer readers to the literature for this aspect of the algorithm (Section 4.7 ). The time complexity of SPIMI is because no sorting of tokens is required and all operations are at most linear in the size of the collection.
iir_4_4	Distributed indexing  clusters   distributed indexing  distributed index term-partitioned index . Most large search engines prefer a document-partitioned index (which can be easily generated from a term-partitioned index). We discuss this topic further in Section 20.3 (page ). The distributed index construction method we describe in this section is an application of MapReduce , a general architecture for distributed computing. MapReduce is designed for large computer clusters. The point of a cluster is to solve large computing problems on cheap commodity machines or nodes that are built from standard parts (processor, memory, disk) as opposed to on a supercomputer with specialized hardware. Although hundreds or thousands of machines are available in such clusters, individual machines can fail at any time. One requirement for robust distributed indexing is, therefore, that we divide the work up into chunks that we can easily assign and - in case of failure - reassign. A master node directs the process of assigning and reassigning tasks to individual worker nodes. The map and reduce phases of MapReduce split up the computing job into chunks that standard machines can process in a short time. The various steps of MapReduce are shown in Figure 4.5 and an example on a collection consisting of two documents is shown in Figure 4.6 . First, the input data, in our case a collection of web pages, are split into splits where the size of the split is chosen to ensure that the work can be distributed evenly (chunks should not be too large) and efficiently (the total number of chunks we need to manage should not be too large); 16 or 64 MB are good sizes in distributed indexing. Splits are not preassigned to machines, but are instead assigned by the master node on an ongoing basis: As a machine finishes processing one split, it is assigned the next one. If a machine dies or becomes a laggard due to hardware problems, the split it is working on is simply reassigned to another machine.  Figure 4.5: An example of distributed indexing with MapReduce. Adapted from Dean and Ghemawat (2004). In general, MapReduce breaks a large computing problem into smaller parts by recasting it in terms of manipulation of key-value pairs . For indexing, a key-value pair has the form (termID,docID). In distributed indexing, the mapping from terms to termIDs is also distributed and therefore more complex than in single-machine indexing. A simple solution is to maintain a (perhaps precomputed) mapping for frequent terms that is copied to all nodes and to use terms directly (instead of termIDs) for infrequent terms. We do not address this problem here and assume that all nodes share a consistent term termID mapping. The map phase of MapReduce consists of mapping splits of the input data to key-value pairs. This is the same parsing task we also encountered in BSBI and SPIMI, and we therefore call the machines that execute the map phase parsers . Each parser writes its output to local intermediate files, the segment files (shown as in Figure 4.5 ). For the reduce phase , we want all values for a given key to be stored close together, so that they can be read and processed quickly. This is achieved by partitioning the keys into term partitions and having the parsers write key-value pairs for each term partition into a separate segment file. In Figure 4.5 , the term partitions are according to first letter: a-f, g-p, q-z, and . (We chose these key ranges for ease of exposition. In general, key ranges need not correspond to contiguous terms or termIDs.) The term partitions are defined by the person who operates the indexing system (Exercise 4.6 ). The parsers then write corresponding segment files, one for each term partition. Each term partition thus corresponds to segments files, where is the number of parsers. For instance, Figure 4.5 shows three a-f segment files of the a-f partition, corresponding to the three parsers shown in the figure. Collecting all values (here: docIDs) for a given key (here: termID) into one list is the task of the inverters in the reduce phase. The master assigns each term partition to a different inverter - and, as in the case of parsers, reassigns term partitions in case of failing or slow inverters. Each term partition (corresponding to segment files, one on each parser) is processed by one inverter. We assume here that segment files are of a size that a single machine can handle (Exercise 4.6 ). Finally, the list of values is sorted for each key and written to the final sorted postings list (``postings'' in the figure). (Note that postings in Figure 4.6 include term frequencies, whereas each posting in the other sections of this chapter is simply a docID without term frequency information.) The data flow is shown for a-f in Figure 4.5 . This completes the construction of the inverted index. Parsers and inverters are not separate sets of machines. The master identifies idle machines and assigns tasks to them. The same machine can be a parser in the map phase and an inverter in the reduce phase. And there are often other jobs that run in parallel with index construction, so in between being a parser and an inverter a machine might do some crawling or another unrelated task. To minimize write times before inverters reduce the data, each parser writes its segment files to its local disk. In the reduce phase, the master communicates to an inverter the locations of the relevant segment files (e.g., of the segment files of the a-f partition). Each segment file only requires one sequential read because all data relevant to a particular inverter were written to a single segment file by the parser. This setup minimizes the amount of network traffic needed during indexing.  Map and reduce functions in MapReduce. In general, the map function produces a list of key-value pairs. All values for a key are collected into one list in the reduce phase. This list is then processed further. The instantiations of the two functions and an example are shown for index construction. Because the map phase processes documents in a distributed fashion, termID-docID pairs need not be ordered correctly initially as in this example. The example shows terms instead of termIDs for better readability. We abbreviate Caesar as C and conquered as c'ed. Figure 4.6 shows the general schema of the MapReduce functions. Input and output are often lists of key-value pairs themselves, so that several MapReduce jobs can run in sequence. In fact, this was the design of the Google indexing system in 2004. What we describe in this section corresponds to only one of five to ten MapReduce operations in that indexing system. Another MapReduce operation transforms the term-partitioned index we just created into a document-partitioned one. MapReduce offers a robust and conceptually simple framework for implementing index construction in a distributed environment. By providing a semiautomatic method for splitting index construction into smaller tasks, it can scale to almost arbitrarily large collections, given computer clusters of sufficient size. Exercises. For splits, segments, and term partitions, how long would distributed index creation take for Reuters-RCV1 in a MapReduce architecture? Base your assumptions about cluster machines on Table 4.1 .
iir_4_5	Dynamic indexing Thus far, we have assumed that the document collection is static. This is fine for collections that change infrequently or never (e.g., the Bible or Shakespeare). But most collections are modified frequently with documents being added, deleted, and updated. This means that new terms need to be added to the dictionary, and postings lists need to be updated for existing terms. The simplest way to achieve this is to periodically reconstruct the index from scratch. This is a good solution if the number of changes over time is small and a delay in making new documents searchable is acceptable - and if enough resources are available to construct a new index while the old one is still available for querying. If there is a requirement that new documents be included quickly, one solution is to maintain two indexes: a large main index and a small auxiliary index that stores new documents. The auxiliary index is kept in memory. Searches are run across both indexes and results merged. Deletions are stored in an invalidation bit vector. We can then filter out deleted documents before returning the search result. Documents are updated by deleting and reinserting them. Each time the auxiliary index becomes too large, we merge it into the main index. The cost of this merging operation depends on how we store the index in the file system. If we store each postings list as a separate file, then the merge simply consists of extending each postings list of the main index by the corresponding postings list of the auxiliary index. In this scheme, the reason for keeping the auxiliary index is to reduce the number of disk seeks required over time. Updating each document separately requires up to disk seeks, where is the average size of the vocabulary of documents in the collection. With an auxiliary index, we only put additional load on the disk when we merge auxiliary and main indexes. Unfortunately, the one-file-per-postings-list scheme is infeasible because most file systems cannot efficiently handle very large numbers of files. The simplest alternative is to store the index as one large file, that is, as a concatenation of all postings lists. In reality, we often choose a compromise between the two extremes (Section 4.7 ). To simplify the discussion, we choose the simple option of storing the index as one large file here. In this scheme, we process each posting times because we touch it during each of merges where is the size of the auxiliary index and the total number of postings. Thus, the overall time complexity is . (We neglect the representation of terms here and consider only the docIDs. For the purpose of time complexity, a postings list is simply a list of docIDs.)  Figure: Logarithmic merging. Each token (termID,docID) is initially added to in-memory index by LM ERGEA DDT OKEN. L OGARITHMICM ERGE initializes and . We can do better than by introducing indexes , , , ...of size , , .... Postings percolate up this sequence of indexes and are processed only once on each level. This scheme is called logarithmic merging (Figure 4.7 ). As before, up to postings are accumulated in an in-memory auxiliary index, which we call . When the limit is reached, the postings in are transferred to a new index that is created on disk. The next time is full, it is merged with to create an index of size . Then is either stored as (if there isn't already an ) or merged with into (if exists); and so on. We service search requests by querying in-memory and all currently valid indexes on disk and merging the results. Readers familiar with the binomial heap data structure will recognize its similarity with the structure of the inverted indexes in logarithmic merging. Overall index construction time is because each posting is processed only once on each of the levels. We trade this efficiency gain for a slow down of query processing; we now need to merge results from indexes as opposed to just two (the main and auxiliary indexes). As in the auxiliary index scheme, we still need to merge very large indexes occasionally (which slows down the search system during the merge), but this happens less frequently and the indexes involved in a merge on average are smaller. Having multiple indexes complicates the maintenance of collection-wide statistics. For example, it affects the spelling correction algorithm in Section 3.3 (page ) that selects the corrected alternative with the most hits. With multiple indexes and an invalidation bit vector, the correct number of hits for a term is no longer a simple lookup. In fact, all aspects of an IR system - index maintenance, query processing, distribution, and so on - are more complex in logarithmic merging. Because of this complexity of dynamic indexing, some large search engines adopt a reconstruction-from-scratch strategy. They do not construct indexes dynamically. Instead, a new index is built from scratch periodically. Query processing is then switched from the new index and the old index is deleted. Exercises. For and , perform a step-by-step simulation of the algorithm in Figure 4.7 . Create a table that shows, for each point in time at which tokens have been processed ( ), which of the three indexes are in use. The first three lines of the table are given below.         2 0 0 0 0     4 0 0 0 1     6 0 0 1 0  
iir_4_6	Other types of indexes In the indexes we have considered so far, postings lists are ordered with respect to docID. As we see in Chapter 5, this is advantageous for compression - instead of docIDs we can compress smaller gaps between IDs, thus reducing space requirements for the index. However, this structure for the index is not optimal when we build ranked (Chapters 6 7 ) - as opposed to Boolean - retrieval systems . In ranked retrieval, postings are often ordered according to weight or impact , with the highest-weighted postings occurring first. With this organization, scanning of long postings lists during query processing can usually be terminated early when weights have become so small that any further documents can be predicted to be of low similarity to the query (see Chapter 6 ). In a docID-sorted index, new documents are always inserted at the end of postings lists. In an impact-sorted index impactordered, the insertion can occur anywhere, thus complicating the update of the inverted index. Security is an important consideration for retrieval systems in corporations. A low-level employee should not be able to find the salary roster of the corporation, but authorized managers need to be able to search for it. Users' results lists must not contain documents they are barred from opening; the very existence of a document can be sensitive information.  Figure: A user-document matrix for access control lists. Element is 1 if user has access to document and 0 otherwise. During query processing, a user's access postings list is intersected with the results list returned by the text part of the index. User authorization is often mediated through access control lists or ACLs. ACLs can be dealt with in an information retrieval system by representing each document as the set of users that can access them (Figure 4.8 ) and then inverting the resulting user-document matrix. The inverted ACL index has, for each user, a ``postings list'' of documents they can access - the user's access list. Search results are then intersected with this list. However, such an index is difficult to maintain when access permissions change - we discussed these difficulties in the context of incremental indexing for regular postings lists in Section 4.5. It also requires the processing of very long postings lists for users with access to large document subsets. User membership is therefore often verified by retrieving access information directly from the file system at query time - even though this slows down retrieval. We discussed indexes for storing and retrieving terms (as opposed to documents) in Chapter 3 . Exercises. Can spelling correction compromise document-level security? Consider the case where a spelling correction is based on documents to which the user does not have access. Exercises. Total index construction time in blocked sort-based indexing is broken down in Table 4.3. Fill out the time column of the table for Reuters-RCV1 assuming a system with the parameters given in Table 4.1 . Table: The five steps in constructing an index for Reuters-RCV1 in blocked sort-based indexing. Line numbers refer to Figure 4.2 .     Step Time     1 reading of collection (line 4)       2 10 initial sorts of records each (line 5)       3 writing of 10 blocks (line 6)       4 total disk transfer time for merging (line 7)       5 time of actual merging (line 7)         total     Table 4.4: Collection statistics for a large collection.   Symbol Statistic Value     # documents 1,000,000,000     # tokens per document 1000     # distinct terms 44,000,000   Repeat Exercise 4.6 for the larger collection in Table 4.4 . Choose a block size that is realistic for current technology (remember that a block should easily fit into main memory). How many blocks do you need? Assume that we have a collection of modest size whose index can be constructed with the simple in-memory indexing algorithm in Figure 1.4 (page ). For this collection, compare memory, disk and time requirements of the simple algorithm in Figure 1.4 and blocked sort-based indexing. Assume that machines in MapReduce have 100 GB of disk space each. Assume further that the postings list of the term the has a size of 200 GB. Then the MapReduce algorithm as described cannot be run to construct the index. How would you modify MapReduce so that it can handle this case? For optimal load balancing, the inverters in MapReduce must get segmented postings files of similar sizes. For a new collection, the distribution of key-value pairs may not be known in advance. How would you solve this problem? Apply MapReduce to the problem of counting how often each term occurs in a set of files. Specify map and reduce operations for this task. Write down an example along the lines of Figure 4.6 . We claimed (on page 4.5 ) that an auxiliary index can impair the quality of collection statistics. An example is the term weighting method idf , which is defined as where is the total number of documents and is the number of documents that term occurs in idf. Show that even a small auxiliary index can cause significant error in idf when it is computed on the main index only. Consider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm Flossie).
iir_4_7	References and further reading Witten et al. (1999, Chapter 5) present an extensive treatment of the subject of index construction and additional indexing algorithms with different tradeoffs of memory, disk space, and time. In general, blocked sort-based indexing does well on all three counts. However, if conserving memory or disk space is the main criterion, then other algorithms may be a better choice. See Witten et al. (1999), Tables 5.4 and 5.5; BSBI is closest to ``sort-based multiway merge,'' but the two algorithms differ in dictionary structure and use of compression. Moffat and Bell (1995) show how to construct an index ``in situ,'' that is, with disk space usage close to what is needed for the final index and with a minimum of additional temporary files (cf. also Harman and Candela (1990)). They give Lesk (1988) and Somogyi (1990) credit for being among the first to employ sorting for index construction. The SPIMI method in Section 4.3 is from (Heinz and Zobel, 2003). We have simplified several aspects of the algorithm, including compression and the fact that each term's data structure also contains, in addition to the postings list, its document frequency and house keeping information. We recommend Heinz and Zobel (2003) and Zobel and Moffat (2006) as up-do-date, in-depth treatments of index construction. Other algorithms with good scaling properties with respect to vocabulary size require several passes through the data, e.g., FAST-INV (Harman et al., 1992, Fox and Lee, 1991). The MapReduce architecture was introduced by Dean and Ghemawat (2004). An open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/. Ribeiro-Neto et al. (1999) and Melnik et al. (2001) describe other approaches to distributed indexing. Introductory chapters on distributed IR are (Baeza-Yates and Ribeiro-Neto, 1999, Chapter 9) and (Grossman and Frieder, 2004, Chapter 8). See also Callan (2000). Lester et al. (2005) and Büttcher and Clarke (2005a) analyze the properties of logarithmic merging and compare it with other construction methods. One of the first uses of this method was in Lucene (http://lucene.apache.org). Other dynamic indexing methods are discussed by Büttcher et al. (2006) and Lester et al. (2006). The latter paper also discusses the strategy of replacing the old index by one built from scratch. Heinz et al. (2002) compare data structures for accumulating the vocabulary in memory. Büttcher and Clarke (2005b) discuss security models for a common inverted index for multiple users. A detailed characterization of the Reuters-RCV1 collection can be found in (Lewis et al., 2004). NIST distributes the collection (see http://trec.nist.gov/data/reuters/reuters.html). Garcia-Molina et al. (1999, Chapter 2) review computer hardware relevant to system design in depth. An effective indexer for enterprise search needs to be able to communicate efficiently with a number of applications that hold text data in corporations, including Microsoft Outlook, IBM's Lotus software, databases like Oracle and MySQL, content management systems like Open Text, and enterprise resource planning software like SAP.
iir_5	Index compression Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems. One benefit of compression is immediately clear. We need less disk space. As we will see, compression ratios of 1:4 are easy to achieve, potentially cutting the cost of storing the index by 75%. There are two more subtle benefits of compression. The first is increased use of caching. Search systems use some parts of the dictionary and the index much more than others. For example, if we cache the postings list of a frequently used query term , then the computations necessary for responding to the one-term query can be entirely done in memory. With compression, we can fit a lot more information into main memory. Instead of having to expend a disk seek when processing a query with , we instead access its postings list in memory and decompress it. As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small. As a result, we are able to decrease the response time of the IR system substantially. Because memory is a more expensive resource than disk space, increased speed owing to caching - rather than decreased space requirements - is often the prime motivator for compression. The second more subtle advantage of compression is faster transfer of data from disk to memory. Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form. For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression. So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists. If the main goal of compression is to conserve disk space, then the speed of compression algorithms is of no concern. But for improved cache utilization and faster disk-to-memory transfer, decompression speeds must be high. The compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression. In this chapter, we define a posting as a docID in a postings list. For example, the postings list (6; 20, 45, 100), where 6 is the termID of the list's term, contains three postings. As discussed in Section 2.4.2 (page ), postings in most search systems also contain frequency and position information; but we will only consider simple docID postings here. See Section 5.4 for references on compressing frequencies and positions. This chapter first gives a statistical characterization of the distribution of the entities we want to compress - terms and postings in large collections (Section 5.1 ). We then look at compression of the dictionary, using the dictionary-as-a-string method and blocked storage (Section 5.2 ). Section 5.3 describes two techniques for compressing the postings file, variable byte encoding and encoding.   Subsections Statistical properties of terms in information retrieval Heaps' law: Estimating the number of terms Zipf's law: Modeling the distribution of terms Dictionary compression Dictionary as a string Blocked storage Postings file compression Variable byte codes Gamma codes References and further reading
iir_5_1	Statistical properties of terms in information retrieval As in the last chapter, we use Reuters-RCV1 as our model collection (see Table 4.2 , page 4.2 ). We give some term and postings statistics for the collection in Table 5.1 . ``'' indicates the reduction in size from the previous line. ``T%'' is the cumulative reduction from unfiltered. The table shows the number of terms for different levels of preprocessing (column 2). The number of terms is the main factor in determining the size of the dictionary. The number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection. The expected size of a positional index is related to the number of positions it must encode (column 4). In general, the statistics in Table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly. Stemming and case folding reduce the number of (distinct) terms by 17% each and the number of nonpositional postings by 4% and 3%, respectively. The treatment of the most frequent words is also important. The rule of 30 states that the 30 most common words account for 30% of the tokens in written text (31% in the table). Eliminating the 150 most common words from indexing (as stop words; cf. Section 2.2.2 , page 2.2.2 ) cuts 25% to 30% of the nonpositional postings. But, although a stop list of 150 words reduces the number of postings by a quarter or more, this size reduction does not carry over to the size of the compressed index. As we will see later in this chapter, the postings lists of frequent words require only a few bits per posting after compression.   Table 5.1: The effect of preprocessing on the number of terms, nonpositional postings, and tokens for Reuters-RCV1. `` '' indicates the reduction in size from the previous line, except that ``30 stop words'' and ``150 stop words'' both use ``case folding'' as their reference line. ``T%'' is the cumulative (``total'') reduction from unfiltered. We performed stemming with the Porter stemmer (Chapter 2 , page 2.2.4 ).                 tokens (number of position       (distinct) terms nonpositional postings entries in postings)                   number T% number T% number T%     unfiltered 484,494     109,971,179     197,879,290         no numbers 473,723 2 2 100,680,242 8 8 179,158,204 9 9     case folding 391,523 17 19 96,969,056 3 12 179,158,204 0 9     30 stop words 391,493 0 19 83,390,443 14 24 121,857,825 31 38     150 stop words 391,373 0 19 67,001,847 30 39 94,516,599 47 52     stemming 322,383 17 33 63,812,300 4 42 94,516,599 0 52    The deltas in the table are in a range typical of large collections. Note, however, that the percentage reductions can be very different for some text collections. For example, for a collection of web pages with a high proportion of French text, a lemmatizer for French reduces vocabulary size much more than the Porter stemmer does for an English-only collection because French is a morphologically richer language than English. The compression techniques we describe in the remainder of this chapter are lossless , that is, all information is preserved. Better compression ratios can be achieved with lossy compression , which discards some information. Case folding, stemming, and stop word elimination are forms of lossy compression. Similarly, the vector space model (Chapter 6 ) and dimensionality reduction techniques like latent semantic indexing (Chapter 18 ) create compact representations from which we cannot fully restore the original collection. Lossy compression makes sense when the ``lost'' information is unlikely ever to be used by the search system. For example, web search is characterized by a large number of documents, short queries, and users who only look at the first few pages of results. As a consequence, we can discard postings of documents that would only be used for hits far down the list. Thus, there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness. Before introducing techniques for compressing the dictionary, we want to estimate the number of distinct terms in a collection. It is sometimes said that languages have a vocabulary of a certain size. The second edition of the Oxford English Dictionary (OED) defines more than 600,000 words. But the vocabulary of most large collections is much larger than the OED. The OED does not include most names of people, locations, products, or scientific entities like genes. These names need to be included in the inverted index, so our users can search for them.   Subsections Heaps' law: Estimating the number of terms Zipf's law: Modeling the distribution of terms
iir_5_1_1	Heaps' law: Estimating the number of terms  Heaps' law.Vocabulary size as a function of collection size (number of tokens) for Reuters-RCV1. For these data, the dashed line is the best least-squares fit. Thus, and . A better way of getting a handle on is Heaps' law , which estimates vocabulary size as a function of collection size: (1)       5.1     (2)  The parameter is quite variable because vocabulary growth depends a lot on the nature of the collection and how it is processed. Case-folding and stemming reduce the growth rate of the vocabulary, whereas including numbers and spelling errors increase it. Regardless of the values of the parameters for a particular collection, Heaps' law suggests that (i) the dictionary size continues to increase with more documents in the collection, rather than a maximum vocabulary size being reached, and (ii) the size of the dictionary is quite large for large collections. These two hypotheses have been empirically shown to be true of large text collections (Section 5.4 ). So dictionary compression is important for an effective information retrieval system.
iir_5_1_2	Zipf's law: Modeling the distribution of terms 5.3 A commonly used model of the distribution of terms in a collection is Zipf's law . It states that, if is the most common term in the collection, is the next most common, and so on, then the collection frequency of the th most common term is proportional to :     (3)    3 Equivalently, we can write Zipf's law as or as where and is a constant to be defined in Section 5.3.2 . It is therefore a power law with exponent . See Chapter 19 , page 19.2.1 , for another power law, a law characterizing the distribution of links on web pages.  Zipf's law for Reuters-RCV1. Frequency is plotted as a function of frequency rank for the terms in the collection. The line is the distribution predicted by Zipf's law (weighted least-squares fit; intercept is 6.95). The log-log graph in Figure 5.2 plots the collection frequency of a term as a function of its rank for Reuters-RCV1. A line with slope -1, corresponding to the Zipf function , is also shown. The fit of the data to the law is not particularly good, but good enough to serve as a model for term distributions in our calculations in Section 5.3 . Exercises. Assuming one machine word per posting, what is the size of the uncompressed (nonpositional) index for different tokenizations based on Table 5.1 ? How do these numbers compare with Table 5.6 ?
iir_5_2	Dictionary compression This section presents a series of dictionary data structures that achieve increasingly higher compression ratios. The dictionary is small compared with the postings file as suggested by Table 5.1 . So why compress it if it is responsible for only a small percentage of the overall space requirements of the IR system? One of the primary factors in determining the response time of an IR system is the number of disk seeks necessary to process a query. If parts of the dictionary are on disk, then many more disk seeks are necessary in query evaluation. Thus, the main goal of compressing the dictionary is to fit it in main memory, or at least a large portion of it, to support high query throughput. Although dictionaries of very large collections fit into the memory of a standard desktop machine, this is not true of many other application scenarios. For example, an enterprise search server for a large corporation may have to index a multiterabyte collection with a comparatively large vocabulary because of the presence of documents in many different languages. We also want to be able to design search systems for limited hardware such as mobile phones and onboard computers. Other reasons for wanting to conserve memory are fast startup time and having to share resources with other applications. The search system on your PC must get along with the memory-hogging word processing suite you are using at the same time.  Figure 5.3: Storing the dictionary as an array of fixed-width entries.   Subsections Dictionary as a string Blocked storage
iir_5_2_1	Dictionary as a string 5.3  For Reuters-RCV1, we need for storing the dictionary in this scheme.  Dictionary-as-a-string storage.Pointers mark the end of the preceding term and the beginning of the next. For example, the first three terms in this example are systile, syzygetic, and syzygial. Using fixed-width entries for terms is clearly wasteful. The average length of a term in English is about eight characters icompresstb1, so on average we are wasting twelve characters (or 24 bytes) in the fixed-width scheme. Also, we have no way of storing terms with more than twenty characters like hydrochlorofluorocarbons and supercalifragilisticexpialidocious. We can overcome these shortcomings by storing the dictionary terms as one long string of characters, as shown in Figure 5.4 . The pointer to the next term is also used to demarcate the end of the current term. As before, we locate terms in the data structure by way of binary search in the (now smaller) table. This scheme saves us 60% compared to fixed-width storage - 24 bytes on average of the 40 bytes 12 bytes on average of the 20 bytes we allocated for terms before. However, we now also need to store term pointers. The term pointers resolve positions, so they need to be bits or 3 bytes long. In this new scheme, we need for the Reuters-RCV1 dictionary: 4 bytes each for frequency and postings pointer, 3 bytes for the term pointer, and bytes on average for the term. So we have reduced the space requirements by one third from 19.211.2 to 10.87.6 MB.  Blocked storage with four terms per block.The first block consists of systile, syzygetic, syzygial, and syzygy with lengths of seven, nine, eight, and six characters, respectively. Each term is preceded by a byte encoding its length that indicates how many bytes to skip to reach subsequent terms.
iir_5_2_2	Blocked storage   5.5        Figure 5.6: Search of the uncompressed dictionary (a) and a dictionary compressed by blocking with (b). By increasing the block size , we get better compression. However, there is a tradeoff between compression and the speed of term lookup. For the eight-term dictionary in Figure 5.6 , steps in binary search are shown as double lines and steps in list search as simple lines. We search for terms in the uncompressed dictionary by binary search (a). In the compressed dictionary, we first locate the term's block by binary search and then its position within the list by linear search through the block (b). Searching the uncompressed dictionary in (a) takes on average steps, assuming each term is equally likely to come up in a query. For example, finding the two terms, aid and box, takes three and two steps, respectively. With blocks of size in (b), we need steps on average, more. For example, finding den takes one binary search step and two steps through the block. By increasing , we can get the size of the compressed dictionary arbitrarily close to the minimum of , but term lookup becomes prohibitively slow for large values of .   One source of redundancy in the dictionary we have not exploited yet is the fact that consecutive entries in an alphabetically sorted list share common prefixes. This observation leads to front coding (Figure 5.7 ). A common prefix is identified for a subsequence of the term list and then referred to with a special character. In the case of Reuters, front coding saves another 2.41.2 MB, as we found in an experiment. Other schemes with even greater compression rely on minimal perfect hashing, that is, a hash function that maps terms onto without collisions. However, we cannot adapt perfect hashes incrementally because each new term causes a collision and therefore requires the creation of a new perfect hash function. Therefore, they cannot be used in a dynamic environment. Even with the best compression scheme, it may not be feasible to store the entire dictionary in main memory for very large text collections and for hardware with limited memory. If we have to partition the dictionary onto pages that are stored on disk, then we can index the first term of each page using a B-tree. For processing most queries, the search system has to go to disk anyway to fetch the postings. One additional seek for retrieving the term's dictionary page from disk is a significant, but tolerable increase in the time it takes to process a query.   Table 5.2: Dictionary compression for Reuters-RCV1.   data structure size in MB     dictionary, fixed-width 19.211.2     dictionary, term pointers into string 10.8 7.6     , with blocking, 10.3 7.1     , with blocking   front coding 7.9 5.9    5.2  Exercises. Estimate the space usage of the Reuters-RCV1 dictionary with blocks of size and in blocked dictionary storage. Estimate the time needed for term lookup in the compressed dictionary of Reuters-RCV1 with block sizes of (Figure 5.6 , b), , and . What is the slowdown compared with (Figure 5.6 , a)?
iir_5_3	Postings file compression   Table: Encoding gaps instead of document IDs. For example, we store gaps 107, 5, 43, ..., instead of docIDs 283154, 283159, 283202, ... for computer. The first docID is left unchanged (only shown for arachnocentric).     encoding postings list                   the docIDs ...   283042   283043   283044   283045 ...       gaps       1   1   1   ...     computer docIDs ...   283047   283154   283159   283202 ...       gaps       107   5   43   ...     arachnocentric docIDs 252000   500100                     gaps 252000 248100                    Recall from Table 4.2 (page 4.2 ) that Reuters-RCV1 has 800,000 documents, 200 tokens per document, six characters per token, and 100,000,000 postings where we define a posting in this chapter as a docID in a postings list, that is, excluding frequency and position information. These numbers correspond to line 3 (``case folding'') in Table 5.1 . Document identifiers are bits long. Thus, the size of the collection is about and the size of the uncompressed postings file is . To devise a more efficient representation of the postings file, one that uses fewer than 20 bits per document, we observe that the postings for frequent terms are close together. Imagine going through the documents of a collection one by one and looking for a frequent term like computer. We will find a document containing computer, then we skip a few documents that do not contain it, then there is again a document with the term and so on (see Table 5.3 ). The key idea is that the gaps between postings are short, requiring a lot less space than 20 bits to store. In fact, gaps for the most frequent terms such as the and for are mostly equal to 1. But the gaps for a rare term that occurs only once or twice in a collection (e.g., arachnocentric in Table 5.3 ) have the same order of magnitude as the docIDs and need 20 bits. For an economical representation of this distribution of gaps, we need a variable encoding method that uses fewer bits for short gaps. To encode small numbers in less space than large numbers, we look at two types of methods: bytewise compression and bitwise compression. As the names suggest, these methods attempt to encode gaps with the minimum number of bytes and bits, respectively.   Subsections Variable byte codes Gamma codes
iir_5_3_1	Variable byte codes     VB encoding. Gaps are encoded using an integral number of bytes. The first bit, the continuation bit, of each byte indicates whether the code ends with this byte (1) or not (0).   docIDs 824 829 215406       gaps   5 214577       VB code 00000110 10111000 10000101 00001101 00001100 10110001      Variable byte (VB) encoding uses an integral number of bytes to encode a gap. The last 7 bits of a byte are ``payload'' and encode part of the gap. The first bit of the byte is a continuation bit . It is set to 1 for the last byte of the encoded gap and to 0 otherwise. To decode a variable byte code, we read a sequence of bytes with continuation bit 0 terminated by a byte with continuation bit 1. We then extract and concatenate the 7-bit parts. Figure 5.8 gives pseudocode for VB encoding and decoding and Table 5.4 an example of a VB-encoded postings list. With VB compression, the size of the compressed index for Reuters-RCV1 is 116 MB as we verified in an experiment. This is a more than 50% reduction of the size of the uncompressed index (see Table 5.6 ). The idea of VB encoding can also be applied to larger or smaller units than bytes: 32-bit words, 16-bit words, and 4-bit words or nibbles . Larger words further decrease the amount of bit manipulation necessary at the cost of less effective (or no) compression. Word sizes smaller than bytes get even better compression ratios at the cost of more bit manipulation. In general, bytes offer a good compromise between compression ratio and speed of decompression. For most IR systems variable byte codes offer an excellent tradeoff between time and space. They are also simple to implement - most of the alternatives referred to in Section 5.4 are more complex. But if disk space is a scarce resource, we can achieve better compression ratios by using bit-level encodings, in particular two closely related encodings: codes, which we will turn to next, and codes (Exercise 5.3.2 ).
iir_5_3_2	Gamma codes   Table 5.5: Some examples of unary and codes. Unary codes are only shown for the smaller numbers. Commas in codes are for readability only and are not part of the actual codes.   number unary code length offset code     0 0           1 10 0   0     2 110 10 0 10,0     3 1110 10 1 10,1     4 11110 110 00 110,00     9 1111111110 1110 001 1110,001     13   1110 101 1110,101     24   11110 1000 11110,1000     511   111111110 11111111 111111110,11111111     1025   11111111110 0000000001 11111111110,0000000001    VB codes use an adaptive number of bytes depending on the size of the gap. Bit-level codes adapt the length of the code on the finer grained bit level. The simplest bit-level code is unary code . The unary code of is a string of 1s followed by a 0 (see the first two columns of Table 5.5 ). Obviously, this is not a very efficient code, but it will come in handy in a moment. How efficient can a code be in principle? Assuming the gaps with are all equally likely, the optimal encoding uses bits for each . So some gaps ( in this case) cannot be encoded with fewer than bits. Our goal is to get as close to this lower bound as possible. A method that is within a factor of optimal is encoding . codes implement variable-length encoding by splitting the representation of a gap into a pair of length and offset. Offset is in binary, but with the leading 1 removed. For example, for 13 (binary 1101) offset is 101. Length encodes the length of offset in unary code. For 13, the length of offset is 3 bits, which is 1110 in unary. The code of 13 is therefore 1110101, the concatenation of length 1110 and offset 101. The right hand column of Table 5.5 gives additional examples of codes. A code is decoded by first reading the unary code up to the 0 that terminates it, for example, the four bits 1110 when decoding 1110101. Now we know how long the offset is: 3 bits. The offset 101 can then be read correctly and the 1 that was chopped off in encoding is prepended: 101 1101 = 13. The length of offset is bits and the length of length is bits, so the length of the entire code is bits. codes are always of odd length and they are within a factor of 2 of what we claimed to be the optimal encoding length . We derived this optimum from the assumption that the gaps between and are equiprobable. But this need not be the case. In general, we do not know the probability distribution over gaps a priori.  Figure 5.9: Entropy as a function of for a sample space with two outcomes and .      entropy   (4)    5.9         It can be shown that the lower bound for the expected length of a code is if certain conditions hold (see the references). It can further be shown that for , encoding is within a factor of 3 of this optimal encoding, approaching 2 for large : (5)        universal In addition to universality, codes have two other properties that are useful for index compression. First, they are prefix free , namely, no code is the prefix of another. This means that there is always a unique decoding of a sequence of codes - and we do not need delimiters between them, which would decrease the efficiency of the code. The second property is that codes are parameter free . For many other efficient codes, we have to fit the parameters of a model (e.g., the binomial distribution) to the distribution of gaps in the index. This complicates the implementation of compression and decompression. For instance, the parameters need to be stored and retrieved. And in dynamic indexing, the distribution of gaps can change, so that the original parameters are no longer appropriate. These problems are avoided with a parameter-free code. How much compression of the inverted index do codes achieve? To answer this question we use Zipf's law, the term distribution model introduced in Section 5.1.2 . According to Zipf's law, the collection frequency is proportional to the inverse of the rank , that is, there is a constant such that:     (6)       (7)     (8)         harmonic number     (9)       (10)  4.2  Figure 5.10: Stratification of terms for estimating the size of a encoded inverted index.            5.10 Encoding the gaps of size with codes, the number of bits needed for the postings list of a term in the th block (corresponding to one row in the figure) is:            (11)   For Reuters-RCV1, 400,000 and (12)  When we run compression on Reuters-RCV1, the actual size of the compressed index is even lower: 101 MB, a bit more than one tenth of the size of the collection. The reason for the discrepancy between predicted and actual value is that (i) Zipf's law is not a very good approximation of the actual distribution of term frequencies for Reuters-RCV1 and (ii) gaps are not uniform. The Zipf model predicts an index size of 251 MB for the unrounded numbers from Table 4.2 . If term frequencies are generated from the Zipf model and a compressed index is created for these artificial terms, then the compressed size is 254 MB. So to the extent that the assumptions about the distribution of term frequencies are accurate, the predictions of the model are correct.   Table: Index and dictionary compression for Reuters-RCV1. The compression ratio depends on the proportion of actual text in the collection. Reuters-RCV1 contains a large amount of XML markup. Using the two best compression schemes, encoding and blocking with front coding, the ratio compressed index to collection size is therefore especially small for Reuters-RCV1: . .   data structure size in MB   dictionary, fixed-width 19.211.2   dictionary, term pointers into string 10.8 7.6   , with blocking, 10.3 7.1   , with blocking   front coding 7.9 5.9   collection (text, xml markup etc) 3600.0   collection (text) 960.0   term incidence matrix 40,000.0   postings, uncompressed (32-bit words) 400.0   postings, uncompressed (20 bits) 250.0   postings, variable byte encoded 116.0   postings, encoded 101.0  Table 5.6 summarizes the compression techniques covered in this chapter. The term incidence matrix (Figure 1.1 , page 1.1 ) for Reuters-RCV1 has size bits or 40 GB. The numbers were the collection (3600 MB and 960 MB) are for the encoding of RCV1 of CD, which uses one byte per character, not Unicode. codes achieve great compression ratios - about 15% better than variable byte codes for Reuters-RCV1. But they are expensive to decode. This is because many bit-level operations - shifts and masks - are necessary to decode a sequence of codes as the boundaries between codes will usually be somewhere in the middle of a machine word. As a result, query processing is more expensive for codes than for variable byte codes. Whether we choose variable byte or encoding depends on the characteristics of an application, for example, on the relative weights we give to conserving disk space versus maximizing query response time. The compression ratio for the index in Table 5.6 is about 25%: 400 MB (uncompressed, each posting stored as a 32-bit word) versus 101 MB () and 116 MB (VB). This shows that both and VB codes meet the objectives we stated in the beginning of the chapter. Index compression substantially improves time and space efficiency of indexes by reducing the amount of disk space needed, increasing the amount of information that can be kept in the cache, and speeding up data transfers from disk to memory. Exercises. Compute variable byte codes for the numbers in Tables 5.3 5.5 . Compute variable byte and codes for the postings list 777, 17743, 294068, 31251336. Use gaps instead of docIDs where possible. Write binary codes in 8-bit blocks. Consider the postings list with a corresponding list of gaps . Assume that the length of the postings list is stored separately, so the system knows when a postings list is complete. Using variable byte encoding: (i) What is the largest gap you can encode in 1 byte? (ii) What is the largest gap you can encode in 2 bytes? (iii) How many bytes will the above postings list require under this encoding? (Count only space for encoding the sequence of numbers.) A little trick is to notice that a gap cannot be of length 0 and that the stuff left to encode after shifting cannot be 0. Based on these observations: (i) Suggest a modification to variable byte encoding that allows you to encode slightly larger gaps in the same amount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What is the largest gap you can encode in 2 bytes? (iv) How many bytes will the postings list in Exercise 5.3.2 require under this encoding? (Count only space for encoding the sequence of numbers.) From the following sequence of -coded gaps, reconstruct first the gap sequence and then the postings sequence: 1110001110101011111101101111011. codes are relatively inefficient for large numbers (e.g., 1025 in Table 5.5 ) as they encode the length of the offset in inefficient unary code. codes differ from codes in that they encode the first part of the code (length) in code instead of unary code. The encoding of offset is the same. For example, the code of 7 is 10,0,11 (again, we add commas for readability). 10,0 is the code for length (2 in this case) and the encoding of offset (11) is unchanged. (i) Compute the codes for the other numbers in Table 5.5 . For what range of numbers is the code shorter than the code? (ii) code beats variable byte code in Table 5.6 because the index contains stop words and thus many small gaps. Show that variable byte code is more compact if larger gaps dominate. (iii) Compare the compression ratios of code and variable byte code for a distribution of gaps dominated by large gaps. Go through the above calculation of index size and explicitly state all the approximations that were made to arrive at Equation 11. For a collection of your choosing, determine the number of documents and terms and the average length of a document. (i) How large is the inverted index predicted to be by Equation 11? (ii) Implement an indexer that creates a -compressed inverted index for the collection. How large is the actual index? (iii) Implement an indexer that uses variable byte encoding. How large is the variable byte encoded index? Table: Two gap sequences to be merged in blocked sort-based indexing   encoded gap sequence of run 1 1110110111111001011111111110100011111001     encoded gap sequence of run 2 11111010000111111000100011111110010000011111010101   To be able to hold as many postings as possible in main memory, it is a good idea to compress intermediate index files during index construction. (i) This makes merging runs in blocked sort-based indexing more complicated. As an example, work out the -encoded merged sequence of the gaps in Table 5.7 . (ii) Index construction is more space efficient when using compression. Would you also expect it to be faster? (i) Show that the size of the vocabulary is finite according to Zipf's law and infinite according to Heaps' law. (ii) Can we derive Heaps' law from Zipf's law?
iir_5_4	References and further reading Heaps' law was discovered by Heaps (1978). See also Baeza-Yates and Ribeiro-Neto (1999). A detailed study of vocabulary growth in large collections is (Williams and Zobel, 2005). Zipf's law is due to Zipf (1949). Witten and Bell (1990) investigate the quality of the fit obtained by the law. Other term distribution models, including K mixture and two-poisson model, are discussed by Manning and Schütze (1999, Chapter 15). Carmel et al. (2001), Büttcher and Clarke (2006), Blanco and Barreiro (2007), and Ntoulas and Cho (2007) show that lossy compression can achieve good compression with no or no significant decrease in retrieval effectiveness. Dictionary compression is covered in detail by Witten et al. (1999, Chapter 4), which is recommended as additional reading. Subsection 5.3.1 is based on (Scholer et al., 2002). The authors find that variable byte codes process queries two times faster than either bit-level compressed indexes or uncompressed indexes with a 30% penalty in compression ratio compared with the best bit-level compression method. They also show that compressed indexes can be superior to uncompressed indexes not only in disk usage, but also in query processing speed. Compared with VB codes, ``variable nibble'' codes showed 5% to 10% better compression and up to one third worse effectiveness in one experiment (Anh and Moffat, 2005). Trotman (2003) also recommends using VB codes unless disk space is at a premium. In recent work, Anh and Moffat (2006a;2005) and Zukowski et al. (2006) have constructed word-aligned binary codes that are both faster in decompression and at least as efficient as VB codes. Zhang et al. (2007) investigate the increased effectiveness of caching when a number of different compression techniques for postings lists are used on modern hardware. codes (Exercise 5.3.2 ) and codes were introduced by Elias (1975), who proved that both codes are universal. In addition, codes are asymptotically optimal for . codes perform better than codes if large numbers (greater than 15) dominate. A good introduction to information theory, including the concept of entropy , is (Cover and Thomas, 1991). While Elias codes are only asymptotically optimal, arithmetic codes (Witten et al., 1999, Section 2.4) can be constructed to be arbitrarily close to the optimum for any . Several additional index compression techniques are covered by Witten et al. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommend using parameterized codes for index compression, codes that explicitly model the probability distribution of gaps for each term. For example, they show that Golomb codes achieve better compression ratios than codes for large collections. Moffat and Zobel (1992) compare several parameterized methods, including LLRUN (Fraenkel and Klein, 1985). The distribution of gaps in a postings list depends on the assignment of docIDs to documents. A number of researchers have looked into assigning docIDs in a way that is conducive to the efficient compression of gap sequences (Moffat and Stuiver, 1996; Blandford and Blelloch, 2002; Silvestri et al., 2004; Blanco and Barreiro, 2006; Silvestri, 2007). These techniques assign docIDs in a small range to documents in a cluster where a cluster can consist of all documents in a given time period, on a particular web site, or sharing another property. As a result, when a sequence of documents from a cluster occurs in a postings list, their gaps are small and can be more effectively compressed. Different considerations apply to the compression of term frequencies and word positions than to the compression of docIDs in postings lists. See Scholer et al. (2002) and Zobel and Moffat (2006). Zobel and Moffat (2006) is recommended in general as an in-depth and up-to-date tutorial on inverted indexes, including index compression. This chapter only looks at index compression for Boolean retrieval. For ranked retrieval (Chapter 6 ), it is advantageous to order postings according to term frequency instead of docID. During query processing, the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked documents found so far. It is not a good idea to precompute and store weights in the index (as opposed to frequencies) because they cannot be compressed as well as integers (see impactordered). Document compression can also be important in an efficient information retrieval system. de Moura et al. (2000) and Brisaboa et al. (2007) describe compression schemes that allow direct searching of terms and phrases in the compressed text, which is infeasible with standard text compression utilities like gzip and compress. Exercises. We have defined unary codes as being ``10'': sequences of 1s terminated by a 0. Interchanging the roles of 0s and 1s yields an equivalent ``01'' unary code. When this 01 unary code is used, the construction of a code can be stated as follows: (1) Write down in binary using bits. (2) Prepend 0s. (i) Encode the numbers in Table 5.5 in this alternative code. (ii) Show that this method produces a well-defined alternative code in the sense that it has the same length and can be uniquely decoded. Unary code is not a universal code in the sense defined above. However, there exists a distribution over gaps for which unary code is optimal. Which distribution is this? Give some examples of terms that violate the assumption that gaps all have the same size (which we made when estimating the space requirements of a -encoded index). What are general characteristics of these terms? Consider a term whose postings list has size , say, . Compare the size of the -compressed gap-encoded postings list if the distribution of the term is uniform (i.e., all gaps have the same size) versus its size when the distribution is not uniform. Which compressed postings list is smaller? Work out the sum in Equation 12 and show it adds up to about 251 MB. Use the numbers in Table 4.2 , but do not round , , and the number of vocabulary blocks.
iir_6	Scoring, term weighting and the vector space model Thus far we have dealt with indexes that support Boolean queries: a document either matches or does not match a query. In the case of large document collections, the resulting number of matching documents can far exceed the number a human user could possibly sift through. Accordingly, it is essential for a search engine to rank-order the documents matching a query. To do this, the search engine computes, for each matching document, a score with respect to the query at hand. In this chapter we initiate the study of assigning a score to a (query, document) pair. This chapter consists of three main ideas. We introduce parametric and zone indexes in Section 6.1 , which serve two purposes. First, they allow us to index and retrieve documents by metadata such as the language in which a document is written. Second, they give us a simple means for scoring (and thereby ranking) documents in response to a query. Next, in Section 6.2 we develop the idea of weighting the importance of a term in a document, based on the statistics of occurrence of the term. In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document. This view is known as vector space scoring. 6.4 7 As we develop these ideas, the notion of a query will assume multiple nuances. In Section 6.1 we consider queries in which specific query terms occur in specified regions of a matching document. Beginning Section 6.2 we will in fact relax the requirement of matching specific regions of a document; instead, we will look at so-called free text queries that simply consist of query terms with no specification on their relative order, importance or where in a document they should be found. The bulk of our study of scoring will be in this latter notion of a query being such a set of terms.   Subsections Parametric and zone indexes Weighted zone scoring Learning weights The optimal weight g Term frequency and weighting Inverse document frequency Tf-idf weighting The vector space model for scoring Dot products Queries as vectors Computing vector scores Variant tf-idf functions Sublinear tf scaling Maximum tf normalization Document and query weighting schemes Pivoted normalized document length References and further reading
iir_6_1	Parametric and zone indexes We have thus far viewed a document as a sequence of terms. In fact, most documents have additional structure. Digital documents generally encode, in machine-recognizable form, certain metadata associated with each document. By metadata, we mean specific forms of data about a document, such as its author(s), title and date of publication. This metadata would generally include fields such as the date of creation and the format of the document, as well the author and possibly the title of the document. The possible values of a field should be thought of as finite - for instance, the set of all dates of authorship. Consider queries of the form ``find documents authored by William Shakespeare in 1601, containing the phrase alas poor Yorick''. Query processing then consists as usual of postings intersections, except that we may merge postings from standard inverted as well as parametric indexes . There is one parametric index for each field (say, date of creation); it allows us to select only the documents matching a date specified in the query. Figure 6.1 illustrates the user's view of such a parametric search. Some of the fields may assume ordered values, such as dates; in the example query above, the year 1601 is one such field value. The search engine may support querying ranges on such ordered values; to this end, a structure like a B-tree may be used for the field's dictionary.  Parametric search.In this example we have a collection with fields allowing us to select publications by zones such as Author and fields such as Language. Zones are similar to fields, except the contents of a zone can be arbitrary free text. Whereas a field may take on a relatively small set of values, a zone can be thought of as an arbitrary, unbounded amount of text. For instance, document titles and abstracts are generally treated as zones. We may build a separate inverted index for each zone of a document, to support queries such as ``find documents with merchant in the title and william in the author list and the phrase gentle rain in the body''. This has the effect of building an index that looks like Figure 6.2. Whereas the dictionary for a parametric index comes from a fixed vocabulary (the set of languages, or the set of dates), the dictionary for a zone index must structure whatever vocabulary stems from the text of that zone.   In fact, we can reduce the size of the dictionary by encoding the zone in which a term occurs in the postings. In Figure 6.3 for instance, we show how occurrences of william in the title and author zones of various documents are encoded. Such an encoding is useful when the size of the dictionary is a concern (because we require the dictionary to fit in main memory). But there is another important reason why the encoding of Figure 6.3 is useful: the efficient computation of scores using a technique we will call weighted zone scoring .  Figure 6.3: Zone index in which the zone is encoded in the postings rather than the dictionary.   Subsections Weighted zone scoring Learning weights The optimal weight g
iir_6_1_1	Weighted zone scoring 6.1 Given a Boolean query and a document , weighted zone scoring assigns to the pair a score in the interval , by computing a linear combination of zone scores, where each zone of the document contributes a Boolean value. More specifically, consider a set of documents each of which has zones. Let such that . For , let be the Boolean score denoting a match (or absence thereof) between and the th zone. For instance, the Boolean score from a zone could be 1 if all the query term(s) occur in that zone, and zero otherwise; indeed, it could be any Boolean function that maps the presence of query terms in a zone to . Then, the weighted zone score is defined to be (13)   ranked Boolean retrieval Worked example. Consider the query shakespeare in a collection in which each document has three zones: author, title and body. The Boolean score function for a zone takes on the value 1 if the query term shakespeare is present in the zone, and zero otherwise. Weighted zone scoring in such a collection would require three weights and , respectively corresponding to the author, title and body zones. Suppose we set and (so that the three weights add up to 1); this corresponds to an application in which a match in the author zone is least important to the overall score, the title zone somewhat more, and the body contributes even more. Thus if the term shakespeare were to appear in the title and body zones but not the author zone of a document, the score of this document would be 0.8. End worked example. How do we implement the computation of weighted zone scores? A simple approach would be to compute the score for each document in turn, adding in all the contributions from the various zones. However, we now show how we may compute weighted zone scores directly from inverted indexes. The algorithm of Figure 6.4 treats the case when the query is a two-term query consisting of query terms and , and the Boolean function is AND: 1 if both query terms are present in a zone and 0 otherwise. Following the description of the algorithm, we describe the extension to more complex queries and Boolean functions.   The reader may have noticed the close similarity between this algorithm and that in Figure 1.6 . Indeed, they represent the same postings traversal, except that instead of merely adding a document to the set of results for a Boolean AND query, we now compute a score for each such document. Some literature refers to the array scores[] above as a set of accumulators . The reason for this will be clear as we consider more complex Boolean functions than the AND; thus we may assign a non-zero score to a document even if it does not contain all query terms.
iir_6_1_2	Learning weights   machine-learned relevance 15  We are provided with a set of training examples, each of which is a tuple consisting of a query and a document , together with a relevance judgment for on . In the simplest form, each relevance judgments is either Relevant or Non-relevant. More sophisticated implementations of the methodology make use of more nuanced judgments. The weights are then ``learned'' from these examples, in order that the learned scores approximate the relevance judgments in the training examples. For weighted zone scoring, the process may be viewed as learning a linear function of the Boolean match scores contributed by the various zones. The expensive component of this methodology is the labor-intensive assembly of user-generated relevance judgments from which to learn the weights, especially in a collection that changes frequently (such as the Web). We now detail a simple example that illustrates how we can reduce the problem of learning the weights to a simple optimization problem. We now consider a simple case of weighted zone scoring, where each document has a title zone and a body zone. Given a query and a document , we use the given Boolean match function to compute Boolean variables and , depending on whether the title (respectively, body) zone of matches query . For instance, the algorithm in Figure 6.4 uses an AND of the query terms for this Boolean function. We will compute a score between 0 and 1 for each (document, query) pair using and by using a constant , as follows:  (14)   training examples      Relevant Non-relevant 6.5 Figure 6.5: An illustration of training examples. For each training example we have Boolean values and that we use to compute a score from (14)  (15)   Relevant Non-relevant   (16)      (17)    17 Picking the best value of in (17) in the formulation of Section 6.1.3 reduces to the problem of minimizing a quadratic function of over the interval . This reduction is detailed in Section 6.1.3 .
iir_6_1_3	The optimal weight g We begin by noting that for any training example for which and , the score computed by Equation 14 is . In similar fashion, we may write down the score computed by Equation 14 for the three other possible combinations of and ; this is summarized in Figure 6.6 .  Figure 6.6: The four possible combinations of and . Let (respectively, ) denote the number of training examples for which and and the editorial judgment is Relevant (respectively, Non-relevant). Then the contribution to the total error in Equation 17 from training examples for which and is (18)    17  (19)  By differentiating Equation 19 with respect to and setting the result to zero, it follows that the optimal value of is (20)  Exercises. When using weighted zone scoring, is it necessary for all zones to use the same Boolean match function? In Example 6.1.1 above with weights and , what are all the distinct score values a document may get? Rewrite the algorithm in Figure 6.4 to the case of more than two query terms. Write pseudocode for the function WeightedZone for the case of two postings lists in Figure 6.4 . Apply Equation 20 to the sample training set in Figure 6.5 to estimate the best value of for this sample. For the value of estimated in Exercise 6.1.3, compute the weighted zone score for each (query, document) example. How do these scores relate to the relevance judgments in Figure 6.5 (quantized to 0/1)? Why does the expression for in (20) not involve training examples in which and have the same value?
iir_6_2	Term frequency and weighting  free text query 1.4 Towards this end, we assign to each term in a document a weight for that term, that depends on the number of occurrences of the term in the document. We would like to compute a score between a query term  and a document , based on the weight of in . The simplest approach is to assign the weight to be equal to the number of occurrences of term  in document . This weighting scheme is referred to as term frequency and is denoted , with the subscripts denoting the term and the document in order. For a document , the set of weights determined by the weights above (or indeed any weighting function that maps the number of occurrences of in to a positive real value) may be viewed as a quantitative digest of that document. In this view of a document, known in the literature as the bag of words model , the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material (in contrast to Boolean retrieval). We only retain information on the number of occurrences of each term. Thus, the document ``Mary is quicker than John'' is, in this view, identical to the document ``John is quicker than Mary''. Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content. We will develop this intuition further in Section 6.3 . Before doing so we first study the question: are all words in a document equally important? Clearly not; in Section 2.2.2 (page ) we looked at the idea of stop words - words that we decide not to index at all, and therefore do not contribute in any way to retrieval and scoring.   Subsections Inverse document frequency Tf-idf weighting
iir_6_2_1	Inverse document frequency collection frequency,  Instead, it is more commonplace to use for this purpose the document frequency , defined to be the number of documents in the collection that contain a term . This is because in trying to discriminate between documents for the purpose of scoring it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term. Figure 6.7: Collection frequency (cf) and document frequency (df) behave differently, as in this example from the Reuters collection. 6.7 How is the document frequency df of a term used to scale its weight? Denoting as usual the total number of documents in a collection by , we define the inverse document frequency of a term as follows:  (21)  Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low. Figure 6.8 gives an example of idf's in the Reuters collection of 806,791 documents; in this example logarithms are to the base 10. In fact, as we will see in Exercise 6.2.2 , the precise base of the logarithm is not material to ranking. We will give on page 11.3.3 a justification of the particular form in Equation 21.
iir_6_2_2	Tf-idf weighting We now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document. The tf-idf weighting scheme assigns to term a weight in document given by  (22)  In other words, assigns to term a weight in document that is  highest when occurs many times within a small number of documents (thus lending high discriminating power to those documents); lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal); lowest when the term occurs in virtually all documents. At this point, we may view each document as a vector with one component corresponding to each term in the dictionary, together with a weight for each component that is given by (22). For dictionary terms that do not occur in a document, this weight is zero. This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3 . As a first step, we introduce the overlap score measure: the score of a document is the sum, over all query terms, of the number of times each of the query terms occurs in . We can refine this idea so that we add up not the number of occurrences of each query term in , but instead the tf-idf weight of each term in . (23)  6.3 23 Exercises. Why is the idf of a term always finite? What is the idf of a term that occurs in every document? Compare this with the use of stop word lists. Consider the table of term frequencies for 3 documents denoted Doc1, Doc2, Doc3 in Figure 6.9 . Figure 6.9: Table of tf values for Exercise  6.2.2. Compute the tf-idf weights for the terms car, auto, insurance, best, for each document, using the idf values from Figure 6.8 . Can the tf-idf weight of a term in a document exceed 1? How does the base of the logarithm in (21) affect the score calculation in (23)? How does the base of the logarithm affect the relative scores of two documents on a given query? If the logarithm in (21) is computed base 2, suggest a simple approximation to the idf of a term.
iir_6_3	The vector space model for scoring In Section 6.2 (page ) we developed the notion of a document vector that captures the relative importance of the terms in a document. The representation of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval operations ranging from scoring documents on a query, document classification and document clustering. We first develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2 ) of queries as vectors in the same vector space as the document collection.   Subsections Dot products Queries as vectors Computing vector scores
iir_6_3_1	Dot products    6.2  bag of words How do we quantify the similarity between two documents in this vector space? A first attempt might consider the magnitude of the vector difference between two document vectors. This measure suffers from a drawback: two documents with very similar content can have a significant vector difference simply because one is much longer than the other. Thus the relative distributions of terms may be identical in the two documents, but the absolute term frequencies of one may be far larger.        cosine similarity      (24)   dot product  inner product     Euclidean lengths         The effect of the denominator of Equation 24 is thus to length-normalize the vectors and to unit vectors and . We can then rewrite (24) as (25)  Worked example. Consider the documents in Figure 6.9 . We now apply Euclidean normalization to the tf values from the table, for each of the three documents in the table. The quantity has the values 30.56, 46.84 and 41.30 respectively for Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these documents are shown in Figure 6.11 .  Figure 6.11: Euclidean normalized tf values for documents in Figure 6.9 . End worked example. Thus, (25) can be viewed as the dot product of the normalized versions of the two document vectors. This measure is the cosine of the angle between the two vectors, shown in Figure 6.10 . What use is the similarity measure ? Given a document (potentially one of the in the collection), consider searching for the documents in the collection most similar to . Such a search is useful in a system where a user may identify a document and seek others like it - a feature available in the results lists of search engines as a more like this feature. We reduce the problem of finding the document(s) most similar to to that of finding the with the highest dot products ( values) . We could do this by computing the dot products between and each of , then picking off the highest resulting values.   Worked example. Figure 6.12 shows the number of occurrences of three terms (affection, jealous and gossip) in each of the following three novels: Jane Austen's Sense and Sensibility (SaS) and Pride and Prejudice (PaP) and Emily Brontë's Wuthering Heights (WH). Of course, there are many other terms occurring in each of these novels. In this example we represent each of these novels as a unit vector in three dimensions, corresponding to these three terms (only); we use raw term frequencies here, with no idf multiplier. The resulting weights are as shown in Figure 6.13.   Now consider the cosine similarities between pairs of the resulting three-dimensional vectors. A simple computation shows that sim((SAS), (PAP)) is 0.999, whereas sim((SAS), (WH)) is 0.888; thus, the two books authored by Austen (SaS and PaP) are considerably closer to each other than to Brontë's Wuthering Heights. In fact, the similarity between the first two is almost perfect (when restricted to the three terms we consider). Here we have considered tf weights, but we could of course use other term weight functions. End worked example. Viewing a collection of documents as a collection of vectors leads to a natural view of a collection as a term-document matrix and jealousy would under stemming be considered as a single dimension. This matrix view will prove to be useful in Chapter 18 .
iir_6_3_2	Queries as vectors There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query jealous gossip. This query turns into the unit vector on the three coordinates of Figures 6.12 and 6.13. The key idea now: to assign to each document a score equal to the dot product (26)  In the example of Figure 6.13, Wuthering Heights is the top-scoring document for this query with a score of 0.509, with Pride and Prejudice a distant second with a score of 0.085, and Sense and Sensibility last with a score of 0.074. This simple example is somewhat misleading: the number of dimensions in practice will be far larger than three: it will equal the vocabulary size . To summarize, by viewing a query as a ``bag of words'', we are able to treat it as a very short document. As a consequence, we can use the cosine similarity between the query vector and a document vector as a measure of the score of the document for that query. The resulting scores can then be used to select the top-scoring documents for a query. Thus we have (27)  6.3.2 6.4 Computing the cosine similarities between the query vector and each document vector in the collection, sorting the resulting scores and selecting the top documents can be expensive -- a single similarity computation can entail a dot product in tens of thousands of dimensions, demanding tens of thousands of arithmetic operations. In Section 7.1 we study how to use an inverted index for this purpose, followed by a series of heuristics for improving on this. Worked example. We now consider the query best car insurance on a fictitious collection with documents where the document frequencies of auto, best, car and insurance are respectively 5000, 50000, 10000 and 1000.  term query document product   tf df idf tf wf   auto 0 5000 2.3 0 1 1 0.41 0 best 1 50000 1.3 1.3 0 0 0 0 car 1 10000 2.0 2.0 1 1 0.41 0.82 insurance 1 1000 3.0 3.0 2 2 0.82 2.46 In this example the weight of a term in the query is simply the idf (and zero for a term not in the query, such as auto); this is reflected in the column header (the entry for auto is zero because the query does not contain the termauto). For documents, we use tf weighting with no use of idf but with Euclidean normalization. The former is shown under the column headed wf, while the latter is shown under the column headed . Invoking (23) now gives a net score of . End worked example.
iir_6_3_3	Computing vector scores  free text query      7  Figure 6.14: The basic algorithm for computing vector space scores. Figure 6.14 gives the basic algorithm for computing vector space scores. The array Length holds the lengths (normalization factors) for each of the documents, whereas the array Scores holds the scores for each of the documents. When the scores are finally computed in Step 9, all that remains in Step 10 is to pick off the documents with the highest scores. The outermost loop beginning Step 3 repeats the updating of Scores, iterating over each query term in turn. In Step 5 we calculate the weight in the query vector for term . Steps 6-8 update the score of each document by adding in the contribution from term . This process of adding in contributions one query term at a time is sometimes known as term-at-a-time scoring or accumulation, and the elements of the array are therefore known as accumulators . For this purpose, it would appear necessary to store, with each postings entry, the weight of term in document (we have thus far used either tf or tf-idf for this weight, but leave open the possibility of other functions to be developed in Section 6.4 ). In fact this is wasteful, since storing this weight may require a floating point number. Two ideas help alleviate this space problem. First, if we are using inverse document frequency , we need not precompute ; it suffices to store at the head of the postings for . Second, we store the term frequency for each postings entry. Finally, Step 12 extracts the top scores - this requires a priority queue data structure, often implemented using a heap. Such a heap takes no more than comparisons to construct, following which each of the top scores can be extracted from the heap at a cost of comparisons. Note that the general algorithm of Figure 6.14 does not prescribe a specific implementation of how we traverse the postings lists of the various query terms; we may traverse them one term at a time as in the loop beginning at Step 3, or we could in fact traverse them concurrently as in Figure 1.6 . In such a concurrent postings traversal we compute the scores of one document at a time, so that it is sometimes called document-at-a-time scoring. We will say more about this in Section 7.1.5 . Exercises. If we were to stem jealous and jealousy to a common stem before setting up the vector space, detail how the definitions of tf and idf should be modified. Recall the tf-idf weights computed in Exercise 6.2.2. Compute the Euclidean normalized document vectors for each of the documents, where each vector has four components, one for each of the four terms. Verify that the sum of the squares of the components of each of the document vectors in Exercise 6.3.3 is 1 (to within rounding error). Why is this the case? With term weights as computed in Exercise 6.3.3, rank the three documents by computed score for the query car insurance, for each of the following cases of term weighting in the query: The weight of a term is 1 if present in the query, 0 otherwise. Euclidean normalized idf.
iir_6_4_2	Maximum tf normalization        (30)        30  smoothing  13        23   23  The method is unstable in the following sense: a change in the stop word list can dramatically alter term weightings (and therefore ranking). Thus, it is hard to tune. A document may contain an outlier term with an unusually large number of occurrences of that term, not representative of the content of that document. More generally, a document in which the most frequent term appears roughly as often as many other terms should be treated differently from one with a more skewed distribution.
iir_6_4_3	Document and query weighting schemes Equation 27 is fundamental to information retrieval systems that use any form of vector space scoring. Variations from one vector space scoring method to another hinge on the specific choices of weights in the vectors and . Figure 6.15 lists some of the principal weighting schemes in use for each of and , together with a mnemonic for representing a specific combination of weights; this system of mnemonics is sometimes called SMART notation, following the authors of an early text retrieval system. The mnemonic for representing a combination of weights takes the form ddd.qqq where the first triplet gives the term weighting of the document vector, while the second triplet gives the weighting in the query vector. The first letter in each triplet specifies the term frequency component of the weighting, the second the document frequency component, and the third the form of normalization used. It is quite common to apply different normalization functions to and . For example, a very standard weighting scheme is lnc.ltc, where the document vector has log-weighted term frequency, no idf (for both effectiveness and efficiency reasons), and cosine normalization, while the query vector uses log-weighted term frequency, idf weighting, and cosine normalization.
iir_6_4_4	Pivoted normalized document length In Section 6.3.1 we normalized each document vector by the Euclidean length of the vector, so that all document vectors turned into unit vectors. In doing so, we eliminated all information on the length of the original document; this masks some subtleties about longer documents. First, longer documents will - as a result of containing more terms - have higher tf values. Second, longer documents contain more distinct terms. These factors can conspire to raise the scores of longer documents, which (at least for some information needs) is unnatural. Longer documents can broadly be lumped into two categories: (1) verbose documents that essentially repeat the same content - in these, the length of the document does not alter the relative weights of different terms; (2) documents covering multiple different topics, in which the search terms probably match small segments of the document but not all of it - in this case, the relative weights of terms are quite different from a single short document that matches the query terms. Compensating for this phenomenon is a form of document length normalization that is independent of term and document frequencies. To this end, we introduce a form of normalizing the vector representations of documents in the collection, so that the resulting ``normalized'' documents are not necessarily of unit length. Then, when we compute the dot product score between a (unit) query vector and such a normalized document, the score is skewed to account for the effect of document length on relevance. This form of compensation for document length is known as pivoted document length normalization .  Figure 6.16: Pivoted document length normalization. Consider a document collection together with an ensemble of queries for that collection. Suppose that we were given, for each query and for each document , a Boolean judgment of whether or not is relevant to the query ; in Chapter 8 we will see how to procure such a set of relevance judgments for a query ensemble and a document collection. Given this set of relevance judgments, we may compute a probability of relevance as a function of document length, averaged over all queries in the ensemble. The resulting plot may look like the curve drawn in thick lines in Figure 6.16 . To compute this curve, we bucket documents by length and compute the fraction of relevant documents in each bucket, then plot this fraction against the median document length of each bucket. (Thus even though the ``curve'' in Figure 6.16 appears to be continuous, it is in fact a histogram of discrete buckets of document length.) On the other hand, the curve in thin lines shows what might happen with the same documents and query ensemble if we were to use relevance as prescribed by cosine normalization Equation 27 - thus, cosine normalization has a tendency to distort the computed relevance vis-à-vis the true relevance, at the expense of longer documents. The thin and thick curves crossover at a point corresponding to document length , which we refer to as the pivot length; dashed lines mark this point on the and axes. The idea of pivoted document length normalization would then be to ``rotate'' the cosine normalization curve counter-clockwise about so that it more closely matches thick line representing the relevance vs. document length curve. As mentioned at the beginning of this section, we do so by using in Equation 27 a normalization factor for each document vector that is not the Euclidean length of that vector, but instead one that is larger than the Euclidean length for documents of length less than , and smaller for longer documents. To this end, we first note that the normalizing term for in the denominator of Equation 27 is its Euclidean length, denoted . In the simplest implementation of pivoted document length normalization, we use a normalization factor in the denominator that is linear in , but one of slope as in Figure 6.17 . In this figure, the axis represents , while the axis represents possible normalization factors we can use. The thin line depicts the use of cosine normalization. Notice the following aspects of the thick line representing pivoted length normalization: It is linear in the document length and has the form (31) where is the cosine normalization value at which the two curves intersect. Its slope is and (3) it crosses the line at piv. 31  (32)     Figure 6.17: Implementing pivoted document length normalization by linear scaling. Of course, pivoted document length normalization is not appropriate for all applications. For instance, in a collection of answers to frequently asked questions (say, at a customer service website), relevance may have little to do with document length. In other cases the dependency may be more complex than can be accounted for by a simple linear pivoted normalization. In such cases, document length can be used as a feature in the machine learning based scoring approach of Section 6.1.2 . Exercises. One measure of the similarity of two vectors is the Euclidean distance (or distance ) between them: (33) Given a query and documents , we may rank the documents in order of increasing Euclidean distance from . Show that if and the are all normalized to unit vectors, then the rank ordering produced by Euclidean distance is identical to that produced by cosine similarities. Compute the vector space similarity between the query ``digital cameras'' and the document ``digital cameras and video cameras'' by filling out the empty columns in Table 6.1 . Assume , logarithmic term weighting (wf columns) for query and document, idf weighting for the query only and cosine normalization for the document only. Treat and as a stop word. Enter term counts in the tf columns. What is the final similarity score? Table 6.1: Cosine computation for Exercise 6.4.4 .   query document   word tf wf df idf tf wf digital     10,000             video     100,000             cameras     50,000             Show that for the query affection, the relative ordering of the scores of the three documents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous gossip. In turning a query into a unit vector in Figure 6.13, we assigned equal weights to each of the query terms. What other principled approaches are plausible? Consider the case of a query term that is not in the set of indexed terms; thus our standard construction of the query vector results in not being in the vector space created from the collection. How would one adapt the vector space representation to handle this case? Refer to the tf and idf values for four terms and three documents in Exercise 6.2.2. Compute the two top scoring documents on the query best car insurance for each of the following weighing schemes: (i) nnn.atc; (ii) ntc.atc. Suppose that the word coyote does not occur in the collection used in Exercises 6.2.2 and 6.4.4. How would one compute ntc.atc scores for the query coyote insurance?
iir_6_5	References and further reading Chapter 7 develops the computational aspects of vector space scoring. Luhn (1957;1958) describes some of the earliest reported applications of term weighting. His paper dwells on the importance of medium frequency terms (terms that are neither too commonplace nor too rare) and may be thought of as anticipating tf-idf and related weighting schemes. Spärck Jones (1972) builds on this intuition through detailed experiments showing the use of inverse document frequency in term weighting. A series of extensions and theoretical justifications of idf are due to Salton and Buckley (1987) Robertson and Jones (1976), Croft and Harper (1979) and Papineni (2001). Robertson maintains a web page (http://www.soi.city.ac.uk/~ser/idf.html) containing the history of idf, including soft copies of early papers that predated electronic versions of journal article. Singhal et al. (1996a) develop pivoted document length normalization. Probabilistic language models (Chapter 11 ) develop weighting techniques that are more nuanced than tf-idf; the reader will find this development in Section 11.4.3 . We observed that by assigning a weight for each term in a document, a document may be viewed as a vector of term weights, one for each term in the collection. The SMART information retrieval system at Cornell (Salton, 1971b) due to Salton and colleagues was perhaps the first to view a document as a vector of weights. The basic computation of cosine scores as described in Section 6.3.3 is due to Zobel and Moffat (2006). The two query evaluation strategies term-at-a-time and document-at-a-time are discussed by Turtle and Flood (1995). The SMART notation for tf-idf term weighting schemes in Figure 6.15 is presented in (Singhal et al., 1996b;1995, Salton and Buckley, 1988). Not all versions of the notation are consistent; we most closely follow (Singhal et al., 1996b). A more detailed and exhaustive notation was developed in Moffat and Zobel (1998), considering a larger palette of schemes for term and document frequency weighting. Beyond the notation, Moffat and Zobel (1998) sought to set up a space of feasible weighting functions through which hill-climbing approaches could be used to begin with weighting schemes that performed well, then make local improvements to identify the best combinations. However, they report that such hill-climbing methods failed to lead to any conclusions on the best weighting schemes.
iir_7	Computing scores in a complete search system Chapter 6 developed the theory underlying term weighting in documents for the purposes of scoring, leading up to vector space models and the basic cosine scoring algorithm of Section 6.3.3 (page ). In this chapter we begin in Section 7.1 with heuristics for speeding up this computation; many of these heuristics achieve their speed at the risk of not finding quite the top documents matching the query. Some of these heuristics generalize beyond cosine scoring. With Section 7.1 in place, we have essentially all the components needed for a complete search engine. We therefore take a step back from cosine scoring, to the more general problem of computing scores in a search engine. In Section 7.2 we outline a complete search engine, including indexes and structures to support not only cosine scoring but also more general ranking factors such as query term proximity. We describe how all of the various pieces fit together in Section 7.2.4 . We conclude this chapter with Section 7.3 , where we discuss how the vector space model for free text queries interacts with common query operators.   Subsections Efficient scoring and ranking Inexact top K document retrieval Index elimination Champion lists Static quality scores and ordering Impact ordering Cluster pruning Components of an information retrieval system Tiered indexes Query-term proximity Designing parsing and scoring functions Putting it all together Vector space scoring and query operator interaction Boolean retrieval Wildcard queries Phrase queries References and further reading
iir_7_1	Efficient scoring and ranking We begin by recapping the algorithm of Figure 6.14 . For a query such as jealous gossip, two observations are immediate: The unit vector has only two non-zero components. In the absence of any weighting for query terms, these non-zero components are equal - in this case, both equal 0.707. For the purpose of ranking the documents matching this query, we are really interested in the relative (rather than absolute) scores of the documents in the collection. To this end, it suffices to compute the cosine similarity from each document unit vector to (in which all non-zero components of the query vector are set to 1), rather than to the unit vector . For any two documents (34)      6.14  7.1  6.3.3   Figure 7.1: A faster algorithm for vector space scores. Given these scores, the final step before presenting results to a user is to pick out the highest-scoring documents. While one could sort the complete set of scores, a better approach is to use a heap to retrieve only the top documents in order. Where is the number of documents with non-zero cosine scores, constructing such a heap can be performed in comparison steps, following which each of the highest scoring documents can be ``read off'' the heap with comparison steps.   Subsections Inexact top K document retrieval Index elimination Champion lists Static quality scores and ordering Impact ordering Cluster pruning
iir_7_1_1	Inexact top K document retrieval Thus far, we have focused on retrieving precisely the highest-scoring documents for a query. We now consider schemes by which we produce documents that are likely to be among the highest scoring documents for a query. In doing so, we hope to dramatically lower the cost of computing the documents we output, without materially altering the user's perceived relevance of the top results. Consequently, in most applications it suffices to retrieve documents whose scores are very close to those of the best. In the sections that follow we detail schemes that retrieve such documents while potentially avoiding computing scores for most of the documents in the collection. Such inexact top- retrieval is not necessarily, from the user's perspective, a bad thing. The top documents by the cosine measure are in any case not necessarily the best for the query: cosine similarity is only a proxy for the user's perceived relevance. In Sections 7.1.2 -7.1.6 below, we give heuristics using which we are likely to retrieve documents with cosine scores close to those of the top documents. The principal cost in computing the output stems from computing cosine similarities between the query and a large number of documents. Having a large number of documents in contention also increases the selection cost in the final stage of culling the top documents from a heap. We now consider a series of ideas designed to eliminate a large number of documents without computing their cosine scores. The heuristics have the following two-step scheme: Find a set of documents that are contenders, where . does not necessarily contain the top-scoring documents for the query, but is likely to have many documents with scores near those of the top . Return the top-scoring documents in .
iir_7_1_2	Index elimination  We only consider documents containing terms whose idf exceeds a preset threshold. Thus, in the postings traversal, we only traverse the postings for terms with high idf. This has a fairly significant benefit: the postings lists of low-idf terms are generally long; with these removed from contention, the set of documents for which we compute cosines is greatly reduced. One way of viewing this heuristic: low-idf terms are treated as stop words and do not contribute to scoring. For instance, on the query catcher in the rye, we only traverse the postings for catcher and rye. The cutoff threshold can of course be adapted in a query-dependent manner. We only consider documents that contain many (and as a special case, all) of the query terms. This can be accomplished during the postings traversal; we only compute scores for documents containing all (or many) of the query terms. A danger of this scheme is that by requiring all (or even many) query terms to be present in a document before considering it for cosine computation, we may end up with fewer than candidate documents in the output. This issue will discussed further in Section 7.2.1 .
iir_7_1_3	Champion lists champion lists fancy lists top docs        champion list  Now, given a query we create a set as follows: we take the union of the champion lists for each of the terms comprising . We now restrict cosine computation to only the documents in . A critical parameter in this scheme is the value , which is highly application dependent. Intuitively, should be large compared with , especially if we use any form of the index elimination described in Section 7.1.2 . One issue here is that the value is set at the time of index construction, whereas is application dependent and may not be available until the query is received; as a result we may (as in the case of index elimination) find ourselves with a set that has fewer than documents. There is no reason to have the same value of for all terms in the dictionary; it could for instance be set to be higher for rarer terms.
iir_7_1_4	Static quality scores and ordering  static quality scores    static  21 The net score for a document is some combination of together with the query-dependent score induced (say) by (27). The precise combination may be determined by the learning methods of Section 6.1.2 , to be developed further in Section 15.4.1 ; but for the purposes of our exposition here, let us consider a simple sum: (35)   24 First, consider ordering the documents in the postings list for each term by decreasing value of . This allows us to perform the postings intersection algorithm of Figure 1.6 . In order to perform the intersection by a single pass through the postings of each query term, the algorithm of Figure 1.6 relied on the postings being ordered by document IDs. But in fact, we only required that all postings be ordered by a single common ordering; here we rely on the values to provide this common ordering. This is illustrated in Figure 7.2 , where the postings are ordered in decreasing order of .  A static quality-ordered index.In this example we assume that Doc1, Doc2 and Doc3 respectively have static quality scores . The first idea is a direct extension of champion lists: for a well-chosen value , we maintain for each term a global champion list of the documents with the highest values for . The list itself is, like all the postings lists considered so far, sorted by a common order (either by document IDs or by static quality). Then at query time, we only compute the net scores (35) for documents in the union of these global champion lists. Intuitively, this has the effect of focusing on documents likely to have large net scores. We conclude the discussion of global champion lists with one further idea. We maintain for each term two postings lists consisting of disjoint sets of documents, each sorted by values. The first list, which we call high, contains the documents with the highest tf values for . The second list, which we call low, contains all other documents containing . When processing a query, we first scan only the high lists of the query terms, computing net scores for any document on the high lists of all (or more than a certain number of) query terms. If we obtain scores for documents in the process, we terminate. If not, we continue the scanning into the low lists, scoring documents in these postings lists. This idea is developed further in Section 7.2.1 .
iir_7_1_5	Impact ordering 7.1.4 6.3.3  document-at-a-time  6.14  term-at-a-time The idea is to order the documents in the postings list of term by decreasing order of . Thus, the ordering of documents will vary from one postings list to another, and we cannot compute scores by a concurrent traversal of the postings lists of all query terms. Given postings lists ordered by decreasing order of , two ideas have been found to significantly lower the number of documents for which we accumulate scores: (1) when traversing the postings list for a query term , we stop after considering a prefix of the postings list - either after a fixed number of documents have been seen, or after the value of has dropped below a threshold; (2) when accumulating scores in the outer loop of Figure 6.14 , we consider the query terms in decreasing order of idf, so that the query terms likely to contribute the most to the final scores are considered first. This latter idea too can be adaptive at the time of processing a query: as we get to query terms with lower idf, we can determine whether to proceed based on the changes in document scores from processing the previous query term. If these changes are minimal, we may omit accumulation from the remaining query terms, or alternatively process shorter prefixes of their postings lists. These ideas form a common generalization of the methods introduced in Sections 7.1.2 -7.1.4 . We may also implement a version of static ordering in which each postings list is ordered by an additive combination of static and query-dependent scores. We would again lose the consistency of ordering across postings, thereby having to process query terms one at time accumulating scores for all documents as we go along. Depending on the particular scoring function, the postings list for a document may be ordered by other quantities than term frequency; under this more general setting, this idea is known as impact ordering.
iir_7_1_6	Cluster pruning cluster pruning Pick documents at random from the collection. Call these leaders. For each document that is not a leader, we compute its nearest leader. followers   Given a query , find the leader that is closest to . This entails computing cosine similarities from to each of the leaders. The candidate set consists of together with its followers. We compute the cosine scores for all documents in this candidate set. The use of randomly chosen leaders for clustering is fast and likely to reflect the distribution of the document vectors in the vector space: a region of the vector space that is dense in documents is likely to produce multiple leaders and thus a finer partition into sub-regions. This illustrated in Figure 7.3 .  Figure 7.3: Cluster pruning. Variations of cluster pruning introduce additional parameters and , both of which are positive integers. In the pre-processing step we attach each follower to its closest leaders, rather than a single closest leader. At query time we consider the leaders closest to the query . Clearly, the basic scheme above corresponds to the case . Further, increasing or increases the likelihood of finding documents that are more likely to be in the set of true top-scoring documents, at the expense of more computation. We reiterate this approach when describing clustering in Chapter 16 (page 16.1 ). Exercises. We suggested above (Figure 7.2 ) that the postings for static quality ordering be in decreasing order of . Why do we use the decreasing rather than the increasing order? When discussing champion lists, we simply used the documents with the largest tf values to create the champion list for . But when considering global champion lists, we used idf as well, identifying documents with the largest values of . Why do we differentiate between these two cases? If we were to only have one-term queries, explain why the use of global champion lists with suffices for identifying the highest scoring documents. What is a simple modification to this idea if we were to only have -term queries for any fixed integer ? Explain how the common global ordering by values in all high and low lists helps make the score computation efficient. Consider again the data of Exercise 6.4.4 with nnn.atc for the query-dependent scoring. Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2. Determine under Equation 35 what ranges of static quality score for Doc3 result in it being the first, second or third result for the query best car insurance. Sketch the frequency-ordered postings for the data in Figure 6.9 . Let the static quality scores for Doc1, Doc2 and Doc3 in Figure 6.11 be respectively 0.25, 0.5 and 1. Sketch the postings for impact ordering when each postings list is ordered by the sum of the static quality score and the Euclidean normalized tf values in Figure 6.11 . The nearest-neighbor problem in the plane is the following: given a set of data points on the plane, we preprocess them into some data structure such that, given a query point , we seek the point in that is closest to in Euclidean distance. Clearly cluster pruning can be used as an approach to the nearest-neighbor problem in the plane, if we wished to avoid computing the distance from to every one of the query points. Devise a simple example on the plane so that with two leaders, the answer returned by cluster pruning is incorrect (it is not the data point closest to ).
iir_7_2_1	Tiered indexes 7.1.2     tiered indexes  champion lists 7.4 6.9  Tiered indexes.If we fail to get results from tier 1, query processing ``falls back'' to tier 2, and so on. Within each tier, postings are ordered by document ID.
iir_7_2_2	Query-term proximity Especially for free text queries on the web (Chapter 19 ), users prefer a document in which most or all of the query terms appear close to each other, because this is evidence that the document has text focused on their query intent. Consider a query with two or more query terms, . Let be the width of the smallest window in a document that contains all the query terms, measured in the number of words in the window. For instance, if the document were to simply consist of the sentence The quality of mercy is not strained, the smallest window for the query strained mercy would be 4. Intuitively, the smaller that is, the better that matches the query. In cases where the document does not contain all of the query terms, we can set to be some enormous number. We could also consider variants in which only words that are not stop words are considered in computing . Such proximity-weighted scoring functions are a departure from pure cosine similarity and closer to the ``soft conjunctive'' semantics that Google and other web search engines evidently use. How can we design such a proximity-weighted scoring function to depend on ? The simplest answer relies on a ``hand coding'' technique we introduce below in Section 7.2.3 . A more scalable approach goes back to Section 6.1.2 - we treat the integer as yet another feature in the scoring function, whose importance is assigned by machine learning, as will be developed further in Section 15.4.1 .
iir_7_2_3	Designing parsing and scoring functions  free text queries The answer of course depends on the user population, the query distribution and the collection of documents. Typically, a query parser is used to translate the user-specified keywords into a query with various operators that is executed against the underlying indexes. Sometimes, this execution can entail multiple queries against the underlying indexes; for example, the query parser may issue a stream of queries: Run the user-generated query string as a phrase query. Rank them by vector space scoring using as query the vector consisting of the 3 terms rising interest rates. If fewer than ten documents contain the phrase rising interest rates, run the two 2-term phrase queries rising interest and interest rates; rank these using vector space scoring, as well. If we still have fewer than ten results, run the vector space query consisting of the three individual query terms.  accumulates evidence The answer depends on the setting. In many enterprise settings we have application builders who make use of a toolkit of available scoring operators, along with a query parsing layer, with which to manually configure the scoring function as well as the query parser. Such application builders make use of the available zones, metadata and knowledge of typical documents and queries to tune the parsing and scoring. In collections whose characteristics change infrequently (in an enterprise application, significant changes in collection and query characteristics typically happen with infrequent events such as the introduction of new document formats or document management systems, or a merger with another company). Web search on the other hand is faced with a constantly changing document collection with new characteristics being introduced all the time. It is also a setting in which the number of scoring factors can run into the hundreds, making hand-tuned scoring a difficult exercise. To address this, it is becoming increasingly common to use machine-learned scoring, extending the ideas we introduced in Section 6.1.2 , as will be discussed further in Section 15.4.1 .
iir_7_2_4	Putting it all together 7.5  A complete search system.Data paths are shown primarily for a free text query. In this figure, documents stream in from the left for parsing and linguistic processing (language and format detection, tokenization and stemming). The resulting stream of tokens feeds into two modules. First, we retain a copy of each parsed document in a document cache. This will enable us to generate results snippets : snippets of text accompanying each document in the results list for a query. This snippet tries to give a succinct explanation to the user of why the document matches the query. The automatic generation of such snippets is the subject of Section 8.7 . A second copy of the tokens is fed to a bank of indexers that create a bank of indexes including zone and field indexes that store the metadata for each document, (tiered) positional indexes, indexes for spelling correction and other tolerant retrieval, and structures for accelerating inexact top- retrieval. A free text user query (top center) is sent down to the indexes both directly and through a module for generating spelling-correction candidates. As noted in Chapter 3 the latter may optionally be invoked only when the original query fails to retrieve enough results. Retrieved documents (dark arrow) are passed to a scoring module that computes scores based on machine-learned ranking (MLR), a technique that builds on Section 6.1.2 (to be further developed in Section 15.4.1 ) for scoring and ranking documents. Finally, these ranked documents are rendered as a results page. Exercises. Explain how the postings intersection algorithm first introduced in Section 1.3 can be adapted to find the smallest integer that contains all query terms. Adapt this procedure to work when not all query terms are present in a document.
iir_7_3	Vector space scoring and query operator interaction Vector space scoring supports so-called free text retrieval, in which a query is specified as a set of words without any query operators connecting them. It allows documents matching the query to be scored and thus ranked, unlike the Boolean, wildcard and phrase queries studied earlier. Classically, the interpretation of such free text queries was that at least one of the query terms be present in any retrieved document. However more recently, web search engines such as Google have popularized the notion that a set of terms typed into their query boxes (thus on the face of it, a free text query) carries the semantics of a conjunctive query that only retrieves documents containing all or most query terms.   Subsections Boolean retrieval Wildcard queries Phrase queries
iir_7_4	References and further reading Heuristics for fast query processing with early termination are described by Persin et al. (1996), Anh et al. (2001), Garcia et al. (2004), Anh and Moffat (2006b). Cluster pruning is investigated by Singitham et al. (2004) and by Chierichetti et al. (2007); see also Section 16.6 (page ). Champion lists are described in Persin (1994) and (under the name top docs ) in Brown (1995), and further developed in Long and Suel (2003), Brin and Page (1998). While these heuristics are well-suited to free text queries that can be viewed as vectors, they complicate phrase queries; see Anh and Moffat (2006c) for an index structure that supports both weighted and Boolean/phrase searches. Carmel et al. (2001) Clarke et al. (2000) and Song et al. (2005) treat the use of query term proximity in assessing relevance. Pioneering work on learning of ranking functions was done by Fuhr (1989), Fuhr and Pfeifer (1994), Cooper et al. (1994), Bartell et al. (1998), Bartell (1994) and by Cohen et al. (1998).
iir_8	Evaluation in information retrieval We have seen in the preceding chapters many alternatives in designing an IR system. How do we know which of these techniques are effective in which applications? Should we use stop lists? Should we stem? Should we use inverse document frequency weighting? Information retrieval has developed as a highly empirical discipline, requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections. In this chapter we begin with a discussion of measuring the effectiveness of IR systems (Section 8.1 ) and the test collections that are most often used for this purpose (Section 8.2 ). We then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results (Section 8.3 ). This includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate. We then extend these notions and develop further measures for evaluating ranked retrieval results (Section 8.4 ) and discuss developing reliable and informative test collections (Section 8.5 ). We then step back to introduce the notion of user utility, and how it is approximated by the use of document relevance (Section 8.6 ). The key utility measure is user happiness. Speed of response and the size of the index are factors in user happiness. It seems reasonable to assume that relevance of results is the most important factor: blindingly fast, useless answers do not make a user happy. However, user perceptions do not always coincide with system designers' notions of quality. For example, user happiness commonly depends very strongly on user interface design issues, including the layout, clarity, and responsiveness of the user interface, which are independent of the quality of the results returned. We touch on other measures of the quality of a system, in particular the generation of high-quality result summary snippets, which strongly influence user utility, but are not measured in the basic relevance ranking paradigm (Section 8.7 ).   Subsections Information retrieval system evaluation Standard test collections Evaluation of unranked retrieval sets Evaluation of ranked retrieval results Assessing relevance Critiques and justifications of the concept of relevance A broader perspective: System quality and user utility System issues User utility Refining a deployed system Results snippets References and further reading
iir_8_1	Information retrieval system evaluation To measure ad hoc information retrieval effectiveness in the standard way, we need a test collection consisting of three things: A document collection A test suite of information needs, expressible as queries A set of relevance judgments, standardly a binary assessment of either relevant or nonrelevant for each query-document pair.  relevant nonrelevant   gold standard  ground truth Relevance is assessed relative to an , not a query. For example, an information need might be: Information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine. wine and red and white and heart and attack and effective 8.5.1 Many systems contain various weights (often known as parameters) that can be adjusted to tune system performance. It is wrong to report results on a test collection which were obtained by tuning these parameters to maximize performance on that collection. That is because such tuning overstates the expected performance of the system, because the weights will be set to maximize performance on one particular set of queries rather than for a random sample of queries. In such cases, the correct procedure is to have one or more development test collections , and to tune the parameters on the development test collection. The tester then runs the system with those weights on the test collection and reports the results on that collection as an unbiased estimate of performance.
iir_8_2	Standard test collections Here is a list of the most standard test collections and evaluation series. We focus particularly on test collections for ad hoc information retrieval system evaluation, but also mention a couple of similar test collections for text classification. The Cranfield collection. This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs. Text Retrieval Conference (TREC) . The U.S. National Institute of Standards and Technology (NIST) has run a large IR test bed evaluation series since 1992. Within this framework, there have been many tracks over a range of different test collections, but the best known test collections are the ones used for the TREC Ad Hoc track during the first 8 TREC evaluations between 1992 and 1999. In total, these test collections comprise 6 CDs containing 1.89 million documents (mainly, but not exclusively, newswire articles) and relevance judgments for 450 information needs, which are called topics and specified in detailed text passages. Individual test collections are defined over different subsets of this data. The early TRECs each consisted of 50 information needs, evaluated over different but overlapping sets of documents. TRECs 6-8 provide 150 information needs over about 528,000 newswire and Foreign Broadcast Information Service articles. This is probably the best subcollection to use in future work, because it is the largest and the topics are more consistent. Because the test document collections are so large, there are no exhaustive relevance judgments. Rather, NIST assessors' relevance judgments are available only for the documents that were among the top returned for some system which was entered in the TREC evaluation for which the information need was developed. In more recent years, NIST has done evaluations on larger document collections, including the 25 million page GOV2 web page collection. From the beginning, the NIST test document collections were orders of magnitude larger than anything available to researchers previously and GOV2 is now the largest Web collection easily available for research purposes. Nevertheless, the size of GOV2 is still more than 2 orders of magnitude smaller than the current size of the document collections indexed by the large web search companies. NII Test Collections for IR Systems ( NTCIR ). The NTCIR project has built various test collections of similar sizes to the TREC collections, focusing on East Asian language and cross-language information retrieval , where queries are made in one language over a document collection containing documents in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-en.html Cross Language Evaluation Forum ( CLEF ). This evaluation series has concentrated on European languages and cross-language information retrieval. See: http://www.clef-campaign.org/ and Reuters-RCV1. For text classification, the most used test collection has been the Reuters-21578 collection of 21578 newswire articles; see Chapter 13 , page 13.6 . More recently, Reuters released the much larger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents; see Chapter 4 , page 4.2 . Its scale and rich annotation makes it a better basis for future research. 20 Newsgroups . This is another widely used text classification collection, collected by Ken Lang. It consists of 1000 articles from each of 20 Usenet newsgroups (the newsgroup name being regarded as the category). After the removal of duplicate articles, as it is usually used, it contains 18941 articles.
iir_8_3	Evaluation of unranked retrieval sets Given these ingredients, how is system effectiveness measured? The two most frequent and basic measures for information retrieval effectiveness are precision and recall. These are first defined for the simple case where an IR system returns a set of documents for a query. We will see later how to extend these notions to ranked retrieval situations. Precision ( ) is the fraction of retrieved documents that are relevant (36) Recall ( ) is the fraction of relevant documents that are retrieved (37)     (38) (39)   An obvious alternative that may occur to the reader is to judge an information retrieval system by its accuracy , that is, the fraction of its classifications that are correct. In terms of the contingency table above, . This seems plausible, since there are two actual classes, relevant and nonrelevant, and an information retrieval system can be thought of as a two-class classifier which attempts to label them as such (it retrieves the subset of documents which it believes to be relevant). This is precisely the effectiveness measure often used for evaluating machine learning classification problems. There is a good reason why accuracy is not an appropriate measure for information retrieval problems. In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category. A system tuned to maximize accuracy can appear to perform well by simply deeming all documents nonrelevant to all queries. Even if the system is quite good, trying to label some documents as relevant will almost always lead to a high rate of false positives. However, labeling all documents as nonrelevant is completely unsatisfying to an information retrieval system user. Users are always going to want to see some documents, and can be assumed to have a certain tolerance for seeing some false positives providing that they get some useful information. The measures of precision and recall concentrate the evaluation on the return of true positives, asking what percentage of the relevant documents have been found and how many false positives have also been returned. The advantage of having the two numbers for precision and recall is that one is more important than the other in many circumstances. Typical web surfers would like every result on the first page to be relevant (high precision) but have not the slightest interest in knowing let alone looking at every document that is relevant. In contrast, various professional searchers such as paralegals and intelligence analysts are very concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it. Individuals searching their hard disks are also often interested in high recall searches. Nevertheless, the two quantities clearly trade off against one another: you can always get a recall of 1 (but very low precision) by retrieving all documents for all queries! Recall is a non-decreasing function of the number of documents retrieved. On the other hand, in a good system, precision usually decreases as the number of documents retrieved is increased. In general we want to get some amount of recall while tolerating only a certain percentage of false positives. A single measure that trades off precision versus recall is the F measure , which is the weighted harmonic mean of precision and recall: (40)     balanced F measure        (41)       Graph comparing the harmonic mean to other means.The graph shows a slice through the calculation of various means of precision and recall for the fixed recall value of 70%. The harmonic mean is always less than either the arithmetic or geometric mean, and often quite close to the minimum of the two numbers. When the precision is also 70%, all the measures coincide. Why do we use a harmonic mean rather than the simpler average (arithmetic mean)? Recall that we can always get 100% recall by just returning all documents, and therefore we can always get a 50% arithmetic mean by the same process. This strongly suggests that the arithmetic mean is an unsuitable measure to use. In contrast, if we assume that 1 document in 10,000 is relevant to the query, the harmonic mean score of this strategy is 0.02%. The harmonic mean is always less than or equal to the arithmetic mean and the geometric mean. When the values of two numbers differ greatly, the harmonic mean is closer to their minimum than to their arithmetic mean; see Figure 8.1 . Exercises. An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall? The balanced F measure (a.k.a. F) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than ``averaging'' (using the arithmetic mean)? Derive the equivalence between the two formulas for F measure shown in Equation 40, given that .
iir_8_4	Evaluation of ranked retrieval results  Figure 8.2: Precision/recall graph. Precision, recall, and the F measure are set-based measures. They are computed using unordered sets of documents. We need to extend these measures (or to define new measures) if we are to evaluate the ranked retrieval results that are now standard with search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top retrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve , such as the one shown in Figure 8.2 . Precision-recall curves have a distinctive saw-tooth shape: if the document retrieved is nonrelevant then recall is the same as for the top documents, but precision has dropped. If it is relevant, then both precision and recall increase, and the curve jags up and to the right. It is often useful to remove these jiggles and the standard way to do this is with an interpolated precision: the interpolated precision at a certain recall level is defined as the highest precision found for any recall level : (42)  The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher). Interpolated precision is shown by a thinner line in Figure 8.2 . With this definition, the interpolated precision at a recall of 0 is well-defined (Exercise 8.4 ).   Recall Interp.   Precision 0.0 1.00 0.1 0.67 0.2 0.63 0.3 0.55 0.4 0.45 0.5 0.41 0.6 0.36 0.7 0.29 0.8 0.13 0.9 0.10 1.0 0.08 Calculation of 11-point Interpolated Average Precision.This is for the precision-recall curve shown in Figure 8.2 .  Examining the entire precision-recall curve is very informative, but there is often a desire to boil this information down to a few numbers, or perhaps even a single number. The traditional way of doing this (used for instance in the first 8 TREC Ad Hoc evaluations) is the 11-point interpolated average precision . For each information need, the interpolated precision is measured at the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. For the precision-recall curve in Figure 8.2 , these 11 values are shown in Table 8.1 . For each recall level, we then calculate the arithmetic mean of the interpolated precision at that recall level for each information need in the test collection. A composite precision-recall curve showing 11 points can then be graphed. Figure 8.3 shows an example graph of such results from a representative good system at TREC 8.  Averaged 11-point precision/recall graph across 50 queries for a representative TREC system.The Mean Average Precision for this system is 0.2553. In recent years, other measures have become more common. Most standard among the TREC community is Mean Average Precision (MAP), which provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. For a single information need, Average Precision is the average of the precision value obtained for the set of top documents existing after each relevant document is retrieved, and this value is then averaged over information needs. That is, if the set of relevant documents for an information need is and is the set of ranked retrieval results from the top result until you get to document , then (43)   Using MAP, fixed recall levels are not chosen, and there is no interpolation. The MAP value for a test collection is the arithmetic mean of average precision values for individual information needs. (This has the effect of weighting each information need equally in the final reported number, even if many documents are relevant to some queries whereas very few are relevant to other queries.) Calculated MAP scores normally vary widely across information needs when measured within a single system, for instance, between 0.1 and 0.7. Indeed, there is normally more agreement in MAP for an individual information need across systems than for MAP scores for different information needs for the same system. This means that a set of test information needs must be large and diverse enough to be representative of system effectiveness across different queries. The above measures factor in precision at all recall levels. For many prominent applications, particularly web search, this may not be germane to users. What matters is rather how many good results there are on the first page or the first three pages. This leads to measuring precision at fixed low levels of retrieved results, such as 10 or 30 documents. This is referred to as ``Precision at '', for example ``Precision at 10''. It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at . An alternative, which alleviates this problem, is R-precision . It requires having a set of known relevant documents , from which we calculate the precision of the top documents returned. (The set may be incomplete, such as when is formed by creating relevance judgments for the pooled top results of particular systems in a set of experiments.) R-precision adjusts for the size of the set of relevant documents: A perfect system could score 1 on this metric for each query, whereas, even a perfect system could only achieve a precision at 20 of 0.4 if there were only 8 documents in the collection relevant to an information need. Averaging this measure across queries thus makes more sense. This measure is harder to explain to naive users than Precision at but easier to explain than MAP. If there are relevant documents for a query, we examine the top results of a system, and find that are relevant, then by definition, not only is the precision (and hence R-precision) , but the recall of this result set is also . Thus, R-precision turns out to be identical to the break-even point , another measure which is sometimes used, defined in terms of this equality relationship holding. Like Precision at , R-precision describes only one point on the precision-recall curve, rather than attempting to summarize effectiveness across the curve, and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal F-measure) or a retrieval level of interest to a particular application (Precision at ). Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on the curve.  Figure 8.4: The ROC curve corresponding to the precision-recall curve in Figure 8.2 . . Another concept sometimes used in evaluation is an ROC curve . (``ROC'' stands for ``Receiver Operating Characteristics'', but knowing that doesn't help most people.) An ROC curve plots the true positive rate or sensitivity against the false positive rate or ( ). Here, sensitivity is just another term for recall. The false positive rate is given by . Figure 8.4 shows the ROC curve corresponding to the precision-recall curve in Figure 8.2 . An ROC curve always goes from the bottom left to the top right of the graph. For a good system, the graph climbs steeply on the left side. For unranked result sets, specificity , given by , was not seen as a very useful notion. Because the set of true negatives is always so large, its value would be almost 1 for all information needs (and, correspondingly, the value of the false positive rate would be almost 0). That is, the ``interesting'' part of Figure 8.2 is , a part which is compressed to a small corner of Figure 8.4 . But an ROC curve could make sense when looking over the full retrieval spectrum, and it provides another way of looking at the data. In many fields, a common aggregate measure is to report the area under the ROC curve, which is the ROC analog of MAP. Precision-recall curves are sometimes loosely referred to as ROC curves. This is understandable, but not accurate. A final approach that has seen increasing adoption, especially when employed with machine learning approaches to ranking svm-ranking is measures of cumulative gain , and in particular normalized discounted cumulative gain ( NDCG ). NDCG is designed for situations of non-binary notions of relevance (cf. Section 8.5.1 ). Like precision at , it is evaluated over some number of top search results. For a set of queries , let be the relevance score assessors gave to document for query . Then, (44)       Exercises. What are the possible values for interpolated precision at a recall level of 0? Must there always be a break-even point between precision and recall? Either show there must be or give a counter-example. What is the relationship between the value of and the break-even point? The Dice coefficient of two sets is a measure of their intersection scaled by their size (giving a value in the range 0 to 1): (45) Show that the balanced F-measure () is equal to the Dice coefficient of the retrieved and relevant document sets. Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result): System 1   R N R N N   N N N R R System 2   N R N N R   R R N N N What is the MAP of each system? Which has a higher MAP? Does this result intuitively make sense? What does it say about what is important in getting a good MAP score? What is the R-precision of each system? (Does it rank the systems the same as MAP?) The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection. R R N N N   N N N R N   R N N N R   N N N N R What is the precision of the system on the top 20? What is the F on the top 20? What is the uninterpolated precision of the system at 25% recall? What is the interpolated precision at 33% recall? Assume that these 20 documents are the complete result set of the system. What is the MAP for the query? Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned. f. What is the largest possible MAP that this system could have? g. What is the smallest possible MAP that this system could have? h. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)-(g). For this example, how large (in absolute terms) can the error for the MAP be by calculating (e) instead of (f) and (g) for this query?
iir_8_5	Assessing relevance To properly evaluate a system, your test information needs must be germane to the documents in the test document collection, and appropriate for predicted usage of the system. These information needs are best designed by domain experts. Using random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distribution of information needs. Given information needs and documents, you need to collect relevance assessments. This is a time-consuming and expensive process involving human beings. For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained. For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query. The most standard approach is pooling , where relevance is assessed over a subset of the collection that is formed from the top documents returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process.   Table 8.2: Calculating the kappa statistic.     Judge 2 Relevance     Yes   No Total Judge 1 Yes 300   20 320 Relevance No 10   70 80   Total 310   90 400 Observed proportion of the times the judges agreed Pooled marginals Probability that the two judges agreed by chance Kappa statistic   A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query. Rather, humans and their relevance judgments are quite idiosyncratic and variable. But this is not a problem to be solved: in the final analysis, the success of an IR system depends on how good it is at satisfying the needs of these idiosyncratic humans, one information need at a time. Nevertheless, it is interesting to consider and measure how much agreement between judges there is on relevance judgments. In the social sciences, a common measure for agreement between judges is the kappa statistic . It is designed for categorical judgments and corrects a simple agreement rate for the rate of chance agreement. (46)      marginal  8.2 Interjudge agreement of relevance has been measured within the TREC evaluations and for medical IR collections. Using the above rules of thumb, the level of agreement normally falls in the range of ``fair'' (0.67-0.8). The fact that human agreement on a binary relevance judgment is quite modest is one reason for not requiring more fine-grained relevance labeling from the test set creator. To answer the question of whether IR evaluation results are valid despite the variation of individual assessors' judgments, people have experimented with evaluations taking one or the other of two judges' opinions as the gold standard. The choice can make a considerable absolute difference to reported scores, but has in general been found to have little impact on the relative effectiveness ranking of either different systems or variants of a single system which are being compared for effectiveness.   Subsections Critiques and justifications of the concept of relevance
iir_8_5_1	Critiques and justifications of the concept of relevance The advantage of system evaluation, as enabled by the standard model of relevant and nonrelevant documents, is that we have a fixed setting in which we can vary IR systems and system parameters to carry out comparative experiments. Such formal testing is much less expensive and allows clearer diagnosis of the effect of changing system parameters than doing user studies of retrieval effectiveness. Indeed, once we have a formal measure that we have confidence in, we can proceed to optimize effectiveness by machine learning methods, rather than tuning parameters by hand. Of course, if the formal measure poorly describes what users actually want, doing this will not be effective in improving user satisfaction. Our perspective is that, in practice, the standard formal measures for IR evaluation, although a simplification, are good enough, and recent work in optimizing formal evaluation measures in IR has succeeded brilliantly. There are numerous examples of techniques developed in formal evaluation settings, which improve effectiveness in operational settings, such as the development of document length normalization methods within the context of TREC ( and 11.4.3 ) and machine learning methods for adjusting parameter weights in scoring (Section 6.1.2 ). That is not to say that there are not problems latent within the abstractions used. The relevance of one document is treated as independent of the relevance of other documents in the collection. (This assumption is actually built into most retrieval systems - documents are scored against queries, not against each other - as well as being assumed in the evaluation methods.) Assessments are binary: there aren't any nuanced assessments of relevance. Relevance of a document to an information need is treated as an absolute, objective decision. But judgments of relevance are subjective, varying across people, as we discussed above. In practice, human assessors are also imperfect measuring instruments, susceptible to failures of understanding and attention. We also have to assume that users' information needs do not change as they start looking at retrieval results. Any results based on one collection are heavily skewed by the choice of collection, queries, and relevance judgment set: the results may not translate from one domain to another or to a different user population. Some of these problems may be fixable. A number of recent evaluations, including INEX, some TREC tracks, and NTCIR have adopted an ordinal notion of relevance with documents divided into 3 or 4 classes, distinguishing slightly relevant documents from highly relevant documents. See Section 10.4 (page ) for a detailed discussion of how this is implemented in the INEX evaluations. One clear problem with the relevance-based assessment that we have presented is the distinction between relevance and marginal relevance : whether a document still has distinctive usefulness after the user has looked at certain other documents (Carbonell and Goldstein, 1998). Even if a document is highly relevant, its information can be completely redundant with other documents which have already been examined. The most extreme case of this is documents that are duplicates - a phenomenon that is actually very common on the World Wide Web - but it can also easily occur when several documents provide a similar precis of an event. In such circumstances, marginal relevance is clearly a better measure of utility to the user. Maximizing marginal relevance requires returning documents that exhibit diversity and novelty. One way to approach measuring this is by using distinct facts or entities as evaluation units. This perhaps more directly measures true utility to the user but doing this makes it harder to create a test collection. Exercises. Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you've written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}. docID Judge 1 Judge 2 1 0 0 2 0 0 3 1 1 4 1 1 5 1 0 6 1 0 7 1 0 8 1 0 9 0 1 10 0 1 11 0 1 12 0 1 Calculate the kappa measure between the two judges. Calculate precision, recall, and of your system if a document is considered relevant only if the two judges agree. Calculate precision, recall, and of your system if a document is considered relevant if either judge thinks it is relevant.
iir_8_6	A broader perspective: System quality and user utility Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies. These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface. In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility.   Subsections System issues User utility Refining a deployed system
iir_8_6_1	System issues There are many practical benchmarks on which to rate an information retrieval system beyond its retrieval quality. These include: How fast does it index, that is, how many documents per hour does it index for a certain distribution over document lengths? (cf. Chapter 4 ) How fast does it search, that is, what is its latency as a function of index size? How expressive is its query language? How fast is it on complex queries? How large is its document collection, in terms of the number of documents or the collection having information distributed across a broad range of topics? measurable
iir_8_6_2	User utility What we would really like is a way of quantifying aggregate user happiness, based on the relevance, speed, and user interface of a system. One part of this is understanding the distribution of people we wish to make happy, and this depends entirely on the setting. For a web search engine, happy search users are those who find what they want. One indirect measure of such users is that they tend to return to the same engine. Measuring the rate of return of users is thus an effective metric, which would of course be more effective if you could also measure how much these users used other search engines. But advertisers are also users of modern web search engines. They are happy if customers click through to their sites and then make purchases. On an eCommerce web site, a user is likely to be wanting to purchase something. Thus, we can measure the time to purchase, or the fraction of searchers who become buyers. On a shopfront web site, perhaps both the user's and the store owner's needs are satisfied if a purchase is made. Nevertheless, in general, we need to decide whether it is the end user's or the eCommerce site owner's happiness that we are trying to optimize. Usually, it is the store owner who is paying us. For an ``enterprise'' (company, government, or academic) intranet search engine, the relevant metric is more likely to be user productivity: how much time do users spend looking for information that they need. There are also many other practical criteria concerning such matters as information security, which we mentioned in Section 4.6 (page ). User happiness is elusive to measure, and this is part of why the standard methodology uses the proxy of relevance of search results. The standard direct way to get at user satisfaction is to run user studies, where people engage in tasks, and usually various metrics are measured, the participants are observed, and ethnographic interview techniques are used to get qualitative information on satisfaction. User studies are very useful in system design, but they are time consuming and expensive to do. They are also difficult to do well, and expertise is required to design the studies and to interpret the results. We will not discuss the details of human usability testing here.
iir_8_6_3	Refining a deployed system If an IR system has been built and is being used by a large number of users, the system's builders can evaluate possible changes by deploying variant versions of the system and recording measures that are indicative of user satisfaction with one variant vs. others as they are being used. This method is frequently used by web search engines. The most common version of this is A/B testing , a term borrowed from the advertising industry. For such a test, precisely one thing is changed between the current system and a proposed system, and a small proportion of traffic (say, 1-10% of users) is randomly directed to the variant system, while most users use the current system. For example, if we wish to investigate a change to the ranking algorithm, we redirect a random sample of users to a variant system and evaluate measures such as the frequency with which people click on the top result, or any result on the first page. (This particular analysis method is referred to as clickthrough log analysis or clickstream mining . It is further discussed as a method of implicit feedback in Section 9.1.7 (page ).) The basis of A/B testing is running a bunch of single variable tests (either in sequence or in parallel): for each test only one parameter is varied from the control (the current live system). It is therefore easy to see whether varying each parameter has a positive or negative effect. Such testing of a live system can easily and cheaply gauge the effect of a change on users, and, with a large enough user base, it is practical to measure even very small positive and negative effects. In principle, more analytic power can be achieved by varying multiple things at once in an uncorrelated (random) way, and doing standard multivariate statistical analysis, such as multiple linear regression. In practice, though, A/B testing is widely used, because A/B tests are easy to deploy, easy to understand, and easy to explain to management.
iir_8_7	Results snippets Having chosen or ranked the documents matching a query, we wish to present a results list that will be informative to the user. In many cases the user will not want to examine all the returned documents and so we want to make the results list informative enough that the user can do a final ranking of the documents for themselves based on relevance to their information need.The standard way of doing this is to provide a snippet , a short summary of the document, which is designed so as to allow the user to decide its relevance. Typically, the snippet consists of the document title and a short summary, which is automatically extracted. The question is how to design the summary so as to maximize its usefulness to the user. The two basic kinds of summaries are static , which are always the same regardless of the query, and dynamic (or query-dependent), which are customized according to the user's information need as deduced from a query. Dynamic summaries attempt to explain why a particular document was retrieved for the query at hand. A static summary is generally comprised of either or both a subset of the document and metadata associated with the document. The simplest form of summary takes the first two sentences or 50 words of a document, or extracts particular zones of a document, such as the title and author. Instead of zones of a document, the summary can instead use metadata associated with the document. This may be an alternative way to provide an author or date, or may include elements which are designed to give a summary, such as the description metadata which can appear in the meta element of a web HTML page. This summary is typically extracted and cached at indexing time, in such a way that it can be retrieved and presented quickly when displaying search results, whereas having to access the actual document content might be a relatively expensive operation. There has been extensive work within natural language processing (NLP) on better ways to do text summarization . Most such work still aims only to choose sentences from the original document to present and concentrates on how to select good sentences. The models typically combine positional factors, favoring the first and last paragraphs of documents and the first and last sentences of paragraphs, with content factors, emphasizing sentences with key terms, which have low document frequency in the collection as a whole, but high frequency and good distribution across the particular document being returned. In sophisticated NLP approaches, the system synthesizes sentences for a summary, either by doing full text generation or by editing and perhaps combining sentences used in the document. For example, it might delete a relative clause or replace a pronoun with the noun phrase that it refers to. This last class of methods remains in the realm of research and is seldom used for search results: it is easier, safer, and often even better to just use sentences from the original document. Dynamic summaries display one or more ``windows'' on the document, aiming to present the pieces that have the most utility to the user in evaluating the document with respect to their information need. Usually these windows contain one or several of the query terms, and so are often referred to as keyword-in-context ( ) snippets, though sometimes they may still be pieces of the text such as the title that are selected for their query-independent information value just as in the case of static summarization. Dynamic summaries are generated in conjunction with scoring. If the query is found as a phrase, occurrences of the phrase in the document will be shown as the summary. If not, windows within the document that contain multiple query terms will be selected. Commonly these windows may just stretch some number of words to the left and right of the query terms. This is a place where NLP techniques can usefully be employed: users prefer snippets that read well because they contain complete phrases.   Dynamic summaries are generally regarded as greatly improving the usability of IR systems, but they present a complication for IR system design. A dynamic summary cannot be precomputed, but, on the other hand, if a system has only a positional index, then it cannot easily reconstruct the context surrounding search engine hits in order to generate such a dynamic summary. This is one reason for using static summaries. The standard solution to this in a world of large and cheap disk drives is to locally cache all the documents at index time (notwithstanding that this approach raises various legal, information security and control issues that are far from resolved) as shown in Figure 7.5 (page ). Then, a system can simply scan a document which is about to appear in a displayed results list to find snippets containing the query words. Beyond simply access to the text, producing a good KWIC snippet requires some care. Given a variety of keyword occurrences in a document, the goal is to choose fragments which are: (i) maximally informative about the discussion of those terms in the document, (ii) self-contained enough to be easy to read, and (iii) short enough to fit within the normally strict constraints on the space available for summaries. Generating snippets must be fast since the system is typically generating many snippets for each query that it handles. Rather than caching an entire document, it is common to cache only a generous but fixed size prefix of the document, such as perhaps 10,000 characters. For most common, short documents, the entire document is thus cached, but huge amounts of local storage will not be wasted on potentially vast documents. Summaries of documents whose length exceeds the prefix size will be based on material in the prefix only, which is in general a useful zone in which to look for a document summary anyway. If a document has been updated since it was last processed by a crawler and indexer, these changes will be neither in the cache nor in the index. In these circumstances, neither the index nor the summary will accurately reflect the current contents of the document, but it is the differences between the summary and the actual document content that will be more glaringly obvious to the end user.
iir_8_8	References and further reading Definition and implementation of the notion of relevance to a query got off to a rocky start in 1953. Swanson (1988) reports that in an evaluation in that year between two teams, they agreed that 1390 documents were variously relevant to a set of 98 questions, but disagreed on a further 1577 documents, and the disagreements were never resolved. Rigorous formal testing of IR systems was first completed in the Cranfield experiments, beginning in the late 1950s. A retrospective discussion of the Cranfield test collection and experimentation with it can be found in (Cleverdon, 1991). The other seminal series of early IR experiments were those on the SMART system by Gerard Salton and colleagues (Salton, 1971b;1991). The TREC evaluations are described in detail by Voorhees and Harman (2005). Online information is available at http://trec.nist.gov/. Initially, few researchers computed the statistical significance of their experimental results, but the IR community increasingly demands this (Hull, 1993). User studies of IR system effectiveness began more recently (Saracevic and Kantor, 1988;1996). The notions of recall and precision were first used by Kent et al. (1955), although the term precision did not appear until later. The (or, rather its complement ) was introduced by van Rijsbergen (1979). He provides an extensive theoretical discussion, which shows how adopting a principle of decreasing marginal relevance (at some point a user will be unwilling to sacrifice a unit of precision for an added unit of recall) leads to the harmonic mean being the appropriate method for combining precision and recall (and hence to its adoption rather than the minimum or geometric mean). Buckley and Voorhees (2000) compare several evaluation measures, including precision at , MAP, and R-precision, and evaluate the error rate of each measure. was adopted as the official evaluation metric in the TREC HARD track (Allan, 2005). Aslam and Yilmaz (2005) examine its surprisingly close correlation to MAP, which had been noted in earlier studies (Buckley and Voorhees, 2000, Tague-Sutcliffe and Blustein, 1995). A standard program for evaluating IR systems which computes many measures of ranked retrieval effectiveness is Chris Buckley's trec_eval program used in the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/. Kekäläinen and Järvelin (2002) argue for the superiority of graded relevance judgments when dealing with very large document collections, and Järvelin and Kekäläinen (2002) introduce cumulated gain-based methods for IR system evaluation in this context. Sakai (2007) does a study of the stability and sensitivity of evaluation measures based on graded relevance judgments from NTCIR tasks, and concludes that NDCG is best for evaluating document ranking. Schamber et al. (1990) examine the concept of relevance, stressing its multidimensional and context-specific nature, but also arguing that it can be measured effectively. (Voorhees, 2000) is the standard article for examining variation in relevance judgments and their effects on retrieval system scores and ranking for the TREC Ad Hoc task. Voorhees concludes that although the numbers change, the rankings are quite stable. Hersh et al. (1994) present similar analysis for a medical IR collection. In contrast, Kekäläinen (2005) analyze some of the later TRECs, exploring a 4-way relevance judgment and the notion of cumulative gain, arguing that the relevance measure used does substantially affect system rankings. See also Harter (1998). Zobel (1998) studies whether the pooling method used by TREC to collect a subset of documents that will be evaluated for relevance is reliable and fair, and concludes that it is. The and its use for language-related purposes is discussed by Carletta (1996). Many standard sources (e.g., Siegel and Castellan, 1988) present pooled calculation of the expected agreement, but Di Eugenio (2004) argue for preferring the unpooled agreement (though perhaps presenting multiple measures). For further discussion of alternative measures of agreement, which may in fact be better, see Lombard et al. (2002) and Krippendorff (2003). Text summarization has been actively explored for many years. Modern work on sentence selection was initiated by Kupiec et al. (1995). More recent work includes (Barzilay and Elhadad, 1997) and (Jing, 2000), together with a broad selection of work appearing at the yearly DUC conferences and at other NLP venues. Tombros and Sanderson (1998) demonstrate the advantages of dynamic summaries in the IR context. Turpin et al. (2007) address how to generate snippets efficiently. Clickthrough log analysis is studied in (Joachims, 2002b, Joachims et al., 2005). In a series of papers, Hersh, Turpin and colleagues show how improvements in formal retrieval effectiveness, as evaluated in batch experiments, do not always translate into an improved system for users (Hersh et al., 2000b, Turpin and Hersh, 2002, Hersh et al., 2000a;2001, Turpin and Hersh, 2001). User interfaces for IR and human factors such as models of human information seeking and usability testing are outside the scope of what we cover in this book. More information on these topics can be found in other textbooks, including (Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) and (Korfhage, 1997), and collections focused on cognitive aspects (Spink and Cole, 2005).
iir_9	Relevance feedback and query expansion In most collections, the same concept may be referred to using different words. This issue, known as synonymy , has an impact on the recall of most information retrieval systems. For example, you would want a search for aircraft to match plane (but only for references to an airplane, not a woodworking plane), and for a search on thermodynamics to match references to heat in appropriate discussions. Users often attempt to address this problem themselves by manually refining a query, as was discussed in Section 1.4 ; in this chapter we discuss ways in which a system can help with query refinement, either fully automatically or with the user in the loop. The methods for tackling this problem split into two major classes: global methods and local methods. Global methods are techniques for expanding or reformulating query terms independent of the query and results returned from it, so that changes in the query wording will cause the new query to match other semantically similar terms. Global methods include: Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2 ) Query expansion via automatic thesaurus generation (Section 9.2.3 ) Techniques like spelling correction (discussed in Chapter 3 ) Relevance feedback (Section 9.1 ) Pseudo relevance feedback, also known as Blind relevance feedback (Section 9.1.6 ) (Global) indirect relevance feedback (Section 9.1.7 )   Subsections Relevance feedback and pseudo relevance feedback The Rocchio algorithm for relevance feedback The underlying theory. The Rocchio (1971) algorithm. Probabilistic relevance feedback When does relevance feedback work? Relevance feedback on the web Evaluation of relevance feedback strategies Pseudo relevance feedback Indirect relevance feedback Summary Global methods for query reformulation Vocabulary tools for query reformulation Query expansion Automatic thesaurus generation References and further reading
iir_9_1	Relevance feedback and pseudo relevance feedback The idea of relevance feedback ( ) is to involve the user in the retrieval process so as to improve the final result set. In particular, the user gives feedback on the relevance of documents in an initial set of results. The basic procedure is: The user issues a (short, simple) query. The system returns an initial set of retrieval results. The user marks some returned documents as relevant or nonrelevant. The system computes a better representation of the information need based on the user feedback. The system displays a revised set of retrieval results.  (a) (b) Relevance feedback searching over images.(a) The user views the initial query results for a query of bike, selects the first, third and fourth result in the top row and the fourth result in the bottom row as relevant, and submits this feedback. (b) The users sees the revised result set. Precision is greatly improved. From http://nayana.ece.ucsb.edu/imsearch/imsearch.html(Newsam et al., 2001). Image search provides a good example of relevance feedback. Not only is it easy to see the results at work, but this is a domain where a user can easily have difficulty formulating what they want in words, but can easily indicate relevant or nonrelevant images. After the user enters an initial query for bike on the demonstration system at: http://nayana.ece.ucsb.edu/imsearch/imsearch.html 9.1 9.1 Figure 9.2 shows a textual IR example where the user wishes to find out about new applications of space satellites.     Subsections The Rocchio algorithm for relevance feedback The underlying theory. The Rocchio (1971) algorithm. Probabilistic relevance feedback When does relevance feedback work? Relevance feedback on the web Evaluation of relevance feedback strategies Pseudo relevance feedback Indirect relevance feedback Summary
iir_9_1_1	The Rocchio algorithm for relevance feedback The Rocchio Algorithm is the classic algorithm for implementing relevance feedback. It models a way of incorporating relevance feedback information into the vector space model of Section 6.3 .  Figure 9.3: The Rocchio optimal query for separating relevant and nonrelevant documents.   Subsections The underlying theory. The Rocchio (1971) algorithm.
iir_9_1_2	Probabilistic relevance feedback Rather than reweighting the query in a vector space, if a user has told us some relevant and nonrelevant documents, then we can proceed to build a . One way of doing this is with a Naive Bayes probabilistic model. If is a Boolean indicator variable expressing the relevance of a document, then we can estimate , the probability of a term appearing in a document, depending on whether it is relevant or not, as: (50) (51)         11 13 11.3.4  50
iir_9_1_3	When does relevance feedback work? The success of relevance feedback depends on certain assumptions. Firstly, the user has to have sufficient knowledge to be able to make an initial query which is at least somewhere close to the documents they desire. This is needed anyhow for successful information retrieval in the basic case, but it is important to see the kinds of problems that relevance feedback cannot solve alone. Cases where relevance feedback alone is not sufficient include: Misspellings. If the user spells a term in a different way to the way it is spelled in any document in the collection, then relevance feedback is unlikely to be effective. This can be addressed by the spelling correction techniques of Chapter 3 . Cross-language information retrieval. Documents in another language are not nearby in a vector space based on term distribution. Rather, documents in the same language cluster more closely together. Mismatch of searcher's vocabulary versus collection vocabulary. If the user searches for laptop but all the documents use the term notebook computer, then the query will fail, and relevance feedback is again most likely ineffective.  Secondly, the relevance feedback approach requires relevant documents to be similar to each other. That is, they should cluster. Ideally, the term distribution in all relevant documents will be similar to that in the documents marked by the users, while the term distribution in all nonrelevant documents will be different from those in relevant documents. Things will work well if all relevant documents are tightly clustered around a single prototype, or, at least, if there are different prototypes, if the relevant documents have significant vocabulary overlap, while similarities between relevant and nonrelevant documents are small. Implicitly, the Rocchio relevance feedback model treats relevant documents as a single cluster, which it models via the centroid of the cluster. This approach does not work as well if the relevant documents are a multimodal class, that is, they consist of several clusters of documents within the vector space. This can happen with: Subsets of the documents using different vocabulary, such as Burma vs. Myanmar A query for which the answer set is inherently disjunctive, such as Pop stars who once worked at Burger King. Instances of a general concept, which often appear as a disjunction of more specific concepts, for example, felines. Relevance feedback is not necessarily popular with users. Users are often reluctant to provide explicit feedback, or in general do not wish to prolong the search interaction. Furthermore, it is often harder to understand why a particular document was retrieved after relevance feedback is applied. Relevance feedback can also have practical problems. The long queries that are generated by straightforward application of relevance feedback techniques are inefficient for a typical IR system. This results in a high computing cost for the retrieval and potentially long response times for the user. A partial solution to this is to only reweight certain prominent terms in the relevant documents, such as perhaps the top 20 terms by term frequency. Some experimental results have also suggested that using a limited number of terms like this may give better results (Harman, 1992) though other work has suggested that using more terms is better in terms of retrieved document quality (Buckley et al., 1994b).
iir_9_1_4	Relevance feedback on the web Some web search engines offer a similar/related pages feature: the user indicates a document in the results set as exemplary from the standpoint of meeting his information need and requests more documents like it. This can be viewed as a particular simple form of relevance feedback. However, in general relevance feedback has been little used in web search. One exception was the Excite web search engine, which initially provided full relevance feedback. However, the feature was in time dropped, due to lack of use. On the web, few people use advanced search interfaces and most would like to complete their search in a single interaction. But the lack of uptake also probably reflects two other factors: relevance feedback is hard to explain to the average user, and relevance feedback is mainly a recall enhancing strategy, and web search users are only rarely concerned with getting sufficient recall. Spink et al. (2000) present results from the use of relevance feedback in the Excite search engine. Only about 4% of user query sessions used the relevance feedback option, and these were usually exploiting the ``More like this'' link next to each result. About 70% of users only looked at the first page of results and did not pursue things any further. For people who used relevance feedback, results were improved about two thirds of the time. An important more recent thread of work is the use of clickstream data (what links a user clicks on) to provide indirect relevance feedback. Use of this data is studied in detail in (Joachims, 2002b, Joachims et al., 2005). The very successful use of web link structure (see Chapter 21 ) can also be viewed as implicit feedback, but provided by page authors rather than readers (though in practice most authors are also readers). Exercises. In Rocchio's algorithm, what weight setting for does a ``Find pages like this one'' search correspond to? Give three reasons why relevance feedback has been little used in web search.
iir_9_1_5	Evaluation of relevance feedback strategies Interactive relevance feedback can give very substantial gains in retrieval performance. Empirically, one round of relevance feedback is often very useful. Two rounds is sometimes marginally more useful. Successful use of relevance feedback requires enough judged documents, otherwise the process is unstable in that it may drift away from the user's information need. Accordingly, having at least five judged documents is recommended. There is some subtlety to evaluating the effectiveness of relevance feedback in a sound and enlightening way. The obvious first strategy is to start with an initial query and to compute a precision-recall graph. Following one round of feedback from the user, we compute the modified query and again compute a precision-recall graph. Here, in both rounds we assess performance over all documents in the collection, which makes comparisons straightforward. If we do this, we find spectacular gains from relevance feedback: gains on the order of 50% in mean average precision. But unfortunately it is cheating. The gains are partly due to the fact that known relevant documents (judged by the user) are now ranked higher. Fairness demands that we should only evaluate with respect to documents not seen by the user. A second idea is to use documents in the residual collection (the set of documents minus those assessed relevant) for the second round of evaluation. This seems like a more realistic evaluation. Unfortunately, the measured performance can then often be lower than for the original query. This is particularly the case if there are few relevant documents, and so a fair proportion of them have been judged by the user in the first round. The relative performance of variant relevance feedback methods can be validly compared, but it is difficult to validly compare performance with and without relevance feedback because the collection size and the number of relevant documents changes from before the feedback to after it. Thus neither of these methods is fully satisfactory. A third method is to have two collections, one which is used for the initial query and relevance judgments, and the second that is then used for comparative evaluation. The performance of both and can be validly compared on the second collection. Perhaps the best evaluation of the utility of relevance feedback is to do user studies of its effectiveness, in particular by doing a time-based comparison: how fast does a user find relevant documents with relevance feedback vs. another strategy (such as query reformulation), or alternatively, how many relevant documents does a user find in a certain amount of time. Such notions of user utility are fairest and closest to real system usage.
iir_9_1_6	Pseudo relevance feedback Pseudo relevance feedback , also known as blind relevance feedback , provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top ranked documents are relevant, and finally to do relevance feedback as before under this assumption.   This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis (Section 9.2 ). It has been found to improve performance in the TREC ad hoc task. See for example the results in Figure 9.5 . But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile.
iir_9_1_7	Indirect relevance feedback We can also use indirect sources of evidence rather than explicit feedback on relevance as the basis for relevance feedback. This is often called implicit (relevance) feedback . Implicit feedback is less reliable than explicit feedback, but is more useful than pseudo relevance feedback, which contains no evidence of user judgments. Moreover, while users are often reluctant to provide explicit feedback, it is easy to collect implicit feedback in large quantities for a high volume system, such as a web search engine. On the web, DirectHit introduced the idea of ranking more highly documents that users chose to look at more often. In other words, clicks on links were assumed to indicate that the page was likely relevant to the query. This approach makes various assumptions, such as that the document summaries displayed in results lists (on whose basis users choose which documents to click on) are indicative of the relevance of these documents. In the original DirectHit search engine, the data about the click rates on pages was gathered globally, rather than being user or query specific. This is one form of the general area of clickstream mining . Today, a closely related approach is used in ranking the advertisements that match a web search query (Chapter 19 ).
iir_9_1_8	Summary Relevance feedback has been shown to be very effective at improving relevance of results. Its successful use requires queries for which the set of relevant documents is medium to large. Full relevance feedback is often onerous for the user, and its implementation is not very efficient in most IR systems. In many cases, other types of interactive retrieval may improve relevance by about as much with less work. Beyond the core ad hoc retrieval scenario, other uses of relevance feedback include: Following a changing information need (e.g., names of car models of interest change over time) Maintaining an information filter (e.g., for a news feed). Such filters are discussed further in Chapter 13 . Active learning (deciding which examples it is most useful to know the class of to reduce annotation costs). Exercises. Under what conditions would the modified query in Equation 49 be the same as the original query ? In all other cases, is closer than to the centroid of the relevant documents? Why is positive feedback likely to be more useful than negative feedback to an IR system? Why might only using one nonrelevant document be more effective than using several? Suppose that a user's initial query is cheap CDs cheap DVDs extremely cheap CDs. The user examines two documents, and . She judges , with the content CDs cheap software cheap CDs relevant and with content cheap thrills DVDs nonrelevant. Assume that we are using direct term frequency (with no scaling and no document frequency). There is no need to length-normalize vectors. Using Rocchio relevance feedback as in Equation 49 what would the revised query vector be after relevance feedback? Assume . Omar has implemented a relevance feedback web search system, where he is going to do relevance feedback based only on words in the title text returned for a page (for efficiency). The user is going to rank 3 results. The first user, Jinxing, queries for: banana slug and the top three titles returned are: banana slug Ariolimax columbianus Santa Cruz mountains banana slug Santa Cruz Campus Mascot Jinxing judges the first two documents relevant, and the third nonrelevant. Assume that Omar's search engine uses term frequency but no length normalization nor IDF. Assume that he is using the Rocchio relevance feedback mechanism, with . Show the final revised query that would be run. (Please list the vector elements in alphabetical order.)
iir_9_2	Global methods for query reformulation In this section we more briefly discuss three global methods for expanding a query: by simply aiding the user in doing so, by using a manual thesaurus, and through building a thesaurus automatically.   Subsections Vocabulary tools for query reformulation Query expansion Automatic thesaurus generation
iir_9_2_1	Vocabulary tools for query reformulation Various user supports in the search process can help the user see how their searches are or are not working. This includes information about words that were omitted from the query because they were on stop lists, what words were stemmed to, the number of hits on each term or phrase, and whether words were dynamically turned into phrases. The IR system might also suggest search terms by means of a thesaurus or a controlled vocabulary. A user can also be allowed to browse lists of the terms that are in the inverted index, and thus find good terms that appear in the collection.
iir_9_2_2	Query expansion  An example of query expansion in the interface of the Yahoo! web search engine in 2006.The expanded query suggestions appear just below the ``Search Results'' bar. In relevance feedback, users give additional input on documents (by marking documents in the results set as relevant or not), and this input is used to reweight the terms in the query for documents. In query expansion on the other hand, users give additional input on query words or phrases, possibly suggesting additional query terms. Some search engines (especially on the web) suggest related queries in response to a query; the users then opt to use one of these alternative query suggestions. Figure 9.6 shows an example of query suggestion options being presented in the Yahoo! web search engine. The central question in this form of query expansion is how to generate alternative or expanded queries for the user. The most common form of query expansion is global analysis, using some form of thesaurus. For each term in a query, the query can be automatically expanded with synonyms and related words of from the thesaurus. Use of a thesaurus can be combined with ideas of term weighting: for instance, one might weight added terms less than original query terms.   Methods for building a thesaurus for query expansion include: Use of a controlled vocabulary that is maintained by human editors. Here, there is a canonical term for each concept. The subject headings of traditional library subject indexes, such as the Library of Congress Subject Headings, or the Dewey Decimal system are examples of a controlled vocabulary. Use of a controlled vocabulary is quite common for well-resourced domains. A well-known example is the Unified Medical Language System (UMLS) used with MedLine for querying the biomedical research literature. For example, in Figure 9.7 , neoplasms was added to a search for cancer. This Medline query expansion also contrasts with the Yahoo! example. The Yahoo! interface is a case of interactive query expansion, whereas PubMed does automatic query expansion. Unless the user chooses to examine the submitted query, they may not even realize that query expansion has occurred. A manual thesaurus. Here, human editors have built up sets of synonymous names for concepts, without designating a canonical term. The UMLS metathesaurus is one example of a thesaurus. Statistics Canada maintains a thesaurus of preferred terms, synonyms, broader terms, and narrower terms for matters on which the government collects statistics, such as goods and services. This thesaurus is also bilingual English and French. An automatically derived thesaurus. Here, word co-occurrence statistics over a collection of documents in a domain are used to automatically induce a thesaurus; see Section 9.2.3 . Query reformulations based on query log mining. Here, we exploit the manual query reformulations of other users to make suggestions to a new user. This requires a huge query volume, and is thus particularly appropriate to web search.
iir_9_2_3	Automatic thesaurus generation As an alternative to the cost of a manual thesaurus, we could attempt to generate a thesaurus automatically by analyzing a collection of documents. There are two main approaches. One is simply to exploit word cooccurrence. We say that words co-occurring in a document or paragraph are likely to be in some sense similar or related in meaning, and simply count text statistics to find the most similar words. The other approach is to use a shallow grammatical analysis of the text and to exploit grammatical relations or grammatical dependencies. For example, we say that entities that are grown, cooked, eaten, and digested, are more likely to be food items. Simply using word cooccurrence is more robust (it cannot be misled by parser errors), but using grammatical relations is more accurate.   The simplest way to compute a co-occurrence thesaurus is based on term-term similarities. We begin with a term-document matrix , where each cell is a weighted count for term and document , with weighting so has length-normalized rows. If we then calculate , then is a similarity score between terms and , with a larger number being better. Figure 9.8 shows an example of a thesaurus derived in basically this manner, but with an extra step of dimensionality reduction via Latent Semantic Indexing, which we discuss in Chapter 18 . While some of the thesaurus terms are good or at least suggestive, others are marginal or bad. The quality of the associations is typically a problem. Term ambiguity easily introduces irrelevant statistically correlated terms. For example, a query for Apple computer may expand to Apple red fruit computer. In general these thesauri suffer from both false positives and false negatives. Moreover, since the terms in the automatic thesaurus are highly correlated in documents anyway (and often the collection used to derive the thesaurus is the same as the one being indexed), this form of query expansion may not retrieve many additional documents. Query expansion is often effective in increasing recall. However, there is a high cost to manually producing a thesaurus and then updating it for scientific and terminological developments within a field. In general a domain-specific thesaurus is required: general thesauri and dictionaries give far too little coverage of the rich domain-particular vocabularies of most scientific fields. However, query expansion may also significantly decrease precision, particularly when the query contains ambiguous terms. For example, if the user searches for interest rate, expanding the query to interest rate fascinate evaluate is unlikely to be useful. Overall, query expansion is less successful than relevance feedback, though it may be as good as pseudo relevance feedback. It does, however, have the advantage of being much more understandable to the system user. Exercises. If is simply a Boolean cooccurrence matrix, then what do you get as the entries in ?
iir_9_3	References and further reading Work in information retrieval quickly confronted the problem of variant expression which meant that the words in a query might not appear in a document, despite it being relevant to the query. An early experiment about 1960 cited by Swanson (1988) found that only 11 out of 23 documents properly indexed under the subject toxicity had any use of a word containing the stem toxi. There is also the issue of translation, of users knowing what terms a document will use. Blair and Maron (1985) conclude that ``it is impossibly difficult for users to predict the exact words, word combinations, and phrases that are used by all (or most) relevant documents and only (or primarily) by those documents''. The main initial papers on relevance feedback using vector space models all appear in Salton (1971b), including the presentation of the Rocchio algorithm (Rocchio, 1971) and the Ide dec-hi variant along with evaluation of several variants (Ide, 1971). Another variant is to regard all documents in the collection apart from those judged relevant as nonrelevant, rather than only ones that are explicitly judged nonrelevant. However, Schütze et al. (1995) and Singhal et al. (1997) show that better results are obtained for routing by using only documents close to the query of interest rather than all documents. Other later work includes Salton and Buckley (1990), Riezler et al. (2007) (a statistical NLP approach to RF) and the recent survey paper Ruthven and Lalmas (2003). The effectiveness of interactive relevance feedback systems is discussed in (Harman, 1992, Buckley et al., 1994b, Salton, 1989). Koenemann and Belkin (1996) do user studies of the effectiveness of relevance feedback. Traditionally Roget's thesaurus has been the best known English language thesaurus (Roget, 1946). In recent computational work, people almost always use WordNet (Fellbaum, 1998), not only because it is free, but also because of its rich link structure. It is available at: http://wordnet.princeton.edu. Qiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus generation. Xu and Croft (1996) explore using both local and global query expansion.
irv-0001	One  INTRODUCTION  Information retrieval is a wide, often loosely-defined term but in these pages I shall be concerned only with automatic information retrieval systems. Automatic as opposed to manual and information as opposed to data or fact. Unfortunately the word information can be very misleading. In the context of information retrieval (IR), information, in the technical meaning given in Shannon's theory of communication, is not readily measured (Shannon and Weaver[1]). In fact, in many cases one can adequately describe the kind of retrieval by simply substituting 'document' for 'information'. Nevertheless, 'information retrieval' has become accepted as a description of the kind of work published by Cleverdon, Salton, Sparck Jones, Lancaster and others. A perfectly straightforward definition along these lines is given by Lancaster[2]: 'Information retrieval is the term conventionally, though somewhat inaccurately, applied to the type of activity discussed in this volume. An information retrieval system does not inform (i.e. change the knowledge of) the user on the subject of his inquiry. It merely informs on the existence (or non-existence) and whereabouts of documents relating to his request.' This specifically excludes Question-Answering systems as typified by Winograd[3] and those described by Minsky[4]]. It also excludes data retrieval systems such as used by, say, the stock exchange for on-line quotations.  To make clear the difference between data retrieval (DR) and information retrieval (IR), I have listed in Table 1.1 some of the distinguishing properties of data and information retrieval. One  Table 1.1 DATA RETRIEVAL OR INFORMATION RETRIEVAL?  Data Retrieval (DR) Information Retrieval (IR)  Matching		Exact match		Partial match, best match  Inference Deduction Induction  Model Deterministic Probabilistic  Classification Monothetic Polythetic  Query language Artificial Natural  Query specification Complete Incomplete  Items wanted Matching Relevant  Error response Sensitive Insensitive  may want to criticise this dichotomy on the grounds that the boundary between the two is a vague one. And so it is, but it is a useful one in that it illustrates the range of complexity associated with each mode of retrieval.  Let us now take each item in the table in turn and look at it more closely. In data retrieval we are normally looking for an exact match, that is, we are checking to see whether an item is or is not present in the file. In information retrieval this may sometimes be of interest but more generally we want to find those items which partially match the request and then select from those a few of the best matching ones.  The inference used in data retrieval is of the simple deductive kind, that is, aRb and bRc then aRc. In information retrieval it is far more common to use inductive inference; relations are only specified with a degree of certainty or uncertainty and hence our confidence in the inference is variable. This distinction leads one to describe data retrieval as deterministic but information retrieval as probabilistic. Frequently Bayes' Theorem is invoked to carry out inferences in IR, but in DR probabilities do not enter into the processing.  Another distinction can be made in terms of classifications that are likely to be useful. In DR we are most likely to be interested in a monothetic classification, that is, one with classes defined by objects possessing attributes both necessary and sufficient to belong to a class. In IR such a classification is one the whole not very useful, in fact more often a polythetic classification is what is wanted. In such a classification each individual in a class will possess only a proportion of all the attributes possessed by all the members of that class. Hence no attribute is necessary nor sufficient for membership to a class.  The query language for DR will generally be of the artificial kind, one with restricted syntax and vocabulary, in IR we prefer to use natural language although there are some notable exceptions. In DR the query is generally a complete specification of what is wanted, in IR it is invariably incomplete. This last difference arises partly from the fact that in IR we are searching for relevant documents as opposed to exactly matching items. The extent of the match in IR is assumed to indicate the likelihood of the relevance of that item. One simple consequence of this difference is that DR is more sensitive to error in the sense that, an error in matching will not retrieve the wanted item which implies a total failure of the system. In IR small errors in matching generally do not affect performance of the system significantly.  Many automatic information retrieval systems are experimental. I only make occasional reference to operational systems. Experimental IR is mainly carried on in a 'laboratory' situation whereas operational systems are commercial systems which charge for the service they provide. Naturally the two systems are evaluated differently. The 'real world' IR systems are evaluated in terms of 'user satisfaction' and the price the user is willing to pay for its service. Experimental IR systems are evaluated by comparing the retrieval experiments with standards specially constructed for the purpose. I believe that a book on experimental information retrieval, covering the design and evaluation of retrieval systems from a point of view which is independent of any particular system, will be a great help to other workers in the field and indeed is long overdue.  Many of the techniques I shall discuss will not have proved themselves incontrovertibly superior to all other techniques, but they have promise and their promise will only be realised when they are understood. Information about new techniques has been so scattered through the literature that to find out about them you need to be an expert before you begin to look. I hope that I will be able to take the reader to the point where he will have little trouble in implementing some of the new techniques. Also, that some people will then go on to experiment with them, and generate new, convincing evidence of their efficiency and effectiveness.  My aim throughout has been to give a complete coverage of the more important ideas current in various special areas of information retrieval. Inevitably some ideas have been elaborated at the expense of others. In particular, emphasis is placed on the use of automatic classification techniques and rigorous methods of measurement of effectiveness. On the other hand, automatic content analysis is given only a superficial coverage. The reasons are straightforward, firstly the material reflects my own bias, and secondly, no adequate coverage of the first two topics has been given before whereas automatic content analysis has been documented very well elsewhere. A subsidiary reason for emphasising automatic classification is that little appears to be known or understood about it in the context of IR so that research workers are loath to experiment with it.
irv-0002	The structure of the book  The introduction presents some basic background material, demarcates the subject and discusses loosely some of the problems in IR. The chapters that follow cover topics in the order in which I would think about them were I about to design an experimental IR system. They begin by describing the generation of machine representations for the information, and then move on to an explanation of the logical structures that may be arrived at by clustering. There are numerous methods for representing these structures in the computer, or in other words, there is a choice of file structures to represent the logical structure, so these are outlined next. Once the information has been stored in this way we are able to search it, hence a discussion of search strategies follows. The chapter on probabilistic retrieval is an attempt to create a formal model for certain kinds of search strategies. Lastly, in an experimental situation all of the above will have been futile unless the results of retrieval can be evaluated. Therefore a large chapter is devoted to ways of measuring the effectiveness of retrieval. In the final chapter I have indulged in a little speculation about the possibilities for IR in the next decade.  The two major chapters are those dealing with automatic classification and evaluation. I have tried to write them in such a way that each can be read independently of the rest of the book (although I do not recommend this for the non-specialist).
irv-0003	Outline  Chapter 2: Automatic Text Analysis - contains a straightforward discussion of how the text of a document is represented inside a computer. This is a superficial chapter but I think it is adequate in the context of this book.  Chapter 3: Automatic Classification - looks at automatic classification methods in general and then takes a deeper look at the use of these methods in information retrieval.  Chapter 4: File Structures - here we try and discuss file structures from the point of view of someone primarily interested in information retrieval.  Chapter 5: Search Strategies - gives an account of some search strategies when applied to document collections structured in different ways. It also discusses the use of feedback.  Chapter 6: Probabilistic Retrieval - describes a formal model for enhancing retrieval effectiveness by using sample information about the frequency of occurrence and co-occurrence of index terms in the relevant and non-relevant documents.  Chapter 7: Evaluation - here I give a traditional view of the measurement of effectiveness followed by an explanation of some of the more promising attempts at improving the art. I also attempt to provide foundations for a theory of evaluation.  Chapter 8: The Future - contains some speculation about the future of IR and tries to pinpoint some areas of research where further work is desperately needed.
irv-0004	Information retrieval  Since the 1940s the problem of information storage and retrieval has attracted increasing attention. It is simply stated: we have vast amounts of information to which accurate and speedy access is becoming ever more difficult. One effect of this is that relevant information gets ignored since it is never uncovered, which in turn leads to much duplication of work and effort. With the advent of computers, a great deal of thought has been given to using them to provide rapid and intelligent retrieval systems. In libraries, many of which certainly have an information storage and retrieval problem, some of the more mundane tasks, such as cataloguing and general administration, have successfully been taken over by computers. However, the problem of effective retrieval remains largely unsolved.  In principle, information storage and retrieval is simple. Suppose there is a store of documents and a person (user of the store) formulates a question (request or query) to which the answer is a set of documents satisfying the information need expressed by his question. He can obtain the set by reading all the documents in the store, retaining the relevant documents and discarding all the others. In a sense, this constitutes 'perfect' retrieval. This solution is obviously impracticable. A user either does not have the time or does not wish to spend the time reading the entire document collection, apart from the fact that it may be physically impossible for him to do so.  When high speed computers became available for non-numerical work, many thought that a computer would be able to 'read' an entire document collection to extract the relevant documents. It soon became apparent that using the natural language text of a document not only caused input and storage problems (it still does) but also left unsolved the intellectual problem of characterising the document content. It is conceivable that future hardware developments may make natural language input and storage more feasible. But automatic characterisation in which the software attempts to duplicate the human process of 'reading' is a very sticky problem indeed. More specifically, 'reading' involves attempting to extract information, both syntactic and semantic, from the text and using it to decide whether each document is relevant or not to a particular request. The difficulty is not only knowing how to extract the information but also how to use it to decide relevance. The comparatively slow progress of modern linguistics on the semantic front and the conspicuous failure of machine translation (Bar-Hillel[5]) show that these problems are largely unsolved.  The reader will have noticed that already, the idea of 'relevance' has slipped into the discussion. It is this notion which is at the centre of information retrieval. The purpose of an automatic retrieval strategy is to retrieve all the relevant documents at the same time retrieving as few of the non-relevant as possible. When the characterisation of a document is worked out, it should be such that when the document it represents is relevant to a query, it will enable the document to be retrieved in response to that query. Human indexers have traditionally characterised documents in this way when assigning index terms to documents. The indexer attempts to anticipate the kind of index terms a user would employ to retrieve each document whose content he is about to describe. Implicitly he is constructing queries for which the document is relevant. When the indexing is done automatically it is assumed that by pushing the text of a document or query through the same automatic analysis, the output will be a representation of the content, and if the document is relevant to the query, a computational procedure will show this.  Intellectually it is possible for a human to establish the relevance of a document to a query. For a computer to do this we need to construct a model within which relevance decisions can be quantified. It is interesting to note that most research in information retrieval can be shown to have been concerned with different aspects of such a model.
irv-0005	An information retrieval system  Let me illustrate by means of a black box what a typical IR system would look like. The diagram shows three components: input, processor and output. Such a trichotomy may seem a little trite, but the components constitute a convenient set of pegs upon which to hang a discussion.  Starting with the input side of things. The main problem here is to obtain a representation of each document and query suitable for a computer to use. Let me emphasise that most computer-based retrieval systems store only a representation of the document (or query) which means that the text of a document is lost once it has been processed for the purpose of generating its representation. A document representative could, for example, be a list of extracted words considered to be significant. Rather than have the computer process the natural language, an alternative approach is to have an artificial language within which all queries and documents can be formulated. There  is some evidence to show that this can be effective (Barber et al.[6]). Of course it presupposes that a user is willing to be taught to express his information need in the language.  When the retrieval system is on-line, it is possible for the user to change his request during one search session in the light of a sample retrieval, thereby, it is hoped, improving the subsequent retrieval run. Such a procedure is commonly referred to as feedback. An example of a sophisticated on-line retrieval system is the MEdigital libraryINE system (McCarn and Leiter[7]). I think it is fair to say that it will be only a short time before all retrieval systems will be on-line.  Secondly, the processor, that part of the retrieval system concerned with the retrieval process. The process may involve structuring the information in some appropriate way, such as classifying it. It will also involve performing the actual retrieval function, that is, executing the search strategy in response to a query. In the diagram, the documents have been placed in a separate box to emphasise the fact that they are not just input but can be used during the retrieval process in such a way that their structure is more correctly seen as part of the retrieval process.  Finally, we come to the output, which is usually a set of citations or document numbers. In an operational system the story ends here. However, in an experimental system it leaves the evaluation to be done.
irv-0006	IR in perspective  This section is not meant to constitute an attempt at an exhaustive and complete account of the historical development of IR. In any case, it would not be able to improve on the accounts given by Cleverdon[8] and Salton[9]]. Although information retrieval can be subdivided in many ways, it seems that there are three main areas of research which between them make up a considerable portion of the subject. They are: content analysis, information structures, and evaluation. Briefly the first is concerned with describing the contents of documents in a form suitable for computer processing; the second with exploiting relationships between documents to improve the efficiency and effectiveness of retrieval strategies; the third with the measurement of the effectiveness of retrieval.  Since the emphasis in this book is on a particular approach to document representation, I shall restrict myself here to a few remarks about its history. I am referring to the approach pioneered by Luhn[10]. He used frequency counts of words in the document text to determine which words were sufficiently significant to represent or characterise the document in the computer (more details about this in the next chapter). Thus a list of what might be called 'keywords' was derived for each document. In addition the frequency of occurrence of these words in the body of the text could also be used to indicate a degree of significance. This provided a simple weighting scheme for the 'keywords' in each list and made available a document representative in the form of a 'weighted keyword description'.  At this point, it may be convenient to elaborate on the use of 'keyword'. It has become common practice in the IR literature to refer to descriptive items extracted from text as keywords or terms. Such items are often the outcome of some process such as, for example, the gathering together of different morphological variants of the same word. In this book, keyword and term will be used interchangeably.  The use of statistical information about distributions of words in documents was further exploited by Maron and Kuhns[11] and Stiles[12] who obtained statistical associations between keywords. These associations provided a basis for the construction of a thesaurus as an aid to retrieval. Much of this early research was brought together with the publication of the 1964 Washington Symposium on Statistical Association Methods for Mechanized Documentation (Stevens et al. [13]).  Sparck Jones has carried on this work using measures of association between keywords based on their frequency of co-occurrence (that is, the frequency with which any two keywords occur together in the same document). She has shown[14] that such related words can be used effectively to improve recall, that is, to increase the proportion of the relevant documents which are retrieved. Interestingly, the early ideas of Luhn are still being developed and many automatic methods of characterisation are based on his early work.  The term information structure (for want of better words) covers specifically a logical organisation of information, such as document representatives, for the purpose of information retrieval. The development in information structures has been fairly recent. The main reason for the slowness of development in this area of information retrieval is that for a long time no one realised that computers would not give an acceptable retrieval time with a large document set unless some logical structure was imposed on it. In fact, owners of large data-bases are still loath to try out new organisation techniques promising faster and better retrieval. The slowness to recognise and adopt new techniques is mainly due to the scantiness of the experimental evidence backing them. The earlier experiments with document retrieval systems usually adopted a serial file organisation which, although it was efficient when a sufficiently large number of queries was processed simultaneously in a batch mode, proved inadequate if each query required a short real time response. The popular organisation to be adopted instead was the inverted file. By some this has been found to be restrictive (Salton[15]). More recently experiments have attempted to demonstrate the superiority of clustered files for on-line retrieval.  The organisation of these files is produced by an automatic classification method. Good[16] and Fairthorne[17] were among the first to suggest that automatic classification might prove useful in document retrieval. Not until several years later were serious experiments carried out in document clustering (Doyle[18]; Rocchio[19]). All experiments so far have been on a small scale. Since clustering only comes into its own when the scale is increased, it is hoped that this book may encourage some large scale experiments by bringing together many of the necessary tools.  Evaluation of retrieval systems has proved extremely difficult. Senko[20] in an excellent survey paper states: 'Without a doubt system evaluation is the most troublesome area in ISR ...', and I am inclined to agree. Despite excellent pioneering work done by Cleverdon et al.[21] in this area, and despite numerous measures of effectiveness that have been proposed (see Robertson[22, 23 ]for a substantial list), a general theory of evaluation had not emerged. I attempt to provide foundations for such a theory in Chapter 7 (page 168).  In the past there has been much debate about the validity of evaluations based on relevance judgments provided by erring human beings. Cuadra and Katter[24]supposed that relevance was measurable on an ordinal scale (one which arises from the operation of rank-ordering) but showed that the position of a document on such a scale was affected by external variables not usually controlled in the laboratory. Lesk and Salton[25] subsequently showed that a dichotomous scale on which a document is either relevant or non-relevant, when subjected to a certain probability of error, did not invalidate the results obtained for evaluation in terms of precision (the proportion of retrieved documents which are relevant) and recall(the proportion of relevant documents retrieved). Today effectiveness of retrieval is still mostly measured in terms of precision and recall or by measures based thereon. There is still no adequate statistical treatment showing how appropriate significance tests may be used (I shall return to this point in the Chapter on Evaluation, page 178). So, after a few decades of research in this area we basically have only precision and recall, and a working hypothesis which states, quoting Cleverdon[26]: 'Within a single system, assuming that a sequence of sub-searches for a particular question is made in the logical order of expected decreasing precision, and the requirements are those stated in the question, there is an inverse relationship between recall and precision, if the results of a number of different searches are averaged.'
irv-0007	Effectiveness and efficiency  Much of the research and development in information retrieval is aimed at improving the effectiveness and efficiency of retrieval. Efficiency is usually measured in terms of the computer resources used such as core, backing store, and C.P.U. time. It is difficult to measure efficiency in a machine independent way. In any case, it should be measured in conjunction with effective-ness to obtain some idea of the benefit in terms of unit cost. In the previous section I mentioned that effectiveness is commonly measured in terms of precision and recall. I repeat here that precision is the ratio of the number of relevant documents retrieved to the total number of documents retrieved, and recall is the ratio of the number of relevant documents retrieved to the total number of relevant documents (both retrieved and not retrieved). The reason for emphasising these two measures is that frequent reference is made to retrieval effectiveness but its detailed discussion is delayed until Chapter 7. It will suffice until we reach that chapter to think of retrieval effectiveness in terms of precision and recall. It would have been possible to give the chapter on evaluation before any of the other material but this, in my view, would have been like putting the cart before the horse. Before we can appreciate the evaluation of observations we need to understand what gave rise to the observations. Hence I have delayed discussing evaluation until some understanding of what makes an information retrieval system tick has been gained. Readers not satisfied with this order can start by first reading Chapter 7 which in any case can be read independently.
irv-0008	Bibliographic remarks  The best introduction to information retrieval is probably got by reading some of the early papers in the field. Luckily many of these have now been collected in book form. I recommend for browsing the books edited by Garvin[27], Kochen[28], Borko[29], Schecter[30 ]and Saracevic[31]. It is also worth noting that some of the papers cited in this book may be found in one of these collections and therefore be readily accessible. A book which is well written and can be read without any mathematical background is one by Lancaster[2]. More recently, a number of books have come out entirely devoted to information retrieval and allied topics, they are Doyle[32], Salton[33], Paice[34], and Kochen[35]. In particular, the latter half of Doyle's book makes interesting reading since it describes what work in IR was like in the early days (the late 1950s to early 1960s). A critical view of information storage and retrieval is presented in the paper by Senko[20]. This paper is more suitable for people with a computer science background, and is particularly worth reading because of its healthy scepticism of the whole subject. Readers more interested in information retrieval in a library context should read Vickery[36].  One early publication worth reading which is rather hard to come by is the report on the Cranfield II project by Cleverdon et al.[21]. This report is not really introductory material but constitutes, in my view, one of the milestones in information retrieval. It is an excellent example of the experimental approach to IR and contains many good ideas which have subsequently been elaborated in the open literature. Time spent on this report is well spent.  Papers on information retrieval have a tendency to get published in journals on computer science and library science. There are, however, a few major journals which are largely devoted to information retrieval. These are, Journal of Documentation, Information Storage and Retrieval*, and Journal of the American Society for Information Science.  Finally, every year a volume in the series Annual Review of Information Science and Technology is edited by C. A. Cuadra. Each volume attempts to cover the new work published in information storage and retrieval for that year. As a source of references to the current literature it is unsurpassed. But they are mainly aimed at the practitioner and as such are a little difficult to read for the uninitiated.
irv-0010	Introduction  Before a computerised information retrieval system can actually operate to retrieve some information, that information must have already been stored inside the computer. Originally it will usually have been in the form od documents. The computer, however, is not likely to have stored the complete text of each document in the natural language in which it was writtten. It will have, instead, a document representative which may have been produced from the documents either manually or automatically.  The starting point of the text analysis process may be the complete document text, an abstract, the title only, or perhaps a list of words only. From it the process must produce a document representative in a form which the computer can handle.  The developments and advances in the process of representation have been reviewed every year by the appropriate chapters of Cuadra's Annual Review of Information Science and Technology*. The reader is referred to them for extensive references. The emphasis in this Chapter is on the statistical (a word used loosely here: it usually simply implies counting) rather than linguistic approaches to automatic text analysis. The reasons for this emphasis are varied. Firstly, there is the limit on space. Were I to attempt a discussion of semantic and syntactic methods applicable to automatic text analysis, it would probably fill another book. Luckily such a book has recently been written by Sparck Jones and Kay[2]. Also Montgomery[3] has written a paper surveying linguistics in information science. Secondly, linguistic analysis has proved to be expensive to implement and it is not clear how to use it to enhance information retrieval. Part of the problem has been that very little progress has been made in formal semantic theory. However, there is some reason for optimism on this front, see, for example, Keenan[4, 5]. Undoubtedly a theory of language will be of extreme importance to the development of intelligent IR systems. But, to date, no such theory has been sufficiently developed for it to be applied successfully to IR. In any case satisfactory, possibly even very good, document retrieval systems can be built without such a theory. Thirdly, the statistical approach has been examined and tried ever since the days of Luhn and has been found to be moderately successful.  The chapter therefore starts with the original ideas of Luhn on which much of automatic text analysis has been built, and then goes on to describe a concrete way of generating document representatives. Furthermore, ways of exploiting and improving document representatives through weighting or classifying keywords are discussed. In passing, some of the evidence for automatic indexing is presented.
irv-0011	Luhn's ideas  In one of Luhn's[6] early papers he states: 'It is here proposed that the frequency of word occurrence in an article furnishes a useful measurement of word significance. It is further proposed that the relative position within a sentence of words having given values of significance furnish a useful measurement for determining the significance of sentences. The significance factor of a sentence will therefore be based on a combination of these two measurements.'  I think this quote fairly summaries Luhn's contribution to automatic text analysis. His assumption is that frequency data can be used to extract words and sentences to represent a document.  Let f be the frequency of occurrence of various word types in a given position of text and r their rank order, that is, the order of their frequency of occurrence, then a plot relating f and r yields a curve similar to the hyperbolic curve in Figure 2.1. This is in fact a curve demonstrating Zipf's Law[7]* which states that the product of the frequency of use of wards and the rank order is approximately constant. Zipf verified his law on American Newspaper English. Luhn used it as a null hypothesis to enable him to specify two cut-offs, an upper and a lower (see Figure 2.1.), thus excluding non-significant words. The words exceeding the upper cut-off were considered to be common and those below the lower cut-off rare, and therefore not contributing significantly to the content of the article. He thus devised a counting technique for finding significant words. Consistent with this he assumed that the resolving power of significant words, by which he meant the ability of words to discriminate content, reached a peak at a rank order position half way between the two cut-offs and from the peak fell off in either direction reducing to almost zero at the cut-off points. A certain arbitrariness is involved in determining the cut-offs. There is no oracle which gives their values. They have to be established by trial and error.  It is interesting that these ideas are really basic to much of the later work in IR. Luhn himself used them to devise a method of automatic abstracting. He went on to develop a numerical measure of significance for sentences based on the number of significant and non-significant words in each portion of the sentence. Sentences were ranked according to their numerical score and the highest ranking were included in the abstract (extract really). Edmundson and Wyllys[8] have gone on to generalise some of Luhn's work by normalising his measurements with respect to the frequency of occurrence of words in general text.  There is no reason why such an analysis should be restricted to just words. It could equally well be applied to stems of words (or phrases) and in fact this has often been done.
irv-0012	Generating document representatives - conflation  Ultimately one would like to develop a text processing system which by menas of computable methods with the minimum of human intervention will generate from the input text (full text, abstract, or title) a document representative adequate for use in an automatic retrieval system. This is a tall order and can only be partially met. The document representative I am aiming for is one consisting simply of a list of class names, each name representing a class of words occurring in the total input text. A document will be indexed by a name if one of its significant words occurs as a member of that class.  Such a system will usually consist of three parts: (1) removal of high frequency words, (2) suffix stripping, (3) detecting equivalent stems.  The removal of high frequency words, 'stop' words or 'fluff' words is one way of implementing Luhn's upper cut-off. This is normally done by comparing the input text with a 'stop list' of words which are to be removed.  Table 2.1 gives a portion of such a list, and demonstrates the kind of words that are involved. The advantages of the process are not only that non-significant words are removed and will therefore not interfere during retrieval, but also that the size of the total document file can be reduced by between 30 and 50 per cent.  The second stage, suffix stripping, is more complicated. A standard approach is to have a complete list of suffixes and to remove the longest possible one.  Table 2.2 lists some suffixes. Unfortunately, context free removal leads to a significant error rate. For example, we may well want UAL removed from FACTUAL but not from EQUAL. To avoid erroneously removing suffixes, context rules are devised so that a suffix will be removed only if the context is right. 'Right' may mean a number of things:  (1) the length of remaining stem exceeds a given number; the default is usually 2;  (2) the stem-ending satisfies a certain condition, e.g. does not end with Q.  Many words, which are equivalent in the above sense, map to one morphological form by removing their suffixes. Others, unluckily, though they are equivalent, do not. It is this latter category which requires special treatment. Probably the simplest method of dealing with it is to construct a list of equivalent stem-endings. For two stems to be equivalent they must match except for their endings, which themselves must appear in the list as equivalent. For example, stems such as ABSORB- and ABSORPT- are conflated because there is an entry in the list defining B and PT as equivalent stem-endings if the preceding characters match.  The assumption (in the context of IR) is that if two words have the same underlying stem then they refer to the same concept and should be indexed as such. This is obviously an over-simplification since words with the same stem, such as NEUTRON AND NEUTRALISE, sometimes need to be distinguished. Even words which are essentially equivalent may mean different things in different contexts. Since there is no cheap way of making these fine distinctions we put up with a certain proportion of errors and assume (correctly) that they will not degrade retrieval effectiveness too much.  It is inevitable that a processing system such as this will produce errors. Fortunately experiments have shown that the error rate tends to be of the order of 5 per cent (Andrews[9]). Lovins [10, 11] using a slightly different approach to stemming also quotes errors of the same order of magnitude.  My description of the three stages has been deliberately undetailed, only the underlying mechanism has been explained. An excellent description of a conflation algorithm, based on Lovins' paper[10] may be found in Andrews[9], where considerable thought is given to implementation efficiency.  Surprisingly, this kind of algorithm is not core limited but limited instead by its processing time.  The final output from a conflation algorithm is a set of classes, one for each stem detected. A class name is assigned to a document if and only if one of its members occurs as a significant word in the text of the document. A document representative then becomes a list of class names. These are often referred to as the documents index terms or keywords.  Queries are of course treated in the same way. In an experimental situation they can be processed at the same time as the documents. In an operational situation, the text processing system needs to be applied to the query at the time that it is submitted to the retrieval system.
irv-0013	Indexing  An index language is the language used to describe documents and requests. The elements of the index language are index terms, which may be derived from the text of the document to be described, or may be arrived at independently. Index languages may be described as pre-coordinate or post-coordinate, the first indicates that terms are coordinated at the time of indexing and the latter at the time of searching. More specifically, in pre-coordinate indexing a logical combination of any index terms may be used as a label to identify a class of documents, whereas in post-coordinate indexing the same class would be identified at search time by combining the classes of documents labelled with the individual index terms.  One last distinction, the vocabulary of an index language may be controlled or uncontrolled. The former refers to a list of approved index terms that an indexer may use, such as for example used by MEdigital libraryARS. The controls on the language may also include hierarchic relationships between the index terms. Or, one may insist that certain terms can only be used as adjectives (or qualifiers). There is really no limit to the kind of syntactic controls one may put on a language.  The index language which comes out of the conflation algorithm in the previous section may be described as uncontrolled, post-coordinate and derived. The vocabulary of index terms at any stage in the evolution of the document collection is just the set of all conflation class names.  There is much controversy about the kind of index language which is best for document retrieval. The recommendations range from the complicated relational languages of Farradane et al.[12] and the Syntol group (see Coates[13] for a description) to the simple index terms extracted by text processing systems just described. The main debate is really about whether automatic indexing is as good as or better than manual indexing. Each can be done to various levels of complexity. However, there seems to be mounting evidence that in both cases, manual and automatic indexing, adding complexity in the form of controls more elaborate than index term weighting do not pay dividends. This has been demonstrated by the results obtained by Cleverdon et al.[14], Aitchison et al.[15], Comparative Systems Laboratory[16] and more recently Keen and Digger [17]. The message is that uncontrolled vocabularies based on natural language achieve retrieval effectiveness comparable to vocabularies with elaborate controls. This is extremely encouraging, since the simple index language is the easiest to automate.  Probably the most substantial evidence for automatic indexing has come out of the SMART Project (1966). Salton[18] recently summarised its conclusions: ' ... on the average the simplest indexing procedures which identify a given document or query by a set of terms, weighted or unweighted, obtained from document or query text are also the most effective'. Its recommendations are clear, automatic text analysis should use weighted terms derived from document excerpts whose length is at least that of a document abstract.  The document representatives used by the SMART project are more sophisticated than just the lists of stems extracted by conflation. There is no doubt that stems rather than ordinary word forms are more effective (Carroll and Debruyn[19]). On top of this the SMART project adds index term weighting, where an index term may be a stem or some concept class arrived at through the use of various dictionaries. For details of the way in which SMART elaborates its document representatives see Salton[20].  In the next sections I shall give a simple discussion of the kind of frequency information that may be used to weight document descriptors and explain the use of automatically constructed term classes to aid retrieval.
irv-0014	Index term weighting  Traditionally the two most important factors governing the effectiveness of an index language have been thought to be the exhaustivity of indexing and the specificity of the index language. There has been much debate about the exact meaning of these two terms. Not wishing to enter into this controversy I shall follow Keen and Digger[17] in giving a working definition of each.  For any document, indexing exhaustivity is defined as the number of different topics indexed, and the index language specificity is the ability of the index language to describe topics precisely. Keen and Digger further define indexing specificity as the level of precision with which a document is actually indexed. It is very difficult to quantify these factors. Human indexers are able to rank their indexing approximately in order of increasing exhaustivity or specificity. However, the same is not easily done for automatic indexing.  It is of some importance to be able to quantify the notions of indexing exhaustivity and specificity because of the predictable effect they have on retrieval effectiveness. It has been recognised (Lancaster[21]) that a high level of exhaustivity of indexing leads to high recall* and low precision*. Conversely, a low level of exhaustivity leads to low recall and high precision. The converse is true for levels of indexing specificity, high specificity leads to high precision and low recall, etc. It would seem, therefore, that there is an optimum level of indexing exhaustivity and specificity for a given user population.  Quite a few people (Sparck Jones[22, 23], Salton and Yang[24]), have attempted to relate these two factors to document collection statistics. For example, exhaustivity can be assumed to be related to the number of index terms assigned to a given document, and specificity related to the number of documents to which a given term is assigned in a given collection. The importance of this rather vague relationship is that the two factors are related to the distribution of index terms in the collection. The relationships postulated are consistent with the observed trade-off between precision and recall just mentioned. Changes in the number of index terms per document lead to corresponding changes in the number of documents per term and vice versa.  I am arguing that in using distributional information about index terms to provide, say, index term weighting we are really attacking the old problem of controlling exhaustivity and specificity.  * These terms are defined in the introduction on page 10.  If we go back to Luhn's original ideas, we remember that he postulated a varying discrimination power for index terms as a function of the rank order of their frequency of occurrence, the highest discrimination power being associated with the middle frequencies. His model was proposed for the selection of significant terms from a document. However, the same frequency counts can be used to provide a weighting scheme for the individual terms in a document. In fact, there is a common weighting scheme in use which gives each index term a weight directly proportional to its frequency of occurrence in the document. At first this scheme would appear to be inconsistent with Luhn's hypothesis that the discrimination power drops off at higher frequencies. However, referring back to Figure 2.1, the scheme would be consistent if the upper cut-off is moved to the point where the peak occurs. It is likely that this is in fact what has happened in experiments using this particular form of weighting.  Attempts have been made to apply weighting based on the way the index terms are distributed in the entire collection. The index term vocabulary of a document collection often has a Zipfian distribution, that is, if we count the number of documents in which each index term occurs and plot them according to rank order, then we obtain the usual hyperbolic shape. Sparck Jones[22] showed experimentally that if there are N documents and an index term occurs in n of them then a weight of log(N/n) + 1 leads to more effective retrieval than if the term were used unweighted. If indexing specificity is assumed to be inversely proportional to the number of documents in which an index term occurs then the weighting can be seen to be attaching more importance to the more specific terms.  The difference between the last mode of weighting and the previous one may be summarised by saying that document frequency weighting places emphasis on content description whereas weighting by specificity attempts to emphasise the ability of terms to discriminate one document from another.  Salton and Yang[24] have recently attempted to combine both methods of weighting by looking at both inter document frequencies and intra document frequencies. Their conclusions are really an extension of those reached by Luhn. By considering both the total frequency of occurrence of a term and its distribution over the documents, that is, how many times it occurs in each document, they were able to draw several conclusions. A term with high total frequency of occurrence is not very useful in retrieval irrespective of its distribution. Middle frequency terms are most useful particularly if the distribution is skewed. Rare terms with a skewed distribution are likely to be useful but less so than the middle frequency ones. Very rare terms are also quite useful but come bottom of the list except for the ones with a high total frequency. The experimental evidence for these conclusions is insufficient to make a more precise statement of their merits.  Salton and his co-workers have developed an interesting tool for describing whether an index is 'good' or 'bad'. They assume that a good index term is one which, when assigned as an index term to a collection of documents, renders the documents as dissimilar as possible, whereas a bad term is one which renders the documents more similar. This is quantified through a term discrimination value which for a particular term measures the increase or decrease in the average dissimilarity between documents on the removal of that term. Therefore, a good term is one which on removal from the collection of documents, leads to a decrease in the average dissimilarity (adding it would hence lead to an increase), whereas a bad term is one which leads on removal to an increase. The idea is that a greater separation between documents will enhance retrieval effectiveness but that less separation will depress retrieval effectiveness. Although superficially this appears reasonable, what really is required is that the relevant documents become less separated in relation to the non-relevant ones. Experiments using the term discrimination model have been reported[25, 26]. A connection between term discrimination and inter document frequency has also been made supporting the earlier results reported by Salton, Wong and Yang[27]. The main results have been conveniently summarised by Yu and Salton[28], where also some formal proofs of retrieval effectiveness improvement are given for strategies based on frequency data. For example, the inverse document frequency weighting scheme described above, that is assigning a weight proportional to log (N/n) + 1, is shown to be formally more effective than not using these weights. Of course, to achieve a proof of this kind some specific assumptions about how to measure effectiveness and how to match documents with queries have to be made. They also establish the effectiveness of a technique used to conflate low frequency terms, which increases recall, and of a technique used to combine high frequency terms into phrases, which increases precision.
irv-0015	Probabilistic indexing  In the past few years, a detailed quantitative model for automatic indexing based on some statistical assumptions about the distribution of words in text has been worked out by Bookstein, Swanson, and Harter[29, 30, 31]. The difference between the terms word-type and word-token is crucial to the understanding of their model. A token instantiates a type, so that it is possible to refer to the occurrence of a word-type WAR; then a particular occurrence at one point in the text of a document (or abstract) will be a word-token. Hence 'the frequency of occurrence of word w in a document' means the number of word-tokens occurring in that document corresponding to a unique word-type. The type/token qualification of a word will be dropped whenever the context makes it clear what is meant when I simply refer to a 'word'.  In their model they consider the difference in the distributional behaviour of words as a guide to whether a word should be assigned as an index term. Their starting point has been the much earlier work by Stone and Rubinoff[32], Damerau[33], and Dennis[34] who showed that the statistical behaviour of 'speciality' words was different from that of 'function' words. They found that function words were closely modelled by a Poisson distribution over all documents whereas specialty words did not follow a Poisson distribution. Specifically, if one is looking at the distribution of a function word w over a set of texts then the probability, f(n), that a text will have n occurrences of the function word w is given by  [ ]  In general the parameter x will vary from word to word, and for a given word should be proportional to the length of the text. We also interpret x as the mean number of occurrences of the w in the set of texts.  The Bookstein-Swanson-Harter model assumes that specialty words are 'content-bearing' whereas function words are not. What this means is that a word randomly distributed according to a Poisson distribution is not informative about the document in which it occurs. At the same time, the fact that a word does not follow a Poisson distribution is assumed to indicate that it conveys information as to what a document is about. This is not an unreasonable view: knowing that the specialty word WAR occurs in the collection one would expect it to occur only in the relatively few documents that are about WAR. On the other hand, one would expect a typical function word such as FOR to be randomly distributed.  The model also assumes that a document can be about a word to some degree. This implies that in general a document collection can be broken up into subsets; each subset being made up of documents that are about a given word to the same degree. The fundamental hypothesis made now is that a content-bearing word is a word that distinguishes more than one class of documents with respect to the extent to which the topic referred to by the word is treated in the documents in each class. It is precisely these words that are the candidates for index terms. These content-bearing words can be mechanically detected by measuring the extent to which their distributions deviate from that expected under a Poisson process. In this model the status of one of these content words within a subset of documents of the same 'aboutness' is one of non-content-bearing, that is, within the given subset it does not discriminate between further subsets.  Harter[31] has identified two assumptions, based upon which the above ideas can be used to provide a method of automatic indexing. The aim is to specify a rule that for any given document will assign it index terms selected from the list of candidates. The assumptions are:  (1) The probability that a document will be found relevant to a request for information on a subject is a function of the relative extent to which the topic is treated in the document.  (2) The number of tokens in a document is a function* of the extent to which the subject referred to by the word is treated in the document.  In these assumptions a 'topic' is identified with the 'subject of the request' and with the 'subject referred to by the word'. Also, only single word requests are considered, although Bookstein and Kraft[35] in a more recent paper have attempted an extension to multi-word requests. The indexing rule based on these assumptions indexes a document with word w if and only if the probability of the document being judged relevant to a request for information on w exceeds some cost function. To calculate the required probability of relevance for a content-bearing word we need to postulate what its distribution would look like. We know that it cannot be a single Poisson distribution, and that it is intrinsic to a content-bearing word that it will distinguish between subsets of documents differing in the extent to which they treat the topic specified by the word. By assumption (2), within one of these subsets the distribution of a content-bearing can however be described by a Poisson process. Therefore, if there are only two such subsets differing in the extent to which they are about a word w then the distribution of w can be described by a mixture of two Poisson distributions. Specifically, with the same notation as before we have  here p1 is the probability of a random document belonging to one of the subsets and x1 and x2 are the mean occurrences in the two classes. This expression shows why the model is sometimes called the 2-Poisson model. It is important to note that it describes the statistical behaviour of a content-bearing word over two classes which are 'about' that word to different extents, these classes are not necessarily the relevant and non-relevant documents although by  * Although Harter[31] uses 'function' in his wording of this assumption, I think 'measure' would have been more appropriate.  assumption (1) we can calculate the probability of relevance for any document from one of these classes. It is the ratio  that is used to make the decision whether to assign an index term w that occurs k times in a document. This ratio is in fact the probability that the particular document belongs to the class which treats w to an average extent of x1 given that it contains exactly k occurrences of w. This ratio is compared with some cost function based on the cost a user is prepared to attach to errors the system might make in retrieval. The details of its specification can be found in the cited papers.  Finally, although tests have shown that this model assigns 'sensible' index terms, it has not been tested from the point of view of its effectiveness in retrieval. Ultimately that will determine whether it is acceptable as a model for automatic indexing.
irv-0016	Discrimination and/or representation  There are two conflicting ways of looking at the problem of characterising documents for retrieval. One is to characterise a document through a representation of its contents, regardless of the way in which other documents may be described, this might be called representation without discrimination. The other way is to insist that in characterising a document one is discriminating it from all, or potentially all, other documents in the collection, this we might call discrimination without representation. Naturally, neither of these extreme positions is assumed in practice, although identifying the two is useful when thinking about the problem of characterisation.  In practice, one seeks some sort of optimal trade-off between representation and discrimination. Traditionally this has been attempted through balancing indexing exhaustively against specificity. Most automatic methods of indexing can be seen to be a mix of representation versus discrimination. In the simple case of removing high frequency words by means of a 'stop' word list we are attempting to increase the level of discrimination between document. Salton's methods based on the discrimination value attempts the same thing. However, it should be clear that when removing possible index terms there must come a stage when the remaining ones cannot adequately represent the contents of documents any more. Bookstein-Swanson-Harter's formal model can be looked upon as one in which the importance of a term in representing the contents of a document is balanced against its importance as a discriminator. They, in fact, attempt to attach a cost function to the trade-ff between the two.  The emphasis on representation leads to what one might call a document-orientation: that is, a total preoccupation with modelling what the document is about. This approach will tend to shade into work on artificial intelligence, particularly of the kind concerned with constructing computer models of the contents of any given piece of natural language text. The relevance of this work in AI, as well as other work, has been conveniently summarised by Smith[36].  This point of view is also adopted by those concerned with defining a concept of 'information', they assume that once this notion is properly explicated a document can be represented by the 'information' it contains[37].  The emphasis on discrimination leads to a query-orientation. This way of looking at things presupposes that one can predict the population of queries likely to be submitted to the IR system. In the light of data about this population of queries, one can then try and characterise documents in the optimal fashion. Recent work attempting to formalise this approach in terms of utility theory has been done by Maron and Cooper[38, 39], although it is difficult to see at this stage how it might be automated.
irv-0017	Automatic keyword classification  Many automatic retrieval systems rely on thesauri to modify queries and document representatives to improve the chance of retrieving relevant documents. Salton[40] has experimented with many different kinds of thesauri and concluded that many of the simple ones justify themselves in terms of improved retrieval effectiveness.  In practice many of thesauri are constructed manually. They have mainly been constructed in two ways:  (1) words which are deemed to be about the same topic are linked;  (2) words which are deemed to be about related things are linked.  The first kind of thesaurus connects words which are intersubstitutible, that is, it puts them into equivalence classes. Then one word could be chosen to represent each class and a list of these words could be used to form a controlled vocabulary. From this an indexer could be instructed to select the words to index a document, or the user could be instructed to select the words to express his query. The same thesaurus could be used in an automatic way to identify the words of a query for the purpose of retrieval.  The second kind of thesaurus uses semantic links between words to, for example, relate them hierarchically. The manually constructed thesaurus used by the MEdigital libraryARS system is of this type.  However, methods have been proposed to construct thesauri automatically. Whereas, the manual thesauri are semantically based (e.g. they recognise synonyms, more general, or more specific relationships) the automatic thesauri tend to be syntactically and statistically based. Again the use of syntax has proved to be of little value, so I shall concentrate on the statistical methods. These are based mainly on the patterns of co-occurrence of words in documents. These 'words' are often the descriptive items which were introduced earlier as terms of keywords.  The basic relationship underlying the automatic construction of keyword classes is as follows: If keyword a and b are substitutible for one another in the sense that we are prepared to accept a document containing one in response to a request containing the other, this will be because they have the same meaning or refer to a common subject or topic. One way of finding out whether two keywords are related is by looking at the documents in which they occur. If they tend to co-occur in the same documents, the chances are that they have to do with the same subject and so can be substituted for one another.  It is not difficult to see that, based on this principle, a classification of keywords can be automatically constructed, of which the classes are used analogously to those of the manual thesaurus mentioned before. More specifically we can identify two main approaches to the use of keyword classifications:  (1) replace each keyword in a document (and query) representative by the name of the class in which it occurs;  (2) replace each keyword by all the keywords occurring in the class to which it belongs.  If we think of a simple retrieval strategy as operating by matching on the descriptors, whether they be keyword names or class names, then 'expanding' representatives in either of these ways will have the effect of increasing the number of matches between document and query, and hence tends to improve recall*. The second way will improve precision as well. Sparck  * Recall is defined in the introduction.  Jones[41] has reported a large number of experiments using automatic keyword classifications and found that in general one obtained a better retrieval performance with the aid of automatic keyword classification than with the unclassified keywords alone.  Unfortunately, even here the evidence has not been conclusive. The work by Minker et al.[42] has not confirmed the findings of Sparck Jones, and in fact they have shown that in some cases keyword classification can be detrimental to retrieval effectiveness. Salton[43], in a review of the work of Minker et al., has questioned their experimental design which leaves the question of the effectiveness of keyword classification still to be resolved by further research.  The discussion of keyword classifications has by necessity been rather sketchy. Readers wishing to pursue it in greater depth should consult Sparck Jones's book[41] on the subject. We shall briefly return to it when we discuss automatic classification methods in Chapter 3.
irv-0018	Normalisation  It is probably useful at this stage to recapitulate and show how a number of levels of normalisation of text is involved in generating document representatives. At the lowest level we have the document which is merely described by a string of words. The first step in normalisation is to remove the 'fluff' words. We now have what traditionally might have been called the 'keywords'. The next stage might be to conflate these words into classes and describe documents by sets of class names which in modern terminology are the keywords or index terms. The next level is the construction of keyword classes by automatic classification. Strictly speaking this is where the normalisation stops.  Index term weighting can also be thought of as a process of normalisation, if the weighting scheme takes into account the number of different index terms per document. For example, we may wish to ensure that a match in one term among ten carries more weight than one among twenty. Similarly, the process of weighting by frequency of occurrence in the total document collection is an attempt to normalise document representatives with respect to expected frequency distributions.
irv-0019	Bibliographic remarks  The early work of H.P. Luhn has been emphasised in this chapter. Therefore, the reader may like to consult the book by Schultz[44] which contains a selection of his papers. In particular, it contains his 1957 and 1958 papers cited in the text. Some other early papers which have had an impact on indexing are Maron and Kuhns[45], and its sequel in Maron[46]. The first paper contains an attempt to construct a probabilistic model for indexing. Batty[47] provides useful background information to the early work on automatic keyword classification. An interesting paper which seems to have been largely ignored in the IR literature is Simon[48]. Simon postulates a stochastic process which will generate a distribution for word frequencies similar to the Zipfian distribution. Doyle[49] examines the role of statistics in text analysis. A recent paper by Sparck Jones[50] compares many of the different approaches to index term weighting. A couple of state-of-the-art reports on automatic indexing are Stevens[51] and Sparck Jones[52]. Salton[53] has compiled a report containing a theory of indexing. Borko[54] has provided a convenient summary of some theoretical approaches to indexing. For an interesting attack on the use of statistical methods in indexing, see Ghose and Dhawle[55].
irv-0021	Introduction  In this chapter I shall attempt to present a coherent account of classification in such a way that the principles involved will be sufficiently understood for anyone wishing to use classification techniques in IR to do so without too much difficulty. The emphasis will be on their application in document clustering, although many of the ideas are applicable to pattern recognition, automatic medical diagnosis, and keyword clustering.  A formal definition of classification will not be attempted; for our purposes it is sufficient to think of classification as describing the process by which a classificatory system is constructed. The word 'classification' is also used to describe the result of such a process. Although indexing is often thought of (wrongly I think) as 'classification' we specifically exclude this meaning. A further distinction to be made is between 'classification' and 'diagnosis'. Everyday language is very ambiguous on this point:  'How would you classify (identify) this?'  'How are these best classified (grouped)?'  The first example refers to diagnosis whereas the second talks about classification proper. These distinctions have been made before in the literature by Kendall[1] and Jardine and Sibson[2].  In the context of information retrieval, a classification is required for a purpose. Here I follow Macnaughton-Smith[3] who states: 'All classifications, even the most general are carried out for some more or less explicit "special purpose" or set of purposes which should influence the choice of [classification] method and the results obtained.' The purpose may be to group the documents in such a way that retrieval will be faster or alternatively it may be to construct a thesaurus automatically. Whatever the purpose the 'goodness' of the classification can finally only be measured by its performance during retrieval. In this way we can side-step the debate about 'natural' and 'best' classifications and leave it to the philosophers (see for example Hempel[4]).  There are two main areas of application of classification methods in IR:  (1) keyword clustering;  (2) document clustering.  The first area is very well dealt with in a recent book by Sparck Jones[5]. Document clustering, although recommended forcibly by Salton and his co-workers, has had very little impact. One possible reason is that the details of Salton's work on document clustering became submerged under the welter of experiments performed on the SMART system. Another is possibly that as the early enthusiasm for clustering wanted, the realisation dawned that significant experiments in this area required quantities of expensive data and large amounts of computer time.  Good[6] and Fairthorne[7] were amongst the first to recommend that automatic classification might prove useful in document retrieval. A clear statement of what is implied by document clustering was made early on by R. M. Hayes[8]: 'We define the organisation as the grouping together of items (e.g. documents, representations of documents) which are then handled as a unit and lose, to that extent, their individual identities. In other words, classification of a document into a classification slot, to all intents and purposes identifies the document with that slot. Thereafter, it and other documents in the slot are treated as identical until they are examined individually. It would appear, therefore, that documents are grouped because they are in some sense related to each other; but more basically, they are grouped because they are likely to be wanted together, and logical relationship is the means of measuring this likelihood.' In the main, people have achieved the 'logical organisation' in two different ways. Firstly, through direct classification of the documents, and secondly via the intermediate calculation of a measure of closeness between documents. The first approach has proved theoretically to be intractable so that any experimental test results cannot be considered to be reliable. The second approach to classification is fairly well documented now, and above all, there are some forceful arguments recommending it in a particular form. It is this approach which is to be emphasised here.  The efficiency of document clustering has been emphasised by Salton[9], he says: 'Clearly in practice it is not possible to match each analysed document with each analysed search request because the time consumed by such operation would be excessive. Various solutions have been proposed to reduce the number of needed comparisons between information items and requests. A particular promising one generates groups of related documents, using an automatic document matching procedure. A representative document group vector is then chosen for each document group, and a search request is initially checked against all the group vectors only. Thereafter, the request is checked against only those individual documents where group vectors show a high score with the request.' Salton believes that although document clustering saves time, it necessarily reduces the effectiveness of a retrieval system. I believe a case has been made showing that on the contrary document clustering has potential for improving the effectiveness (Jardine and van Rijsbergen[10]).
irv-0022	Measures of association  Some classification methods are based on a binary relationship between objects. On the basis of this relationship a classification method can construct a system of clusters. The relationship is described variously as 'similarity', 'association' and 'dissimilarity'. Ignoring dissimilarity for the moment as it will be defined mathematically later, the other two terms mean much the same except that 'association' will be reserved for the similarity between objects characterised by discrete-state attributes. The measure of similarity is designed to quantify the likeness between objects so that if one assumes it is possible to group objects in such a way that an object in a group is more like the other members of the group than it is like any object outside the group, then a cluster method enables such a group structure to be discovered.  Informally speaking, a measure of association increases as the number or proportion of shared attribute states increases. Numerous coefficients of association have been described in the literature, see for example Goodman and Kruskal[11, 12], Kuhns[13], Cormack[14] and Sneath and Sokal[15]. Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised. Intuitively, one would expect this since most measures incorporate the same information. Lerman[16] has investigated the mathematical relationship between many of the measures and has shown that many are monotone with respect to each other. It follows that a cluster method depending only on the rank-ordering of the association values would given identical clusterings for all these measures.  There are five commonly used measures of association in information retrieval. Since in information retrieval documents and requests are most commonly represented by term or keyword lists, I shall simplify matters by assuming that an object is represented by a set of keywords and that the counting measure | . | gives the size of the set. We can easily generalise to the case where the keywords have been weighted, by simply choosing an appropriate measure (in the measure-theoretic sense).  The simplest of all association measures is  |X [[intersection]] Y| Simple matching coefficient  which is the number of shared index terms. This coefficient does not take into account the sizes of X and Y. The following coefficients which have been used in document retrieval take into account the information provided by the sizes of X and Y.  These may all be considered to be normalised versions of the simple matching coefficient. Failure to normalise leads to counter intuitive results as the following example shows:  then |X1| = 1 |Y1| = 1 |X1 [[intersection]] Y2| = 1 =gt; S1 = 1S2 = 1  |X2| = 10 |Y2| = 10 |X2 [[intersection]] Y2| = 1 =gt; S1 = 1S2 = 1/10  S1 (X1, Y1) = S1 (X2, Y2) which is clearly absurd since X1 and Y1 are identical representatives whereas X2 and Y2 are radically different. The normalisation for S2, scales it between ) and 1, maximum similarity being indicated by 1.  Doyle[17] hinted at the importance of normalisation in an amusing way: 'One would regard the postulate "All documents are created equal" as being a reasonable foundation for a library description. Therefore one would like to count either documents or things which pertain to documents, such as index tags, being careful of course to deal with the same number of index tags for each document. Obviously, if one decides to describe the library by counting the word tokens of the text as "of equal interest" one will find that documents contribute to the description in proportion to their size, and the postulate "Big documents are more important than little documents" is at odds with "All documents are created equal" '.  I now return to the promised mathematical definition of dissimilarity. The reasons for preferring the 'dissimilarity' point of view are mainly technical and will not be elaborated here. Interested readers can consult Jardine and Sibson[2] on the subject, only note that any dissimilarity function can be transformed into a similarity function by a simple transformation of the form s = (1 + d)[-1] but the reverse is not always true.  If P is the set of objects to be clustered, a pairwise dissimilarity coefficient D is a function from P x P to the non-negative real numbers. D, in general, satisfies the following conditions:  D1 D(X, Y) gt;= 0 for all X, Y [[propersubset]] P  D2 D(X, X) = 0 for all X [[propersubset]]P  D3 D(X, Y) = D(Y, X) for all X, Y [[propersubset]] P  Informally, a dissimilarity coefficient is a kind of 'distance' function. In fact, many of the dissimilarity coefficients satisfy the triangle inequality:  D4 D(X, Y) lt;= D(X, Z) + D(Y, Z)  which may be recognised as the theorem from Euclidean geometry which states that the sum of the lengths of two sides of a triangle is always greater than the length of the third side.  An example of a dissimilarity coefficient satisfying D1 - D4 is  where (X [[Delta]] Y) = (X [[union]] Y) - (X [[intersection]] Y) is the symmetric different of sets X and Y. It is simply related to Dice's coefficient by  and is monotone with respect to Jaccard's coefficient subtracted from 1. To complete the picture, I shall express this last DC in a different form. Instead of representing each document by a set of keywords, we represent it by a binary string where the absence or presence of the ith keyword is indicated by a zero or one in the ith position respectively. In that case  where summation is over the total number of different keywords in the document collection.  Salton considered document representatives as binary vectors embedded in an n-dimensional Euclidean space, where n is the total number of index terms.  can then be interpreted as the cosine of the angular separation of the two binary vectors X and Y. This readily generalises to the case where X and Y are arbitrary real vectors (i.e. weighted keyword lists) in which case we write  where (X, Y) is the inner product and || . || the length of a vector. If the space is Euclidean then for  X = (x1, ..., xn) and Y = (y1, ..., yn)  we get  Some authors have attempted to base a measure of association on a probabilistic model[18]. They measure the association between two objects by the extent to which their distributions deviate from stochastic independence. This way of measuring association will be of particular importance when in Chapter 5 I discuss how the association between index terms is to be used to improve retrieval effectiveness. There I use the expected mutual information measure to measure association. For two discrete probability distributions P(xi) and P(xj) it can be defined as follows:  When xi and xj are independent P(xi)P(xj) = P(xi,xj) and so I(xi,xj) = 0. Also I(xixj) = 0. Also I(xixj) = I(xjxi) which shows that it is symmetric. It also has the nice property of being invariant under one-to-one transformations of the co-ordinates. Other interesting properties of this measure may be found in Osteyee and Good[19]. Rajski[20] shows how I(xixj) may be simply transformed into a distance function on discrete probability distributions. I(xixj) is often interpreted as a measure of the statistical information contained in xi about xj (or vice versa). When we apply this function to measure the association between two index terms, say i and j, then xi and xj are binary variables. Thus P(xi = 1) will be the probability of occurrence of the term i and similarly P(xi = 0) will be the probability of its non-occurrence. The extent to which two index terms i and j are associated is then measured by I(xixj) which measures the extent to which their distributions deviate from stochastic independence.  A function very similar to the expected mutual information measure was suggested by Jardine and Sibson[2] specifically to measure dissimilarity between two classes of objects. For example, we may be able to discriminate two classes on the basis of their probability distributions over a simple two-point space {1, 0}. Thus let P1(1), P1(0) and P2(1), P2(0) be the probability distributions associated with class I and II respectively. Now on the basis of the difference between them we measure the dissimilarity between I and II by what Jardine and Sibson call the Information Radius, which is  Here u and v are positive weights adding to unit. This function is readly generalised to multi-state, or indeed continuous distribution. It is also easy to shown that under some interpretation the expected mutual information measure is a special case of the information radius. This fact will be of some importance in Chapter 6. To see it we write P1(.) and P2(.) as two conditional distributions P(./w1) and P(./w2). If we now interpret u = P(./w1) and v = P(./w2), that is the prior probability of the conditioning variable in P(./wi), then on substituting into the expression for the information radius and using the identities.  P(x) = P(x/w1) P(w1) + P(x/w2) P(w2) x = 0, 1  P(x/wi) = P(x/wi) P(x) i = 1, 2  we recover the expected mutual information measure I(x,wi).
irv-0023	Classification methods  Let me start with a description of the kind of data for which classification methods are appropriate. The data consists of objects and their corresponding descriptions. The objects may be documents, keywords, hand written characters, or species (in the last case the objects themselves are classes as opposed to individuals). The descriptors come under various names depending on their structure:  (1) multi-state attributes (e.g. colour)  (2) binary-state (e.g. keywords)  (3) numerical (e.g. hardness scale, or weighted keywords)  (4) probability distributions.  The fourth category of descriptors is applicable when the objects are classes. For example, the leaf width of a species of plants may be described by a normal distribution of a certain mean and variance. It is in an attempt to summarise and simplify this kind of data that classification methods are used.  Some excellent surveys of classification methods now exist, to name but a few, Ball[21], Cormack[14] and Dorofeyuk[22]. In fact, methods of classification are now so numerous, that Good[23] has found it necessary to give a classification of classification.  Sparck Jones[24]has provided a very clear intuitive break down of classification methods in terms of some general characteristics of the resulting classificatory system. In what follows the primitive notion of 'property' will mean feature of an object. I quote:  (1) Relation between properties and classes  (a) monothetic  (b) polythetic  (2) Relation between objects and classes  (a) exclusive  (b) overlapping  (3) Relation between classes and classes  (a) ordered  (b) unordered  The first category has been explored thoroughly by numerical taxonomists. An early statement of the distinction between monothetic and polythetic is given by Beckner[25]: 'A class is ordinarily defined by reference to a set of properties which are both necessary and sufficient (by stipulation) for membership in the class. It is possible, however, to define a group K in terms of a set G of properties f1, f2, . . . , fn in a different manner. Suppose we have an aggregate of individuals (we shall not yet call them a class) such that  (1) each one possesses a large (but unspecified) number of the properties in G;  (2) each f in G is possessed by large number of these individuals; and  (3) no f in G is possessed by every individual in the aggregate.'  The first sentence of Beckner's statement refers to the classical Aristotelian definition of a class, which is now termed monothetic. The second part defines polythetic.  To illustrate the basic distinction consider the following example (Figure 3.1) of 8 individuals (1-8) and 8 properties (A-H). The possession of a property is indicated by a plus sign. The individuals 1-4 constitute a polythetic group each individual possessing three out of four of the properties A,B,C,D. The other 4 individuals can be split into two monothetic classes {5,6} and {7,8}. The distinction between monothetic and polythetic is a particularly easy one to make providing the properties are of a simple kind, e.g. binary-state attributes. When the properties are more complex the definitions are rather more difficult to apply, and in any case are rather arbitrary.  The distinction between overlapping and exclusive is important both from a theoretical and practical point of view. Many classification methods can be viewed as data-simplification methods. In the process of classification information is discarded so that the members of one class are indistinguishable. It is in an attempt to minimise the amount of information thrown away, or to put it differently, to have a classification which is in some sense 'closest' to the original data, that overlapping classes are allowed. Unfortunately this plays havoc with the efficiency of implementation for a particular application. A compromise can be adopted in which the classification methods generates overlapping classes in the first instance and is finally 'tidied up' to give exclusive classes.  An example of an ordered classification is a hierarchy. The classes are ordered by inclusion, e.g. the classes at one level are nested in the classes at the next level. To give a simple example of unordered classification is more difficult. Unordered classes generally crop up in automatic thesaurus construction. The classes sought for a thesaurus are those which satisfy certain homogeneity and isolation conditions but in general cannot be simply related to each other. (See for example the use and definition of clumps in Needham[26].) For certain applications ordering is irrelevant, whereas for others such as document clustering it is of vital importance. The ordering enables efficient search strategies to be devised.  The discussion about classification has been purposely vague up to this point. Although the break down scheme discussed gives some insight into classification method. Like all categorisations it isolates some ideal types; but any particular instance will often fall between categories or be a member of a large proportion of categories.  Let me know be more specific about current (and past) approaches to classification, particularly in the context of information retrieval.
irv-0024	The cluster hypothesis  Before describing the battery of classification methods that are now used in information retrieval, I should like to discuss the underlying hypothesis for their use in document clustering. This hypothesis may be simply stated as follows: closely associated documents tend to be relevant to the same requests. I shall refer to this hypothesis as the Cluster Hypothesis.  A basic assumption in retrieval systems is that documents relevant to a request are separated from those which are not relevant, i.e. that the relevant documents are more like one another than they are like non-relevant documents. Whether this is true for a collection can be tested as follows. Compute the association between all pairs of documents:  (a) both of which are relevant to a request, and  (b) one of which is relevant and the other non-relevant.  Summing over a set of requests gives the relative distribution of relevant-relevant (R-R) and relevant-non-relevant (R-N-R) associations of a collection. Plotting the relative frequency against strength of association for two hypothetical collections X and Y we might get distributions as shown in Figure 3.2.  From these it is apparent:  (a) that the separation for collection X is good while for Y it is poor; and  (b) that the strength of the association between relevant documents is greater for X than for Y.  Figure 3.2. R-R is the distribution of relevant-relevant associations, and R-N-R is the distribution of relevant-non-relevant associations.  It is this separation between the distributions that one attempts to exploit in document clustering. It is on the basis of this separation that I would claim that document clustering can lead to more effective retrieval than say a linear search. A linear search ignores the relationship that exists between documents. If the hypothesis is satisfied for a particular collection (some promising results have been published in Jardine and van Rijsbergen[10], and van Rijsbergen and Sparck Jones[27] for three test collections), then it is clear that structuring the collection in such a way that the closely associated documents appear in one class, will not only speed up the retrieval but may also make it more effective, since a class once found will tend to contain only relevant and no non-relevant documents.  I should add that these conclusions can only be verified, finally, by experimental work on a large number of collections. One reason for this is that although it may be possible to structure a document collection so that relevant documents are brought together there is no guarantee that a search strategy will infallibly find the class of documents containing the relevant documents. It is a matter for experimentation whether one can design search strategies which will do the job. So far most experiments in document clustering have been moderately successful but by no means conclusive.  Note that the Cluster Hypothesis refers to given document descriptions. The object of making permanent or temporary changes to a description by such techniques as keyword classifications can therefore be expressed as an attempt to increase the distance between the two distributions R-R and R-N-R. That is, we want to make it more likely that we will retrieve relevant documents and less likely that we will retrieve non-relevant ones.  As can be seen from the above, the Cluster Hypothesis is a convenient way of expressing the aim of such operations as document clustering. Of course, it does not say anything about how the separation is to be exploited.
irv-0025	The use of clustering in information retrieval  There are a number of discussions in print now which cover the use of clustering in IR. The most important of these are by Litofsky[28], Crouch[29], Prywes and Smith[30] and Fritzche[31]. Rather than repeat their chronological treatment here, I shall instead try to isolate the essential features of the various cluster methods.  In choosing a cluster method for use in experimental IR, two, often conflicting, criteria have frequently been used. The first of these, and in my view the most important at this stage of the development of the subject, is the theoretical soundness of the method. By this I mean that the method should satisfy certain criteria of adequacy. To list some of the more important of these:  (1) the method produces a clustering which is unlikely to be altered drastically when  further objects are incorporated, i.e. it is stable under growth.  (2) the method is stable in the sense that small errors in the description of the objects  lead to small changes in the clustering;  (3) the method is independent of the initial ordering of the objects.  These conditions have been adapted from Jardine and Sibson[2]. The point is that any cluster method which does not satisfy these conditions is unlikely to produce any meaningful experimental results. Unfortunately not many cluster methods do satisfy these criteria, probably because algorithms implementing them tend to be less efficient than ad hoc clustering algorithms.  The second criterion for choice is the efficiency of the clustering process in terms of speed and storage requirements. In some experimental work this has been the overriding consideration. But it seems to me a little early in the day to insist on efficiency even before we know much about the behaviour of clustered files in terms of the effectiveness of retrieval (i.e. the ability to retrieve wanted and hold back unwanted documents.) In any case, many of the 'good' theoretical methods (ones which are likely to produce meaningful experimental results) can be modified to increase the efficiency of their clustering process.  Efficiency is really a property of the algorithm implementing the cluster method. It is sometimes useful to distinguish the cluster method from its algorithm, but in the context of IR this distinction becomes slightly less than useful since many cluster methods are defined by their algorithm, so no explicit mathematical formulation exists.  In the main, two distinct approaches to clustering can be identified:  (1) the clustering is based on a measure of similarity between the objects to be clustered;  (2) the cluster method proceeds directly from the object descriptions.  The most obvious examples of the first approach are the graph theoretic methods which define clusters in terms of a graph derived from the measure of similarity. This approach is best explained with an example (see Figure 3.3). Consider a set of objects to be clustered. We compute a numerical value for each pair of objects indicating their similarity. A graph corresponding to this set of similarity values is obtained as follows: A threshold value is decided upon, and two objects are considered linked if their similarity value is above the threshold. The cluster definition is simply made in terms of the graphical representation.  A string is a connected sequence of objects from some starting point.  A connected component is a set of objects such that each object is connected to at least one other member of the set and the set is maximal with respect to this property.  A maximal complete subgraph is a subgraph such that each node is connected to every other node in the subgraph and the set is maximal with respect to this property, i.e. if one further  node were included anywhere the completeness condition would be violated. An example of each is given in Figure 3.4. These methods have been used extensively in keyword clustering by Sparck Jones and Jackson[32], Augustson and Minker[33] and Vaswani and Cameron[34].  A large class of hierarchic cluster methods is based on the initial measurement of similarity. The most important of these is single-link which is the only one to have extensively used in document retrieval. It satisfies all the criteria of adequacy mentioned above. In fact, Jardine and Sibson[2] have shown that under a certain number of reasonable conditions single-link is the only hierarchic method satisfying these important criteria. It will be discussed in some detail in the next section.  A further class of cluster methods based on measurement of similarity is the class of so-called 'clump' methods. They proceed by seeking sets which satisfy certain cohesion and isolation conditions defined in terms of the similarity measure. The computational difficulties of this approach have largely caused it to be abandoned. An attempt to generate a hierarchy of clumps was made by van Rijsbergen[35] but, as expected, the cluster definition was so strict that very few sets could be found to satisfy it.  Efficiency has been the overriding consideration in the definition of the algorithmically defined cluster methods used in IR. For this reason most of these methods have tended to proceed directly from object description to final classification without an intermediate calculation of a similarity measure. Another distinguishing characteristic of these methods is that they do not seek an underlying structure in the data but attempt to impose a suitable structure on it. This is achieved by restricting the number of clusters and by bounding the size of each cluster.  Rather than give a detailed account of all the heuristic algorithms, I shall instead discuss some of the main types and refer the reader to further developments by citing the appropriate authors. Before proceeding, we need to define some of the concepts used in designing these algorithms.  The most important concept is that of cluster representative variously called cluster profile, classification vector, or centroid. It is simply an object which summaries and represents the objects in the cluster. Ideally it should be near to every object in the cluster in some average sense; hence the use of the term centroid. The similarity of the objects to the representative is measured by a matching function (sometimes called similarity or correlation function). The algorithms also use a number of empirically determined parameters such as:  (1) the number of clusters desired;  (2) a minimum and maximum size for each cluster;  (3) a threshold value on the matching function, below which an object will not be included in a cluster;  (4) the control of overlap between clusters;  (5) an arbitrarily chosen objective function which is optimised.  Almost all of the algorithms are iterative, i.e. the final classification is achieved by iteratively improving an intermediate classification. Although most algorithms have been defined only for one-level classification, they can obviously be extended to multi-level classification by the simple device of considering the clusters at one level as the objects to be classified at the next level.  Probably the most important of this kind of algorithm is Rocchio's clustering algorithm[36] which was developed on the SMART project. It operates in three stages. In the first stage it selects (by some criterion) a number of objects as cluster centres. The remaining objects are then assigned to the centres or to a 'rag-bag' cluster (for the misfits). On the basis of the initial assignment the cluster representatives are computed and all objects are once more assigned to the clusters. The assignment rules are explicitly defined in terms of thresholds on a matching function. The final clusters may overlap (i.e. an object may be assigned to more than one cluster). The second stage is essentially an iterative step to allow the various input parameters to be adjusted so that the resulting classification meets the prior specification of such things as cluster size, etc. more nearly. The third stage is for 'tidying up'. Unassigned objects are forcibly assigned, and overlap between clusters is reduced.  Most of these algorithms aim at reducing the number of passes that have to be made of the file of object descriptions. There are a small number of clustering algorithms which only require one pass of the file of object descriptions. Hence the name 'Single-Pass Algorithm' for some of them. Basically they operate as follows:  (1) the object descriptions are processed serially;  (2) the first object becomes the cluster representative of the first cluster;  (3) each subsequent object is matched against all cluster representatives existing at its processing time;  (4) a given object is assigned to one cluster (or more if overlap is allowed) according to some condition on the matching function;  (5) when an object is assigned to a cluster the representative for that cluster is recomputed;  (6) if an object fails a certain test it becomes the cluster representative of a new cluster.  Once again the final classification is dependent on input parameters which can only be determined empirically (and which are likely to be different for different sets of objects) and must be specified in advance.  The simplest version of this kind of algorithm is probably one due to Hill[37]. Subsequently, many variations have been produced mainly the result of changes in the assignment rules and definition of cluster representatives. (See for example Rieber and Marathe[38], Johnson and Lafuente[39] and Etzweiler and Martin[40].)  Related to the single-pass approach is the algorithm of MacQueen[41] which starts with an arbitrary initial partition of the objects. Cluster representatives are computed for the members (sets) of the partition, and objects are reallocated to the nearest cluster representative.  A third type of algorithm is represented by the work of Dattola[42]. His algorithm is based on an earlier algorithm by Doyle. As in the case of MacQueen, it starts with an initial arbitrary partition and set of cluster representatives. The subsequent processing reallocates the objects, some ending up in a 'rag-bag' cluster (cf. Rocchio). After each reallocation the cluster representative is recomputed, but the new cluster representative will only replace the old one if the new representative turns out to be nearer in some sense to the objects in the new cluster than the old representative. Dattola's algorithm has been used extensively by Murray[43] for generating hierarchic classifications. Related to Dattola's approach is that due to Crouch[29]. Crouch spends more time obtaining the initial partition (he calls them categories) and the corresponding cluster representatives. The initial phase is termed the 'categorisation stage', which is followed by the 'classification stage'. The second stage proceeds to reallocate objects in the normal way. His work is of some interest because of the extensive comparisons he made between the algorithms of Rocchio, Rieber and Marathe, Bonner (see below) and his own.  One further algorithm that should be mentioned here is that due to Litofsky[28]. His algorithm is designed only to work for objects described by binary state attributes. It uses cluster representatives and matching functions in an entirely different way. The algorithm shuffles objects around in an attempt to minimise the average number of different attributes present in the members of each cluster. The clusters are characterised by sets of attribute values where each set is the set of attributes common to all members of the cluster. The final classification is a hierarchic one. (For further details about this approach see also Lefkovitz[44].)  Finally, the Bonner[45] algorithm should be mentioned. It is a hybrid of the graph-theoretic and heuristic approaches. The initial clusters are specified by graph-theoretic methods (based on an association measure), and then the objects are reallocated according to conditions on the matching function.  The major advantage of the algorithmically defined cluster methods is their speed: order n log n  (where n is the number of objects to be clustered) compared with order n[2] for the methods based on association measures. However, they have disadvantages. The final classification depends on the order in which the objects are input to the cluster algorithm, i.e. it suffers from the defect of order dependence. In additional the effects of errors in the object descriptions are unpredictable.  One obvious omission from the list of cluster methods is the group of mathematically or statistically based methods such as Factor Analysis and Latest Class Analysis. Although both methods were originally used in IR (see Borko and Bernick[46], Baker[47]) they have now largely been superseded by the cluster methods described above.  The method of single-link avoids the disadvantages just mentioned. Its appropriateness for document clustering is discussed here.
irv-0026	Single-link  The dissimilarity coefficient is the basic input to a single-link clustering algorithm. The output is a hierarchy with associated numerical levels called a dendrogram. Frequently the hierarchy is represented by a tree structure such that each node represents a cluster. The two representations are shown side by side in Figure 3.5 for the same set of objects {A,B,C,D,E}. The clusters are: {A,B}, {C}, {D}, {E} at level L1, {A,B}, {C,D,E} at level L2, and {A,B,C,D,E} at level L3. At each level of the hierarchy one can identify a set of classes, and as one moves up the hierarchy the classes at the lower levels are nested in the classes at the higher levels. A mathematical definition of a dendrogram exists, but is of little use, so will be omitted. Interested readers should consult Jardine and Sibson[2].  To give the reader a better feel for a single-link classification, there is a worked example (see Figure 3.6). A DC (dissimilarity coefficient) can be characterised by a set of graphs, one for each value taken by the DC. The different values taken by the DC in the example are L = .1, .2, .3, .4. The graph at each level is given by a set of vertices corresponding to the objects to be clustered, and any two vertices are linked if their dissimilarity is at most equal to the value of the level L. It should be clear that these graphs characterise the DC completely. Given the graphs and their interpretation a DC can be recovered, and vice versa. Graphs at values other than those taken by the DC are simply the same as at the next smallest value actually taken by the DC, for example, compare the graphs at L = .15 and L = .1.  It is now a simple matter to define single-link in terms of these graphs; at any level a single-link cluster is precisely the set of vertices of a connected component of the graph at that level. In the diagram I have enclosed each cluster with a dotted line. Note that whereas the graphs at any two distinct values taken by the DC will be different, this is not necessarily the case for the  corresponding clusters at those levels. It may be that by increasing the level the links introduced between vertices do not change the total number of connected vertices in a component. For example, the clusters at levels .3 and .4 are the same. The hierarchy is achieved by varying the level from the lowest possible value, increasing it through successive values of the DC until all objects are contained in one cluster. The reason for the name single-link is now apparent: for an object to belong to a cluster it needs to be linked to only one other member of the cluster.  This description immediately leads to an inefficient algorithm for the generation of single-link classes. It was demonstrated in the example above. It simply consists of thresholding the DC at increasing levels of dissimilarity. The binary connection matrices are then calculated at each threshold level, from which the connected components can easily be extracted. This is the basis for many published single-link algorithms. From the point of view of IR, where one is trying to construct a searchable tree it is too inefficient (see van Rijsbergen[48] for an appropriate implementation).
irv-0027	The appropriateness of stratified hierarchic cluster methods  There are many other hierarchic cluster methods, to name but a few: complete-link, average-link, etc. For a critique of these methods see Sibson[49]. My concern here is to indicate their appropriateness for document retrieval. It is as well to realise that the kind of retrieval intended is one in which the entire cluster is retrieved without any further subsequent processing of the documents in the cluster. This is in contrast with the methods proposed by Rocchio, Litofsky, and Crouch who use clustering purely to help limit the extent of a linear search.  Stratified systems of clusters are appropriate because the level of a cluster can be used in retrieval strategies as a parameter analogous to rank position or matching function threshold in a linear search. Retrieval of a cluster which is a good match for a request at a low level in the hierarchy tends to produce high precision* but low recall*; just as a cut-off at a low rank position in a linear search tends to yield high precision but low recall. Similarly, retrieval of a cluster which is a good match for a request at a high level in the hierarchy tends to produce high recall but low precision. Hierarchic systems of clusters are appropriate for three reasons. First, very efficient strategies can be devised to search a hierarchic clustering. Secondly, construction of a hierarchic systems is much faster than construction of a non-hierarchic (that is, stratified but overlapping) system of clusters. Thirdly, the storage requirements for a hierarchic structure are considerably less than for a non-hierarchic structure, particularly during the classification phase.  Given that hierarchic methods are appropriate for document clustering the question arises: 'Which method?' The answer is that under certain conditions (made precise in Jardine and Sibson[2]) the only acceptable stratified hierarchic cluster method is single-link. Let me immediately qualify this by saying that it applies to a method which operates from a dissimilarity coefficient (or some equivalent variant), and does not take into account methods based directly on the object descriptions.  * See introduction for definition.
irv-0028	Single-link and the minimum spanning tree  The single-link tree (such as the one shown in Figure 3.5) is closely related to another kind of tree: the minimum spanning tree, or MST, also derived from a dissimilarity coefficient (Gower and Ross[50]). This second tree is quite different from the first, the nodes instead of representing clusters represent the individual objects to be clustered. The MST is the tree of minimum length connecting the objects, where by 'length' I mean the sum of the weights of the connecting links in the tree. Similarly we can define a maximum spanning tree as one of maximum length. Whether we are interested in a minimum or maximum spanning tree depends entirely on the application we have in mind. For convenience we will concentrate on the minimum spanning tree since it derives naturally from a dissimilarity coefficient and is more common anyway. (In Chapter 6 we shall have cause to use a maximum spanning tree based on the expected mutual information measure.) Given the minimum spanning tree then the single-link clusters are obtained by deleting links from the MST in order of decreasing length; the connected sets after each deletion are the single-link clusters. The order of deletion and the structure of the MST ensure that the clusters will be nested into a hierarchy.  The MST contains more information than the single-link hierarchy and only indirectly information about the single-link clusters. Thus, although we can derive the single-link hierarchy from it by a simple thresholding process, we cannot reverse this and uniquely derive the MST from the single-link hierarchy. It is interesting to consider in the light of this whether the MST would not be more suitable for document clustering than the single-link hierarchy. Unfortunately, it does not seem possible to update a spanning tree dynamically. To add a new object to a single-link hierarchy is relatively straightforward but to add one to an MST is much more complicated.  The representation of the single-link hierarchy through an MST has proved very useful in connecting single-link with other clustering techniques[51]. For example, Boulton and Wallace[52] have shown, using the MST representation, that under suitable assumptions the single-link hierarchy will minimise their information measure of classification. They see classification as a way of economically describing the original object descriptions, and the best classification is one which does it most economically in an information-theoretic sense. It is interesting that the MST has, independently of their work, been used to reduce storage when storing object descriptions, which amounts to a practical application of their result[53].
irv-0029	Implication of classification methods  It is fairly difficult to talk about the implementation of an automatic classification method without at the same time referring to the file structure representing it inside the computer. Nevertheless, there are a few remarks of importance which can be made.  Just as in many other computational problems, it is possible to trade core storage and computation time. In experimental IR, computation time is likely to be at a premium and a classification process can usually be speeded up by using extra storage.  One important decision to be made in any retrieval system concerns the organisation of storage. Usually part of the file structure will be kept in fast store and the rest on backing store. In experimental IR we are interested in a flexible system and getting experiments done quickly. Therefore, frequently much or all of a classification structure is kept in fast store although this would never be done in an operational system where the document collections are so much bigger.  Another good example of the difference in approach between experimental and operational implementations of a classification is in the permanence of the cluster representatives. In experiments we often want to vary the cluster representatives at search time. In fact, we require that each cluster representative can be quickly specified and implemented at search time. Of course, were we to design an operational classification, the cluster representatives would be constructed once and for all at cluster time.  Probably one of the most important features of a classification implementation is that it should be able to deal with a changing and growing document collection. Adding documents to the classification should not be too difficult. For instance, it should not be necessary to take the document classification 'off the air' for lengthy periods to update it. So, we expect the classification to be designed in such a way that a new batch of documents can be readily inserted without reclassifying the entire set of both old and new documents.  Although many classification algorithms claim this feature, the claim is almost invariably not met. Because of the heuristic nature of many of the algorithms, the updated classification is not the same as it would have been if the increased set had been classified from scratch. In addition, many of the updating strategies mess up the classification to such an extent that it becomes necessary to throw away the classification after a series of updates and reclassify completely.  These comments tend to apply to the n log n classification methods. Unfortunately, they are usually recommended over the n[2] methods for two reasons. Firstly, because n log n is considerably less than n[2], and secondly because the time increases only as log n for the n log n methods but as n for the n[2] methods. On the face of it these are powerful arguments. However, I think they mislead. If we assume that the n log n methods cannot be updated without reclassifying each time and that the n[2] methods can (for example, single-link), then the correct comparison is between  where n1 lt; n2 lt; . . . lt; nt = N, and t is the number of updates. In the limit when n is a continuous variable and the sum becomes an integral we are better off with N[2]. In the discrete case the comparison depends rather on the size of the updates ni - ni - 1. So unless we can design an n log n dependence as extra documents are added, we may as well stick with the n[2] methods which satisfy the soundness conditions and preserve n[2] dependence during updating.  In any case, if one is willing to forego some of the theoretical adequacy conditions then it is possible to modify the n[2] methods to 'break the n[2] barrier'. One method is to sample from the document collection and construct a core clustering using an n[2] method on the sample of the documents. The remainder of the documents can then be fitted into the core clustering by a very fast assignment strategy, similar to a search strategy which has log n dependence. A second method is to initially do a coarse clustering of the document collection and then apply the finer classification method of the n[2] kind to each cluster in turn. So, if there are N documents and we divide into k coarse clusters by a method that has order N time dependence (e.g. Rieber and Marathe's method) then the total cluster time will be of order N + [[Sigma]] (N/k)[2] which will be less than N[2].  Another comment to be made about n log n methods is that although they have this time dependence in theory, examination of a number of the algorithms implementing them shows that they actually have an n[2] dependence (e.g. Rocchio's algorithm). Furthermore, most n log n methods have only been tested on single-level classifications and it is doubtful whether they would be able to preserve their n log n dependence if they were used to generate hierarchic classifications (Senko[54]).  In experiments where we are often dealing with only a few thousand documents, we may find that the proportionality constant in the n log n method is so large that the actual time taken for clustering is greater than that for an n[2] method. Croft[55] recently found this when he compared the efficiency of SNOB (Boulton and Wallace[56]), an n log n cluster method, with single-link. In fact, it is possible to implement single-link in such a way that the generation of the similarity values is overlapped in real time with the cluster generation process.  The implementation of classification algorithms for use in IR is by necessity different from implementations in other fields such as for example numerical taxonomy. The major differences arise from differences in the scale and in the use to which a classification structure is to be put.  In the case of scale, the size of the problem in IR is invariably such that for cluster methods based on similarity matrices it becomes impossible to store the entire similarity matrix, let alone allow random access to its elements. If we are to have a reasonably useful cluster method based on similarity matrices we must be able to generate the similarity matrix in small sections, use each section to update the classification structure immediately after it has been generated and then throw it away. The importance of this fact was recognised by Needham[57]. van Rijsbergen[48] has described an implementation of single-link which satisfies this requirement.  When a classification is to be used in IR, it affects the design of the algorithm to the extent that a classification will be represented by a file structure which is  (1) easily updated;  (2) easily searched; and  (3) reasonably compact.  Only (3) needs some further comment. It is inevitable that parts of the storage used to contain a classification will become redundant during an updating phase. This being so it is of some importance to be able to reuse this storage, and if the redundant storage becomes excessive to be able to process the file structure in such a way that it will subsequently reside in one contiguous part of core. This 'compactness' is particularly important during experiments in which the file structure is read into core before being accessed.
irv-0030	Conclusion  Let me briefly summarise the logical structure of this chapter. It started very generally with a descriptive look at automatic classification and its uses. It then discussed association measures which form the basis of an important class of classification methods. Next came a breakdown of classification methods. This was followed by a statement of the hypothesis underlying the use of automatic classification in document clustering. It went on to examine in some detail the use of classification methods in IR leading up to recommendation of single-link for document clustering. Finally we made some practical points about implementation.  This chapter ended on a rather practical note. We continue in this vein in the next chapter where we discuss file structures. These are important if we are to appreciate how it is that we can get dictionaries, document clustering, search strategies, and such like to work inside a computer.
irv-0031	Bibliographic remarks  In recent years a vast literature on automatic classification has been generated. One reason for this is that applications for these techniques have been found in such diverse fields as Biology, Pattern Recognition, and Information Retrieval. The best introduction to the field is still provided by Sneath and Sokal[15] (a much revised and supplemented version of their earlier book) which looks at automatic classification in the context of numerical taxonomy. Second to this, I would recommend a collection of papers edited by Cole[58].  A book and a report on cluster analysis with a computational emphasis are Anderberg[59] and Wishart[60] respectively. Both given listings of Fortran programs for various cluster methods. Other books with a numerical taxonomy emphasis are Everitt[61], Hartigan[62]and Clifford and Stephenson[63]. A recent book with a strong statistical flavour is Van Ryzin[64].  Two papers worth singling out are Sibson[65] and Fisher and Van Ness[66]. The first gives a very lucid account of the foundations of cluster methods based on dissimilarity measures. The second does a detailed comparison of some of the more well-known cluster methods (including single-link) in terms of such conditions on the clusters as connectivity and convexity.  Much of the early work in document clustering was done on the SMART project. An excellent idea of its achievement in this area may be got by reading ISR-10 (Rocchio[36]), ISR-19 (Kerchner[67]), ISR-20 (Murray[43]), and Dattola[68]. Each has been predominantly concerned with document clustering.  There are a number of areas in IR where automatic classification is used which have not been touched on in this chapter. Probably the most important of these is the use of 'Fuzzy Sets' which is an approach to clustering pioneered by Zadeh[69]. Its relationship with the measurement of similarity is explicated in Zadeh[70]. More recently it has been applied in document clustering by Negoita[71], Chan[72] and Radecki[73].  One further interesting area of application of clustering techniques is in the clustering of citation graphs. A measure of closeness is defined between journals as a function of the frequency with which they cite one another. Groups of closely related journals can thus be isolated (Disiss[74]). Related to this is the work of Preparata and Chien[75] who study citation patterns between documents so that mutually cited documents can be stored as closely together as possible. The early work of Ivie[76] was similarly motivated in that he proposed to collect feedback information from users showing which pairs of documents were frequently was then taken as proportional to the strength of association, and documents more closely associated were made more readily accessible than those less closely associated.  Finally, the reader may be interested in pursuing the use of cluster methods in pattern recognition since some of the ideas developed there are applicable to IR. Both Duda and Hart[77]and Watanabe[78] devote a chapter to clustering in the context of pattern recognition.
irv-0033	Introduction  This chapter is mainly concerned with the way in which file structures are used in document retrieval. Most surveys of file structures address themselves to applications in data management which is reflected in the terminology used to describe the basic concepts. I shall (on the whole) follow Hsiao and Harary[1] whose terminology is perhaps slightly non-standard but emphasises the logical nature of file structures. A further advantage is that it enables me to bridge the gap between data management and document retrieval easily. A few other good references on file structures are Roberts[2], Bertziss[3], Dodd[4], and Climenson[5].
irv-0034	Logical or physical organisation and data independence  There is one important distinction that must be made at the outset when discussing file structures. And that is the difference between the logical and physical organisation of the data. On the whole a file structure will specify the logical structure of the data, that is the relationships that will exist between data items independently of the way in which these relationships may actually be realised within any computer. It is this logical aspect that we will concentrate on. The physical organisation is much more concerned with optimising the use of the storage medium when a particular logical structure is stored on, or in it. Typically for every unit of physical store there will be a number of units of the logical structure (probably records) to be stored in it. For example, if we were to store a tree structure on a magnetic disk, the physical organisation would be concerned with the best way of packing the nodes of the tree on the disk given the access characteristics of the disk.  The work on data bases has been very much concerned with a concept called data independence. The aim of this work is to enable programs to be written independently of the logical structure of the data they would interact with. The independence takes the following form, should the file structure overnight be changed from an inverted to a serial file the program should remain unaffected. This independence is achieved by interposing a data model between the user and the data base. The user sees the data model rather than the data base, and all his programs communicate with the model. The user therefore has no interest in the structure of the file.  There is a school of thought that says that says that applications in library automation and information retrieval should follow this path as well[6,7]. And so it should. Unfortunately, there is still much debate about what a good data model should look like. Furthermore, operational implementations of some of the more advanced theoretical systems do not exist yet. So any suggestion that an IR system might be implemented through a data base package should still seem premature. Also, the scale of the problems in IR is such that efficient implementation of the application still demands close scrutiny of the file structure to be used.  Nevertheless, it is worth taking seriously the trend away from user knowledge of file structures, a trend that has been stimulated considerably by attempts to construct a theory of data[8,9]. There are a number of proposals for dealing with data at an abstract level. The best known of these by now is the one put forward by Codd[8], which has become known as the relational model. In it data are described by n-tuples of attribute values. More formally if the data is described by relations, a relation on a set of domains D1, . . . , Dn can be represented by a set of ordered n-tuples each of the form (d1, . . . , dn) where di [[propersubset]] Di. As it is rather difficult to cope with general relations, various levels (three in fact) of normalisation have been introduced restricting the kind of relations allowed.  A second approach is the hierarchical approach. It is used in many existing data base systems. This approach works as one might expect: data is represented in the form of hierarchies. Although it is more restrictive than the relational approach it often seems to be the natural way to proceed. It can be argued that in many applications a hierarchic structure is a good approximation to the natural structure in the data, and that the resulting loss in precision of representation is worth the gain in efficiency and simplicity of representation.  The third approach is the network approach associated with the proposals by the Data Base Task Group of CODASYL. Here data items are linked into a network in which any given link between two items exists because it satisfies some condition on the attributes of those items, for example, they share an attribute. It is more general than the hierarchic approach in the sense that a node can have any number of immediate superiors. It is also equivalent to the relational approach in descriptive power.  The whole field of data base structures is still very much in a state of flux. The advantages and disadvantages of each approach are discussed very thoroughly in Date[10], who also gives excellent annotated citations to the current literature. There is also a recent Computing Survey[ll] which reviews the current state of the art. There have been some very early proponents of the relational approach in IR, as early as 1967 Maron[12] and Levien[13] discussed the design and implementation of an IR system via relations, be it binary ones. Also Prywes and Smith in their review chapter in the Annual Review of Information Science and Technology more recently recommended the DBTG proposals as ways of implementing IR systems[7].  Lurking in the background of any discussion of file structures nowadays is always the question whether data base technology will overtake all. Thus it may be that any application in the field of library automation and information retrieval will be implemented through the use of some appropriate data base package. This is certainly a possibility but not likely to happen in the near future. There are several reasons. One is that data base systems are general purpose systems whereas automated library and retrieval systems are special purpose. Normally one pays a price for generality and in this case it is still too great. Secondly, there now is a considerable investment in providing special purpose systems (for example, MARC)[14] and this is not written off very easily. Nevertheless a trend towards increasing use of data-base technology exists and is well illustrated by the increased prominence given to it in the Annual Review of Information Science and Technology.
irv-0035	A language for describing file structures  Like all subjects in computer science the terminology of file structures has evolved higgledy-piggledy without much concern for consistency, ambiguity, or whether it was possible to make the kind of distinctions that were important. It was only much later that the need for a well-defined, unambiguous language to describe file structures became apparent. In particular, there arose a need to communicate ideas about file structures without getting bogged down by hardware considerations.  This section will present a formal description of file structures. The framework described is important for the understanding of any file structure. The terminology is based on that introduced by Hsiao and Harary (but also see Hsiao[15] and Manola and Hsiao[16]). Their terminology has been modified and extended by Severance[17], a summary of this can be found in van Rijsbergen[18]. Jonkers[19] has formalised a different framework which provides an interesting contrast to the one described here.
irv-0036	Basic terminology  Given a set of 'attributes' A and a set of 'values' V, then a record R is a subset of the cartesian product A x V in which each attribute has one and only one value. Thus R is a set of ordered pairs of the form (an attribute, its value). For example, the record for a document which has been processed by an automatic content analysis algorithm would be  R = {(K1, x1), (K2, x2), . . . (Km, xm)}  The Ki 's are keywords functioning as attributes and the value xi can be thought of as a numerical weight. Frequently documents are simply characterised by the absence or presence of keywords, in which case we write  R = {Kt1, Kt2, . . . , Kti}  where Kti is present if xti = 1 and is absent otherwise.  Records are collected into logical units called files. They enable one to refer to a set of records by name, the file name. The records within a file are often organised according to relationships between the records. This logical organisation has become known as a file structure (or data structure).  It is difficult in describing file structures to keep the logical features separate from the physical ones. The latter are characteristics forced upon us by the recording media (e.g. tape, disk). Some features can be defined abstractly (with little gain) but are more easily understood when illustrated concretely. One such feature is a field. In any implementation of a record, the attribute values are usually positional, that is the identity of an attribute is given by the position of its attribute value within the record. Therefore the data within a record is registered sequentially and has a definite beginning and end. The record is said to be divided into fields and the nth field carries the nth attribute value. Pictorially we have an example of a record with associated fields in Figure 4.1.  The fields are not necessarily constant in length. To find the value of the attribute K4, we first find the address of the record R (which is actually the address of the start of the record) and read the data in the 4th field.  In the same picture I have also shown some fields labelled Pi. They are addresses of other records, and are commonly called pointers. Now we have extended the definition of a record to a set of attribute-value pairs and pointers. Each pointer is usually associated with a particular attribute-value pair. For example, (see Figure 4.2) pointers could be used to link all records for which the value x1 (of attribute K1) is a, similarly for x2 equal to b, etc.  To indicate that a record is the last record pointed to in a list of records we use the null pointer [[logicaland]]. The pointer associated with attribute K in record R will be called a K-pointer. An attribute (keyword) that is used in this way to organise a file is called a key.  The unify the discussion of file structures we need some further concepts. Following Hsiao and Harary again, we define a list L of records with respect to a keyword K, or more briefly a K-list as a set of records containing K such that:  (1) the K-pointers are distinct;  (2) each non-null K-pointer in L gives the address of a record within L;  (3) there is a unique record in L not pointed to by any record containing K; it is called the beginning of the list; and  (4) there is a unique record in L containing the null K-pointer; it is the end of the list.  (Hsiao and Harary state condition (2) slightly differently so that no two K-lists have a record in common; this only appears to complicate things.)  From our previous example:  K1-list : R1, R2, R5  K2-list : R2, R4  K4-list : R1, R2, R3  Finally, we need the definition of a directory of a file. Let F be a file whose records contain just m different keywords K1, K2, . . . , Km. Let ni be the number of records containing the keyword Ki, and hi be the number of Ki-lists in F. Furthermore, we denote by aij the beginning address of the jth Ki-list. Then the directory is the set of sequences  (Ki, ni, hi, ai1, ai2, . . . aihi) i = 1, 2, . . . m  We are now in a position to give a unified treatment of sequential files, inverted files, index-sequential files and multi-list files.
irv-0037	Sequential files  A sequential file is the most primitive of all file structures. It has no directory and no linking pointers. The records are generally organised in lexicographic order on the value of some key. In other words, a particular attribute is chosen whose value will determine the order of the records. Sometimes when the attribute value is constant for a large number of records a second key is chosen to give an order when the first key fails to discriminate.  The implementation of this file structure requires the use of a sorting routine.  Its main advantages are:  (1) it is easy to implement;  (2) it provides fast access to the next record using lexicographic order.  Its disadvantages:  (1) it is difficult to update - inserting a new record may require moving a large proportion of the file;  (2) random access is extremely slow.  Sometimes a file is considered to be sequentially organised despite the fact that it is not ordered according to any key. Perhaps the date of acquisition is considered to be the key value, the newest entries are added to the end of the file and therefore pose no difficulty to updating.
irv-0038	Inverted files  The importance of this file structure will become more apparent when Boolean Searches are discussed in the next chapter. For the moment we limit ourselves to describing its structure.  An inverted file is a file structure in which every list contains only one record. Remember that a list is defined with respect to a keyword K, so every K-list contains only one record. This implies that the directory will be such that ni = hi for all i, that is, the number of records containing Ki will equal the number of Ki-lists. So the directory will have an address for each record containing Ki . For document retrieval this means that given a keyword we can immediately locate the addresses of all the documents containing that keyword. For the previous example let us assume that a non-black entry in the field corresponding to an attribute indicates the presence of a keyword and a black entry its absence. Then the directory will point to the file in the way shown in Figure 4.3. The definition of an inverted files does not require that the addresses in the directory are in any order. However, to facilitate operations such as conjunction ('and') and disjunction ('or') on any two inverted lists, the addresses are normally kept in record number order. This means that 'and' and 'or' operations can be performed with one pass through both lists. The penalty we pay is of course that the inverted file becomes slower to update.
irv-0039	Index-sequential files  An index-sequential file is an inverted file in which for every keyword Ki , we have ni = hi = 1 and a11 lt;a21 . . . lt;am1. This situation can only arise if each record has just one unique keyword, or one unique attribute-value. In practice therefore, this set of records may be order sequentially by a key. Each key value appears in the directory with the associated address of its record. An obvious interpretation of a key of this kind would be the record number. In our example none of the attributes would do the job except the record number. Diagrammatically the index-sequential file would therefore appear as shown in Figure 4.4. I have deliberately written Ri instead of Ki to emphasise the nature of the key.  In the literature an index-sequential file is usually thought of as a sequential file with a hierarchy of indices. This does not contradict the previous definition, it merely describes the way in which the directory is implemented. It is not surprising therefore that the indexes ('index' = 'directory' here) are often oriented to the characteristics of the storage medium. For example (see Figure 4.5) there might be three levels of indexing: track, cylinder and master. Each entry in the track index will contain enough information to locate the start of the track, and the key of the last record in the track which is also normally the highest value on that track. There is a track index for each cylinder. Each entry in the cylinder index gives the last record on each cylinder and the address of the track index for that cylinder. If the cylinder index itself is stored on tracks, then the master index will give the highest key referenced for each track of the cylinder index and the starting address of that track.  No mention has been made of the possibility of overflow during an updating process. Normally provision is made in the directory to administer an overflow area. This of course increases the number of book-keeping entries in each entry of the index.
irv-0040	Multi-lists  A multi-list is really only a slightly modified inverted file. There is one list per keyword, i.e. hi = 1. The records containing a particular keyword Ki are chained together to form the Ki-list and the start of the Ki-list is given in the directory, as illustrated in Figure 4.6. Since there is no K3-list, the field reserved for its pointer could well have been omitted. So could any blank pointer field, so long as no ambiguity arises as to which pointer belongs to which keyword. One way of ensuring this, particularly if the data values (attribute-values) are fixed format, is to have the pointer not pointing to the beginning of the record but pointing to the location of the next pointer in the chain.  The multi-list is designed to overcome the difficulties of updating an inverted file. The addresses in the directory of an inverted file are normally kept in record-number order. But, when the time comes to add a new record to the file, this sequence must be maintained, and inserting the new address can be expensive. No such problem arises with the multi-list, we update the appropriate K-lists by simply chaining in the new record. The penalty we pay for this is of course the increase in search time. This is in fact typical of many of the file structures. Inherent in their design is a trade-off between search time and update time.
irv-0041	Cellular multi-lists  A further modification of the multi-list is inspired by the fact that many storage media are divided into pages, which can be retrieved one at a time. A K-list may cross several page boundaries which means that several pages may have to be accessed to retrieve one record. A modified multi-list structure which avoids this is called a cellular multi-list. The K-lists are limited so that they will not cross the page (cell) boundaries.  At this point the full power of the notation introduced before comes into play. The directory for a cellular multi-list will be the set of sequences  (Ki, ni, hi, ai1, . . . aihi) i = 1, 2, . . . , m  where the hi have been picked to ensure that a Ki-list does not cross a page boundary. In an implementation, just as in the implementation of an index-sequential file, further information will be stored with each address to enable the right page to be located for each key value.
irv-0042	Ring structures  A ring is simply a linear list that closes upon itself. In terms of the definition of a K-list, the beginning and end of the list are the same record. This data-structure is particularly useful to show classification of data.  Let us suppose that a set of documents  {Dl, D2, D3, D4, D5, D6, D7, D8}  has been classified into four groups, that is  {(Dl, D2), (D3, D4), (D5, D6), (D7, D8)}  Furthermore these have themselves been classified into two groups,  {((Dl, D2), (D3, D4)), ((D5, D6), (D7, D8))}  The dendrogram for this structure would be that given in Figure 4.7. To represent this in storage by means of ring structures is now a simple matter (see Figure 4.8).  The Di indicates a description (representation) of a document. Notice how the rings at a lower level are contained in those at a higher level. The field marked Ci normally contains some identifying information with respect to the ring it subsumes. For example, C1 in some way identifies the class of documents {D1, D2}.  Were we to group documents according to the keywords they shared, then for each keyword we would have a group of documents, namely, those which had that keyword in common. Ci would then be the field containing the keyword uniting that particular group. The rings would of course overlap (Figure 4.9), as in this example:  D1 = {K1, K2}  D2 = {K2, K3}  D3 = {K1, K4}  The usefulness of this kind of structure will become more apparent when we discuss searching of classifications. If each ring has associated with it a record which contains identifying information for its members, then, a search strategy searching a structure such as this will first look at Ci (or Ki in the second example) to determine whether to proceed or abandon the search.
irv-0043	Threaded lists  In this section an elementary knowledge of list processing will be assumed. Readers who are unfamiliar with this topic should consult the little book by Foster[20].  A simple list representation of the classification  ((D1, D2), (D3, D4)), ((D5, D6), (D7, D8))  is given in Figure 4.10. Each sublist in this structure has associated with it a record containing only two pointers. (We can assume that Di is really a pointer to document Di.) The function of the pointers should be clear from the diagram. The main thing to note, however, is that the record associated with a list does not contain any identifying information.  A modification of the implementation of a list structure like this which makes it resemble a set of ring structures is to make the right hand pointer of the last element of a sublist point back to the head of the sublist. Each sublist has become effectively a ring structure. We now have what is commonly called a threaded list (see Figure 4.11). The representation I have given is a slight oversimplification in that we need to flag which elements are data elements (giving access to the documents Di) and which elements are just pointer elements. The major advantage associated with a threaded list is that it can be traversed without the aid of a stack. Normally when traversing a conventional list structure the return addresses are stacked, whereas in the threaded list they have been incorporated in the data structure.  One disadvantage associated with the use of list and ring structures for representing classifications is that they can only be entered at the 'top'. An additional index giving entry to the structure at each of the data elements increases the update speed considerably.  Another modification of the simple list representation has been studied extensively by Stanfel[21,22] and Patt[23]. The individual elements (or cells) of the list structure are modified to incorporate one extra field, so that instead of each element looking like this  where the Pis are pointers and S is a symbol. Otherwise no essential change has been made to the simple representation. This structure has become known as the Doubly Chained Tree. Its properties have mainly been investigated for storing variable length keys, where each key is made up by selecting symbols from a finite (usually small) alphabet. For example, let {A,B,C} be the set of key symbols and let R1, R2, R3, R4, R5 be five records to be stored. Let us assign keys made of the 3 symbols, to the record as follows:  AAA R1  AB R2  AC R3  BB R4  BC R5  An example of a doubly chained tree containing the keys and giving access to the records is given in Figure 4.12. The topmost element contains no symbol, it merely functions as the start of the structure. Given an arbitrary key its presence or absence is detected by matching it against keys in the structure. Matching proceeds level by level, once a matching symbol has been found at one level, the P1 pointer is followed to the set of alternative symbols at the next level down. The matching will terminate either:  (1) when the key is exhausted, that is, no more key symbols are left to match; or  (2) when no matching symbol is found at the current level.  For case (1) we have:  (a) the key is present if the P1 pointer in the same cell as the last matching symbol  now points to a record;  (b) P1 points to a further symbol, that is, the key 'falls short' and is therefore not in  the structure.  For case (2), we also have that the key is not in the structure, but now there is a mismatch.  Stanfel and Patt have concentrated on generating search trees with minimum expected search time, and preserving this property despite updating. For the detailed mathematics demonstrating that this is possible the reader is referred to their cited work.
irv-0044	Trees  Although computer scientists have adopted trees as file structures, their properties were originally investigated by mathematicians. In fact, a substantial part of the Theory of Graphs is devoted to the study of trees. Excellent books on the mathematical aspects of trees (and graphs) have been written by Berge[24], Harary et al.,[25 ]and Ore[26]. Harary's book also contains a useful glossary of concepts in graph theory. In addition Bertziss[3] and Knuth[27] discuss topics in graph theory with applications in information processing.  There are numerous definitions of trees. I have chosen a particularly simple one from Berge. If we think of a graph as a set of nodes (or points or vertices) and a set of lines (or edges) such that each line connects exactly two nodes, then a tree is defined to be a finite connected graph with no cycles, and possessing at least two nodes. To define a cycle we first define a chain. We represent the line uk joining two nodes x and y by uk = [x,y]. A chain is a sequence of lines, in which each line uk has one node in common with the preceding line uk-1, and the other vertex in common with the succeeding line uk+1. An example of a chain is [a,x1], [x1,x2], [x2,x3], [x3,b]. A cycle is a finite chain which begins at a node and terminates at the same node (i.e. in the example a = b).  Berge gives the following theorem showing many equivalent characterisations of trees.  Theorem. Let H be a graph with at least n nodes, where n gt; 1; any one of the following equivalent properties characterises a tree.  (1) H is connected and does not possess any cycles.  (2) H contains no cycles and has n - 1 lines.  (3) H is connected and has n - l lines.  (4) H is connected but loses this property if any line is deleted.  (5) Every pair of nodes is connected by one and only one chain.  One thing to be noticed in the discussion so far is that no mention has been made of a direction associated with a line. In most applications in computer science (and IR) one node is singled out as special. This node is normally called the root of the tree, and every other node in the tree can only be reached by starting at the root and proceeding along a chain of lines until the node sought is reached. Implicitly therefore, a direction is associated with each line. In fact, when one comes to represent a tree inside a computer by a list structure, often the addresses are stored in a way which allows movement in only one direction. It is convenient to think of a tree as a directed graph with a reserved node as the root of the tree. Of course, if one has a root then each path (directed chain) starting at the root will eventually terminate at a particular node from which no further branches will emerge. These nodes are called the terminal nodes of the tree.  By now it is perhaps apparent that when we were talking about ring structures and threaded lists in some of our examples we were really demonstrating how to implement a tree structure. The dendrogram in Figure 4.7 can easily be represented as a tree (Figure 4.13). The documents are stored at the terminal nodes and each node represents a class (cluster) of documents. A search for a particular set of documents would be initiated at the root and would proceed along the arrows until the required class was found.  Another example of a tree structure is the directory associated with an index-sequential file. It was described as a hierarchy of indexes, but could equally well have been described as a tree structure.  The use of tree structures in computer science dates back to the early 1950s when it was realised that the so-called binary search could readily be represented by a binary tree. A binary tree is one in which each node (except the terminal nodes) has exactly two branches leaving it. A binary search is an efficient method for detecting the presence or absence of a key value among a set of keys. It presupposes that the keys have been sorted. It proceeds by successive division of the set, at each division discarding half the current set as not containing the sought key. When the set contains N sorted keys the search time is of order log2N. Furthermore, after some thought one can see how this process can be simply represented by a binary tree.  Unfortunately, in many applications one wants the ability to insert a key which has been found to be absent. If the keys are stored sequentially then the time taken by the insertion operation may be of order N. If one, however, stores the keys in a binary tree this lengthy insert time may be overcome, both search and insert time will be of order log2N. The keys are stored at the nodes, at each node a left branch will lead to 'smaller' keys, a right branch will lead to 'greater' keys. A search terminating on a terminal node will indicate that the key is not present and will need to be inserted.  The structure of the tree as it grows is largely dependent on the order in which new keys are presented. Search time may become unnecessarily long because of the lop-sidedness of the tree. Fortunately, it can be shown (Knuth[28]) that random insertions do not change the expected log2N time dependence of the tree search. Nevertheless, methods are available to prevent the possibility of degenerate trees. These are trees in which the keys are stored in such a way that the expected search time is far from optimal. For example, if the keys were to arrive for insertion already ordered then the tree to be built would simply be as shown in Figure 4.14.  It would take us too far afield for me to explain the techniques for avoiding degenerate trees. Essentially, the binary tree is maintained in such a way that at any node the subtree on the left branch has approximately as many levels as the subtree on the right branch. Hence the name balanced tree for such a tree. The search paths in a balanced tree will never be more than 45 per cent longer than the optimum. The expected search and insert times are still of order log N. For further details the reader is recommended to consult Knuth[28].  So far we have assumed that each key was equally likely as a search argument. If one has data giving the probability that the search argument is Ki (a key already in the tree), and the probability that the search argument lies between Ki and Ki+1, then again techniques are known for reordering the tree to optimise the expected search time. Essentially one makes sure that the more frequently accessed keys have the shortest search paths from the root. One well-known technique used when only the second set of probabilities is known, and the others assigned the value zero, is the Hu-Tucker algorithm. Again the interested reader may consult Knuth.  At this point it is probably a good idea to point out that these efficiency considerations are largely irrelevant when it comes to representing a document classification by a tree structure. The situation in document retrieval is different in the following aspects:  (1) we do not have a useful linear ordering on the documents;  (2) a search request normally does not seek the absence or presence of a document.  In fact, what we do have is that documents are more or less similar to each other, and a request seeks documents which in some way best match the request. A tree structure representing a document classification is therefore chosen so that similar documents may be close together. Therefore to rearrange a tree structure to satisfy some 'balancedness' criterion is out of the question. The search efficiency is achieved by bringing together documents which are likely to be required together.  This is not to say that the above efficiency considerations are unimportant in the general context of IR. Many operations, such as the searching of a dictionary, and using a suffix stripping algorithm can be made very efficient by appropriately structuring the binary tree.  The discussion so far has been limited to binary trees. In many applications this two-way split is inappropriate. The natural way to represent document classifications is by a general tree structure, where there is no restriction on the number of branches leaving a node. Another example is the directory of an index sequential file which is normally represented by an m-way tree, where m is the number of branches leaving a node.  Finally, more comments are in order about the manipulation of tree structures in mass storage devices. Up to now we have assumed that to follow a set of pointers poses no particular problems with regard to retrieval speed. Unfortunately, present random access devices are sufficiently slow for it to be impossible to allow an access for, say, each node in a tree. There are ways of partitioning trees in such a way that the number of disk accesses during a tree search can be reduced. Essentially, it involves storing a number of nodes together in one 'page' of disk storage. During a disk access this page is brought into fast memory, is then searched, and the next page to be accessed is determined.
irv-0045	Scatter storage or hash addressing  One file structure which does not relate very well to the ones mentioned before is known as Scatter Storage. The technique by which the file structure is implemented is often called Hash Addressing. Its underlying principle is appealingly simple. Given that we may access the data through a number of keys Ki, then the address of the data in store is located through a key transformation function f which when applied to Ki evaluates to give the address of the associated data. We are assuming here that with each key is associated only one data item. Also for convenience we will assume that each record (data and key) fits into one location, whose address is in the image space of f. The addresses given by the application of f to the keys Ki are called the hash addresses and f is called a hashing function. Ideally f should be such that it spreads the hash addresses uniformly over the available storage. Of course this would be achieved if the function were one-to-one. Unfortunately this cannot be so because the range of possible key values is usually considerably larger than the range of the available storage addresses. Therefore, given any hashing function we have to contend with the fact that two distinct keys Ki and Kj are likely to map to the same address f(Ki) (=f(Kj)). Before I explain some of the ways of dealing with this I shall give a few examples of hashing functions.  Let us assume that the available storage is of size 2[m] then three simple transformations are as follows:  (1) if Ki is the key, then take the square of its binary representation and select m bits from the middle of the result;  (2) cut the binary representation of Ki into pieces each of m bits and add these together. Now select the m least significant bits of the sum as the hash address;  (3) divide the integer corresponding to Ki by the length of the available store 2[m] and use the remainder as the hash address.  Each of these methods has disadvantages. For example, the last one may given the same address rather frequently if there are patterns in the keys. Before using a particular method, the reader is advised to consult the now extensive literature on the subject, e.g. Morris[29], or Lum et al.[30].  As mentioned before there is the problem of collisions, that is, when two distinct keys hash to the same address. The first point to be made about this problem is that it destroys some of the simplicity of hashing. Initially it may have been thought that the key need not be stored with the data at the hash address. Unfortunately this is not so. No matter what method we use to resolve collisions we still need to store the key with the data so that at search time when a key is hashed we can distinguish its data from the data associated with keys which have hashed to the same address.  There are a number of strategies for dealing with collisions. Essentially they fall into two classes, those which use pointers to link together collided keys and those which do not. Let us first look at the ones which do not use pointers. These have a mechanism for searching the store, starting at the address where the collision occurred, for an empty storage location if a record needs to be inserted, or, for a matching key value at retrieval time. The simplest of these advances from the hash address each time moving along a fixed number of locations, say s, until an empty location or the matching key value is found. The collision strategy thus traces out a well defined sequence of locations. This method of dealing with collisions is called the linear method. The tendency with this method is to store collided records as closely to the initial hash address as possible. This leads to an undesirable effect called primary clustering. In this context all this means is that the records tend to concentrate in groups or bunch-up. It destroys the uniform nature of the hashing function. To be more precise, it is desirable that hash addresses are equally likely, however, the first empty location at the end of a collision sequence increases in likelihood in proportion to the number of records in the collision sequence. To see this one needs only to realise that a key hashed to any location in the sequence will have its record stored at the end of the sequence. Therefore big groups of records tend to grow even bigger. This phenomenon is aggravated by a small step size s when seeking an empty location. Sometimes s = 1 is used in which case the collision strategy is known as the open addressing technique. Primary clustering is also worse when the hash table (available storage) is relatively full.  Variations in the linear method which avoid primary clustering involve making the step size a variable. One way is to set s equal to ai + bi[2] on the ith step. Another is to invoke a random number generator which calculates the step size afresh each time. These last two collision handling methods are called the quadratic and random method respectively. Although they avoid primary clustering they are nevertheless subject to secondary clustering, which is caused by keys hashing to the same address and following the same sequence in search of an empty location. Even this can be avoided, see for example Bell and Kaman[31].  The second class of collision handling methods involves extra storage space which is used to chain together collided records. When a collision occurs at a hash address it may be because it is the head of a chain of records which have all hashed to that address, or it may be that a record is stored there which belongs to a chain starting at some other address. In both cases a free location is needed which in the first case is simply linked in and stores the new record, in the second case the intermediate chain element is moved to the free location and the new record is stored at its own hash address thus starting a new chain (a one-element chain so far). A variation on this method is to use a two-level store. At the first level we have a hash table, at the second level we have a bump table which contains all the collided records. At a hash address in the hash table we will find either, a record if no collisions have taken place at that address, or, a pointer to a chain of records which collided at that address. This latter chaining method has the advantage that records need never be moved once they have been entered in the bump table. The storage overhead is larger since records are put in the bump table before the hash table is full.  For both classes of collision strategies one needs to be careful about deletions. For the linear, quadratic etc. collision handling strategies we must ensure that when we delete a record at an address we do not make records which collided at that address unreachable. Similarly with the chaining method we must ensure that a deleted record does not leave a gap in the chain, that is, after deletion the chain must be reconnected.  The advantages of hashing are several. Firstly it is simple. Secondly its insertion and search strategies are identical. Insertion is merely a failed search. If Ki is the hashed key, then if a search of the collision sequence fails to turn up a match in Ki, its record is simply inserted at the end of the sequence at the next free location. Thirdly, the search time is independent of the number of keys to be inserted.  The application of hashing in IR has tended to be in the area of table construction and look-up procedures. An obvious application is when constructing the set of conflation classes during text processing. In Chapter 2, I gave an example of a document representative as simply a list of class names, each name standing for a set of equivalent words. During a retrieval operation, a query will first be converted into a list of class names. To do this each significant word needs to be looked up in a dictionary which gives the name of the class to which it belongs. Clearly there is a case for hashing. We simply apply the hashing function to the word and find the name of the conflation class to which it belongs at the hash address. A similar example is given in great detail by Murray[32].  Finally, let me recommend two very readable discussions on hashing, one is in Page and Wilson[33], the other is in Knuth's third volume[28].
irv-0046	Clustered files  It is now common practice to refer to a file processed by a clustering algorithm as a clustered file, and to refer to the resulting structure as a file structure. For example Salton[34] (p. 288) lists a clustered file as an alternative organisation to inverted, serial, chained files, etc. Although it may be convenient terminologically, it does disguise the real status of cluster methods. Cluster methods (or automatic classification methods) are more profitably discussed at the level of abstraction at which relations are discussed in connection with data bases, that is, in a thoroughly data independent way. In other words, selecting an appropriate cluster method and implementing it are two separate problems. Unfortunately not all users of clustering techniques see it this way, and so the current scene is rather confused. One factor contributing to the confusion is that clustering techniques have been used at a very low level of implementation of system software, for example, to reduce the number of page exceptions in a virtual memory. Therefore, those who use clustering merely to increase retrieval efficiency (in terms of storage and speed) will tend to see a classification structure as a file structure, whereas those who see clustering as a means of discovering (or summarising) some inherent structure in the data will look upon the same structure as a description of the data. Of course, this description may be used to achieve more efficient retrieval (and in IR more effective retrieval in terms of say precision and recall). Furthermore, if one looks carefully at some of the implementations of cluster methods one discovers that the classificatory system is represented inside the computer by one of the more conventional file structures.
irv-0047	Bibliographic remarks  There is now a vast literature on file structures although there are very few survey articles. Where possible I shall point to some of the more detailed discussions which emphasise an application in IR. Of course the chapter on file organisation in the Annual Review is a good source of references as well. Chapter 7 of Salton's latest book contains a useful introduction to file organisation techniques[34].  A general article on data structures of a more philosophical nature well worth reading is Mealey[32].  A description of the use of a sequential file in an on-line environment may be found in Negus and Hall[36]. The effectiveness and efficiency of an inverted file has been extensively compared with a file structure based on clustering by Murray[37]. Ein-Dor[38] has done a comprehensive comparison between an inverted file and a tree structured file. It is hard to find a discussion of an index-sequential file which makes special reference to the needs of document retrieval. Index-sequential organisation is now considered to be basic software which can be used to implement a variety of other file organisations. Nevertheless it is worth studying some of the aspects of its implementation. For this I recommend the paper by McDonell and Montgomery[39] who give a detailed description of an implementation for a mini-computer. Multi-lists and cellular multi-lists are fairly well covered by Lefkovitz[40]. Ring structures have been very popular in CAD and have been written up by Gray[4l]. Extensive use was made of a modified threaded list by van Rijsbergen[42] in his cluster-based retrieval experiments. The doubly chained tree has been adequately dealt with by Stanfel[21,22] and Patt[23].  Work on tree structures in IR goes back a long way as illustrated by the early papers by Salton[43] and Sussenguth[44]. Trees have always attracted much attention in computer science, mainly for the ability to reduce expected search times in data retrieval. One of the earliest papers on this topic is by Windley[45] but the most extensive discussion is still to be found in Knuth[28] where not only methods of construction are discussed but also techniques of reorganisation.  More recently a special kind of tree, called a trie, has attracted attention. This is a tree structure which has records stored at its terminal nodes, and discriminators at the internal nodes. A discriminator at a node is made up from the attributes of the records dominated by that node. Or as Knuth puts it: 'A trie is essentially a M-ary tree whose nodes are M-place vectors with components corresponding to digits or characters. Each node on level l represents the set of all keys that begin with a certain sequence of l characters; the node specifies an M-way branch depending on the (l + 1)st character.' Tries were invented by Fredkins[46], further considered by Sussenguth[44], and more recently studied by Burkhard[47], Rivest[48], and Bentley[49]. The use of tries in data retrieval where one is interested in either a match or mismatch is very similar to the construction of hierarchic document classification, where each node of the tree representing the hierarchy is also associated with a 'discriminator' used to direct the search for relevant documents (see for example Figure 5.3 in Chapter 5).  The use of hashing in document retrieval is dealt with in Higgins and Smith[50], and Chous[51].  It has become fashionable to refer to document collections which have been clustered as clustered files. I have gone to some pains to avoid the use of this terminology because of the conceptual difference that exists between a structure which is inherent in the data and can be discovered by clustering, and an organisation of the data to facilitate its manipulation inside a computer. Unfortunately this distinction becomes somewhat blurred when clustering techniques are used to generate a physical organisation of data. For example, the work by Bell et al.[52] is of this nature. Furthermore, it has recently become popular to cluster records simply to improve the efficiency of retrieval. Buckhard and Keller[53] base the design of a file structure on maximal complete subgraphs (or cliques). Hatfield and Gerald[54] have designed a paging algorithm for a virtual memory store based on clustering. Simon and Guiho[55] look at methods for preserving 'clusters' in the data when it is mapped onto a physical storage device.  Some of the work that has been largely ignored in this chapter, but which is nevertheless of importance when considering the implementation of a file structure, is concerned directly with the physical organisation of a storage device in terms of block sizes, etc. Unfortunately, general statements about this are rather hard to make because the organisation tends to depend on the hardware characteristics of the device and computer. Representative of work in this area is the paper by Lum et al.[56].
irv-0049	Introduction  So far very little has been said about the actual process by which the required information is located. In the case of document retrieval the information is the subset of documents which are deemed to be relevant to the query. In Chapter 4, occasional reference was made to search efficiency, and the appropriateness of a file structure for searching. The kind of search that is of interest, is not the usual kind where the result of the search is clear cut, either yes, the item is present, or no, the item is absent. Good discussions of these may be found in Knuth[1] and Salton[2]. They are of considerable importance when dictionaries need to be set-up or consulted during text processing. However, we are more interested in search strategies in which the documents retrieved may be more or less relevant to the request.  All search strategies are based on comparison between the query and the stored documents. Sometimes this comparison is only achieved indirectly when the query is compared with clusters (or more precisely with the profiles representing the clusters).  The distinctions made between different kinds of search strategies can sometimes be understood by looking at the query language, that is the language in which the information need is expressed. The nature of the query language often dictates the nature of the search strategy. For example, a query language which allows search statements to be expressed in terms of logical combinations of keywords normally dictates a Boolean search. This is a search which achieves its results by logical (rather than numerical) comparisons of the query with the documents. However, I shall not examine query languages but instead capture the differences by talking about the search mechanisms.
irv-0050	Boolean search  A Boolean search strategy retrieves those documents which are 'true' for the query. This formulation only makes sense if the queries are expressed in terms of index terms (or keywords) and combined by the usual logical connectives AND, OR, and NOT. For example, if the query Q = (K1 AND K2) OR (K3 AND (NOT K4)) then the Boolean search will retrieve all documents indexed by K1 and K2, as well as all documents indexed by K3 which are not indexed by K4.  An obvious way to implement the Boolean search is through the inverted file. We store a list for each keyword in the vocabulary, and in each list put the addresses (or numbers) of the documents containing that particular word. To satisfy a query we now perform the set operations, corresponding to the logical connectives, on the Ki-lists. For example, if  K1 -list : D1, D2, D3, D4  K2 -list : D1, D2  K3 -list : D1, D2, D3  K4 -list : D1  and Q = (K1 AND K2) OR (K3 AND (NOT K4))  then to satisfy the (K1 AND K2) part we intersect the K1 and K2 lists, to satisfy the (K3 AND (NOT K4)) part we subtract the K4 list from the K3 list. The OR is satisfied by now taking the union of the two sets of documents obtained for the parts. The result is the set {D1, D2, D3} which satisfies the query and each document in it is 'true' for the query.  A slight modification of the full Boolean search is one which only allows AND logic but takes account of the actual number of terms the query has in common with a document. This number has become known as the co-ordination level. The search strategy is often called simple matching. Because at any level we can have more than one document, the documents are said to be partially ranked by the co-ordination levels.  For the same example as before with the query Q = K1 AND K2 AND K3 we obtain the following ranking:  Co-ordination level  3 D1, D2  2 D3  1 D4  In fact, simple matching may be viewed as using a primitive matching function. For each document D we calculate |D [[intersection]] Q|, that is the size of the overlap between D and Q, each represented as a set of keywords. This is the simple matching coefficient mentioned in Chapter 3.
irv-0051	Matching functions  Many of the more sophisticated search strategies are implemented by means of a matching function. This is a function similar to an association measure, but differing in that a matching function measures the association between a query and a document or cluster profile, whereas an association measure is applied to objects of the same king. Mathematically the two functions have the same properties; they only differ in their interpretations.  There are many examples of matching functions in the literature. Perhaps the simplest is the one associated with the simple matching search strategy.  If M is the matching function, D the set of keywords representing the document, and Q the set representing the query, then:  is another example of a matching function. It is of course the same as Dice's coefficient of Chapter 3.  A popular one used by the SMART project, which they call cosine correlation, assumes that the document and query are represented as numerical vectors in t-space, that is Q = (q1, q2, . . , qt) and D = (d1, d2, . . ., dt) where qi and di are numerical weights associated with the keyword i. The cosine correlation is now simply  or, in the notation for a vector space with a Euclidean norm,  where [[theta]] is the angle between vectors Q and D.
irv-0052	Serial search  Although serial searches are acknowledge to be slow, they are frequently still used as parts of larger systems. They also provide a convenient demonstration of the use of matching functions.  Suppose there are N documents Di in the system, then the serial search proceeds by calculating N values M(Q, Di) the set of documents to be retrieved is determined. There are two ways of doing this:  (1) the matching function is given a suitable threshold, retrieving the documents above the threshold and discarding the ones below. If T is the threshold, then the retrieved set B is the set {Di |M(Q, Di) gt; T}.  (2) the documents are ranked in increasing order of matching function value. A rank position R is chosen as cut-off and all documents below the rank are retrieved so that B = {Di |r(i) lt; R} where r(i) is the rank position assigned to Di. The hope in each case is that the relevant documents are contained in the retrieved set.  The main difficulty with this kind of search strategy is the specification of the threshold or cut-off. It will always be arbitrary since there is no way of telling in advance what value for each query will produce the best retrieval.
irv-0053	Cluster representatives  Before we can sensibly talk about search strategies applied to clustered document collections, we need to say a little about the methods used to represent clusters. Whereas in a serial search we need to be able to match queries with each document in the file, in a search of a clustered file we need to be able to match queries with clusters. For this purpose clusters are represented by some kind of profile (a much overworked word), which here will be called a cluster representative. It attempts to summarise and characterise the cluster of documents.  A cluster representative should be such that an incoming query will be diagnosed into the cluster containing the documents relevant to the query. In other words we expect the cluster representative to discriminate the relevant from the non-relevant documents when matched against any query. This is a tall order, and unfortunately there is no theory enabling one to select the right kind of cluster representative. One can only proceed experimentally. There are a number of 'reasonable' ways of characterising clusters; it then remains a matter for experimental test to decide which of these is the most effective.  Let me first give an example of a very primitive cluster representative. If we assume that the clusters are derived from a cluster method based on a dissimilarity measure, then we can represent each cluster at some level of dissimilarity by a graph (see Figure 5.2). Here A and B are two clusters. The nodes represent documents and the line between any two nodes indicates  that their corresponding documents are less dissimilar than some specified level of dissimilarity. Now, one way of representing a cluster is to select a typical member from the cluster. A simple way of doing this is to find that document which is linked to the maximum number of other documents in the cluster. A suitable name for this kind of cluster representative is the maximally linked document. In the clusters A and B illustrated, there are pointers to the candidates. As one would expect in some cases the representative is not unique. For example, in cluster B we have two candidates. To deal with this, one either makes an arbitrary choice or one maintains a list of cluster representatives for that cluster. The motivation leading to this particular choice of cluster representative is given in some detail in van Rijsbergen[3] but need not concern us here.  Let us now look at other ways of representing clusters. We seek a method of representation which in some way 'averages' the descriptions of the members of the clusters. The method that immediately springs to mind is one in which one calculates the centroid (or centre of gravity) of the cluster. If {D1, D2, . . ., Dn} are the documents in the cluster and each Di is represented by a numerical vector (d1, d2, . . ., dt) then the centroid C of the cluster is given by  where ||Di|| is usually the Euclidean norm, i.e.  More often than not the documents are not represented by numerical vectors but by binary vectors (or equivalently, sets of keywords). In that case we can still use a centroid type of cluster representative but the normalisation is replaced with a process which thresholds the components of the sum [[Sigma]]Di. To be more precise, let Di now be a binary vector, such that a 1 in the jth position indicates the presence of the jth keyword in the document and a 0 indicates the contrary. The cluster representative is now derived from the sum vector  (remember n is the number of documents in the cluster) by the following procedure. Let C = (c1, c2, . . . ct) be the cluster representative and [Di]j the jth component of the binary vector Di, then two methods are:  So, finally we obtain as a cluster representative a binary vector C. In both cases the intuition is that keywords occurring only once in the cluster should be ignored. In the second case we also normalise out the size n of the cluster.  There is some evidence to show that both these methods of representation are effective when used in conjunction with appropriate search strategies (see, for example, van Rijsbergen[4] and Murray[5]). Obviously there are further variations on obtaining cluster representatives but as in the case of association measures it seems unlikely that retrieval effectiveness will change very much by varying the cluster representatives. It is more likely that the way the data in the cluster representative is used by the search strategy will have a larger effect.  There is another theoretical way of looking at the construction of cluster representatives and that is through the notion of a maximal predictor for a cluster[6]. Given that, as before, the documents Di in a cluster are binary vectors then a binary cluster representative for this cluster is a predictor in the sense that each component (ci) predicts that the most likely value of that attribute in the member documents. It is maximal if its correct predictions are as numerous as possible. If one assumes that each member of a cluster of documents D1, . . ., Dn is equally likely then the expected total number of incorrect predicted properties (or simply the expected total number of mismatches between cluster representative and member documents since everything in binary) is,  This can be rewritten as  The expression (*) will be minimised, thus maximising the number of correct predictions, when C = (c1, . . . , ct) is chosen in such a way that  is a minimum. This is achieved by  So in other words a keyword will be assigned to a cluster representative if it occurs in more than half the member documents. This treats errors of prediction caused by absence or presence of keywords on an equal basis. Croft[7] has shown that it is more reasonable to differentiate the two types of error in IR applications. He showed that to predict falsely 0 (cj = 0) is more costly than to predict falsely a 1 (cj = 1). Under this assumption the value of [1]/2 appearing is (3) is replaced by a constant less than [1]/2, its exact value being related to the relative importance attached to the two types of prediction error.  Although the main reason for constructing these cluster representatives is to lead a search strategy to relevant documents, it should be clear that they can also be used to guide a search to documents meeting some condition on the matching function. For example, we may want to retrieve all documents Di which match Q better than T, i.e.  {Di |M (Q, Di) gt; T}  For more details about the evaluation of cluster representative (3) for this purpose the reader should consult the work of Yu et al. [8,9].  One major objection to most work on cluster representatives is that it treats the distribution of keywords in clusters as independent. This is not very realistic. Unfortunately, there does not appear to be any work to remedy the situation except that of Ardnaudov and Govorun[10].  Finally, it should be noted that cluster methods which proceed directly from document descriptions to the classification without first computing the intermediate dissimilarity coefficient, will need to make a choice of cluster representative ab initio. These cluster representatives are then 'improved' as the algorithm, adjusting the classification according to some objective function, steps through its iterations.
irv-0054	Cluster-based retrieval  Cluster-based retrieval has as its foundation the cluster hypothesis, which states that closely associated documents tend to be relevant to the same requests. Clustering picks out closely associated documents and groups them together into one cluster. In Chapter 3, I discussed many ways of doing this, here I shall ignore the actual mechanism of generating the classification and concentrate on how it may be searched with the aim of retrieving relevant documents.  Suppose we have a hierarchic classification of documents then a simple search strategy goes as follows (refer to Figure 5.3 for details). The search starts at the root of the tree, node 0 in the example. It proceeds by evaluating a matching function at the nodes immediately descendant from node 0, in the example the nodes 1 and 2. This pattern repeats itself down the tree. The search is directed by a decision rule, which on the basis of comparing the values of a matching function at each stage decides which node to expand further. Also, it is necessary to have a stopping rule which terminates the search and forces a retrieval. In Figure 5.3 the decision rule is: expand the node corresponding to the maximum value of the matching function achieved within a filial set. The stopping rule is: stop if the current maximum is less than the previous maximum. A few remarks about this strategy are in order:  (1) we assume that effective retrieval can be achieved by finding just one cluster;  (2) we assume that each cluster can be adequately represented by a cluster represent- ative for the purpose of locating the cluster containing the relevant documents;  (3) if the maximum of the matching function is not unique some special action, such as a look-ahead, will need to be taken;  (4) the search always terminates and will retrieve at least one document.  An immediate generalisation of this search is to allow the search to proceed down more than one branch of the tree so as to allow retrieval of more than one cluster. By necessity the decision rule and stopping rule will be slightly more complicated. The main difference being that provision must be made for back-tracking. This will occur when the search strategy estimates (based on the current value of the matching function) that further progress down a branch is a waste of time, at which point it may or may not retrieve the current cluster. The search then returns (back-tracks) to a previous branching point and takes an alternative branch down the tree.  The above strategies may be described as top-down searches. A bottom-up search is one which enters the tree at one of its terminal nodes, and proceeds in an upward direction towards the root of the tree. In this way it will pass through a sequence of nested clusters of increasing size. A decision rule is not required; we only need a stopping rule which could be simply a cut-off. A typical search would seek the largest cluster containing the document represented by the starting node and not exceeding the cut-off in size. Once this cluster is found, the set of documents in it is retrieved. To initiate the search in response to a request it is necessary to know in advance one terminal node appropriate for that request. It is not unusual to find that a user will already known of a document relevant to his request and is seeking other documents similar to it. This 'source' document can thus be used to initiate a bottom-up search. For a systematic evaluation of bottom-up searches in terms of efficiency and effectiveness see Croft[7].  If we now abandon the idea of having a multi-level clustering and accept a single-level clustering, we end up with the approach to document clustering which Salton and his co-workers have worked on extensively. The appropriate cluster method is typified by Rocchio's algorithm described in Chapter 3. The search strategy is in part a serial search. It proceeds by first finding the best (or nearest) cluster(s) and then looking within these. The second stage is achieved by doing a serial search of the documents in the selected cluster(s). The output is frequently a ranking of the documents so retrieved.
irv-0055	Interactive search formulation  A user confronted with an automatic retrieval system is unlikely to be able to express his information need in one go. He is more likely to want to indulge in a trial-and-error process in which he formulates his query in the light of what the system can tell him about his query. The kind of information that he is likely to want to use for the reformulation of his query is:  (1) the frequency of occurrence in the data base of his search terms;  (2) the number of documents likely to be retrieved by his query;  (3) alternative and related terms to be the ones used in his search;  (4) a small sample of the citations likely to be retrieved; and  (5) the terms used to index the citations in (4).  All this can be conveniently provided to a user during his search session by an interactive retrieval system. If he discovers that one of his search terms occurs very frequently he may wish to make it more specific by consulting a hierarchic dictionary which will tell him what his options are. Similarly, if his query is likely to retrieve too many documents he can make it more specific.  The sample of citations and their indexing will give him some idea of what kind of documents are likely to be retrieved and thus some idea of how effective his search terms have been in expressing his information need. He may modify his query in the light of this sample retrieval. This process in which the user modifies his query based on actual search results could be described as a form of feedback.  Examples, both operational and experimental, of systems providing mechanisms of this kind are MEdigital libraryINE[11] and MEDUSA[12] both based on the MEdigital libraryARS system. Another interesting sophisticated experimental system is that described by Oddy[13].  We now look at a mathematical approach to the use of feedback where the system automatically modifies the query.
irv-0056	Feedback  The word feedback is normally used to describe the mechanism by which a system can improve its performance on a task by taking account of past performance. In other words a simple input-output system feeds back the information from the output so that this may be used to improve the performance on the next input. The notion of feedback is well established in biological and automatic control systems. It has been popularised by Norbert Wiener in his book Cybernetics. In information retrieval it has been used with considerable effect.  Consider now a retrieval strategy that has been implemented by means of a matching function M. Furthermore, let us suppose that both the query Q and document representatives D are t-dimensional vectors with real components where t is the number of index terms. Because it is my purpose to explain feedback I will consider its applications to a serial search only.  It is the aim of every retrieval strategy to retrieve the relevant documents A and withhold the non-relevant documents `A. Unfortunately relevance is defined with respect to the user's semantic interpretation of his query. From the point of view of the retrieval system his formulation of it may not be ideal. An ideal formulation would be one which retrieved only the relevant documents. In the case of a serial search the system will retrieve all D for which M(Q,D) gt; T and not retrieve any D for which M(Q,D) lt;= T, where T is a specified threshold. It so happens that in the case where M is the cosine correlation function, i.e.  the decision procedure  M(Q,D) - T gt; 0  corresponds to a linear discriminant function used to linearly separate two sets A and `A in R[t]. Nilsson[14] has discussed in great detail how functions such as this may be 'trained' by modifying the weights qi to discriminate correctly between two categories. Let us suppose for the moment that A and `A are known in advance, then the correct query formulation Q0 would be one for which  M(Q0,D) gt; T whenever D [[propersubset]] A  and  M(Q0,D) lt;= T whenever D [[propersubset]] `[[Alpha]]  The interesting thing is that starting with any Q we can adjust it iteratively using feedback information so that it will converge to Q0. There is a theorem (Nilsson[14], page 81) which states that providing Q0 exists there is an iterative procedure which will ensure that Q will converge to Q0 in a finite number of steps.  The iterative procedure is called the fixed-increment error correction procedure.  It goes as follows:  Qi = Qi-1 + cD if M(Qi-1, D) - T lt;= 0  and D [[propersubset]] A  Qi = Qi-1 - cD if M(Qi-1, D) - T gt; 0  and D [[propersubset]] `A  and no change made to Qi-1 if it diagnoses correctly. c is the correction increment, its value is arbitrary and is therefore usually set to unit. In practice it may be necessary to cycle through the set of documents several times before the correct set of weights are achieved, namely those which will separate A and `A linearly (this is always providing a solution exists).  The situation in actual retrieval is not as simple. We do not know the sets A and `A in advance, in fact A is the set we hope to retrieve. However, given a query formulation Q and the documents retrieved by it we can ask the user to tell the system which of the documents retrieved were relevant and which were not. The system can then automatically modify Q so that at least it will be able to diagnose correctly those documents that the user has seen. The assumption is that this will improve retrieval on the next run by virtue of the fact that its performance is better on a sample.  Once again this is not the whole story. It is often difficult to fix the threshold T in advance so that instead documents are ranked in decreasing matching value on output. It is now more difficult to define what is meant by an ideal query formulation. Rocchio[15] in his thesis defined the optimal query Q0 as one which maximised:  If M is taken to be the cosine function (Q, D) /||Q || ||D || then it is easy to show that [[Phi]] is maximised by  where c is an arbitrary proportionality constant.  If the summations instead of being over A and `A are now made over A [[intersection]] Bi and `A [[intersection]] Bi where Bi is the set of retrieved documents on the ith iteration, then we have a query formulation which is optimal for Bi a subset of the document collection. By analogy to the linear classifier used before, we now add this vector to the query formulation on the ith step to get:  where wi and w2 are weighting coefficients. Salton[2] in fact used a slightly modified version. The most important difference being that there is an option to generate Qi+1 from Qi, or Q, the original query. The effect of all these adjustments may be summarised by saying that the query is automatically modified so that index terms in relevant retrieved documents are given more weight (promoted) and index terms in non-relevant documents are given less weight (demoted).  Experiments have shown that relevance feedback can be very effective. Unfortunately the extent of the effectiveness is rather difficult to gauge, since it is rather difficult to separate the contribution to increased retrieval effectiveness produced when individual documents move up in rank from the contribution produced when new documents are retrieved. The latter of course is what the user most cares about.  Finally, a few comments about the technique of relevance feedback in general. It appears to me that its implementation on an operational basis may be more problematic. It is not clear how users are to assess the relevance, or non-relevance of a document from such scanty evidence as citations. In an operational system it is easy to arrange for abstracts to be output but it is likely that a user will need to browse through the retrieved documents themselves to determine their relevance after which he is probably in a much better position to restate his query himself.
irv-0057	Bibliographic remarks  The book by Lancaster and Fayen[16] contains details of many operational on-line systems. Barraclough[17] has written an interesting survey article about on-line searching. Discussions on search strategies are usually found embedded in more general papers on information retrieval. There are, however, a few specialist references worth mentioning.  Anew classic paper on the limitations of a Boolean search is Verhoeff et al.[18]. Miller[19] has tried to get away from a simple Boolean search by introducing a form of weighting although maintaining essentially a Boolean search. Angione[20] discusses the equivalence of Boolean and weighted searching. Rickman[21] has described a way of introducing automatic feedback into a Boolean search. Goffman[22] has investigated an interesting search strategy based on the idea that the relevance of a document to a query is conditional on the relevance of other documents to that query. In an early paper by Hyvarinen[23], one will find an information-theoretic definition of the 'typical member' cluster representative. Negoita[24] gives a theoretical discussion of a bottom-up search strategy in the context of cluster-based retrieval. Much of the early work on relevance feedback done on the SMART project has now been reprinted in Salton[25]. Two other independence pieces of work on feedback are Stanfel[26] and Bono[27].
irv-0059	Introduction  So far in this book we have made very little use of probability theory in modelling any sub-system in IR. The reason for this is simply that the bulk of the work in IR is non-probabilistic, and it is only recently that some significant headway has been made with probabilistic methods[1,2,3]. The history of the use of probabilistic methods goes back as far as the early sixties but for some reason the early ideas never took hold. In this chapter I shall be describing methods of retrieval, i.e. searching and stopping rules, based on probabilistic considerations. In Chapter 2 I dealt with automatic indexing based on a probabilistic model of the distribution of word tokens within a document (text); here I will be concerned with the distribution of index terms over the set of documents making up a collection or file. I shall be relying heavily on the familiar assumption that the distribution of index terms throughout the collection, or within some subset of it, will tell us something about the likely relevance of any given document.  Perhaps it is as well to warn the reader that some of the material in this chapter is rather mathematical. However, I believe that the framework of retrieval discussed in this chapter is both elegant and potentially extremely powerful*. Although the work on it has been rather recent and thus some may feel that it should stand the test of time, I think it probably represents the most important break-through in IR in the last few years. Therefore I unashamedly make this chapter theoretical, since the theory must be thoroughly understood if any further progress is to be made. There are a number of equivalent ways of presenting the basic theory; I have chosen to present it in such a way that connections with other fields such as pattern recognition are easily made. I shall have more to say about other formulations in the Bibliographic Remarks at the end of the chapter.  The fundamental mathematical tool for this chapter is Bayes' Theorem: most of the equations derive directly from it. Although the underlying mathematics may at first look a little complicated the interpretation is rather simple. So, let me try and immediately given some interpretation of what is to follow.  * This was recognised by Maron in his 'The Logic Behind a Probabilistic Interpretation' as early as 1964[4].  Remember that the basic instrument we have for trying to separate the relevant from the non-relevant documents is a matching function, whether it be that we are in a clustered environment or an unstructured one. The reasons for picking any particular matching function have never been made explicit, in fact mostly they are based on intuitive argument in conjunction with Ockham's Razor. Now in this chapter I shall attempt to use simple probability theory to tell us what a matching function should look like and how it should be used. The arguments are mainly theoretical but in my view fairly conclusive. The only remaining doubt is about the acceptability of the assumptions, which I shall try and bring out as I go along. The data used to fix such a matching function are derived from the knowledge of the distribution of the index terms throughout the collection of some subset of it. If it is defined on some subset of documents then this subset can be defined by a variety of techniques: sampling, clustering, or trial retrieval. The data thus gathered are used to set the values of certain parameters associated with the matching function. Clearly, should the data contain relevance information then the process of defining the matching function can be iterated by some feedback mechanism similar to the one due to Rocchio described in the previous chapter. In this way the parameters of the matching function can be 'learnt'. It is on matching functions derived from relevance information that we shall concentrate.  It will be assumed in the sequel that the documents are described by binary state attributes, that is, absence or presence of index terms. This is not a restriction on the theory, in principle the extension to arbitrary attributes can be worked out, although it is not clear that this would be worth doing[5].
irv-0060	Estimation or calculation of relevance  When we search a document collection, we attempt to retrieve relevant documents without retrieving non-relevant ones. Since we have no oracle which will tell us without fail which documents are relevant and which are non-relevant we must use imperfect knowledge to guess for any given document whether it is relevant or non-relevant. Without going into the philosophical paradoxes associated with relevance, I shall assume that we can only guess at relevance through summary data about the document and its relationships with other documents. This is not an unreasonable assumption particularly if one believes that the only way relevance can ultimately be decided is for the user to read the full text. Therefore, a sensible way of computing our guess is to try and estimate for any document its probability of relevance  PQ (relevance/document)  where the Q is meant to emphasise that it is for a specific query. It is not clear at all what kind of probability this is (see Good[6] for a delightful summary of different kinds), but if we are to make sense of it with a computer and the primitive data we have, it must surely be one based on frequency counts. Thus our probability of relevance is a statistical notion rather than a semantic one, but I believe that the degree of relevance computed on the basis of statistical analysis will tend to be very similar to one arrived at one semantic grounds. Just as a matching function attaches a numerical score to each document and will vary from document to document so will the probability, for some it will be greater than for others and of course it will depend on the query. The variation between queries will be ignored for now, it only becomes important at the evaluation stage. So we will assume only one query has been submitted to the system and we are concerned with  P (relevance/document).  Let us now assume (following Robertson[7]) that:  (1) The relevance of a document to a request is independent of other documents  in the collection.  With this assumption we can now state a principle, in terms of probability of relevance, which shows that probabilistic information can be used in an optimal manner in retrieval. Robertson attributes this principle to W. S Cooper although Maron in 1964 already claimed its optimality[4].  The probability ranking principle. If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose, the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data.  Of course this principle raises many questions as to the acceptability of the assumptions. For example, the Cluster Hypothesis, that closely associated documents tend to be relevant to the same requests, explicitly assumes the contrary of assumption (1). Goffman[8] too, in his work has gone to some pains to make an explicit assumption of dependence. I quote: 'Thus, if a document x has been assessed as relevant to a query s, the relevance of the other documents in the file X may be affected since the value of the information conveyed by these documents may either increase or decrease as a result of the information conveyed by the document x.' Then there is the question of the way in which overall effectiveness is to be measured. Robertson in his paper shows the probability ranking principle to hold if we measure effectiveness in terms of Recall and Fallout. The principle also follows simply from the theory in this chapter. But this is not the place to argue out these research questions, however, I do think it reasonable to adopt the principle as one upon which to construct a probabilistic retrieval model. One word of warning, the probability ranking principle can only be shown to be true for one query. It does not say that the performance over a range of queries will be optimised, to establish a result of this kind one would have to be specific about how one would average the performance across queries.  The probability ranking principle assumes that we can calculate P(relevance/document), not only that, it assumes that we can do it accurately. Now this is an extremely troublesome assumption and it will occupy us some more further on. The problem is simply that we do not know which are the relevant documents, nor do we know how many there are so we have no way of calculating P(relevance/document). But we can, by trial retrieval, guess at P(relevance/ document) and hopefully improve our guess by iteration. To simplify matters in the subsequent discussion I shall assume that the statistics relating to the relevant and non-relevant documents are available and I shall use them to build up the pertinent equations. However, at all times the reader should be aware of the fact that in any practical situation the relevance information must be guessed at (or estimated).  So returning now to the immediate problem which is to calculate, or estimate, P(relevance/ document). For this we use Bayes' Theorem, which relates the posterior probability of relevance to the prior probability of relevance and the likelihood of relevance after observing a document. Before we plunge into a formal expression of this I must introduce some symbols which will make things a little easier as we go along.
irv-0061	Basic probabilistic model*  Since we are assuming that each document is described by the presence/absence of index terms any document can be represented by a binary vector,  x = (x1,x2, . . ., xn)  where xi = 0 or 1 indicates absence or presence of the ith index term. We also assume that there are two mutually exclusive events,  w1 = document is relevant  w2 = document is non-relevant.  * The theory that follows is at first rather abstract, the reader is asked to bear with it, since we soon return to the  nuts and bolts of retrieval.  So, in terms of these symbols, what we wish to calculate for each document is P(w1/x) and perhaps P(w2/x) so that we may decide which is relevant and which is non-relevant. This is a slight change in objective from simply producing a ranking, we also wish the theory to tell us how to cut off the ranking. Therefore we formulate the problem as a decision problem. Of course we cannot estimate P(wi/x) directly so we must find a way of estimating it in terms of quantities we do know something about. Bayes' Theorem tells us that for discrete distributions  Here P(wi) is the prior probability of relevance (i=1) or non-relevance (i=2), P(x/wi) is proportional to what is commonly known as the likelihood of relevance or non-relevance given x; in the continuous case this would be a density function and we would write p(x/wi). Finally,  which is the probability of observing x on a random basis given that it may be either relevant or non-relevant. Again this would be written as a density function p(x) in the continuous case. Although P(x) (or p(x) ) will mostly appear as a normalising factor (i.e. ensuring that P(w1/x) + P(w2/x) = 1) it is in some ways the function we know most about, it does not require a knowledge of relevance for it to be specified. Before I discuss how we go about estimating the right hand side of Bayes' Theorem I will show how the decision for or against relevance is made.  The decision rule we use is in fact well known as Bayes' Decision Rule. It is  [P (w1/x) gt; P(w2/x) -gt; x is relevant, x is non-relevant] * D1  The expression D1 is a short hand notation for the following: compare P (w1/x) with P(w2/x) if the first is greater than the second then decide that x is relevant otherwise decide x is non-relevant. The case P(w1/x) = P(w2/x) is arbitrarily dealt with by deciding non-relevance. The basis for the rule D1 is simply that it minimises the average probability of error, the error of assigning a relevant document as non-relevant or vice versa. To see this note that for any x the probability of error is  * The meaning of [E -gt; p,q] is that if E is true then decide p, otherwise decide q.  In other words once we have decided one way (e.g. relevant) then the probability of having made an error is clearly given by the probability of the opposite way being the case (e.g. non-relevant). So to make this error as small as possible for any given x we must always pick that wi for which P (w1/x) is largest and by implication for which the probability of error is the smallest. To minimise the average probability of error we must minimise  This sum will be minimised by making P (error/x) as small as possible for each x since P(error/x) and P(x) are always positive. This is accomplished by the decision rule D1 which now stands as justified.  Of course average error is not the only sensible quantity worth minimising. If we associate with each type of error a cost we can derive a decision rule which will minimise the overall risk. The overall risk is an average of the conditional risks R(wi/x) which itself in turn is defined in terms of a cost function lij. More specifically lij is the loss incurred for deciding wi when wj is the case. Now the associated expected loss when deciding wi is called the conditional risk and is given by  R (wi/x) - li1P(w1/x) + li2P(w2/x) i = 1, 2  The overall risk is a sum in the same way that the average probability of error was, R (wi/x) now playing the role of P(wi/x). The overall risk is minimised by  [R (w1/x) lt; R (w2/x) -gt; x is relevant, x is non-relevant] D2  D1 and D2 can be shown to be equivalent under certain conditions. First we rewrite D1, using Bayes' Theorem, in a form in which it will be used subsequently, viz.  [P( x/w1) P (w1) gt; P( x/w2) P(w2) -gt; x is relevant, x is non-relevant] D3  Notice that P(x) has disappeared from the equation since it does not affect the outcome of the decision. Now, using the definition R (wi/x) it is easy to show that  [R (w1/x) lt; R (w2/x) ] [[equivalence]] [(l21 - l11) P( x/w1) P(w1) gt; (l12 - l22) P( x/w2) P(w2)]  When a special loss function is chosen, namely,  which implies that no loss is assigned to a correct decision (quite reasonable) and unit loss to any error (not so reasonable), then we have  [R (w1/x) lt; R (w2/x) [[equivalence]] P(x/w1) P (w1) gt; P(x/w2) P(w2)]  which shows the equivalence of D2 and D3, and hence of D1 and D2 under a binary loss function.  This completes the derivation of the decision rule to be used to decide relevance or non-relevance, or to put it differently to retrieve or not to retrieve. So far no constraints have been put on the form of P(x/w1), therefore the decision rule is quite general. I have set up the problem as one of deciding between two classes thereby ignoring the problem of ranking for the moment. One reason for this is that the analysis is simpler, the other is that I want the analysis to say as much as possible about the cut-off value. When ranking, the cut-off value is usually left to the user; within the model so far one can still rank, but the cut-off value will have an interpretation in terms of prior probabilities and cost functions. The optimality of the probability ranking principle follows immediately from the optimality of the decision rule at any cut-off. I shall now go on to be more precise about the exact form of the probability functions in the decision rule.
irv-0062	Form of retrieval function  The previous section was rather abstract and left the connection of the various probabilities with IR rather open. Although it is reasonable for us to want to calculate P(relevance/document) it is not at all clear as to how this should be done or whether the inversion through Bayes' Theorem is the best way of getting at it. Nevertheless, we will proceed assuming that P(x/wi) is the appropriate function to estimate. This function is of course a joint probability function and the interaction between the components of x may be arbitrarily complex. To derive a workable decision rule a simplifying assumption about P(x/wi) will have to be made. The conventional mathematically convenient way of simplifying P(x/wi) is to assume the component variables xi of x to be stochastically independent. Technically this amounts to making the major assumption  P(x/wi) = P(x1/wi) P(x2/wi) ... P(xn/wi) A1  Later I shall show how this stringent assumption may be relaxed. We also for the moment ignore the fact that assuming independence conditional on both w1 and w2 separately has implications about the dependence conditional on w1 [[logicalor]] w2.  Let us now take the simplified form of P(x/wi) and work out what the decision rule will look like. First we define some variables  pi = Prob (xi = 1/w1)  qi = Prob (xi = 1/w2).  In words pi(qi) is the probability that if the document is relevant (non-relevant) that the ith index term will be present. The corresponding probabilities for absence are calculated by subtracting from 1, i.e. 1 - pi = Prob (xi = 0/w1). The likelihood functions which enter into D3 will now look as follows  To appreciate how these expressions work, the reader should check that P((0,1,1,0,0,1)/w1) = (1 - p1)p2p3(1 - p4)(1 - p5)p6. Substituting for P(x/wi) in D3 and taking logs, the decision rule will be transformed into a linear discriminant function.  where the constants ai, bi and e are obvious.  and  The importance of writing it this way, apart from its simplicity, is that for each document x to calculate g(x) we simply add the coefficients ci for those index terms that are present, i.e. for those ci for which xi = 1. The ci are often looked up as weights; Robertson and Sparck Jones[1] call ci a relevance weight, and Salton calls exp(ci) the term relevance. I shall simply refer to it as a coefficient or a weight. Hence the name weighting function for g(x).  The constant C which has been assumed the same for all documents x will of course vary from query to query, but it can be interpreted as the cut-off applied to the retrieval function. The only part that can be varied with respect to a given query is the cost function, and it is this variation which will allow us to retrieve more or less documents. To see this let us assume that l11 = l22 = 0 and that we have some choice in setting the ratio l21/l11 by picking a value for the relative importance we attach to missing a relevant document compared with retrieving a non-relevant one. In this way we can generate a ranking, each rank position corresponding to a different ratio l21/l12.  Let us now turn to the other part of g(x), namely ci and let us try and interpret it in terms of the conventional 'contingency' table.  There will be one such table for each index term; I have shown it for the index term i although the subscript i has not been used in the cells. If we have complete information about the relevant and non-relevant documents in the collection then we can estimate pi by r/R and qi by (n - r)/(N - R). Therefore g(x) can be rewritten as follows:  This is in fact the weighting formula F4 used by Robertson and Sparck Jones1 in their so called retrospective experiments. For later convenience let us set  There are a number of ways of looking at Ki. The most interesting interpretation of Ki is to say that it measures the extent to which the ith term can discriminate between the relevant and non-relevant documents.  Typically the 'weight' Ki(N,r,n,R) is estimated from a contingency table in which N is not the total number of documents in the system but instead is some subset specifically chosen to enable Ki to be estimated. Later I will use the above interpretation of Ki to motivate another function similar to Ki to measure the discrimination power of an index term.
irv-0063	The index terms are not independent  Although it may be mathematically convenient to assume that the index terms are independent it by no means follows that it is realistic to do so. The objection to independence is not new, in 1964 H. H. Williams[9] expressed it this way: 'The assumption of independence of words in a document is usually made as a matter of mathematical convenience. Without the assumption, many of the subsequent mathematical relations could not be expressed. With it, many of the conclusions should be accepted with extreme caution.' It is only because the mathematics become rather intractable if dependence is assumed that people are quick to assume independence. But, 'dependence is the norm rather than the contrary' to quote the famous probability theorist De Finetti[10]. Therefore the correct procedure is to assume dependence and allow the analysis to simplify to the independent case should the latter be true. When speaking of dependence here we mean stochastic dependence; it is not intended as logical dependence although this may imply stochastic dependence. For IR data, stochastic dependence is simply measured by a correlation function or in some other equivalent way. The assumption of dependence could be crucial when we are trying to estimate P(relevance/document) in terms of P(x/wi) since the accuracy with which this latter probability is estimated will no doubt affect the retrieval performance. So our immediate task is to make use of dependence (correlation) between index terms to improve our estimate of P(x/wi) on which our decision rule rests.  In general the dependence can be arbitrarily complex as the following identity illustrates,  P(x) = P(x1)P(x2/x1)P(x3/x1,x2) ... P(xn/x1,x2, ... , xn - 1)  Therefore, to capture all dependence data we would need to condition each variable in turn on a steadily increasing set of other variables. Although in principle this may be possible, it is likely to be computationally inefficient, and impossible in some instances where there is insufficient data to calculate the high order dependencies. Instead we adopt a method of approximation to estimate P(x) which captures the significant dependence information. Intuitively this may be described as one which looks at each factor in the above expansion and selects from the conditioning variables one particular variable which accounts for most of the dependence relation. In other words we seek a product approximation of the form  where (m1, m2, ..., mn) is a permutation of the integers 1,2, ..., n and j(.) is a function mapping i into integers less than i, and P(xi/xm0) is P(xi). An example for a six component vector x = (x1, ..., x6) might be  Pt(x) = P(x1)P(x2/x1)P(x3/x2)P(x4/x2)P(x5/x2)P (x6/x5)  Notice how similar the A2 assumption is to the independence assumption A1, the only difference being that in A2 each factor has a conditioning variable associated with it. In the example the permutation (m1, m2, ..., m6) is (1,2, ..., 6) which is just the natural order, of course the reason for writing the expansion for Pt(x) the way I did in A2 is to show that a permutation of (1,2, ..., 6) must be sought that gives a good approximation. Once this permutation has been found the variables could be relabelled so as to have the natural order again.  The permutation and the function j(.) together define a dependence tree and the corresponding Pt(x) is called a probability distribution of (first-order) tree dependence. The tree corresponding to our six variable example is shown in Figure 6.1. The tree shows which variable appears either side of the conditioning stroke in P(./.). Although I have chosen to  write the function Pt(x) the way I did with xi as the unconditioned variable, and hence the root of the tree, and all others consistently conditioned each on its parent node, in fact any one of the nodes of the tree could be singled out as the root as long as the conditioning is done consistently with respect to the new root node. (In Figure 6.1 the 'direction' of conditioning is marked by the direction associated with an edge.) The resulting Pt(x) will be the same as can easily be shown by using the fact that  Applying this to the link between the root node x1 and its immediate descendant x2 in the example will shift the root to x2 and change the expansion to  Pt(x1, x2, ... x6) = P(x2)P(x1)/x2)P(x3/x2)P(x4/x2)P(x5/x2)P (x6/x5)  Of course, to satisfy the rule about relabelling we would exchange the names '1' and '2'. All expansions transformed in this way are equivalent in terms of goodness of approximation to P(x). It is therefore the tree which represents the class of equivalent expansions. Clearly there are a large number of possible dependence trees, the approximation problem we have is to find the best one; which amounts to finding the best permutation and mapping j(.).  In what follows I shall assume that the relabelling has been done and that xmi = xi.
irv-0064	Selecting the best dependence trees  Our problem now is to find a probability function of the form Pt(x) on a set of documents which is the best approximation to the true joint probability function P(x), and of course a better approximation than the one afforded by making assumption A1*. The set on which the approximation is defined can be arbitrary, it might be the entire collection, the relevant documents (w1), or the non-relevant documents (w2). For the moment I shall leave the set unspecified, all three are important. However, when constructing a decision rule similar to D4 we shall have to approximate P(x/w1) and P(x/w2).  The goodness of the approximation is measured by a well known function (see, for example, Kullback[12]); if P(x) and Pa(x) are two discrete probability distributions then  * That this is indeed the case is shown by Ku and Kullback[11].  is a measure of the extent to which Pa(x) approximates P(x). In terms of this function we want to find a distribution of tree dependence Pt(x) such that I(P,Pt) is a minimum. Or to put it differently to find the dependence tree among all dependence trees which will make I(P,Pt) as small as possible.  If the extent to which two index terms i and j deviate from independence is measured by the expected mutual information measure (EMIM) (see Chapter 3, p 41).  then the best approximation Pt(x), in the sense of minimising I(P,Pt), is given by the maximum spanning tree (MST) (see Chapter 3, p.56) on the variables x1, x2, ..., xn . The spanning tree is derived from the graph whose nodes are the index terms 1,2, ..., n, and whose edges are weighted with I(xi,xj). The MST is simply the tree spanning the nodes for which the total weight  is a maximum. This is a highly condensed statement of how the dependence tree is arrived at, unfortunately a fuller statement would be rather technical. A detailed proof of the optimisation procedure can be found in Chow and Liu[13]. Here we are mainly interested in the application of the tree structure.  One way of looking at the MST is that it incorporates the most significant of the dependences between the variables subject to the global constraint that the sum of them should be a maximum. For example, in Figure 6.1 the links between the variables (nodes, x1, ..., x6) have been put in just because the sum  I(x1,x2) +I(x2,x3) + I(x2,x4) + I(x2,x5) + I(x5/x6)  is a maximum. Any other sum will be less than or equal to this sum. Note that it does not mean that any individual weight associated with an edge in the tree will be greater than one not in the tree, although this will mostly be the case.  Once the dependence tree has been found the approximating distribution can be written down immediately in the form A2. From this I can derive a discriminant function just as I did in the independent case.  ti = Prob (xi = 1/xj(i) = 1)  ri = Prob (xi = 1/xj(i) = 0) and r1 = Prob (x1 = 1)  P(xi /xj(i)) = [ti[xi](1 - ti)[1] [- xi]] [xj(i) []ri[xi ](1 - ri)[1] [- xi]] [1] [- xj(i)]  then  This is a non-linear weighting function which will simplify to the one derived from A1 when the variables are assumed to be independent, that is, when ti = ri. The constant has the same interpretation in terms of prior probabilities and loss function. The complete decision function is of course  g(x) = log P(x/w1) - log P(x/w2)  which now involves the calculation (or estimation) of twice as many parameters as in the linear case. It is only the sum involving xj(i) which make this weighting function different from the linear one, and it is this part which enables a retrieval strategy to take into account the fact that xi depends on xj(i). When using the weighting function a document containing xj(i), or both xi and xj(i), will receive a contribution from that part of the weighting function.  It is easier to see how g(x) combines different weights for different terms if one looks at the weights contributed to g(x) for a given document x for different settings of a pair of variables xi ,xj(i). When xi = 1 and xj(i) = 0 the weight contributed is  and similarly for the other three settings of xi and xj(i).  This shows how simple the non-linear weighting function really is. For example, given a document in which i occurs but j(i) does not, then the weight contributed to g(x) is based on the ratio of two probabilities. The first is the probability of occurrence of i in the set of relevant documents given that j(i) does not occur, the second is the analogous probability computed on the non-relevant documents. On the basis of this ratio we decide how much evidence there is for assigning x to the relevant or non-relevant documents. It is important to remember at this point that the evidence for making the assignment is usually based on an estimate of the pair of probabilities.
irv-0065	Estimation of parameters  The use of a weighting function of the kind derived above in actual retrieval requires the estimation of pertinent parameters. I shall here deal with the estimation of ti and ri for the non-linear case, obviously the linear case will follow by analogy. To show what is involved let me given an example of the estimation process using simple maximum likelihood estimates. The basis for our estimates is the following 2-by-2 table.  Here I have adopted a labelling scheme for the cells in which [x] means the number of occurrences in the cell labelled x. Ignoring for the moment the nature of the set on which this table is based; our estimates might be as follows:  In general we would have two tables of this kind when setting up our function g(x), one for estimating the parameters associated with P(x/w1) and one for P(x/w2). In the limit we would have complete knowledge of which documents in the collection were relevant and which were not. Were we to calculate the estimates for this limiting case, this would only be useful in showing what the upper bound to our retrieval would be under this particular model. More realistically, we would have a sample of documents, probably small (not nesessarily random), for which the relevance status of each document was known. This small set would then be the source data for any 2-by-2 tables we might wish to construct. The estimates therefore would be biased in an unavoidable way.  The estimates shown above are examples of point estimates. There are a number of ways of arriving at an appropriate rule for point estimation. Unfortunately the best form of estimation rule is still an open problem[14]. In fact, some statisticians believe that point estimation should not be attempted at all[15]. However in the context of IR it is hard to see how one can avoid making point estimates. One major objection to any point estimation rule is that in deriving it some 'arbitrary' assumptions are made. Fortunately in IR there is some chance of justifying these assumptions by pointing to experimental data gathered from retrieval systems, thereby removing some of the arbitrariness.  Two basic assumptions made in deriving any estimation rule through Bayesian decision theory are:  (1) the form of the prior distribution on the parameter space, i.e. in our case the assumed  probability distribution on the possible values of the binomial parameter; and  (2) the form of the loss function used to measure the error made in estimating the  parameter.  Once these two assumptions are made explicit by defining the form of the distribution and loss function, then, together with Bayes' Principle which seeks to minimise the posterior conditional expected loss given the observations, we can derive a number of different estimation rules. The statistical literature is not much help when deciding which rule is to be preferred. For details the reader should consult van Rijsbergen[2] where further references to the statistical literature are given. The important rules of estimating a proportion p all come in the form  where x is the number of successes in n trials, and a and b are parameters dictated by the particular combination of prior and loss function. Thus we have a whole class of estimation rules. For example when a=b=0 we have the usual estimate x/n, and when a=b=[1]/2 we have a rule attributed to Sir Harold Jeffreys by Good[16]. This latter rule is in fact the rule used by Robertson and Sparck Jones[1] in their estimates. Each setting of a and b can be justified in terms of the reasonableness of the resulting prior distribution. Since what is found reasonable by one man is not necessarily so for another, the ultimate choice must rest on performance in an experimental test. Fortunately in IR we are in a unique position to do this kind of test.  One important reason for having estimation rules different from the simple x/n, is that this is rather unrealistic for small samples. Consider the case of one sample (n = 1) and the trial result x = 0 (or x = 1) which would result in the estimate for p as p = 0 (or p = 1). This is clearly ridiculous, since in most cases we would already know with high probability that  0 lt; p lt; 1. To overcome this difficulty we might try and incorporate this prior knowledge in a distribution on the possible values of the parameter we are trying to estimate. Once we have accepted the feasibility of this and have specified the way in which estimation error is to be measured, Bayes' Principle (or some other principle) will usually lead to a rule different from x/n.  This is really as much as I wish to say about estimation rules, and therefore I shall not push the technical discussion on this points any further; the interested reader should consult the readily accessible statistical literature.
irv-0066	Recapitulation  At this point I should like to summarise the formal argument thus far so that we may reduce it to simple English. One reason for doing this now is that so far I have stuck closely to what one might call a 'respectable' theoretical development. But as in most applied subjects, in IR when it comes to implementing or using a theory one is forced by either inefficiency or inadequate data to diverge from the strict theoretical model. Naturally one tries to diverge as little as possible, but it is of the essence of research that heuristic modifications to a theory are made so as to fit the real data more closely. One obvious consequence is that it may lead to a better new theory.  The first point to make then, is that, we have been trying to estimate P(relevance/document), that is, the probability of relevance for a given document. Although I can easily write the preceding sentence it is not at all clear that it will be meaningful. Relevance in itself is a difficult notion, that the probability of relevance means something can be objected to on the same grounds that one might object to the probability of Newton's Second Law of Motion being the case. Some would argue that the probability is either one or zero depending on whether it is true or false. Similarly one could argue for relevance. The second point is that the probability P(relevance/document) can be got at by considering the inverse probability P(x/relevance), thus relating the two through Bayes' Theorem. It is not that I am questioning the use of Bayes' Theorem when applied to probabilities, which is forced upon us anyhow if we want to use probability theory consistently, no, what I am questioning is that P(x/relevance) means something in IR and hence can lead us to P(relevance/x). I think that we have to assume that it does, and realise that this assumption will enable us to connect P(relevance/x) with the distributional information about index terms.  To approach the problem in this way would be useless unless one believed that for many index terms the distribution over the relevant documents is different from that over the non-relevant documents. If we assumed the contrary, that is P(x/relevance) = P(x/non-relevance) then the P(relevance/document) would be the same as the prior probability of P(relevance), constant for all documents and hence incapable of discriminating them which is of no use in retrieval. So really we are assuming that there is indirect information available through the joint distribution of index terms over the two sets which will enable us to discriminate them. Once we have accepted this view of things then we are also committed to the formalism derived above. The commitment is that we must guess at P(relevance/document) as accurately as we can, or equivalently guess at P(document/relevance) and P(relevance), through the distributional knowledge we have of the attributes (e.g. index terms) of the document.  The elaboration in terms of ranking rather than just discrimination is trivial: the cut-off set by the constant in g(x) is gradually relaxed thereby increasing the number of documents retrieved (or assigned to the relevant category). The result that the ranking is optimal follows from the fact that at each cut-off value we minimise the overall risk. This optimality should be treated with some caution since it assumes that we have got the form of the P(x/wi)'s right and that our estimation rule is the best possible. Neither of these are likely to be realised in practice.  If one is prepared to let the user set the cut-off after retrieval has taken place then the need for a theory about cut-off disappears. The implication is that instead of working with the ratio  we work with the ratio  In the latter case we do not see the retrieval problem as one of discriminating between relevant and non-relevant documents, instead we merely wish to compute the P(relevance/x) for each document x and present the user with documents in decreasing order of this probability. Whichever way we look at it we still require the estimation of two joint probability functions.  The decision rules derived above are couched in terms of P(x/wi). Therefore one would suppose that the estimation of these probabilities is crucial to the retrieval performance, and of course the fact that they can only be estimated is one explanation for the sub-optimality of the performance. To facilitate the estimation one makes assumptions about the form of P(x/wi). An obvious one is to assume stochastic independence for the components of x. But in general I think this is unrealistic because it is in the nature of information retrieval that index terms will be related to one another. To quote an early paper of Maron's on this point: 'To do this [enlarge upon a request] one would need to program a computing machine to make a statistical analysis of index terms so that the machine will "know" which terms are most closely associated with one another and can indicate the most probable direction in which a given request should be enlarged' [Maron's italics][4]. Therefore a more realistic approach is to assume some sort of dependence between the terms when estimating P(x/w1) and P(x/w2) (or P(x)).  I will now proceed to discuss ways of using this probabilistic model of retrieval and at the same time discuss some of the practical problems that arise. At first I will hardly modify the model at all. But then I will discuss a way of using it which does not necessarily accord strictly with the assumptions upon which it was built in the first place. Naturally the justification for any of this will lie in the province of experimental tests of which many still remain to be done[17]. But first I shall explain a minor modification arising from the need to reduce the dimensionality of our problem.
irv-0067	The curse of dimensionality  In deriving the decision rules I assumed that a document is represented by an n-dimensional vector where n is the size of the index term vocabulary. Typically n would be very large, and so the dimension of the (binary) document vectors is always likely to be greater than the number of samples used to estimate the parameters in the decision function. That this will lead to problems has been pointed out repeatedly in the pattern recognition literature. Although the analysis of the problem in pattern recognition applies to IR as well, the solutions are not directly applicable. In pattern recognition the problem is: given the number of samples that have been used to 'train' the decision function (our weighting function), is there an optimum number of measurements that can be made of an unknown pattern so that the average probability of correct assignment can be maximised? In our case how many index terms can we legitimately use to decide on relevance. Hughes[18] shows that for a very general probabilistic structure the number of measurements is surprisingly small even though reasonably sized samples are used to 'train' the decision function.  Ideally one would like to be able to choose a (small) subset of index terms to which the weighting function g(.) would be restricted thereby maximising the average probability of correct assignment. In pattern recognition there are complicated techniques for doing just that for the equivalent problem. In information retrieval we are fortunate in that there is a natural way in which the dimensionality of the problem can be reduced. We accept that the query terms are a fair guide to the best features to be used in the application of g(.) to decide between relevance and non-relevance. Therefore rather than computing the weighting function for all possible terms we restrict g(.) to the terms specified in the query and possibly their close associates. This would be as if during the retrieval process all documents are projected from a high dimensional space into a subspace spanned by a small number of terms.
irv-0068	Computational details  I now turn to some of the more practical details of computing g(x) for each x when the variables xi are assumed to be stochastically dependent. The main aim of this section will be to demonstrate that the computations involved are feasible. The clearest way of doing this is to discuss the calculation of each 'object' EMIM, MST, and g(.) separately and in that order.
irv-0069	1. Calculation of EMIM  The calculation of the expected mutual information measure can be simplified. Then EMIM itself can be approximated to reduce the computation time even further. We take the simplification first.  When computing I(xi,xj) for the purpose of constructing an MST we need only to know the rank ordering of the I(xi,xj)'s. The absolute values do not matter. Therefore if we use simple maximum likelihood estimates for the probabilities based on the data contained in the following table (using the same notation as on p.125).  then I(xi,xj) will be strictly monotone with  This is an extremely simple formulation of EMIM and easy to compute. Consider the case when it is P(x) we are trying to calculate. The MST is then based on co-occurrence data derived from the entire collection. Once we have this (i.e. [1]) and know the number of documents ([9]) in the file then any inverted file will contain the rest of the frequency data needed to fill in the counts in the other cells. That is from [5] and [7] given by the inverted file we can deduce [2] [3] [4] [6] and [8].  The problem of what to do with zero entries in one of the cells 1 to 4 is taken care of by letting 0 log 0 = 0. The marginals cannot be zero since we are only concerned with terms that occur at least once in the documents.  Next we discuss the possibility of approximation. Maron and Kuhns[19] in their early work used  d(xi,xj) = P(xi = 1, xj = 1) - P(xi =1) P(xj = 1) (*)  to measure the deviation from independence for any two index terms i and j. Apart from the log this is essentially the first term of the EMIM expansion. An MST (dependence tree) constructed on the basis of (*) clearly would not lead to an optimal approximation of P(x/wi) but the fit might be good enough and certainly the corresponding tree can be calculated more efficiently based on (*) than one based on the full EMIM. Similarly Ivie[20] used  as a measure of association. No doubt there are other ways of approximating the EMIM which are easier to compute, but whether they can be used to find a dependence tree leading to good approximation of the joint probability function must remain a matter for experimental test.
irv-0070	2. Calculation of MST  There are numerous published algorithms for generating an MST from pairwise association measures, the most efficient probably being the recent one due to Whitney[21]. The time dependence of his algorithm is 0(n[2]) where n is the number of index terms to be fitted into the tree. This is not a barrier to its use on large data sets, for it is easy to partition the data by some coarse clustering technique as recommended on p.59, after which the total spanning tree can be generated by applying the MST algorithm to each cluster of index terms in turn. This will reduce the time dependence from 0(n[2]) to 0(k[2]) where k lt;lt; n.  It is along these lines that Bentley and Friedman[22] have shown that by exploiting the geometry of the space in which the index terms are points the computation time for generating the MST can be shown to be almost always 0(n log n). Moreover if one is prepared to accept a spanning tree which is almost an MST then a computation time of 0(n log n) is guaranteed.  One major inefficiency in generating the MST is of course due to the fact that all n(n-1)/2 associations are computed whereas only a small number are in fact significant in the sense that they are non-zero and could therefore be chosen for a weight of an edge in the spanning tree. However, a high proportion are zero and could safely be omitted. Unfortunately, the only way we can ignore them is to first compute them. Croft[23] in a recent design for the single-link algorithm has discovered a way of ignoring associations without first computing them. It does however presuppose that a file and its inverted form are available, so that if this is not so some computation time would need to be invested in the inversion. It may be that a similar algorithm could be devised for computing an MST.
irv-0071	3. Calculation of g(x)  It must be emphasised that in the non-linear case the estimation of the parameters for g(x) will ideally involve a different MST for each of P(x/w1) and P(x/w2). Of course one only has complete information about the distribution of index terms in the relevant/non-relevant sets in an experimental situation. The calculation of g(x) using complete information may be of interest when deriving upper bounds for retrieval effectiveness under the model as for example was done for the independent case in Robertson and Sparck Jones[1]. In an operational situation where no relevant documents are known in advance, the technique of relevance feedback would have to be used to estimate the parameters repeatedly so that the performance may converge to the upper bound. That in theory the convergence will take place is guaranteed by the convergence theorem for the linear case at least as discussed on p. 106 in Chapter 5. The limitations mentioned there also apply here.  There is a choice of how one would implement the model for g(x) depending on whether one is interested in setting the cut-off a prior or a posteriori. In the former case one is faced with trying to build an MST for the index terms occurring in the relevant documents and one for the ones occurring in the non-relevant documents. Since one must do this from sample information the dependence trees could be far from optimal. One heuristic way of meeting the situation is to construct a dependence tree for the whole collection. The structure of this tree is then assumed to be the structure for the two dependence trees based on the relevant and non-relevant documents. P(x/w1) and P(x/w2) are then calculated by computing the conditional probabilities for the connected nodes dictated by the one dependence tree. How good this particular approximation is can only be demonstrated by experimental test.  If one assumes that the cut-off is set a posteriori then we can rank the documents according to P(w1/x) and leave the user to decide when he has seen enough. In other words we use the form  to calculate (estimate) the probability of relevance for each document x. Now here we only need to estimate for P(x/w1), since top calculate P(x) we simply use the spanning tree for the entire collection without considering relevance information at all. This second approach has some advantages (ignoring the absence of an explicit mention of cut-off), one being that if dependence is assumed on the entire collection then this is consistent with assuming independence on the relevant documents, which from a computational point of view would simplify things enormously. Although independence on w1 is unlikely it nevertheless may be forced upon us by the fact that we can never get enough information by sampling or trial retrieval to measure the extent of the dependence.
irv-0072	Some of the arguments advanced in the previous section can be construed as implying that the only dependence tree we have enough information to construct is the one on the entire document collection. Let us pursue this line of argument a little further. To construct a dependence tree for index terms without using relevance information is similar to constructing an index term classification. In Chapter 3 I pointed out the relationship between the MST and single-link, which shows that the one is not very different from the other. This leads directly to the idea that perhaps the dependence tree could be used in the same way as one would a term clustering.  The basic idea underlying term clustering was explained in Chapter 2. This could be summarised by saying that based on term clustering various strategies for term deletion and addition can be implemented. Forgetting about 'deletion' for the moment, it is clear how the dependence tree might be used to add in terms to, or expand, the query. The reason for doing this was neatly put by Maron in 1964: 'How can one increase the probability of retrieving a class of documents that includes relevant material not otherwise selected? One obvious method suggests itself: namely, to enlarge the initial request by using additional index terms which have a similar or related meaning to those of the given request'[4]. The assumption here is that 'related meaning' can be discovered through statistical association. Therefore I suggest that given a query, which is an incomplete specification of the information need and hence the relevant documents, we use the document collection (through the dependence tree) to tell us what other terms not already in the query may be useful in retrieving relevant documents. Thus I am claiming that index terms directly related (i.e. connected) to a query term in the dependence tree are likely to be useful in retrieval. In a sense I have reformulated the hypothesis on which term clustering is based (see p.31). Let me state it formally now, and call it the Association Hypothesis:  If an index term is good at discriminating relevant from non-relevant documents then any closely associated index term is also likely to be good at this.  The way we interpret this hypothesis is that a term in the query used by a user is likely to be there because it is a good discriminator and hence we are interested in its close associates. The hypothesis does not specify the way in which association between index terms is to be measured although in this chapter I have made a case for using EMIM. Neither does it specify a measure of 'discrimination', this I consider in the next section. The Association Hypothesis in some ways is a dual to the Cluster Hypothesis (p. 45) and can be tested in the same way.
irv-0073	Discrimination power of an index term  On p. 120 I defined  and in fact there made the comment that it was a measure of the power of term i to discriminate between relevant and non-relevant documents. The weights in the weighting function derived from the independence assumption A1 are exactly these Ki's. Now if we forget for the moment that these weights are a consequence of a particular model and instead consider the notion of discrimination power of an index term on its own merits. Certainly this is not a novel thing to do, Salton in some of his work has sought effective ways of measuring the 'discrimination value' of index terms[24]. It seems reasonable to attach to any index term that enters into the retrieval process a weight related to its discrimination power. Ki as a measure of this power is slightly awkward in that it becomes undefined when the argument of the log function becomes zero. We therefore seek a more 'robust' function for measuring discrimination power. The function I am about to recommend for this purpose is indeed more robust, has an interesting interpretation, and enables me to derive a general result of considerable interest in the next section. However, it must be emphasised that it is only an example of a function which enables some sense to be make of the notion 'discrimination power' in this and the next section. It should therefore not be considered unique although it is my opinion that any alternative way of measuring discrimination power in this context would come very close to the measure I suggest here.  Instead of Ki I suggest using the information radius, defined in Chapter 3 on p. 42, as a measure of the discrimination power of an index term. It is a close cousin of the expected mutual information measure a relationship that will come in useful later on. Using u and v as positive weights such as u + v = 1 and the usual notation for the probability functions we can write the information radius as follows:  The interesting interpretation of the information radius that I referred to above is illustrated most easily in terms of continuous probability functions. Instead of using the densities p(./w1) and p(./w2) I shall use the corresponding probability measure u1 and u2. First we define the average of two directed divergencies[25],  R (u1, u2/v) = uI (u1/v) +vI (u2/v)  where I(ui/v) measures the expectation on ui of the information in favour of rejecting v for ui given by making an observation; it may be regarded as the information gained from being told to reject v in favour of ui. Now the information radius is the minimum  thereby removing the arbitrary v. In fact it turns out that the minimum is achieved when  v = u u1 + v u2  that is, an average of the two distributions to be discriminated. If we now adopt u and v as the prior probabilities then v is in fact given by the density  p(x) = p(x/w1) P(w1) + p(x/w2) P(w2)  defined over the entire collection without regard to relevance. Now of this distribution we are reasonably sure, the distribution u1 and u2 we are only guessing at; therefore it is reasonable when measuring the difference between u1 and u2 that v should incorporate as much of the information that is available. The information radius does just this.  There is one technical problem associated with the use of the information radius, or any other 'discrimination measure' based on all four cells of the contingency table, which is rather difficult to resolve. As a measure of discrimination power it does not distinguish between the different contributions made to the measure by the different cells. So, for example, an index term might be a good discriminator because it occurs frequently in the non-relevant documents and infrequently in the relevant documents. Therefore, to weight an index term proportional to the discrimination measure whenever it is present in a document is exactly the wrong thing to do. It follows that the data contained in the contingency table must be used when deciding on a weighting scheme.
irv-0074	Discrimination gain hypothesis  In the derivation above I have made the assumption of independence or dependence in a straightforward way. I have assumed either independence on both w1 and w2, or dependence. But, as implied earlier, this is not the only way of making these assumptions. Robertson and Sparck Jones[1] make the point that assuming independence on the relevant and non-relevant documents can imply dependence on the total set of documents. To see this consider two index terms i and j, and  P(xi, xj) = P(xi, xj /w1)P(w1) + P(xi, xi /w2) P (w2)  P(xi) P( xj) = [P(xi /w1)P(w1) + P(xi, w2) P (w2)] [P(xj /w1) P(w1) + P(xj,w2) P (w2)]  If we assume conditional independence on both w1 and w2 then  P(xi, xj) = P(xi, /w1) P(xj, w1) P(w1) + P(xi /w2) P(xj/ w2) P (w2)  For unconditional independence as well, we must have  P(xi, xj) = P(xi) P(xj)  This will only happen when P(w1) = 0 or P(w2) = 0, or P(xi/ w1) = P(xi/w2), or P(xj/w1) = P(xj /w2), or in words, when at least one of the index terms is useless at discriminating relevant from non-relevant documents. In general therefore conditional independence will imply unconditional dependence. Now let us assume that the index terms are indeed conditionally independence then we get the following remarkable results.  Kendall and Stuart[26] define a partial correlation coefficient for any two distributions by  where [[rho]] (.,./W) and [[rho]] (.,.) are the conditional and ordinary correlation coefficients respectively. Now if X and Y are conditionally independent then  [[rho]] (X, Y/W) = 0  which implies using the expression for the partial correlation that  [[rho]] (X, Y) = [[rho]] (X, W) [[rho]] (Y, W)  Since  | [[rho]] (X, Y) | lt;= 1 , | [[rho]] (X, W) | lt;= 1 , | [[rho]] (Y, W) | lt;= 1  this in turn implies that under the hypothesis of conditional independence  | [[rho]] (X, Y) | lt; | [[rho]] (X, W) | or | [[rho]] (Y, W) | (**)  Hence if W is a random variable representing relevance then the correlation between it and either index term is greater than the correlation between the index terms.  Qualitatively I shall try and generalise this to functions other than correlation coefficients, Linfott[27] defines a type of informational correlation measure by  rij = (1 - exp (-2I (xi, xj) ) )[1/2 ]0 lt;= rij lt;= 1  or  where I (xi, xj) is the now familiar expected mutual information measure. But rij reduces to the standard correlation coefficient [[rho]] (.,.) if (xi, xj) is normally distributed. So it is not unreasonable to assume that for non-normal distributions rij will behave approximately like [[rho]] (.,.) and will in fact satisfy (**) as well. But rij is strictly monotone with respect to I (x,i, xj) so it too will satisfy (**). Therefore we can now say that under conditional independence the information contained in one index term about another is less than the information contained in either term about the conditioning variable W. In symbols we have  I (xi, xj) lt; I (xi, W) or I (xj, W),  where I (., W) is the information radius with its weights interpreted as prior probabilities. Remember that I (.,W) was suggested as the measure of discrimination power. I think this result deserves to be stated formally as an hypothesis when W is interpreted as relevance.  Discrimination Gain Hypothesis: Under the hypothesis of conditional independence the statistical information contained in one index term about another is less than the information contained in either index term about relevance.  I must emphasise that the above argument leading to the hypothesis is not a proof. The argument is only a qualitative one although I believe it could be tightened up. Despite this it provides (together with the hypothesis) some justification and theoretical basis for the use of the MST based on I (xi, xj) to improve retrieval. The discrimination hypothesis is a way of firming up the Association Hypothesis under conditional independence.  One consequence of the discrimination hypothesis is that it provides a rationale for ranking the index terms connected to a query term in the dependence tree in order of I(term, query term) values to reflect the order of discrimination power values. The basis for this is that the more strongly connected an index term is to the query term (measured by EMIM) the more discriminatory it is likely to be. To see what is involved more clearly I have shown an example set-up in Figure 6.2. Let us suppose that x1 is the variable corresponding to the query term and that I (x1, x2) lt; I (x1, x3) lt; I (x1, x4) lt; I (x1, x5) then our hypothesis says that without knowing in advance how good a discriminator each of the index terms 2,3,4,5 is, it is reasonable to assume that I (x2, W) lt; I (x3, W) lt; I (x4, W) lt;I (x5, W). Clearly we cannot guarantee that the index terms will satisfy the last ordering but it is the best we can do given our ignorance.
irv-0075	Bibliographic remarks  The basis background reading for this chapter is contained in but a few papers. One approach to probabilistic weighting based on relevance data derives from the work of Yu and his collaborators[28,29]. The other is contained in the already frequently cited paper of Robertson and Sparck Jones[1]. Unfortunately, both these approaches rely heavily on the assumption of stochastic independence. My own paper[2] and the one of Bookstein and Kraft[3] are the only ones I know of, which try and construct a model without this assumption. Perhaps an earlier paper by Negoita should be mentioned here which discusses an attempt to use non-linear decision functions in IR[30]. Robertson's recent progress in documentation on models gives a useful summary of some of the more recent work[31].  According to Doyle[32] (p.267), Maron and Kuhns[19] were the first to describe in the open literature the use of association (statistical co-occurrence) of index terms as a means of enlarging and sharpening the search. However, Doyle himself was already working on similar ideas in the late fifties[33] and produced a number of papers on 'associations' in the early sixties[34,35]. Stiles in 1961[36], already apparently aware of Maron and Kuhns work, gave an explicit procedure for using terms co-occurring significantly with search terms, and not unlike the method based on the dependence tree described in this chapter. He also used the [[chi]][2] to measure association between index terms which is mathematically very similar to using the expected mutual information measure, although the latter is to be preferred when measuring dependence (see Goodman and Kruskal for a discussion on this point[37]). Stiles was very clear about the usefulness of using associations between index terms, he saw that through them one was 'able to locate documents relevant to a request even though the document had not been indexed by the term used in the request'[36].  The model in this chapter also connects with two other ideas in earlier research. One is the idea of inverse document frequency weighting already discussed in Chapter 2. The other is the idea of term clustering. Taking the weighting idea first, this in fact goes back to the early paper by Edmundson and Wyllys[38], we can write  or in words, for any document the probability of relevance is inversely proportional the probability with which it will occur on a random basis. If the P(document) is assumed to be the product of the probabilities of the individual index terms being either present or absent in the document then after taking logs we have the inverse document frequency weighting principle. It assumes that the likelihood P(document/relevance) is constant for all documents. Why it is exactly that this principle works so well is not yet clear (but see Yu and Salton's recent theoretical paper[39]).  The connection with term clustering was already made earlier on in the chapter. The spanning tree can be looked upon as a classification of the index terms. One of the important consequences of the model described in this chapter is that it lays down precisely how the tree should be used in retrieval. Earlier work in this area was rather ad hoc and did not lead to conclusive results[40].  It should be clear now that the quantitative model embodies within one theory such diverse topics as term clustering, early association analysis, document frequency weighting, and relevance weighting.
irv-0077	Much effort and research has gone into solving the problem of evaluation of information retrieval systems. However, it is probably fair to say that most people active in the field of information storage and retrieval still feel that the problem is far from solved. One may get an idea of the extent of the effort by looking at the numerous survey articles that have been published on the topic (see the regular chapter in the Annual Review on evaluation). Nevertheless, new approaches to evaluation are constantly being published (e.g. Cooper[1]; Jardin and van Rijsbergen[2]; Heine[3]).  In a book of this nature it will be impossible to cover all work to date about evaluation. Instead I shall attempt to explicate the conventional, most commonly used method of evaluation, followed by a survey of the more promising attempts to improve on the older methods of evaluation.  To put the problem of evaluation in perspective let me pose three questions: (1) Why evaluate? (2) What to evaluate? (3) How to evaluate? The answers to these questions pretty well cover the whole field of evaluation. There is much controversy about each and although I do not wish to add to the controversy I shall attempt an answer to each one in turn.  The answer to the first question is mainly a social and economic one. The social part is fairly intangible, but mainly relates to the desire to put a measure on the benefits (or disadvantages) to be got from information retrieval systems. I use 'benefit' here in a much wider sense than just the benefit accruing due to acquisition of relevant documents. For example, what benefit will users obtain (or what harm will be done) by replacing the traditional sources of information by a fully automatic and interactive retrieval system? Studies to gauge this are going on but results are hard to interpret. For some kinds of retrieval systems the benefit may be more easily measured than for others (compare statute or case law retrieval with document retrieval). The economic answer amounts to a statement of how much it is going to cost you to use one of these systems, and coupled with this is the question 'is it worth it?'. Even a simple statement of cost is difficult to make. The computer costs may be easy to estimate, but the costs in terms of personal effort are much harder to ascertain. Then whether it is worth it or not depends on the individual user.  It should be apparent now that in evaluating an information retrieval system we are mainly concerned with providing data so that users can make a decision as to (1) whether they want such a system (social question) and (2) whether it will be worth it. Furthermore, these methods of evaluation are used in a comparative way to measure whether certain changes will lead to an improvement in performance. In other words, when a claim is made for say a particular search strategy, the yardstick of evaluation can be applied to determine whether the claim is a valid one.  The second question (what to evaluate?) boils down to what can we measure that will reflect the ability of the system to satisfy the user. Since this book is mainly concerned with automatic document retrieval systems I shall answer it in this context. In fact, as early as 1966, Cleverdon gave an answer to this. He listed six main measurable quantities:  (1) The coverage of the collection, that is, the extent to which the system includes relevant matter;  (2) the time lag, that is, the average interval between the time the search request is made and the time an answer is given;  (3) the form of presentation of the output;  (4) the effort involved on the part of the user in obtaining answers to his search requests;  (5) the recall of the system, that is, the proportion of relevant material actually retrieved in answer to a search request;  (6) the precision of the system, that is, the proportion of retrieved material that is actually relevant.  It is claimed that (1)-(4) are readily assessed. It is recall and precision which attempt to measure what is now known as the effectiveness of the retrieval system. In other words it is a measure of the ability of the system to retrieve relevant documents while at the same time holding back non-relevant one. It is assumed that the more effective the system the more it will satisfy the user. It is also assumed that precision and recall are sufficient for the measurement of effectiveness.  There has been much debate in the past as to whether precision and recall are in fact the appropriate quantities to use as measures of effectiveness. A popular alternative has been recall and fall-out (the proportion of non-relevant documents retrieved). However, all the alternatives still require the determination of relevance in some way. The relationship between the various measures and their dependence on relevance will be made more explicit later. Later in the chapter a theory of evaluation is presented based on precision and recall. The advantages of basing it on precision and recall are that they are:  (1) the most commonly used pair;  (2) fairly well understood quantities.  The final question (How to evaluate?) has a large technical answer. In fact, most of the remainder of this chapter may be said to be concerned with this. It is interesting to note that the technique of measuring retrieval effectiveness has been largely influenced by the particular retrieval strategy adopted and the form of its output. For example, when the output is a ranking of documents an obvious parameter such as rank position is immediately available for control. Using the rank position as cut-off, a series of precision recall values could then be calculated, one part for each cut-off value. The results could then be summarised in the form of a set of points joined by a smooth curve. The path along the curve would then have the immediate interpretation of varying effectiveness with the cut-off value. Unfortunately, the kind of question this form of evaluation does not answer is, for example, how many queries did better than average and how many did worse? Nevertheless, we shall need to spend more time explaining this approach to the measurement of effectiveness since it is the most common approach and needs to be understood.  Before proceeding to the technical details relating to the measurement of effectiveness it is as well to examine more closely the concept of relevance which underlies it.
irv-0078	Relevance  Relevance is a subjective notion. Different users may differ about the relevance or non-relevance of particular documents to given questions. However, the difference is not large enough to invalidate experiments which have been made with document collections for which test questions with corresponding relevance assessments are available. These questions are usually elicited from bona fide users, that is, users in a particular discipline who have an information need. The relevance assessments are made by a panel of experts in that discipline. So we now have the situation where a number of questions exist for which the 'correct' responses are known. It is a general assumption in the field of IR that should a retrieval strategy fare well under a large number of experimental conditions then it is likely to perform well in an operational situation where relevance is not known in advance.  There is a concept of relevance which can be said to be objective and which deserves mention as an interesting source of speculation. This notion of relevance has been explicated by Cooper[4]. It is properly termed 'logical relevance'. Its usefulness in present day retrieval systems is limited. However, it can be shown to be of some importance when it is related to the development of question-answering systems, such as the one recently designed by T. Winograd at Massachusetts Institute of Technology.  Logical relevance is most easily explicated if the questions are restricted to the yes-no type. This restriction may be lifted - for details see Cooper's original paper. Relevance is defined in terms of logical consequence. To make this possible a question is represented by a set of sentences. In the case of a yes-no question it is represented by two formal statements of the form 'p' and 'not-p'. For example, if the query were 'Is hydrogen a halogen element?', the part of statements would be the formal language equivalent of 'Hydrogen is a halogen element' and 'Hydrogen is not a halogen element'. More complicated questions of the 'which' and 'whether' type can be transformed in this manner, for details the reader is referred to Belnap[5,6]. If the two statements representing the question are termed component statements then the subset of the set of stored sentences is a premiss set for a component statement if an only if the component statement is a logical consequence of that subset. (Note we are now temporarily talking about stored sentences rather than stored documents.) A minimal premiss set for a component statement is one that is as small as possible in the sense that if any of its members were deleted, the component statement would no longer be a logical consequence of the resulting set. Logical relevance is now defined as a two-place relation between stored sentences and information need representations (that is, the question represented as component statements). The final definition is as follows:  A stored sentence is logically relevant to (a representation of) an information need if and only if it is a member of some minimal premiss set of stored sentences for some component statement of that need.  Although logical relevance is initially only defined between sentences it can easily be extended to apply to stored documents. A document is relevant to an information need if and only if it contains at least one sentence which is relevant to that need.  Earlier on I stated that this notion of relevance was only of limited use at the moment. The main reason for this is that the kind of system which would be required to implement a retrieval strategy which would retrieve only the logically relevant documents has not been built yet. However, the components of such a system do exist to a certain extent. Firstly, theorem provers, which can prove theorems within formal languages such as the first-order predicate calculus, have reached quite a level of sophistication now (see, for example, Chang and Lee[7]). Secondly, Winograd's system is capable of answering questions about its simple universe blocks in natural language. In principle this system could be extended to construct a universe of documents, that is, the content of a document is analysed and incorporated into the universe of currently 'understood' documents. It may be that the scale of a system of this kind will be too large for present day computers; only the future will tell.  Saracevic[8] has given a thorough review of the notion of relevance in information science. Robertson[9] has summarised some of the more recent work on probabilistic interpretations of relevance.
irv-0079	Precision and recall, and others  We now leave the speculations about relevance and return to the promised detailed discussion of the measurement of effectiveness. Relevance will once again be assumed to have its broader meaning of 'aboutness' and 'appropriateness', that is, a document is ultimately determined to be relevant or not by the user. Effectiveness is purely a measure of the ability of the system to satisfy the user in terms of the relevance of documents retrieved. Initially, I shall concentrate on measuring effectiveness by precision and recall; a similar analysis could be given for any pair of equivalent measures.  It is helpful at this point to introduce the famous 'contingency' table which is not really a contingency table at all.  A large number of measures of effectiveness can be derived from this table. To list but a few:  ( | . | is the counting measure)  There is a functional relationship between all three involving a parameter called generality (G) which is a measure of the density of relevant documents in the collection. The relationship is:  For each request submitted to a retrieval system one of these tables can be constructed. Based on each one of these tables a precision-recall value can be calculated. If the output of the retrieval strategy depends on a parameter, such as rank position or co-ordination level (the number of terms a query has in common with a document), it can be varied to give a different table for each value of the parameter and hence a different precision-recall value. If [[lambda]] is the parameter, then P[[lambda]] denotes precision, R[[lambda]] recall, and a precision-recall value will be denoted by the ordered pair (R[[lambda]] , P[[lambda]] ). The set of ordered pairs makes up the precision-recall graph. Geometrically when the points have been joined up in some way they make up the precision-recall curve. The performance of each request is usually given by a precision-recall curve (see Figure 7.1). To measure the overall performance of a system, the set of surves, one for each request, is combined in some way to produce an average curve.  * For a derivation of this relation from Bayes' Theorem, the reader should consult the author's recent paper on retrieval effectiveness[10].
irv-0080	Averaging techniques  The method of pooling or averaging of the individual P-R curves seems to have depended largely on the retrieval strategy employed. When retrieval is done by co-ordination level, micro-evaluation is adopted. If S is the set of requests then:  where As is the set of documents relevant to request s. If [[lambda]] is the co-ordination level, then:  where B[[lambda]]s is the set of documents retrieved at or above the co-ordination level [[lambda]]. The points (R[[lambda]] , P[[lambda]] ) are now calculated as follows:  Figure 7.2 shows graphically what happens when two individual P-R curves are combined in this way. The raw data are given in Table 7.1.  An alternative approach to averaging is macro-evaluation which can be independent of any parameter such as co-ordination level. The average curve is obtained by specifying a set of standard recall values for which average precision values are calculated by averaging over all queries the individual precision values corresponding to the standard recall values. Often no unique precision value corresponds exactly so it becomes necessary to interpolate.  Table 7.1. THE RAW DATA FOR THE MICRO-EVALUATION IN FIGURE 7.2
irv-0081	Interpolation  Many interpolation techniques have been suggested in the literature. See, for example, Keen[11].  Figure 7.3 shows a typical P-R graph for a single query. The points A, B, C and D, I shall call the observed points, since these are the only points observed directly during an experiment the others may be inferred from these. Thus given that A = (R1, P1) has been observed, then the next point B is the one corresponding to an increase in recall, which follows from a unit increase in the number of relevant documents retrieved. Between any two observed points the recall remains constant, since no more relevant documents are retrieved.  It is an experimental fact that average precision-recall graphs are monotonically decreasing. Consistent with this, a linear interpolation estimates the best possible performance between any two adjacent observed points. To avoid inflating the experimental results it is probably better to perform a more conservative interpolation as follows:  Let (R[[lambda]] , P[[lambda]] ) be the set of precision-recall values obtained by varying some parameter [[lambda]]. To obtain the set of observed points we specify a subset of the parameters [[lambda]]. Thus (R[[theta]] , P[[theta]] ) is an observed point if [[theta]] corresponds to a value of [[lambda]] at which an increase in recall is produced. We now have:  Gs = (R[[theta]]s, P[[theta]]s )  the set of observed points for a request. To interpolate between any two points we define:  Ps(R) = {sup P : R' gt;= R s.t. (R', P) [[propersubset]] Gs}  where R is a standard recall value. From this we obtain the average precision value at the standard recall value R by:  The set of observed points is such that the interpolated function is monotonically decreasing. Figure 7.3 shows the effect of the interpolation procedure, essentially it turns the P-R curve into a step-function with the jumps at the observed points. A necessary consequence of its monotonicity is that the average P-R curve will also be monotonically decreasing. It is possible to define the set of observed points in such a way that the interpolate function is not monotonically decreasing. In practice, even for this case, we have that the average precision-recall curve is monotonically decreasing.  In Figure 7.4 we illustrate the interpolation and averaging process.
irv-0082	Composite measures  Dissatisfaction in the past with methods of measuring effectiveness by a pair of numbers (e.g. precision and recall) which may co-vary in a loosely specified way has led to attempts to invest composite measures. These are still based on the 'contingency' table but combine parts of it into a single number measure. Unfortunately many of these measures are rather ad hoc and cannot be justified in any rational way. The simplest example of this kind of measure is the sum of precision and recall  S = P + R  This is simply related to a measure suggested by Borko  BK = P + R - 1  More complicated ones are  Vickery's measure V can be shown to be a special case of a general measure which will be derived below.  Some single-number measures have derivations which can be justified in a rational manner. Some of them will be given individual attention later on. Suffice it here to point out that it is the model underlying the derivation of these measures that is important.
irv-0083	The Swets model*  As early as 1963 Swets[12] expressed dissatisfaction with existing methods of measuring retrieval effectiveness. His background in signal detection led him to formulate an evaluation model based on statistical decision theory. In 1967 he evaluated some fifty different retrieval methods from the point of view of his model[13]. The results of his evaluation were encouraging but not conclusive. Subsequently, Brookes[14] suggested some reasonable modifications to Swets' measure of effectiveness, and Robertson[15] showed that the suggested modifications were in fact simply related to an alternative measure already suggested by Swets. * Bookstein[16] has recently re-examined this model showing how Swets implicitly relied on an 'equal variance' assumption.  It is interesting that although the Swets model is theoretically attractive and links IR measurements to a ready made and well-developed statistical theory, it has not found general acceptance amongst workers in the field.  Before proceeding to an explanation of the Swets model, it is as well to quote in full the conditions that the desired measure of effectiveness is designed to meet. At the beginning of his 1967 report Swets states:  'A desirable measure of retrieval performance would have the following properties: First, it would express solely the ability of a retrieval system to distinguish between wanted and unwanted items - that is, it would be a measure of "effectiveness" only, leaving for separate consideration factors related to cost or "efficiency". Second, the desired measure would not be confounded by the relative willingness of the system to emit items - it would express discrimination power independent of any "acceptance criterion" employed, whether the criterion is characteristic of the system or adjusted by the user. Third, the measure would be a single number - in preference, for example, to a pair of numbers which may co-vary in a loosely specified way, or a curve representing a table of several pairs of numbers - so that it could be transmitted simply and immediately apprehended. Fourth, and finally, the measure would allow complete ordering of different performances, and assess the performance of any one system in absolute terms - that is, the metric would be a scale with a unit, a true zero, and a maximum value. Given a measure with these properties, we could be confident of having a pure and valid index of how well a retrieval system (or method) were performing the function it was primarily designed to accomplish, and we could reasonably ask questions of the form "Shall we pay X dollars for Y units of effectiveness?".'  He then goes on to claim that 'The measure I proposed [in 1963], one drawn from statistical decision theory, has the potential [my italics] to satisfy all four desiderata'. So, what is this measure?  To arrive at the measure, we must first discuss the underlying model. Swets defines the basic variables Precision, Recall, and Fallout in probabilistic terms.  Recall = an estimate of the conditional probability that an item will be  retrieved given that it is relevant [we denote this P(B/A)].  Precision = an estimate of the conditional probability that an item will be  relevant given that it is retrieved [i.e. P(A/B)].  Fallout = an estimate of the conditional probability that an item will be  retrieved given that it is non-relevant [i.e. P(B/`A].  He accepts the validity of measuring the effectiveness of retrieval by a curve either precision-recall or recall-fallout generated by the variation of some control variable [[lambda]] (e.g. co-ordination level). He seeks to characterise each curve by a single number. He rejects precision-recall in favour of recall-fallout since he is unable to do it for the former but achieves limited success with the latter.  In the simplest case we assume that the variable [[lambda]] is distributed normally on the set of relevant and non-relevant documents. The two distributions are given respectively by N(u1, [[sigma]]1) and N(u2, [[sigma]]2). The density functions are given by [[florin]]1 ([[lambda]]|A) and [[florin]]2 ([[lambda]]|`A). We may picture the distribution as shown in Figure 7.5.  The usual set-up in IR is now to define a decision rule in terms of [[lambda]], to determine which documents are retrieved (the acceptance criterion). In other words we specify [[lambda]]c such that a document for which the associated [[lambda]] exceeds [[lambda]]c is retrieved. We now measure the effectiveness of a retrieval strategy by measuring some appropriate variables (such as R and P, or R and F) at various values of [[lambda]]c. It turns out that the differently shaded areas under the curves in Figure 7.5 correspond to recall and fallout. Moreover, we find the operating characteristic (OC) traced out by the point (F[[lambda]], R[[lambda]]) due to variation in [[lambda]]c is a smooth curve fully determined by two points, in the general case of unequal variance, and by one point in the special case of equal variance. To see this one only needs to plot the (F[[lambda]], R[[lambda]]) points on double probability paper (scaled linearly for the normal deviate) to find that the points lie on a straight line. A slope of 45deg. corresponds to equal variance, and otherwise the slope is given by the ratio of [[sigma]]1 and [[sigma]]2. Figure 7.6 shows the two cases. Swets now suggests, regardless of  slope, that the distance 0I (actually [[radical]]20I) be used as a measure of effectiveness. This amounts to using:  which is simply the difference between the means of the distribution normalised by the average standard deviation. Unfortunately this measure does rather hide the fact that a high S1 value may be due to a steep slope. The slope, and S1, would have to be given which fails to meet Swets' second condition. We, also, still have the problem of deciding between two strategies whose OC's intersect and hence have different S1 values and slopes.  Brookes[14] in an attempt to correct for the S1 bias towards systems with slopes much greater than unity suggested a modification to S1. Mathematically Brookes's measure is  Brookes also gives statistical reasons for preferring S2 to S1 which need not concern us here. Geometrically S2 is the perpendicular distance from 0 to OC (see Figure 7.6).  Interestingly enough, Robertson[15] showed that S2 is simply related to the area under the Recall-Fallout curve. In fact, the area is a strictly increasing function of S2. It also has the appealing interpretation that it is equal to the percentage of correct choices a strategy will make when attempting to select from a pair of items, one drawn at random from the non-relevant set and one drawn from the relevant set. It does seem therefore that S2 goes a long way to meeting the requirements laid down by Swets. However, the appropriateness of the model is questionable on a number of grounds. Firstly, the linearity of the OC curve does not necessarily imply that [[lambda]] is normally distributed in both populations, although they will be 'similarly' distributed. Secondly, [[lambda]] is assumed to be continuous which certainly is not the case for the data checked out both by Swets and Brookes, in which the co-ordination level used assumed only integer values. Thirdly, there is no evidence to suggest that in the case of more sophisticated matching functions, as used by the SMART system, that the distributions will be similarly distributed let alone normally. Finally the choice of fallout rather than precision as second variable is hard to justify. The reason is that the proportion of non-relevant retrieved for large systems is going to behave much like the ratio of 'non-relevant' retrieved to 'total documents in system'. For comparative purposes 'total document' may be ignored leaving us with 'non-relevant retrieved' which is complementary to 'relevant retrieved'. But now we may as well use precision instead of fallout.
irv-0084	The Robertson model - the logistic transformation  Robertson in collaboration with Teather has developed a model for estimating the probabilities corresponding to recall and fallout[17]. The estimation procedure is unusual in that in making an estimate of these probabilities for a single query it takes account of two things: one, the amount of data used to arrive at the estimates, and two, the averages of the estimates over all queries. The effect of this is to 'pull' an estimate closer to the overall mean if it seems to be an outlyer whilst at the same time counterbalancing the 'pull' in proportion to the amount of data used to make the estimate in the first place. There is now some evidence to show that this pulling-in-to-the-mean is statistically a reasonable thing to do[18].  Using the logit transformation for probabilities, that is  the basic quantitative model for a single query j they propose is  logit [[theta]]j1 = [[alpha]]j + [[Delta]]j  logit [[theta]]j2 = [[alpha]]j - [[Delta]]j  Here [[theta]]j1 and [[theta]]j2 are probabilities corresponding to recall and fallout respectively as defined in the previous section. The parameters [[alpha]]j and [[Delta]]j are to be interpreted as follows:  [[alpha]]j measures the specificity of the query formulation; [[Delta]]j measures the separation  of relevant and non-relevant documents.  For a given query j if the query i has been formulated in a more specific way than j, one would expect the recall and fallout to decrease, i.e.  [[theta]]i1 lt; [[theta]]j1 and [[theta]]i2 lt; [[theta]]j2  Also, if for query i the system is better at separating the non-relevant from the relevant documents than it is for query j one would expect the recall to increase and the fallout to decrease, i.e.  [[theta]]i1 gt; [[theta]]j1 and [[theta]]i2 lt; [[theta]]j2  Given that logit is a monotonic transformation, these interpretations are consistent with the simple quantitative model defined above.  To arrive at an estimation procedure for [[alpha]]j and [[Delta]]j is a difficult technical problem and the interested reader should consult Robertson's thesis[19]. It requires certain assumptions to be made about [[alpha]]j and [[Delta]]j , the most important of which is that the {[[alpha]]j }and {[[Delta]]j }are independent and normally distributed. These assumptions are rather difficult to validate. The only evidence produced so far derives the distribution of {[[alpha]]j } for certain test data. Unfortunately, these estimates, although they are unimodally and symmetrically distributed themselves, can only be arrived at by using the normality assumption. In the case of [[Delta]]j it has been found that it is approximately constant across queries so that a common-[[Delta]] model is not unreasonable:  logit [[theta]]j1 = [[alpha]]j1 + [[Delta]]  logit [[theta]]j2 = [[alpha]]j2 - [[Delta]]  From them it would appear that [[Delta]] could be a candidate for a single number measure of effectiveness. However, Robertson has gone to some pains to warn against this. His main argument is that these parameters are related to the behavioural characteristics of an IR system so that if we were to adopt [[Delta]] as a measure of effectiveness we could be throwing away vital information needed to make an extrapolation to the performance of other systems.
irv-0085	The Cooper model - expected search length  In 1968, Cooper[20] stated: 'The primary function of a retrieval system is conceived to be that of saving its users to as great an extent as is possible, the labour of perusing and discarding irrelevant documents, in their search for relevant ones'. It is this 'saving' which is measured and is claimed to be the single index of merit for retrieval systems. In general the index is applicable to retrieval systems with ordered (or ranked) output. It roughly measures the search effort which one would expect to save by using the retrieval system as opposed to searching the collection at random. An attempt is made to take into account the varying difficulty of finding relevant documents for different queries. The index is calculated for a query of a precisely specified type. It is assumed that users are able to quantify their information need according to one of the following types:  (a) only one relevant document is wanted;  (b) some arbitrary number n is wanted;  (c) all relevant documents are wanted;  (4) a given proportion of the relevant documents is wanted, etc.  Thus, the index is a measure of performance for a query of given type. Here we shall restrict ourselves to Type 2 queries. For further details the reader is referred to Cooper[20].  The output of a search strategy is assumed to be a weak ordering of documents. I have defined this concept on page 118 in a different context. We start by first considering a special case, namely a simple ordering, which is a weak ordering such that for any two distinct elements e1 and e2 it is never the case that e1 R e2 and e2 R e1 (where R is the order relation). This simply means that all the documents in the output are ordered linearly with no two or more documents at the same level of the ordering. The search length is now defined as the number of non-relevant documents a user must scan before his information need (in terms of the type quantification above) is satisfied. For example, consider a ranking of 20 documents in which the relevant ones are distributed as in Figure 7.7. A Type 2 query with n = 2 would have search length 2, with n = 6 it would have search length 3.  Unfortunately the ranking generated by a matching function is rarely a simple ordering, but more commonly a weak ordering. This means that at any given level in the ranking, there is at l;east one document (probably more) which makes the search length inappropriate since the order of documents within a level is random. If the information need is met at a certain level in the ordering then depending on the arrangement of the relevant documents within that level we shall get different search lengths. Nevertheless we can use an analogous quantity which is the expected search length. For this we need to calculate the probability of each possible search length by juggling (mentally) the relevant and non-relevant documents in the level at which the user need is met.  For example, consider the weak ordering in Figure 7.8. If the query is of Type 2 with n = 6 then the need is met at level 3. The possible search lengths are 3, 4, 5 or 6  depending on how many non-relevant documents precede the sixth relevant document. We can ignore the possible arrangements within levels 1 and 2; their contributions are always the same. To compute the expected search length we need the probability of each possible search length. We get at this by considering first the number of different ways in which two relevant documents could be distributed among five, it is ([5]2) = 10. Of these 4 would result in a search length of 3, 3 in a search length of 4, 2 in a search length of 5 and 1 in a search length of 6. Their corresponding probabilities are therefore, 4/10, 3/10, 2/10 and 1/10. The expected search length is now:  (4/10) . 3 + (3/10) . 4 + (2/10) . 5 + (1/10) . 6 = 4  The above procedure leads immediately to a convenient 'intuitive' derivation of a formula for the expected search length. It seems plausible that the average results of many random searches through the final level (level at which need is met) will be the same as for a single search with the relevant documents spaced 'evenly' throughout that level. First we enumerate the variables:  (a) q is the query of given type;  (b) j is the total number of documents non-relevant to q in all levels preceding the final;  (c) r is the number of relevant documents in the final level;  (d) i is the number of non-relevant documents in the final level;  (e) s is the number of relevant documents required from the final level to satisfy the need according its type.  Now, to distribute the r relevant documents evenly among the non-relevant documents, we partition the non-relevant documents into r + 1 subsets each containing i /(r + 1) documents. The expected search length is now:  As a measure of effectiveness ESL is sufficient if the document collection and test queries are fixed. In that case the overall measure is the mean expected search length  where Q is the set of queries. This statistic is chosen in preference to any other for the property that it is minimised when the total expected search length  To extend the applicability of the measure to deal with varying test queries and document collections, we need to normalise the ESL in some way to counter the bias introduced because:  (1) queries are satisfied by different numbers of documents according to the type of the query and therefore can be expected to have widely differing search lengths;  (2) the density of relevant documents for a query in one document collection may be significantly different from the density in another.  The first item suggests that the ESL per desired relevant document is really what is wanted as an index of merit. The second suggests normalising the ESL by a factor proportional to the expected number of non-relevant documents collected for each relevant one. Luckily it turns out that the correction for variation in test queries and for variation in document collection can be made by comparing the ESL with the expected random search length (ERSL). This latter quantity can be arrived at by calculating the expected search length when the entire document collection is retrieved at one level. The final measure is therefore:  which has been called the expected search length reduction factor by Cooper. Roughly it measures improvement over random retrieval. The explicit form for ERSL is given by:  where  (1) R is the total number of documents in the collection relevant to q;  (2) I is the total number of documents in the collection non-relevant to q;  (3) S is the total desired number of documents relevant to q.  The explicit form for ESL was given before. Finally, the overall measure for a set of queries Q is defined, consistent with the mean ESL, to be  which is known as the mean expected search length reduction factor.  Within the framework as stated at the head of this section this final measure meets the bill admirably. However, its acceptability as a measure of effectiveness is still debatable (see, for example, Senko[21]). It totally ignores the recall aspect of retrieval, unless queries are evaluated which express the need for a certain proportion of the relevant documents in the system. It therefore seems to be a good substitute for precision, one which takes into account order of retrieval and user need.  For a further defence of its subjective nature see Cooper[1]. A spirited attack on Cooper's position can be found in Soergel[22].
irv-0086	The SMART measures  In 1966, Rocchio gave a derivation of two overall indices of merit based on recall and precision. They were proposed for the evaluation of retrieval systems which ranked documents, and were designed to be independent of cut-off.  The first of these indices is normalised recall. It roughly measures the effectiveness of the ranking in relation to the best possible and worst possible ranking. The situation is illustrated in Figure 7.9 for 25 documents where we plot on the y-axis and the ranks on the x-axis.  Normalised recall (Rnorm) is the area between the actual case and the worst as a proportion of the area between the best and the worst. If n is the number of relevant documents, and ri the rank at which the ith document is retrieved, then the area between the best and actual case can be shown to be (after a bit of algebra):  (see Salton[23], page 285).  A convenient explicit form of normalised recall is:  where N is the number of documents in the system and N - n the area between the best and the worst case (to see this substitute ri = N - i + 1 in the formula for Ab - Aa). The form ensures that Rnorm lies between 0 (for the worst case) and 1 (for the best case).  In an analogous manner normalised precision is worked out. In Figure 7.10 we once more have three curves showing (1) the best case, (2) the actual case, and (3) the worst case in terms of the precision values at different rank positions.  The calculation of the areas is a bit more messy but simple to do (see Salton[23], page 298). The area between the actual and best case is now given by:  The log function appears as a result of approximating [[Sigma]] 1/r by its continuous analogue [[integral]] 1/r dr, which is logr + constant.  The area between the worst and best case is obtained in the same way as before using the same substitution, and is:  The explicit form, with appropriate normalisation, for normalised precision is therefore:  Once again it varies between 0 (worst) and 1 (best).  A few comments about these measures are now in order. Firstly their behaviour is consistent in the sense that if one of them is 0 (or 1) then the other is 0 (or 1). In other words they both agree on the best and worst performance. Secondly, they differ in the weights assigned to arbitrary positions of the precision-recall curve, and these weights may differ considerably from those which the user feels are pertinent (Senko[21]). Or, as Salton[23] (page 289) puts it: 'the normalised precision measure assigns a much larger weight to the initial (low) document ranks than to the later ones, whereas the normalised recall measure assigns a uniform weight to all relevant documents'. Unfortunately, the weighting is arbitrary and given. Thirdly, it can be shown that normalised recall and precision have interpretations as approximations to the average recall and precision values for all possible cut-off levels. That is, if R (i) is the recall at rank position i, and P (i) the corresponding precision value, then:  Fourthly, whereas Cooper has gone to some trouble to take account of the random element introduced by ties in the matching function, it is largely ignored in the derivation of Pnorm and Rnorm.  One further comment of interest is that Robertson15 has shown that normalised recall has an interpretation as the area under the Recall-Fallout curve used by Swets.  Finally mention should be made of two similar but simpler measures used by the SMART system. They are:  and do not take into account the collection size N, n is here the number of relevant documents for the particular test query.
irv-0087	A normalised symmetric difference  Let us now return to basics and consider how it is that users could simply measure retrieval effectiveness. We are considering the common situation where a set of documents is retrieved in response to a query, the possible ordering of this set is ignored. Ideally the set should consist only of documents relevant to the request, that is giving 100 per cent precision and 100 per cent recall (and by implication 0 per cent fallout). In practice, however, this is rarely the case, and the retrieved set consists of both relevant and non-relevant documents. The situation may therefore be pictured as shown in Figure 7.11, where A is the set of relevant documents, B the set of retrieved documents, and A [[intersection]] B the set of retrieved documents which are relevant.  Now, an intuitive way of measuring the adequacy of the retrieved set is to measure the size of the shaded area. Or to put it differently, to measure to what extent the two sets do not match. The area is in fact the symmetric difference: A [[Delta]] B (or A [[union]] B - A [[intersection]] B). Since we are more interested in the proportion (rather than absolute number) of relevant and non-relevant documents retrieved, we need to normalise this measure. A simple normalisation gives:  which is a simple composite measure.  The preceding argument in itself is not sufficient to justify the use of this particular composite measure. However, I shall now introduce a framework within which a general measure may be derived which among others has E as one of its special cases.
irv-0088	Foundation*  Problems of measurement have arisen in physics, psychology, and more recently, the social sciences. Clarification of these problems has been sought with the help of the theory of measurement. I shall attempt to do the same for information retrieval. My purpose is to construct a framework, based on the mathematical theory of measurement within which measures of effectiveness for retrieval systems can be derived. The basic mathematical notions underlying the measurement ideas will be introduced, but for their deeper understanding the reader is referred to the excellent book by Krantz et al.[24]. It would be fair to say that the theory developed there is applied here. Also of interest are the books by Ellis[25] and Lieberman[26].  The problems of measurement in information retrieval differ from those encountered in the physical sciences in one important aspect. In the physical sciences there is usually an empirical ordering of the quantities we wish to measure. For example, we can establish empirically by means of a scale which masses are equal, and which are greater or less than others. Such a situation does not hold in information retrieval. In the case of the measurement of effectiveness by precision and recall, there is no absolute sense in which one can say that one particular pair of precision-recall values is better or worse than some other pair, or, for that matter, that they are comparable at all. However, to leave it at that is to admit defeat. There is  * The next three sections are substantially the same as those appearing in my paper: 'Foundations of evaluation', Journal of Documentation, 30, 365-373 (1974). They have been included with the kind permission of the Managing Editor of Aslib.  no reason why we cannot postulate a particular ordering, or, to put it more mildly, why we can not show that a certain model for the measurement of effectiveness has acceptable properties. The immediate consequence of proceeding in this fashion is that each property ascribed to the model may be challenged. The only defence one has against this is that:  (1) all properties ascribed are consistent;  (2) they bring out into the open all the assumptions made in measuring effectiveness;  (3) each property has an acceptable interpretation;  (4) the model leads to a plausible measure of effectiveness.  It is as well to point out here that it does not lead to a unique measure, but it does show that certain classes of measures can be regarded as being equivalent.
irv-0089	The model  We start by examining the structure which it is reasonable to assume for the measurement of effectiveness. Put in other words, we examine the conditions that the factors determining effectiveness can be expected to satisfy. We limit the discussion here to two factors, namely precision and recall, although this is no restriction, different factors could be analysed, and, as will be indicated later, more than two factors can simplify the analysis.  If R is the set of possible recall values and P is the set of possible precision values then we are interested in the set R x P with a relation on it. We shall refer to this as a relational structure and denote it lt;R x P, gt;= gt; where gt;= is the binary relation on R x P. (We shall use the same symbol for less than or equal to, the context will make clear what the domain is.) All we are saying here is that for any given point (R, P) we wish to be able to say whether it indicates more, less or equal effectiveness than that indicated by some other point. The kind of order relation is a weak order. To be more precise:  Definition 1. The relational structure lt;R x P, gt;= gt; is a weak order if and only if for e1, e2, e3 [[propersubset]] R x P the following axioms are satisfied:  (1) Connectedness: either e1 gt;= e2 or e2 gt;= e1  (2) Transitivity: if e1 gt;= e2 and e2 gt;= e3 then e1 gt;= e3  We insist that if two pairs can be ordered both ways then (R1, P1) ~ (R2, P2), i.e. equivalent not necessarily equal. The transitivity condition is obviously desirable.  We now turn to a second condition which is commonly called independence. This notion captures the idea that the two components contribute their effects independently to the effectiveness.  Definition 2. A relation gt;= on R x P is independent if and only if, for R1, R2 [[propersubset]] R, (R1, P) gt;= (R2, P ) for some P [[propersubset]] P implies (R1, P' ) gt;= (R2, P' ) for every P' [[propersubset]] P; and for P1, P2 [[propersubset]] P, (R, P1) gt;= (R, P 2) for some R [[propersubset]] R implies (R', P1) gt;= (R', P 2) for every R '[[propersubset]] R.  All we are saying here is, given that at a constant recall (precision) we find a difference in effectiveness for two values of precision (recall) then this difference cannot be removed or reversed by changing the constant value.  We now come to a condition which is not quite as obvious as the preceding ones. To make it more meaningful I shall need to use a diagram, Figure 7.12, which represents the ordering we have got so far with definitions 1 and 2. The lines l1 and l2 are lines of equal effectiveness that is any two points (R, P ), (R', P' ) [[propersubset]]li are such that (R, P) ~ (R ', P ') (where ~ indicates equal effectiveness). Now let us assume that we have the points on l1 and l2 a but wish to deduce the relative ordering in between these two lines. One may think of this as an interpolation procedure.  Definition 3 (Thomsen condition). For every R1, R2 , R3 [[propersubset]] R and P1, P2, P3 [[propersubset]] P, (R1, P3) ~ (R3, P 2) and (R3, P1) ~ (R2, P 3) imply that (R1, P1) ~ (R2, P 2).  Intuitively this can b e reasoned as follows. The intervals R1 R3 and P2 P 3 are equivalent since an increase in the R-factor by R1 R3 and an increase in the P-factor by P2 P3 starting from (R1 , P3) lead to the same effectiveness (points on l2). It therefore follows that a decrease in each factor starting from equal effectiveness, in this case the two points (R3, R1) and (R2 , P3) on l1, should lead to equal effectiveness.  The fourth condition is one concerned with the continuity of each component. It makes precise what intuitively we would expect when considering the existence of intermediate values.  Definition 4 (Restricted Solvability). A relation gt;= on R x P satisfies restricted solvability provided that:  (1) whenever R, `R, R [[propersubset]] R and P, P' [[propersubset]] P for which (`R, P') gt;= (R, P) gt;= (R, P') then there exists R [[propersubset]] R s.t. (R, P') ~ (R, P);  (2) a similar condition holds on the second component.  In other words we are ensuring that the equation (R', P') ~ (R, P) is soluble for R' provided that there exist `R, R such that (`R, P') gt;= (R, P') gt;= (R, P'). An assumption of continuity of the precision and recall factors would ensure this.  The fifth condition is not limiting in any way but needs to be stated. It requires, in a precise way, that each component is essential.  Definition 5. Component R is essential if and only if there exist R1, R2 [[propersubset]] R and P1 [[propersubset]] P such that it is not the case that (R1, P1) ~ (R2, P1). A similar definition holds for P.  Thus we require that variation in one while leaving the other constant gives a variation in effectiveness.  Finally we need a technical condition which will not be explained here, that is the Archimedean property for each component. It merely ensures that the intervals on a component are comparable. For details the reader is referred to Krantz et al.  We now have six conditions on the relational structure lt;R x P, gt;= gt; which in the theory of measurement are necessary and sufficient conditions* for it to be an additive conjoint structure. This is enough for us to state the main representation theorem. It is a theorem asserting that if a given relational structure satisfies certain conditions (axioms), then a homomorphism into the real numbers is often referred to as a scale. Measurement may therefore be regarded as the construction of homomorphisms for empirical relational structures of interest into numerical relational structures that are useful.  In our case we can therefore expect to find real-valued functions [[Phi]]1 on R and [[Phi]]2 on P and a function F from Re x Re into Re, 1:1 in each variable, such that, for all R, R' [[propersubset]] R and P, P' [[propersubset]] P we have:  (R, P) gt;= (R', P') lt;=gt; F [[[Phi]]1 (R ), [[Phi]]2 (P )] gt;= F [[[Phi]]1 (R' ), [[Phi]]2 (P' )]  (Note that although the same symbol gt;= is used, the first is a binary relation on R x P, the second is the usual one on Re, the set of reals.)  In other words there are numerical scales [[Phi]]i on the two components and a rule F for combining them such that the resultant measure preserves the qualitative ordering of effectiveness. When such a representation exists we say that the structure is decomposable. In this representation the components (R and P) contribute to the effectiveness measure independently. It is not true that all relational structures are decomposable. What is true, however, is that non-decomposable structures are extremely difficult to analyse.  A further simplification of the measurement function may be achieved by requiring a special kind of non-interaction of the components which has become known as additive independence. This requires that the equation for decomposable structures is reduced to:  (R, P) gt;= (R', P' ) lt;=gt; [[Phi]]1 (R ) + [[Phi]]2 (P ) gt;= [[Phi]]1 (R' ) + [[Phi]]2 (P' )  where F is simply the addition function. An example of a non-decomposable structure is given by:  (R, P) gt;= (R', P') lt;=gt; [[Phi]]1 (R ) + [[Phi]]2 (P ) + [[Phi]]1 (R ) [[Phi]]2 (P ) gt;= [[Phi]]1 (R' ) + [[Phi]]2 (P' ) +  + [[Phi]]1 (R' )[[Phi]]2 (P' )  * It can be shown that (starting at the other end) given an additively independent representation the properties defined in 1 and 3, and the Archimedean property are necessary. The structural conditions 4 and 5 are sufficient.  Here the term [[Phi]]1 [[Phi]]2 is referred to as the interaction term, its absence accounts for the non-interaction in the previous condition.  We are now in a position to state the main representation theorem.
irv-0090	Theorem  Suppose lt;R x P, gt;= gt; is an additive conjoint structure, then there exist functions, [[Phi]]1 from R, and [[Phi]]2 from P into the real numbers such that, for all R, R' [[propersubset]] R and P, P' [[propersubset]] P:  (R, P) gt;= (R', P' ) lt;=gt; [[Phi]]1 (R ) + [[Phi]]2 (P ) gt;= [[Phi]]1 (R' ) + [[Phi]]2 (P' )  If [[Phi]]i['] are two other functions with the same property, then there exist constants [[Theta]] gt; 0, [[gamma]]1, and [[gamma]]2 such that  [[Phi]]1['] = [[Theta]][[Phi]]1 + [[gamma]]1 [[Phi]]2['] = [[Theta]][[Phi]]2 + [[gamma]]2  The proof of this theorem may be found in Krantz et al.[15].  Let us stop and take stock of this situation. So far we have discussed the properties of an additive conjoint structure and justified its use for the measurement of effectiveness based on precision and recall. We have also shown that an additively independent representation (unique up to a linear transformation) exists for this kind of relational structure. The explicit form of [[Phi]]i has been left unspecified. To determine the form of [[Phi]]i we need to introduce some extrinsic considerations. Although the representation F = [[Phi]]1 + [[Phi]]2 , this is not the most convenient form for expressing the further conditions we require of F, nor for its interpretation. So, in spite of the fact that we are seeking an additively independent representation we consider conditions on a general F. It will turn out that the F which is appropriate can be simply transformed into an additive representation. The transformation is f (F) = - (F - 1)[-1] which is strictly monotonically increasing in the range 0 lt;= F lt;= 1, which is the range of interest. In any case, when measuring retrieval effectiveness any strictly monotone transformation of the measure will do just as well.
irv-0091	Explicit measures of effectiveness  I shall now argue for a specific form of [[Phi]]i and F, based on a model for the user. In other words, the form [[Phi]]i and F are partly determined by the user. We start by showing how the ordering on R x P in fact induces an ordering of intervals on each factor. From Figure 7.13 we have that (R3, P1) gt;= (R1, P2), (R3, P1) gt;= (R1, P1) and (R1, P2) gt;= (R1, P1). Therefore the increment (interval) R1R3 is preferred to the increment P1P2. But (R2, P2) gt;= (R4, P1), which gives P1 P2 is preferred to R2 R4. Hence R1 R3 gt;=1 R2, R4 where gt;=1 is the induced order relation on R. We now have a method of comparing each interval on R with a fixed interval on P.  Since we have assumed that effectiveness is determined by precision and recall we have committed ourselves to the importance of proportions of documents rather than absolute numbers. Consistent with this is the assumption of decreasing marginal effectiveness. Let me illustrate this with an example. Suppose the user is willing to sacrifice one unit of precision for an increase of one unit of recall, but will not sacrifice another unit of precision for a further unit increase in recall, i.e.  (R + 1, P - 1) gt; (R, P)  but  (R + 1, P) gt; (R + 2, P - 1)  We conclude that the interval between R + 1 and R exceeds the interval between P and P - 1 whereas the interval between R + 1 and R + 2 is smaller. Hence the marginal effectiveness of recall is decreasing. (A similar argument can be given for precision.) The implication of this for the shape of the curves of equal effectiveness is that they are convex towards the origin.  Finally, we incorporate into our measurement procedure the fact that users may attach different relative importance to precision and recall. What we want is therefore a parameter (ß) to characterise the measurement function in such a way that we can say: it measures the effectiveness of retrieval with respect to a user who attaches ß times as much importance to recall as precision. The simplest way I know of quantifying this is to specify the P/R ratio at which the user is willing to trade an increment in precision for an equal loss in recall.  Definition 6. The relative importance a user attaches to precision and recall is the P/R ratio at which [[partialdiff]]E/ [[partialdiff]]R = [[partialdiff]]E/ [[partialdiff]]P, where E = E(P, R) is the measure of effectiveness based on precision and recall.  Can we find a function satisfying all these conditions? If so, can we also interpret it in an intuitively simple way? The answer to both these questions is yes. It involves:  The scale functions are therefore, [[Phi]]1(P) = [[alpha]](1/P), and [[Phi]]2(R) = (1 - [[alpha]]) (1/R). The 'combination' function F is now chosen to satisfy definition 6 without violating the additive independence. We get:  We now have the effectiveness measure. In terms of P and R it will be:  To facilitate interpretation of the function, we transform according to [[alpha]] = 1/(ß[2] + 1), and find that [[partialdiff]]E/ [[partialdiff]]R = [[partialdiff]]E/ [[partialdiff]]P when P/R = ß. If A is the set of relevant documents and B the set of retrieval documents, then:  E now gives rise to the following special cases:  (1) When [[alpha]] = 1/2 (ß = 1) E = |A [[Delta]] B | / (|A | + |B |), a normalised symmetric difference between sets A and B (A [[Delta]] B = A [[union]] B - A [[intersection]] B). It corresponds to a user who attaches equal importance to precision and recall.  (2) E -gt; 1 - R when [[alpha]] -gt; 0 (ß -gt; *), which corresponds to a user who attaches no important to precision.  (3) E -gt; 1 - P when [[alpha]] -gt; 1 (ß -gt; 0), which corresponds to a user who attaches no importance to recall.  It is now a simple matter to show that certain other measures given in the literature are special cases of the general form E. By the representation theorem, the [[Phi]]i 's are uniquely determined up to a linear transformation, that is, [[Phi]]i['] = [[Theta]][[Phi]]i + [[gamma]]i would serve equally well as scale functions. If we now set [[Phi]]1['] = 2[[Phi]]1 - 1/2, [[Phi]]2['] = 2[[Phi]]2 - 1/2, and ß = 1 then we have:  which is the measure recommended by Heine[3].  One final example is the measure suggested by Vickery in 1965 which was documented by Cleverdon et al.[27]. Here we set:  which is Vickery's measure (apart from a scale factor of 100).  To summarise, we have shown that it is reasonable to assume that effectiveness in terms of precision and recall determines an additive conjoint structure. This guarantees the existence of an additively independent representation. We then found the representation satisfying some user requirements and also having special cases which are simple to interpret.  The analysis is not limited to the two factors precision and recall, it could equally well be carried out for say the pair fallout and recall. Furthermore, it is not necessary to restrict the model to two factors. If appropriate variables need to be incorporated the model readily extends to n factors. In fact, for more than two dimensions the Thomsen condition is not required for the representation theorem.
irv-0092	Presentation of experimental results  In my discussion of micro-, macro-evaluation, and expected search length, various ways of averaging the effectiveness measure of the set of queries arose in a natural way. I now want to examine the ways in which we can summarise our retrieval results when we have no a priori reason to suspect that taking means is legitimate.  In this section the discussion will be restricted to single number measures such as a normalised symmetric difference, normalised recall, etc. Let us use Z to denote any arbitrary measure. The test queries will be Qi and n in number. Our aim in all this is to make statements about the relative merits of retrieval under different conditions a,b,c, . . . in terms of the measure of effectiveness Z. The 'conditions' a,b,c, . . . may be different search strategies, or information structures, etc. In other words, we have the usual experimental set-up where we control a variable and measure how its change influences retrieval effectiveness. For the moment we restrict these comparisons to one set of queries and the same document collection.  The measurements we have therefore are {Za(Q1), Za(Q2), . . . }, {Zb(Q1), Zb(Q2), . . . }, {Zc(Q1), Zc(Q2), . . . }, . . . where Zx(Q1) is the value of Z when measuring the effectiveness of the response to Qi under conditions x. If we now wish to make an overall comparison between these sets of measurements we could take means and compare these. Unfortunately, the distributions of Z encountered are far from bell-shaped, or symmetric for that matter, so that the mean is not a particularly good 'average' indicator. The problem of summarising IR data has been a hurdle every since the beginning of the subject. Because of the non-parametric nature of the data it is better not to quote a single statistic but instead to show the variation in effectiveness by plotting graphs. Should it be necessary to quote 'average' results it is important that they are quoted alongside the distribution from which they are derived.  There are a number of ways of representing sets of Z-values graphically. Probably the most obvious one is to use a scatter diagram, where the x-axis is scaled for Za and the y-axis for Zb and each plotted point is the pair (Za(Qi), Zb(Qi)). The number of points plotted will equal the number of queries. If we now draw a line at 45[[ring]] to the x-axis from the origin we will be able to see what proportion of the queries did better under condition a than under condition b. There are two disadvantages to this method of representation: the comparison is limited to two conditions, and it is difficult to get an idea of the extent to which two conditions differ.  A more convenient way of showing retrieval results of this kind is to plot them as cumulative frequency distributions, or as they are frequently called by statisticians empirical distribution functions. Let {Z(Q1), Z(Q2), . . . , Z(Qn)} be a set of retrieval results then the empirical distribution function F(z) is a function of z which equals the proportion of Z(Qi)'s which are less than or equal to z. To plot this function we divide the range of z into intervals. If we assume that 0 lt;= z lt;= 1, then a convenient set of intervals is ten. The distributions will take the general shape as shown in Figure 7.14. When the measure Z is such that the smaller its value the more effective the retrieval, then the higher the curve the better. It is quite simple to read off the various quantiles. For example, to find the median we only need to find the z-value corresponding to 0.5 on the F(z) axis. In our diagrams they are 0.2 and 0.4 respectively for conditions a and b.  I have emphasised the measurement of effectiveness from the point of view of the user. If we now wish to compare retrieval on different document collections with different sets of queries then we can still use these measures to indicate which system satisfies the user more. On the other hand, we cannot thereby establish which system is more effective in its retrieval operations. It may be that in system A the sets of relevant documents constitute a smaller proportion of the total set of documents than is the case in system B. In other words, it is much harder to find the relevant documents in system B than in system A. So, any direct comparison must be weighted by the generality measure which gives the number of relevant documents as a proportion of the total number of documents. Alternatively one could use fallout which measures the proportion of non-relevant documents retrieved. The important point here is to be clear about whether we are measuring user satisfaction or system effectiveness.
irv-0093	Significance tests  Once we have our retrieval effectiveness figures we may wish to establish that the difference in effectiveness under two conditions is statistically significant. It is precisely for this purpose that many statistical tests have been designed. Unfortunately, I have to agree with the findings of the Comparative Systems Laboratory[28] in 1968, that there are no known statistical tests applicable to IR. This may sound like a counsel of defeat but let me hasten to add that it is possible to select a test which violates only a few of the assumptions it makes. Two good sources which spell out the pre-conditions for non-parametric tests are Siegal[29] and Conover[30]. A much harder but also more rewarding book on non-parametrics is Lehmann[31].  Parametric tests are inappropriate because we do not know the form of the underlying distribution. In this class we must include the popular t-test. The assumptions underlying its use are given in some detail by Siegel (page 19), needless to say most of these are not met by IR data. One obvious failure is that the observations are not drawn from normally distributed populations.  On the face of it non-parametric tests might provide the answer. There are some tests for dealing with the case of related samples. In our experimental set-up we have one set of queries which is used in different retrieval environments. Therefore, without questioning whether we have random samples, it is clear that the sample under condition a is related to the sample under condition b. When in this situation a common test to use has been the Wilcoxon Matched-Pairs test. Unfortunately again some important assumptions are not met. The test is done on the difference Di = Za (Qi) - Zb (Qi), but it is assumed that Di is continuous and that it is derived from a symmetric distribution, neither of which is normally met in IR data.  It seems therefore that some of the more sophisticated statistical tests are inappropriate. There is, however, one simple test which makes very few assumptions and which can be used providing its limitations are noted. This one is known in the literature as the sign test (Siegel[29], page 68 and Conover[30], page 121). It is applicable in the case of related samples. It makes no assumptions about the form of the underlying distribution. It does, however, assume that the data are derived from a continuous variable and that the Z (Qi) are statistically independent. These two conditions are unlikely to be met in a retrieval experiment. Nevertheless, given that some of the conditions are not met, it can be used conservatively.  The way it works is as follows: Let {Za (Q1), Za (Q2), . . .,}, {Zb (Q1), Zb (Q2). . .,} be our two sets of measurements under conditions a and b respectively. Within each pair (Za (Qi), Zb (Qi)) a comparison is made, and each pair is classified as ' + ' if Za (Qi) gt; Zb (Qi), as ' - ' if Za (Qi) lt; Zb (Qi) or 'tie' if Za (Qi) = Za (Qi). Pairs which are classified as 'tie' are removed from the analysis thereby reducing the effective number of measurements. The null hypothesis we wish to test is that:  P (Za gt; Zb ) = P (Za lt; Zb ) = [1]/2  Under this hypothesis we expect the number of pairs which have Za gt; Zb to equal the number of pairs which have Za lt; Zb . Another way of stating this is that the two populations from which Za and Zb are derived have the same median.  In IR this test is usually used as a one-tailed test, that is, the alternative hypothesis prescribes the superiority of retrieval under condition a over condition b, or vice versa. A table for small samples n lt;= 25 giving the probability under the null hypothesis for each possible combination of '+''s and '-''s may be found in Siegal[29] (page 250). To give the reader a feel for the values involved: in a sample of 25 queries the null hypothesis will be rejected at the 5 per cent level if there are at least 14 differences in the direction predicted by the alternative hypothesis.  The use of the sign test raises a number of interesting points. The first of these is that unlike the Wilcoxon test it only assumes that the Z's are measured on an ordinal scale, that is, the magnitude of |Za - Zb | is not significant. This is a suitable feature since we are usually only seeking to find which strategy is better in an average sense and do not wish the result to be unduly influenced by excellent retrieval performance on one query. The second point is that some care needs to be taken when comparing Za and Zb. Because our measure of effectiveness can be calculated to infinite precision we may be insisting on a difference when in fact it only occurs in the tenth decimal place. It is therefore important to decide beforehand at what value of [[propersubset]] we will equate Za and Zb when |Za - Zb | lt;= [[propersubset]].  Finally, although I have just explained the use of the sign test in terms of single number measures, it is also used to detect a significant difference between precision-recall graphs. We now interpret the Z's as precision values at a set of standard recall values. Let this set be SR = {0,1, 0.2, . . ., 1.0}, then corresponding to each R[[propersubset]] SR we have a pair (Pa (R) Pb (R)). The Pa's and Pb's are now treated in the same way as the Za's and Zb's. Note that when doing the evaluation this way, the precision-recall values will have already been averaged over the set of queries by one of the ways explained before.
irv-0094	Bibliographic remarks  Quite a number of references to the work on evaluation have already been given in the main body of the chapter. Nevertheless, there are still a few important ones worth mentioning.  Buried in the report by Keen Digger[32] (Chapter 16) is an excellent discussion of the desirable properties of any measure of effectiveness. It also gives a checklist indicating which measure satisfies what. It is probably worth repeating here that Part I of Robertson's paper[33] contains a discussion of measures of effectiveness based on the 'contingency' table as well as a list showing who used what measure in their experiments. King and Bryant[34] have written a book on the evaluation of information services and products emphasising the commercial aspects. Goffman and Newill[35] describe a methodology for evaluation in general.  A parameter which I have mentioned in passing but which deserves closer study in generality. Salton[36] has recently done a study of its effect on precision and fallout for different sized document collections.  The trade-off between precision and recall has for a long time been the subject of debate. Cleverdon[37] who has always been involved in this debate has now restated his position. Heine[38], in response to this, has attempted to further clarify the trade-off in terms of the Swets model.  Guazzo[39] and Cawkell[40] describe an approach to the measurement of retrieval effectiveness based on information theory.  The notion of relevance has at all times attracted much discussion. An interesting early philosophical paper on the subject is by Weiler[41]. Goffman[42] has done an investigation of relevance in terms of Measure Theory. And more recently Negoita[43] has examined the notion in terms of different kinds of logics.  A short paper by Good[44] which is in sympathy with the approach based on a theory of measurement given here, discusses the evaluation of retrieval systems in terms of expected utility.  One conspicuous omission from this chapter is any discussion of cost-effectiveness. The main reason for this is that so far very little of importance can be said about it. A couple of attempts to work out mathematical cost models for IR are Cooper[45] and Marschak[46].
issr-0001	1 Introduction to Information Retrieval  Systems  1.1       Definition of Information Retrieval System  1.2       Objectives of Information Retrieval Systems L3      Functional Overview  1.4       Relationship to Database Management Systems  1.5       Digital Libraries and Data Warehouses  1.6       Summary  This chapter defines an Information Storage and Retrieval System (called an Information Retrieval System for brevity) and differentiates between information retrieval and database management systems. Tied closely to the definition of an Information Retrieval System are the system objectives. It is satisfaction of the objectives that drives those areas that receive the most attention in development. For example, academia pursues all aspects of information systems, investigating new theories, algorithms and heuristics to advance the knowledge base. Academia does not worry about response time, required resources to implement a system to support thousands of users nor operations and maintenance costs associated with system delivery. On the other hand, commercial institutions are not always concerned with the optimum theoretical approach, but the approach that minimizes development costs and increases the salability of their product. This text considers both view points and technology states. Throughout this text, information retrieval is viewed from both the theoretical and practical viewpoint.  The functional view of an Information Retrieval System is introduced to put into perspective the technical areas discussed in later chapters. As detailed algorithms and architectures are discussed, they are viewed as subfiinctions within a total system. They are aiso correlated to the major objective of an Information Retrieval System which  is minimization of human resources required in the Chapter 1  finding of needed information to accomplish a task. As with any discipline, standard measures are identified to compare the value of different algorithms. In information systems, precision and recall are the key metrics used in evaluations. Early introduction of these concepts in this chapter will help the reader in understanding the utility of the detailed algorithms and theory introduced throughout this text.  There is a potential for confusion in the understanding of the differences between Database Management Systems (DBMS) and Information Retrieval Systems. It is easy to confuse the software that optimizes functional support of each type of system with actual information or structured data that is being stored and manipulated. The importance of the differences lies in the inability of a database management system to provide the functions needed to process "information." The opposite, an information system containing structured data, also suffers major functional deficiencies. These differences are discussed in detail in Section 1.4.
issr-0002	1.1  Definition of Information Retrieval System  An Information Retrieval System is a system that is capable of storage, retrieval, and maintenance of information. Information in this context can be composed of text (including numeric and date data), images, audio, video and other multi-media objects. Although the form of an object in an Information Retrieval System is diverse, the text aspect has been the only data type that lent itself to fall functional processing. The other data types have been treated as highly informative sources, but are primarily linked for retrieval based upon search of the text. Techniques are beginning to emerge to search these other media types (e.g., EXCALIBUR's Visual RetrievaiWare, VIRAGE video indexer). The focus of this book is on research and implementation of search, retrieval and representation of textual and multimedia sources. Commercial development of pattern matching against other data types is starting to be a common function integrated within the total information system. In some systems the text may only be an identifier to display another associated data type that holds the substantive information desired by the system's users (e.g., using closed captioning to locate video of interest.) The term uuser" in this book represents an end user of the information system who has minimal knowledge of computers and technical fields in general.  The term "item" is used to represent the smallest complete unit that is processed and manipulated by the system. The definition of item varies by how a specific source treats information. A complete document, such as a book, newspaper or magazine could be an item. At other times each chapter, or article may be defined as an item. As sources vary and systems include more complex processing, an item may address even lower levels of abstraction such as a contiguous passage of text or a paragraph. For readability, throughout this book the terms "item" and "document" are not in this rigorous definition, but used Introduction to Information Retrieval Systems  interchangeably. Whichever is used, they represent the concept of an item. For most of the book it is best to consider an item as text. But in reality an item may be a combination of many modals of information. For example a video news program could be considered an item. It is composed of text in the form of closed captioning, audio text provided by the speakers, and the video images being displayed. There are multiple "tracks" of information possible in a single item. They are typically correlated by time. Where the text discusses multimedia information retrieval keep this expanded model in mind.  An Information Retrieval System consists of a software program that facilitates a user in finding the information the user needs. The system may use standard computer hardware or specialized hardware to support the search subftmction and to convert non-textual sources to a searchable media (e.g., transcription of audio to text). The gauge of success of an information system is how well it can minimize the overhead for a user to find the needed information. Overhead from a user's perspective is the time required to find the information needed, excluding the time for actually reading the relevant data. Thus search composition, search execution, and reading non-relevant items are all aspects of information retrieval overhead.  The first Information Retrieval Systems originated with the need to organize information in central repositories (e.g., libraries) (Hyman-82). Catalogues were created to facilitate the identification and retrieval of items. Chapter 3 reviews the history of cataloging and indexing. Original definitions focused on "documents" for information retrieval (or their surrogates) rather than the multi-media integrated information that is now available (Minker-77, Minker77.)  As computers became commercially available, they were obvious candidates for the storage and retrieval of text. Early introduction of Database Management Systems provided an ideal platform for electronic manipulation of the indexes to information (Rather-77). Libraries followed the paradigm of their catalogs and references by migrating the format and organization of their hardcopy information references into structured databases. These remain as a primary mechanism for researching sources of needed information and play a major role in available Information Retrieval Systems. Academic research that was pursued through the 1980s was constrained by the paradigm of the indexed structure associated with libraries and the lack of computer power to handle large (gigabyte) text databases. The Military and other Government entities have always had a requirement to store and search large textual databases. As a result they began many independent developments of textual Information Retrieval Systems. Given the large quantities of data they needed to process, they pursued both research and development of specialized hardware and unique software solutions incorporating Commercial Off The Shelf (COTS) products where possible. The Government has been the major fending source of research into Information Retrieval Systems. With the advent of inexpensive powerful personnel computer processing systems and   high   speed,   large  capacity  secondary  storage  products,   it   has  become Chapter  commercially feasible to provide large textual information databases for the average user. The introduction and exponential growth of the Internet along with its initial WAIS (Wide Area Information Servers) capability and more recently advanced search servers (e.g., INFOSEEK, EXCITE) has provided a new avenue for access to terabytes of information (over 800 million indexable pages Lawrence-99.) The algorithms and techniques to optimize the processing and access of large quantities of textual data were once the sole domain of segments of the Government, a few industries, and academics. They have now become a needed capability for large quantities of the population with significant research and development being done by the private sector. Additionally the volumes of nontextual information are also becoming searchable using specialized search capabilities. Images across the Internet are searchable from many web sites such as WEBSEEK, DITTO.COM, ALTAVISTA/IMAGES. News organizations such as the BBC are processing the audio news they have produced and are making historical audio news searchable via the audio transcribed versions of the news. Major video organizations such as Disney are using video indexing to assist in finding specific images in their previously produced videos to use in future videos or incorporate in advertising. With exponential growth of multi-media on the Internet capabilities such as these are becoming common place. Information Retrieval exploitation of multi-media is still in its infancy with significant theoretical and practical knowledge missing.
issr-0003	1.2 Objectives of Information Retrieval Systems  The general objective of an Information Retrieval System is to minimize the overhead of a user locating needed information. Overhead can be expressed as the time a user spends in all of the steps leading to reading an item containing the needed information (e.g., query generation, query execution, scanning results of query to select items to read, reading non-relevant items). The success of an information system is very subjective, based upon what information is needed and the willingness of a user to accept overhead. Under some circumstances, needed information can be defined as all information that is in the system that relates to a user's need. In other cases it may be defined as sufficient information in the system to complete a task, allowing for missed data. For example, a financial advisor recommending a billion dollar purchase of another company needs to be sure that all relevant, significant information on the target company has been located and reviewed in writing the recommendation. In contrast, a student only requires sufficient references in a research paper to satisfy the expectations of the teacher, which never is all inclusive. A system that supports reasonable retrieval requires fewer features than one which requires comprehensive retrieval. In many cases comprehensive retrieval is a negative feature because it overloads the user with more information than is needed. This makes it more difficult for the user to filter the relevant but non-useful information from the critical items. In information retrieval the term  "relevant"  item  is used to represent an  item Introduction to Information Retrieval Systems  containing the needed information. In reality the definition of relevance is not a binary classification but a continuous function. From a user's perspective "relevant" and "needed" are synonymous. From a system perspective, information could be relevant to a search statement (i.e., matching the criteria of the search statement) even though it is not needed/relevant to user (e.g., the user already knew the information). A discussion on relevance and the natural redundancy of relevant information is presented in Chapter 11.  The two major measures commonly associated with information systems are precision and recall. When a user decides to issue a search looking for information on a topic, the total database is logically divided into four segments shown in Figure 1.1. Relevant items are those documents that contain information that helps the searcher in answering his question. Non-relevant items are those items that do not provide any directly useful information. There are two possibilities with respect to each item: it can be retrieved or not retrieved by the user's query. Precision and recall are defined as:  Precision =  Number _ Re trieved_ Re levant Number   Total   Retrieved  /fai  w  Relevant Not Retrieved  Non-Relevant Not Retrieved  Non-Relevant Retrieved  Figure 1.1  Effects of Search on Total Document Space  Recall =  Number ___ Re trieved_ Re levant Number   Possible   Re levant  where Number^Possible^Relevant are the number of relevant items in the database. Number_Total_Retieved Is the total number of items retrieved from the query.    NumberJRetrievedJlelevant is the number of items retrieved that are Chapter 1  relevant to the user's search need. Precision measures one aspect of information retrieval overhead for a user associated with a particular search. If a search has a 85 per cent precision, then 15 per cent of the user effort is overhead reviewing nonrelevant items. Recall gauges how well a system processing a particular query is able to retrieve the relevant items that the user is interested in seeing. Recall is a very useful concept, but due to the denominator, is non-calculable in operational systems. If the system knew the total set of relevant items in the database, it would have retrieved them. Figure 1.2a shows the values of precision and recall as the number of items retrieved increases, under an optimum query where every returned item is relevant. There are "N" relevant items in the database. Figures 1.2b and 1.2c show the optimal and currently achievable relationships between Precision and Recall (Harman-95). In Figure 1.2a the basic properties of precision (solid line) and recall (dashed line) can be observed. Precision starts off at 100 per cent and maintains that value as long as relevant items are retrieved. Recall starts off close to zero and increases as long as relevant items are retrieved until all possible relevant items have been retrieved. Once all "N" relevant items have been retrieved, the only items being retrieved are non-relevant. Precision is directly affected by retrieval of non-relevant items and drops to a number close to zero. Recall is not effected by retrieval of non-relevant items and thus remains at 100 per  100  Percent  N Number Items Retrieved  1.2a Ideal Precision and Recall  1.0  10  Recall  Figure 1.2b Ideal Precision/Recall Graph Introduction to Information Retrieval Systems  Precision  1.0  0.8 0.6 0.4  0.2 0.0  0.0  0.2  0.4 Recall  0.6  0.8  1.0  Figure 1.2c Achievable Precision/Recall Graph  cent once achieved. Precision/Recall graphs show how values for precision and recall change within a search results file (Hit file) as viewed from the most relevant to least relevant item. As with Figure 1.2a, in the ideal case every item retrieved is relevant. Thus precision stays at 100 per cent (1.0). Recall continues to increase by moving to the right on the x-axis until it also reaches the 100 per cent (1.0) point. Although Figure 1.2c stops here, continuation stays at the same x-axis location (recall never changes) but precision decreases down the y-axis until it gets close to the x-axis as more non-relevant are discovered and precision decreases. Figure 1.2c is from the latest TREC conference (see Chapter 11) and is representative of current capabilities.  To understand the implications of Figure 1.2c, its usefiil to describe the implications of a particular point on the precision/recall graph. Assume that there are 100 relevant items in the data base and from the graph at precision of .3 (i.e., 30 per cent) there is an associated recall of .5 (i.e., 50 per cent). This means there would be 50 relevant items in the Hit file from the recall value. A precision of 30 per cent means the user would likely review 167 items to find the 50 relevant items.  The first objective of an Information Retrieval System is support of user search generation. There are natural obstacles to specification of the information a user needs that come from ambiguities inherent in languages, limits to the user's ability to express what information is needed and differences between the user's vocabulary corpus and that of the authors of the items in the database.   Natural Chapter 1  languages suffer from word ambiguities such as homographs and use of acronyms that allow the same word to have multiple meanings (e.g., the word "field" or the acronym "U.S.")- Disambiguation techniques exist but introduce significant system overhead in processing power and extended search times and often require interaction with the user.  Many users have trouble in generating a good search statement. The typical user does not have significant experience with nor even the aptitude for Boolean logic statements. The use of Boolean logic is a legacy from the evolution of database management systems and implementation constraints. Until recently, commercial systems were based upon databases. It is only with the introduction of Information Retrieval Systems such as RetrievalWare, TOPIC, AltaVista, Infoseek and INQUERY that the idea of accepting natural language queries is becoming a standard system feature. This allows users to state in natural language what they are interested in finding. But the completeness of the user specification is limited by the user's willingness to construct long natural language queries. Most users on the Internet enter one or two search terms.  Multi-media adds an additional level of complexity in search specification. Where the modal has been converted to text (e.g., audio transcription, OCR) the normal text techniques are still applicable. But query specification when searching for an image, unique sound, or video segment lacks any proven best interface approaches. Typically they are achieved by having prestored examples of known objects in the media and letting the user select them for the search (e.g., images of leaders allowing for searches on "Tony Blair".) This type specification becomes more complex when coupled with Boolean or natural language textual specifications.  In addition to the complexities in generating a query, quite often the user is not an expert in the area that is being searched and lacks domain specific vocabulary unique to that particular subject area. The user starts the search process with a general concept of the information required, but not have a focused definition of exactly what is needed. A limited knowledge of the vocabulary associated with a particular area along with lack of focus on exactly what information is needed leads to use of inaccurate and in some cases misleading search terms. Even when the user is an expert in the area being searched, the ability to select the proper search terms is constrained by lack of knowledge of the author's vocabulary. All writers have a vocabulary limited by their life experiences, environment where they were raised and ability to express themselves. Other than in very technical restricted information domains, the user's search vocabulary does not match the author's vocabulary. Users usually start with simple queries that suffer from failure rates approaching 50% (Nordlie-99).  Thus, an Information Retrieval System must provide tools to help overcome the search specification problems discussed above. In particular the search tools must assist the user automatically and through system interaction in developing a search specification that represents the need of the user and the writing style of diverse authors (see figure 1.3) and multi-media specification. Introduction to Information Retrieval Systems  Authors' Vocabulary on the Concept in the Item  User's  General  Vocabulary  Figure 1.3 Vocabulary Domains  In addition to finding the information relevant to a user's needs, an objective of an information system is to present the search results in a format that facilitates the user in determining relevant items. Historically data has been presented in an order dictated by how it was physically stored. Typically, this is in arrival to the system order, thereby always displaying the results of a search sorted by time. For those users interested in current events this is useful. But for the majority of searches it does not filter out less useful information. Information Retrieval Systems provide functions that provide the results of a query in order of potential relevance to the user. This, in conjunction with user search status (e.g., listing titles of highest ranked items) and item formatting options, provides the user with features to assist in selection and review of the most likely relevant items first. Even more sophisticated techniques use item clustering, item summarization and link analysis to provide additional item selection insights (see Chapter 8.) Other features such as viewing only "unseen" items also help a user who can not complete the item review process in one session. In the area of Question/Answer systems that is coming into focus in Information Retrieval, the retrieved items are not returned to the user. Instead the answer to their question - or a short segment of text that contains the answer - is what is returned. This is a more complex process then summarization since the results need to be focused on the specific information need versus general area of the users query. The approach to this problem most used in TREC - S was to first perform a search using existing 10                                                                                               Chapter  algorithms, then to syntactically parse the highest ranked retrieved items looking for specific passages that answer the question. See Chapter 11 for more details.  Multi-media information retrieval adds a significant layer of complexity on how to display multi-modal results. For example, how should video segments potentially relevant to a user's query be represented for user review and selection? It could be represented by two thumbnail still images of the start and end of the segment, or should the major scene changes be represented (the latter technique would avoid two pictures of the news announcer versus the subject of the video segment.)
issr-0004	1.3 Functional Overview  A total Information Storage and Retrieval System is composed of four major functional processes: Item Normalization, Selective Dissemination of Information (i.e., "Mail"), archival Document Database Search, and an Index Database Search along with the Automatic File Build process that supports Index Files. Commercial systems have not integrated these capabilities into a single system but supply them as independent capabilities. Figure 1.4 shows the logical view of these capabilities in a single integrated Information Retrieval System. Boxes are used in the diagram to represent Jdinctions while disks represent data storage.
issr-0005	1.3.1 Item Normalization  The first step in any integrated system is to normalize the incoming items  to a standard format.   In addition to translating multiple external formats that  might be received into a single consistent data structure that can be manipulated by the functional processes,  item normalization provides logical restructuring of the  item. Additional operations during item normalization are needed to create a searchable data structure: Identification of processing tokens (e.g., words), characterization of the tokens, and stemming (e.g., removing word endings) of the  tokens. The original item or any of its logical subdivisions Is available for the user to display. The processing tokens and their characterization are used to define the searchable text from the total received text. Figure 1.5 shows the normalization  process.  Standardizing the Input takes the different external formats of Input data and performs the translation to the formats acceptable to the system. A system may have a single format for all Items or allow multiple formats. One example of standardization could be translation of foreign languages Into Unicode. Every language has a different internal binary encoding for the characters in the language. One standard encoding that covers English, French, Spanish, etc. is ISO-Latin.   The are other Internal encodings for other language groups  such as Introduction to Information Retrieval Systems  11  Russian (e.g, KOI-7, KOI-8), Japanese, Arabic, etc.    Unicode is an evolving international standard based upon 16 bits (two bytes) that will be able to represent  ITEM INPUT  ITEM NORMALIZATION  SELECTIVE  DISSEMINATION OF  INFORMATION  (MAIL)  DOCUMENT FILE CREATION  AUTOMATIC FILE BUILD (AFB)  CO  o O  PUBLIC INDEXING         PRIVATE INDEXING       Figure 1.4 Total Information Retrieval System 12  Chapter 1  STANDARDIZE INPUT  LOGICAL  SUBSETTING  (ZONING)  IDENTIFY  PROCESSING  TOKENS  UPDATE DOCUMENT FILE  APPLY STOPLISTS (STOP ALGORITHMS)  CHARACTERIZE TOKENS  APPLY STEMMING  CREATE  SEARCHABLE DATA STRUCTURE  Figure 1.5 The Text Normalization Process  all languages. Unicode based upon UTF-8, using multiple 8-bit bytes, is becoming the practical Unicode standard. Having all of the languages encoded into a single format allows for a single browser to display the languages and potentially a single search system to search them. Of course such a search engine would have to have the capability of understanding the linguistic model for all the languages to allow for correct tokenization (e.g., word boundaries, stemming, word stop lists, etc.) of each language. Introduction to Information Retrieval Systems                                              13  Multi-media adds an extra dimension to the normalization process. In addition to normalizing the textual input, the multi-media input also needs to be standardized. There are a lot of options to the standards being applied to the normalization. If the input is video the likely digital standards will be either MPEG-2, MPEG-1, AVI or Real Media. MPEG (Motion Picture Expert Group) standards are the most universal standards for higher quality video where Real Media is the most common standard for lower quality video being used on the Internet. Audio standards are typically WAV or Real Media (Real Audio). Images vary from JPEG to BMP. In all of the cases for multi-media, the input analog source is encoded into a digital format. To index the modal different encodings of the same input may be required (see Section 1.3.5 below). But the importance of using an encoding standard for the source that allows easy access by browsers is greater for multi-media then text that already is handled by all interfaces.  The next process is to parse the item into logical sub-divisions that have meaning to the user. This process, called "Zoning," is visible to the user and used to increase the precision of a search and optimize the display. A typical item is sub-divided into zones, which may overlap and can be hierarchical, such as Title, Author, Abstract, Main Text, Conclusion, and References. The term "Zone" was selected over field because of the variable length nature of the data identified and because it is a logical sub-division of the total item, whereas the term "fields" has a connotation of independence. There may be other source-specific zones such as "Country" and "Keyword." The zoning information is passed to the processing token identification operation to store the information, allowing searches to be restricted to a specific zone. For example, if the user is interested in articles discussing "Einstein" then the search should not include the Bibliography, which could include references to articles written by "Einstein." Zoning differs for multi-media based upon the source structure. For a news broadcast, zones may be defined as each news story in the input. For speeches or other programs, there could be different semantic boundaries that make sense from the user's perspective.  Once a search is complete, the user wants to efficiently review the results to locate the needed information. A major limitation to the user is the size of the display screen which constrains the number of items that are visible for review. To optimize the number of items reviewed per display screen, the user wants to display the minimum data required from each item to allow determination of the possible relevance of that item. Quite often the user will only display zones such as the Title or Title and Abstract. This allows multiple items to be displayed per screen. The user can expand those items of potential interest to see the complete text.  Once the standardization and zoning has been completed, information (i.e., words) that are used in the search process need to be identified in the item. The term processing token is used because a "word" is not the most efficient unit on which to base search structures. The first step in identification of a processing token consists of determining a word. Systems determine words by dividing input symbols into three classes: valid word symbols, inter-word symbols, and special processing symbols.   A word   is defined as a contiguous set of   word symbols 14                                                                                                Chapter  bounded by inter-word symbols. In many systems inter-word symbols are nonsearchable and should be carefully selected. Examples of word symbols are alphabetic characters and numbers. Examples of possible inter-word symbols are blanks, periods and semicolons. The exact definition of an inter-word symbol is dependent upon the aspects of the language domain of the items to be processed by the system. For example, an apostrophe may be of little importance if only used for the possessive case in English, but might be critical to represent foreign names in the database. Based upon the required accuracy of searches and language characteristics, a trade off is made on the selection of inter-word symbols. Finally there are some symbols that may require special processing. A hyphen can be used many ways, often left to the taste and judgment of the writer (Bernstein-84). At the end of a line it is used to indicate the continuation of a word. In other places it links independent words to avoid absurdity, such as in the case of "small business men." To avoid interpreting this as short males that run businesses, it would properly be hyphenated "small-business men." Thus when a hyphen (or other special symbol) is detected a set of rules are executed to determine what action is to be taken generating one or more processing tokens.  Next, a Stop List/Algorithm is applied to the list of potential processing tokens. The objective of the Stop function is to save system resources by eliminating from the set of searchable processing tokens those that have little value to the system. Given the significant increase in available cheap memory, storage and processing power, the need to apply the Stop function to processing tokens is decreasing. Nevertheless, Stop Lists are commonly found in most systems and consist of words (processing tokens) whose frequency and/or semantic use make them of no value as a searchable token. For example, any word found in almost every item would have no discrimination value during a search. Parts of speech, such as articles (e.g., "the"), have no search value and are not a useful part of a user's query. By eliminating these frequently occurring words the system saves the processing and storage resources required to incorporate them as part of the searchable data structure. Stop Algorithms go after the other class of words, those found very infrequently.  Ziph (Ziph-49) postulated that, looking at the frequency of occurrence of the unique words across a corpus of items, the majority of unique words are found to occur a few times. The rank-frequency law of Ziph is:  Frequency * Rank = constant  where Frequency is the number of times a word occurs and rank is the rank order  of the word. The law was later derived analytically using probability and information theory (Fairthorne-69). Table 1.1 shows the distribution of words in the first TREC test database (Harman-93), a database with over one billion characters and 500,000 items. In Table LI, WSJ is Wall Street Journal (1986-89), AP is AP Newswire (1989), ZIFF - Information from Computer Select disks, FR Federal Register (1989), and DOE - Short abstracts from Department of Energy. Introduction to Information Retrieval Systems  15  The highly precise nature of the words only found once or twice in the database reduce the probability of their being in the vocabulary of the user and the terms are almost never included in searches. Eliminating these words saves on storage and access structure (e.g., dictionary - see Chapter 4) complexities. The best technique to eliminate the majority of these words is via a Stop algorithm versus trying to list them individually. Examples of Stop algorithms are:  Stop all numbers greater than "999999"  (this was selected to allow dates to be searchable)  Stop any processing token that has numbers and characters intermixed  The algorithms are typically source specific, usually eliminating unique item numbers that are frequently found in systems and have no search value.  In some systems (e.g., INQUIRE DBMS), inter-word symbols and Stop words are not included in the optimized search structure (e.g., inverted file structure, see Chapter 4) but are processed via a scanning of potential hit documents after inverted file search reduces the list of possible relevant items. Other systems never allow interword symbols to be searched.  Source WSJ AP ZIFF FR DOE  Size in Mbytes 295 266 251 258 190  Median number terms/record 182 353 181 313 82  Average number terms/record 329 375 412 1017 89  Number Unique Terms 156,298 197,608 173,501 126,258 186,225  Number         of Terms Occurring Once 64,656 89,627 85,992 58,677 95,782  Average number terms occurrences gt; 1 199 174 165 106 159  Table LI Distribution of words in TREC Database (from TREC-1 Conference Proceedings, Harmon-93)  The next step in finalizing on processing tokens is identification of any specific word characteristics. The characteristic is used in systems to assist in disambiguation of a particular word.   Morphological analysis of the processing 16                                                                                                Chapter 1  token's part of speech is included here. Thus, for a word such as "plane," the system understands that it could mean "level or flat" as an adjective, "aircraft or facet" as a noun, or "the act of smoothing or evening" as a verb. Other characteristics may classify a token as a member of a higher class of tokens such as "European Country" or "Financial Institution." Another example of characterization is if upper case should be preserved. In most systems upper/lower case is not preserved to avoid the system having to expand a term to cover the case where it is the first word in a sentence. But, for proper names, acronyms and organizations, the upper case represents a completely different use of the processing token versus it being found in the text. "Pleasant Grant" should be recognized as a person's name versus a "pleasant grant" that provides funding. Other characterizations that are typically treated separately from text are numbers and dates.  Once the potential processing token has been identified and characterized, most systems apply stemming algorithms to normalize the token to a standard semantic representation. The decision to perform stemming is a trade off between precision of a search (i.e., finding exactly what the query specifies) versus standardization to reduce system overhead in expanding a search term to similar token representations with a potential increase in recall. For example, the system must keep singular, plural, past tense, possessive, etc. as separate searchable tokens and potentially expand a term at search time to all its possible representations, or just keep the stem of the word, eliminating endings. The amount of stemming that is applied can lead to retrieval of many non-relevant items. The major stemming algorithms used at this time are described in Chapter 4. Some systems such as RetrievalWare, that use a large dictionary/thesaurus, looks up words in the existing dictionary to determine the stemmed version in lieu of applying a sophisticated algorithm.  Once the processing tokens have been finalized, based upon the stemming algorithm, they are used as updates to the searchable data structure. The searchable data structure is the internal representation (i.e., not visible to the user) of items that the user query searches. This structure contains the semantic concepts that represent the items in the database and limits what a user can find as a result of their search. When the text is associated with video or audio multi-media, the relative time from the start of the item for each occurrence of the processing token is needed to provide the correlation between the text and the multi-media source. Chapter 4 Introduces the internal data structures that are used to store the searchable data structure for textual items and Chapter 5 provides the algorithms for creating the data to be stored based upon the identified processing tokens.
issr-0006	1.3.2 Selective Dissemination of Information  The Selective Dissemination of Information (Mail) Process (see Figure 1.4) provides the capability to dynamically compare newly received items in the information system against standing statements of interest of users and deliver the Introduction to Information Retrieval Systems                                              17  item to those users whose statement of interest matches the contents of the item. The Mail process is composed of the search process, user statements of interest (Profiles) and user mail files. As each item is received, it is processed against every user's profile. A profile contains a typically broad search statement along with a list of user mail files that will receive the document if the search statement in the profile is satisfied. User search profiles are different than ad hoc queries in that they contain significantly more search terms (10 to 100 times more terms) and cover a wider range of interests. These profiles define all the areas in which a user is interested versus an ad hoc query which is frequently focused to answer a specific question. It has been shown in recent studies that automatically expanded user profiles perform significantly better than human generated profiles (Harman95).  When the search statement is satisfied, the item is placed in the Mail File(s) associated with the profile. Items in Mail files are typically viewed in time of receipt order and automatically deleted after a specified time period (e.g., after one month) or upon command from the user during display. The dynamic asynchronous updating of Mail Files makes it difficult to present the results of dissemination in estimated order of likelihood of relevance to the user (ranked order). This is discussed in Chapter 2.  Very little research has focused exclusively on the Mail Dissemination process. Most systems modify the algorithms they have established for retrospective search of document (item) databases to apply to Mail Profiles. Dissemination differs from the ad hoc search process in that thousands of user profiles are processed against one item versus the inverse and there is not a large relatively static database of items to be used in development of relevance ranking weights for an item.  Both implementers and researchers have treated the dissemination process as independent from the rest of the information system. The general assumption has been that the only knowledge available in making decisions on whether an incoming item is of interest is the user's profile and the incoming item. This restricted view has produced suboptimal systems forcing the user to receive redundant information that has little value. If a total Information Retrieval System view is taken, then the existing Mail and Index files are also potentially available during the dissemination process. This would allow the dissemination profile to be expanded to include logic against existing files. For example, assume an index file (discussed below) exists that has the price of oil from Mexico as a value in a field with a current value of $30. An analyst will be less interested in items that discuss Mexico and $30 oil prices then items that discuss Mexico and prices other than $30 (i.e., looking for changes). Similarly, if a Mail file already has many items on a particular topic, it would be useful for a profile to not disseminate additional items on the same topic, or at least reduce the relative importance that the system assigns to them (i.e., the rank value).  Selective Dissemination of Information has not yet been applied to multimedia sources.   In some cases where the audio is transformed into text, existing 18                                                                                                Chapter 1  textual algorithms have been applied to the transcribed text (e.g., the DARPA's TIDES Portal), but little research has gone into dissemination techniques for multimedia sources.
issr-0007	1.3.3  Document Database Search  The Document Database Search Process (see Figure 1.4) provides the capability for a query to search against all items received by the system. The Document Database Search process is composed of the search process, user entered queries (typically ad hoc queries) and the document database which contains all items that have been received, processed and stored by the system. It is the retrospective search source for the system. If the user is on-line, the Selective Dissemination of Information system delivers to the user items of interest as soon as they are processed into the system. Any search for information that has already been processed into the system can be considered a "retrospective" search for information. This does not preclude the search to have search statements constraining it to items received in the last few hours. But typically the searches span far greater time periods. Each query is processed against the total document database. Queries differ from profiles in that they are typically short and focused on a specific area of interest. The Document Database can be very large, hundreds of millions of items or more. Typically items in the Document Database do not change (i.e., are not edited) once received. The value of much information quickly decreases over time. These facts are often used to partition the database by time and allow for archiving by the time partitions. Some user interfaces force the user to indicate searches against items received older than a specified time, making use of the partitions of the Document database. The documents in the Mail files are also in the document database, since they logically are input to both processes.
issr-0008	1.3.4  index Database Search  When an item is determined to be of interest, a user may want to save it for future reference.   This is in effect filing it.   In an information system this is  accomplished via the index process. In this process the user can logically store an item in a file along with additional index terms and descriptive text the user wants to associate with the item. It is also possible to have index records that do not reference an item, but contain all the substantive information in the index itself. In this case the user is reading items and extracting the information of interest, never needing to go back to the original item. A good analogy to an index file is the card catalog in a library. Another perspective is to consider Index Files as structured databases whose records can optionally reference items in the Document Database. The Index Database Search Process (see Figure 1.4) provides the capability to create indexes and search them. The user may search the index and retrieve the index and/or the document It references- The system also provides the capability to search the index and  then search the items referenced by the index records that Introduction to Information Retrieval Systems                                              19  satisfied the index portion of the query. This is called a combined file search. In an ideal system the index record could reference portions of items versus the total item.  There are two classes of index files: Public and Private Index files. Every user can have one or more Private Index files leading to a very large number of files. Each Private Index file references only a small subset of the total number of items in the Document Database. Public Index files are maintained by professional library services personnel and typically index every item in the Document Database. There is a small number of Public Index files. These files have access lists (i.e., lists of users and their privileges) that allow anyone to search or retrieve data. Private Index files typically have very limited access lists.  To assist the users in generating indexes, especially the professional indexers, the system provides a process called Automatic File Build shown in Figure 1.4 (also called Information Extraction). This capability processes selected incoming documents and automatically determine potential indexing for the item. The rules that govern which documents are processed for extraction of index information and the index term extraction process are stored in Automatic File Build Profiles. When an item is processed it results in creation of Candidate Index Records. As a minimum, certain citation data can be determined and extracted as part of this process assisting in creation of Public Index Files. Examples of this information are author(s), date of publication, source, and references. More complex data, such as countries an item is about or corporations referenced, have high rates of identification. The placement in an index file facilitates normalizing the terminology, assisting the user in finding items. It also provides a basis for programs that analyze the contents of systems trying to identify new information relationships (i.e., data mining). For more abstract concepts the extraction technology is not accurate and comprehensive enough to allow the created index records to automatically update the index files. Instead the candidate index record, along with the item it references, are stored in a file for review and edit by a user prior to actual update of an index file.  The capability to create Private and Public Index Files is frequently implemented via a structured Database Management System. This has introduced new challenges in developing the theory and algorithms that allow a single integrated perspective on the information in the system. For example, how to use the single instance information in index fields and free text to provide a single system value of how the index/referenced item combination satisfies the user's search statement. Usually the issue is avoided by treating the aspects of the search that apply to the structured records as a first level constraint identifying a set of items that satisfy that portion of the query. The resultant items are then searched using the rest of the query and the functions associated with information systems. The evaluation of relevance is based only on this later step. An example of how this limits the user is if part of the index is a field called "Country." This certainly allows the user to constrain his results to only those countries of interest (e.g., Peru or Mexico).   But because the relevance function is only associated with the portion 20                                                                                                Chapter 1  of the query associated with the item, there is no way for the user to ensure that Peru items have more importance to the retrieval than Mexican items.
issr-0009	1.3.5 Multimedia Database Search  Chapter 10 provides additional details associated with multi-media search against different modalities of information. From a system perspective, the multimedia data is not logically its own data structure, but an augmentation to the existing structures in the Information Retrieval System. It will reside almost entirely in the area described as the Document Database. The specialized indexes to allow search of the multi-media (e.g., vectors representing video and still images, text created by audio transcription) will be augmented search structures. The original source will be kept as normalized digital real source for access possibly in their own specialized retrieval servers (e.g., the Real Media server, ORACLE Video Server, etc.) The correlation between the multi-media and the textual domains will be either via time or positional synchronization. Time synchronization is the example of transcribed text from audio or composite video sources. Positional synchronization is where the multi-media is localized by a hyperlink in a textual item. The synchronization can be used to increase the precision of the search process. Added relevance weights should be assigned when the multi-media search and the textual search result in hits in close proximity. For example when the image of Tony Blair is found in the section of a video where the transcribed audio is discussingTony Blair, then the hit is more likely then when either event occurs independently. The same would be true when the JPEG image hits on Tony Blair in a textual paragraph discussing him in an HTML item.  Making the multi-media data part of the Document Database also implies that the linking of it to Private and Public Index files will also operate the same way as with text.
issr-0010	1.4 Relationship to Database Management Systems  There are two major categories of systems available to process items:  Information Retrieval Systems and Data Base Management Systems (DBMS). Confusion   can  arise  when  the  software  systems   supporting  each   of these  applications get confiised with the data they are manipulating. An Information Retrieval System is software that has the features and functions required to  manipulate "information" items versus a DBMS that is optimized to handle "structured" data. Information is fuzzy text. The term "fuzzy*"' is used to imply the  results from the minimal standards or controls on the creators of the text items. The author is trying to present concepts, ideas and abstractions along with supporting facts. As such, there is minimal consistency in the vocabulary and styles of items discussing the exact same issue. The searcher has to be omniscient to specify all search term possibilities in the query. Introduction to Information Retrieval Systems                                              21  Structured data is well defined data (facts) typically represented by tables. There is a semantic description associated with each attribute within a table that well defines that attribute. For example, there is no confusion between the meaning of "employee name" or "employee salary" and what values to enter in a specific database record. On the other hand, if two different people generate an abstract for the same item, they can be different. One abstract may generally discuss the most important topic in an item. Another abstract, using a different vocabulary, may specify the details of many topics. It is this diversity and ambiguity of language that causes the fuzzy nature to be associated with information items. The differences in the characteristics of the data is one reason for the major differences in functions required for the two classes of systems.  With structured data a user enters a specific request and the results returned provide the user with the desired information. The results are frequently tabulated and presented in a report format for ease of use. In contrast, a search of "information" items has a high probability of not finding all the items a user is looking for. The user has to refine his search to locate additional items of interest. This process is called "iterative search." An Information Retrieval System gives the user capabilities to assist the user in finding the relevant items, such as relevance feedback (see Chapters 2 and 7). The results from an information system search are presented in relevance ranked order. The confusion comes when DBMS software is used to store "information." This is easy to implement, but the system lacks the ranking and relevance feedback features that are critical to an information system. It is also possible to have structured data used in an information system (such as TOPIC). When this happens the user has to be very creative to get the system to provide the reports and management information that are trivially available in a DBMS.  From a practical standpoint, the integration of DBMS's and Information Retrieval Systems is very important. Commercial database companies have already integrated the two types of systems. One of the first commercial databases to integrate the two systems into a single view is the INQUIRE DBMS. This has been available for over fifteen years. A more current example is the ORACLE DBMS that now offers an imbedded capability called CONVECTIS, which is an informational retrieval system that uses a comprehensive thesaurus which provides the basis to generate "themes" for a particular item. CONVECTIS also provides standard statistical techniques that are described in Chapter 5. The INFORMIX DBMS has the ability to link to RetrievalWare to provide integration of structured data and information along with functions associated with Information Retrieval Systems.
issr-0011	1.5 Digital Libraries and Data Warehouses  Two other systems frequently described in the context of information retrieval are Digital Libraries and Data Warehouses (or DataMarts).   There is 22                                                                                                Chapter 1  significant overlap between these two systems and an Information Storage and Retrieval System. All three systems are repositories of information and their primary goal is to satisfy user information needs. Information retrieval easily dates back to Vannevar Bush's 1945 article on thinking (Bush-45) that set the stage for many concepts in this area. Libraries have been in existence since the beginning of writing and have served as a repository of the intellectual wealth of society. As such, libraries have always been concerned with storing and retrieving information in the media it is created on. As the quantities of information grew exponentially, libraries were forced to make maximum use of electronic tools to facilitate the storage and retrieval process. With the worldwide interneting of libraries and information sources (e.g., publishers, news agencies, wire services, radio broadcasts) via the Internet, more focus has been on the concept of an electronic library. Between 1991 and 1993 significant interest was placed on this area because of the interest in U.S. Government and private funding for making more information available in digital form (Fox-93). During this time the terminology evolved from electronic libraries to digital libraries. As the Internet continued its exponential growth and project funding became available, the topic of Digital Libraries has grown. By 1995 enough research and pilot efforts had started to support the 1st ACM International Conference on Digital Libraries (Fox-96).  There remain significant discussions on what is a digital library. Everyone starts with the metaphor of the traditional library. The question is how do the traditional library functions change as they migrate into supporting a digital collection. Since the collection is digital and there is a worldwide communications infrastructure available, the library no longer must own a copy of information as long as it can provide access. The existing quantity of hardcopy material guarantees that we will not have all digital libraries for at least another generation of technology improvements. But there is no question that libraries have started and will continue to expand their focus to digital formats. With direct electronic access available to users the social aspects of congregating in a library and learning from librarians, friends and colleagues will be lost and new electronic collaboration equivalencies will come into existence (Wiederhold-95).  Indexing is one of the critical disciplines in library science and significant effort has gone into the establishment of indexing and cataloging standards. Migration of many of the library products to a digital format introduces both opportunities and challenges. The fiill text of items available for search makes the index process a value added effort as described in Section 1.3. Another important library service is a source of search intermediaries to assist users in finding information. With the proliferation of information available in electronic form, the role of search intermediary will shift from an expert in search to being an expert in source analysis. Searching will identify so much information in the global Internet information space that identification of the "pedigree" of information is required to understand its value. This will become the new refereeing role of a library.  Information Storage and Retrieval technology has addressed a small subset of the issues associated with Digital Libraries. The focus has been on the search and retrieval of textual data with no concern for establishing standards on Introduction to Information Retrieval Systems                                              23  the contents of the system. It has also ignored the issues of unique identification and tracking of information required by the legal aspects of copyright that restrict functions within a library environment. Intellectual property rights in an environment that is not controlled by any country and their set of laws has become a major problem associated with the Internet. The conversion of existing hardcopy text, images (e.g., pictures, maps) and analog (e.g., audio, video) data and the storage and retrieval of the digital version is a major concern to Digital Libraries. Information Retrieval Systems are starting to evolve and incorporate digitized versions of these sources as part of the overall system. But there is also a lot of value placed on the original source (especially printed material) that is an issue to Digital Libraries and to a lesser concern to Information Reteval systems. Other issues such as how to continue to provide access to digital information over many years as digital formats change have to be answered for the long term viability of digital libraries.  The term Data Warehouse comes more from the commercial sector than academic sources. It comes from the need for organizations to control the proliferation of digital information ensuring that it is known and recoverable. Its goal is to provide to the decision makers the critical information to answer future direction questions. Frequently a data warehouse is focused solely on structured databases. A data warehouse consists of the data, an information directory that describes the contents and meaning of the data being stored, an input function that captures data and moves it to the data warehouse, data search and manipulation tools that allow users the means to access and analyze the warehouse data and a delivery mechanism to export data to other warehouses, data marts (small warehouses or subsets of a larger warehouse), and external systems.  Data warehouses are similar to information storage and retrieval systems in that they both have a need for search and retrieval of information. But a data warehouse is more focused on structured data and decision support technologies. In addition to the normal search process, a complete system provides a flexible set of analytical tools to "mine" the data. Data mining (originally called Knowledge Discovery in Databases - KDD) is a search process that automatically analyzes data and extract relationships and dependencies that were not part of the database design. Most of the research focus is on the statistics, pattern recognition and artificial intelligence algorithms to detect the hidden relationships of data. In reality the most difficult task is in preprocessing the data from the database for processing by the algorithms. This differs from clustering in information retrieval in that clustering is based upon known characteristics of items, whereas data mining does not depend upon known relationships. For more detail on data mining see the November 1996 Communications of the ACM (Vol. 39, Number 11) that focuses on this topic.
issr-0012	24                                                                                                 Chapter 1  1.6 Summary  Chapter 1 places into perspective a total Information Storage and Retrieval System. This perspective introduces new challenges to the problems that need to be theoretically addressed and commercially implemented. Ten years ago commercial implementation of the algorithms being developed was not realistic, allowing theoreticians to limit their focus to very specific areas. Bounding a problem is still essential in deriving theoretical results. But the commercialization and insertion of this technology into systems like the Internet that are widely being used changes the way problems are bounded. From a theoretical perspective, efficient scalability of algorithms to systems with gigabytes and terabytes of data, operating with minimal user search statement information, and making maximum use of all functional aspects of an information system need to be considered. The dissemination systems using persistent indexes or mail files to modify ranking algorithms and combining the search of structured information fields and free text into a consolidated weighted output are examples of potential new areas of investigation.  The best way for the theoretician or the commercial developer to understand the importance of problems to be solved is to place them in the context of a total vision of a complete system. Understanding the differences between Digital Libraries and Information Retrieval Systems will add an additional dimension to the potential future development of systems. The collaborative aspects of digital libraries can be viewed as a new source of information that dynamically could interact with information retrieval techniques. For example, should the weighting algorithms and search techniques discussed later in this book vary against a corpus based upon dialogue between people versus statically published material? During the collaboration, in certain states, should the system be automatically searching for reference material to support the collaboration?
issr-0013	2 Information Retrieval System  Capabilities  2.1     Search Capabilties  2.2    Browse Capabilities  2.3    Miscellaneous Capabilities  2.4    Standards  2.5     Summary  This chapter discusses the major functions that are available in an Information Retrieval System. Search and browse capabilities are crucial to assist the user in locating relevant items. The search capabilities address both Boolean and Natural Language queries. The algorithms used for searching are called Boolean, natural language processing and probabilistic. Probabilistic algorithms use frequency of occurrence of processing tokens (words) in determining similarities between queries and items and also in predictors on the potential relevance of the found item to the searcher. Chapter 4 discusses in detail the data structures used to support the algorithms, and Chapters 5 and 7 describe the algorithms. The majority of existing commercial systems are based upon Boolean query and search capabilities. The newer systems such as TOPIC, RetrievalWare, and INQUERY all allow for natural language queries. With the introduction of multimedia searches comes new problems in how to represent queries in the different modalities and how to present the results of a search.  Given the imprecise nature of the search algorithms, Browse functions to assist the user in filtering the search results to find relevant information are very important. To allow different systems to inter-operate there are evolving standards in both the language and architecture areas. Standardization of the interfaces between systems will have the same effect on information systems that acceptance 28                                                                                               Chapter 2  of the Structured Query Language (SQL) has had in the Database Management System field. It will allow independent service providers to develop tools and augmentations that will be applicable to any Information Retrieval System accelerating the development of functions needed by the user. Examples of these functions are information visualization tools and query expansion tools.
issr-0014	2.1 Search Capabilities  The objective of the search capability is to allow for a mapping between a user's specified need and the items in the information database that will answer that need. The search query statement is the means that the user employs to communicate a description of the needed information to the system. It can consist of natural language text in composition style and/or query terms (referred to as terms in this book) with Boolean logic indicators between them. How the system translates the search query into processing steps to find the potential relevant items is described in later chapters. One concept that has occasionally been implemented in commercial systems (e.g., RetrievalWare), and holds significant potential for assisting in the location and ranking of relevant items, is the "weighting" of search terms. This would allow a user to indicate the importance of search terms in either a Boolean or natural language interface. Given the following natural language query statement where the importance of a particular search term is indicated by a value in parenthesis between 0.0 and 1.0 with 1.0 being the most important:  Find articles that discuss automobile emissions(.9) or sulfur dioxide(.3) on the farming industry.  the system would recognize in its importance ranking and item selection process that automobile emissions are far more important than items discussing sulfur dioxide problems.  The search   statement may apply to the  complete  item  or  contain  additional parameters limiting it to a logical division of the item (i.e., to a zone).  As discussed in Chapter 1, this restriction is useful in reducing retrieval of nonrelevant items by limiting the search to those subsets of the item whose use of a  particular word is consistent with the user's search objective. Finding a name in a Bibliography does not necessarily mean the item is about that person. Recent research has shown that for longer items, restricting a query statement to be satisfied within a contiguous subset of the document (passage searching) provides improved precision (Buckley-95, Wilkinson-95). Rather than allowing the search statement to be satisfied anywhere within a document it may be required to be satisfied within a 100 word contiguous subset of the item (Callan-94).  Based upon the algorithms used in a system many different fiinctions are associated with the system's understanding the search statement. The fiinctions define the relationships between the terms in the search statement (e.g., Boolean, Information Retrieval System Capabilities                                                     29  Natural Language, Proximity, Contiguous Word Phrases, and Fuzzy Searches) and the interpretation of a particular word (e.g., Term Masking, Numeric and Date Range, Contiguous Word Phrases, and Concept/Thesaurus expansion).  Rather than continuing the use of the term processing token to represent the searchable units extracted from an item, the terminology "word" or "term" is also used in some contexts as an approximation that is intuitively more meaningful to the reader.
issr-0015	2.1.1 Boolean Logic  Boolean logic allows a user to logically relate multiple concepts together to define what information is needed. Typically the Boolean functions apply to processing tokens identified anywhere within an item. The typical Boolean operators are AND, OR, and NOT. These operations are implemented using set intersection, set union and set difference procedures. A few systems introduced the concept of "exclusive or" but it is equivalent to a slightly more complex query using the other operators and is not generally useful to users since most users do not understand it. Placing portions of the search statement in parentheses are used to overtly specify the order of Boolean operations (i.e., nesting function). If parentheses are not used, the system follows a default precedence ordering of operations (e.g., typically NOT then AND then OR). In the examples of effects of Boolean operators given in Figure 2.1, no precedence order is given to the operators and queries are processed Left to Right unless parentheses are included. Most commercial systems do not allow weighting of Boolean queries. A technique to allow weighting Boolean queries is described in Chapter 7. Some of the deficiencies of use of Boolean operators in information systems are summarized by Belkin and Croft (Belkin-89).  A special type of Boolean search is called "M of N" logic. The user lists a set of possible search terms and identifies, as acceptable, any item that contains a subset of the terms. For example, "Find any item containing any two of the following terms: "AA," "BB," "CC." This can be expanded into a Boolean search that performs an AND between all combinations of two terms and "OR"s the results together ((AA AND BB) or (AA AND CC) or (BB AND CC)). Some search examples and their meanings are given in Figure 2.1. Most Information Retrieval Systems allow Boolean operations as well as allowing for the natural language interfaces discussed in Section 2.1.8. As noted in Chapter 1, very little attention has been focused on integrating the Boolean search functions and weighted information retrieval techniques into a single search result.
issr-0016	30                                                                                                Chapter 2  2.1.2 Proximity  Proximity is used to restrict the distance allowed within an item between two search terms. The semantic concept is that the closer two terms are  SEARCH STATEMENT         SYSTEM OPERATION  COMPUTER OR PROCESSOR NOT Select all items discussing Computers MAINFRAME                                        and/or Processors that do not discuss  Mainframes  COMPUTER OR (PROCESSOR NOT Select all items discussing Computers MAINFRAME)                                       and/or items that discuss Processors and  do not discuss Mainframes  COMPUTER AND NOT PROCESSOR Select all items that discuss computers OR MAINFRAME                                   and not processors or mainframes in the  item  Figure 2.1 Use of Boolean Operators  found in a text the more likely they are related in the description of a particular concept. Proximity is used to increase the precision of a search. If the terms COMPUTER and DESIGN are found within a few words of each other then the item is more likely to be discussing the design of computers than if the words are paragraphs apart. The typical format for proximity is:  TERM! within "m" "units" of TERM2  The distance operator "m" is an integer number and units are in Characters,  Words, Sentences, or Paragraphs. Certain items may have other semantic units that would prove useful in specifying the proximity operation. For very structured items, distances in characters prove useful. For items containing imbedded images (e.g., digital photographs), text between the images could help in precision when  the objective is in locating a certain image. Sometimes the proximity relationship  contains a direction operator indicating the direction (before or after) that the second term must be found within the number of units specified. The default is either direction. A special case of the Proximity operator is the Adjacent (ADJ) operator that normally has a distance operator of one and a forward only direction (i.e., in WAIS). Another special case Is where the distance is set to zero meaning within the same semantic unit. Some proximity search statement examples and their meanings are given in Figure 2.2.
issr-0017	Information Retrieval System Capabilities                                                    31  2.1.3 Contiguous Word Phrases  A Contiguous Word Phrase (CWP) is both a way of specifying a query term and a special search operator. A Contiguous Word Phrase is two or more words that are treated as a single semantic unit. An example of a CWP is "United States of America." It is four words that specify a search term representing a  SEARCH STATEMENT                         SYSTEM OPERATION  "Venetian" ADJ "Blind"                         would   find    items   that   mention    a  Venetian Blind on a window but not items discussing a Blind Venetian  "United"     within     five     words     of   would   hit   on    "United   States   and "American"                                              American  interests," "United Airlines  and American Airlines" not on "United States of America and the American dream"  "Nuclear"  within  zero paragraphs  of   would find items that have "nuclear" "clean-up"                                                and "clean-up" in the same paragraph.  Figure 2.2 Use of Proximity  single specific semantic concept (a country) that can be used with any of the operators discussed above. Thus a query could specify "manufacturing" AND "United States of America" which returns any item that contains the word "manufacturing" and the contiguous words "United States of America."  A contiguous word phrase also acts like a special search operator that is similar to the proximity (Adjacency) operator but allows for additional specificity. If two terms are specified, the contiguous word phrase and the proximity operator using directional one word parameters or the Adjacent operator are identical. For contiguous word phrases of more than two terms the only way of creating an equivalent search statement using proximity and Boolean operators is via nested Adjacencies which are not found in most commercial systems. This is because Proximity and Boolean operators are binary operators but contiguous word phrases are an "N"ary operator where "N" is the number of words in the CWP.  Contiguous Word Phrases are called Literal Strings in WAIS and Exact Phrases in RetrievalWare. In WAIS multiple Adjacency (ADJ) operators are used to define a Literal String (e.g., "United" ADJ "States" ADJ "of ADJ "America").
issr-0018	32                                                                                                Chapter 2  2.1.4 Fuzzy Searches  Fuzzy Searches provide the capability to locate spellings of words that are similar to the entered search term. This function is primarily used to compensate for errors in spelling of words. Fuzzy searching increases recall at the expense of decreasing precision (i.e., it can erroneously identify terms as the search term). In the process of expanding a query term fuzzy searching includes other terms that have similar spellings, giving more weight (in systems that rank output) to words in the database that have similar word lengths and position of the characters as the entered term. A Fuzzy Search on the term "computer" would automatically include the following words from the information database: "computer," "compiler," "conputer," "computter," "compute." An additional enhancement may lookup the proposed alternative spelling and if it is a valid word with a different meaning, include it in the search with a low ranking or not include it at all (e.g., "commuter"). Systems allow the specification of the maximum number of new terms that the expansion includes in the query. In this case the alternate spellings that are "closest" to the query term is included. "Closest" is a heuristic function that is system specific.  Fuzzy searching has its maximum utilization in systems that accept items that have been Optical Character Read (OCRed). In the OCR process a hardcopy item is scanned into a binary image (usually at a resolution of 300 dots per inch or more). The OCR process is a pattern recognition process that segments the scanned in image into meaningful subregions, often considering a segment the area defining a single character. The OCR process will then determine the character and translate it to an internal computer encoding (e.g., ASCII or some other standard for other than Latin based languages). Based upon the original quality of the hardcopy this process introduces errors in recognizing characters. With decent quality input, systems achieves in the 90 - 99 per cent range of accuracy. Since these are character errors throughout the text, fuzzy searching allows location of items of interest compensating for the erroneous characters.
issr-0019	2.1.5 Term Masking  Term masking is the ability to expand a query term by masking a portion of the term and accepting as valid any processing token that maps to the unmasked portion of the term. The value of term masking is much higher in systems that do  not perform stemming or only provide a very simple stemming algorithm. There are two types of search term masking: fixed length and variable length. Sometimes they are called fixed and variable length "don't care" functions.  Fixed length masking is a single position mask. It masks out any symbol in a particular position or the lack of that position in a word. Figure 2.3 gives an example of fixed term masking.   It not only allows any character in the masked Information Retrieval System Capabilities                                                   33  position, but also accepts words where the position does not exist. Fixed length term masking is not frequently used and typically not critical to a system.  Variable length "don't cares" allows masking of any number of characters within a processing token. The masking may be in the front, at the end, at both front and end, or imbedded. The first three of these cases are called suffix search, prefix search and imbedded character string search, respectively. The use of an imbedded variable length don't care is seldom used. Figure 2.3 provides examples of the use of variable length term masking. If "*" represents a variable length don't care then the following are examples of its use:  ""COMPUTER"                            Suffix Search  "COMPUTER*"                            Prefix Search  "?COMPUTER*"                          Imbedded String Search  Of the options discussed, trailing "don't cares" (prefix searches) are by far the most common. In operational systems they are used in 80-90 per cent of the search terms (Kracsony-81) and in may cases are a default without the user having to specify it.
issr-0020	2.1.6 Numeric and Date Ranges  Term masking is useful when applied to words, but does not work for finding ranges of numbers or numeric dates. To find numbers larger than "125," using a term "125*" will not find any number except those that begin with the  SEARCH STATEMENT                    SYSTEM OPERATION  multiSnational                                          Matches"multi-national,"  "multinational," "multinational" but does not match "multi national" since it is two processing tokens.  *computer*                                              Matches,"minicomputer"  "microcomputer" or "computer"  comput*                                                  Matches  "computers,"     "computing,"  "computes"  *comput*                                                 Matches          "microcomputers"  "minicomputing," "compute"  Figure 2.3 Term Masking 34                                                                                                Chapter 2  digits "125." Systems, as part of their normalization process, characterizes words as numbers or dates. This allows for specialized numeric or date range processing against those words. A user could enter inclusive (e.g., "125-425" or "4/2/935/2/95" for numbers and dates) to infinite ranges ("gt;125," "lt;=233," representing "Greater Than" or "Less Than or Equal") as part of a query.
issr-0021	2.1.7 Concept/Thesaurus Expansion  Associated with both Boolean and Natural Language Queries is the ability to expand the search terms via Thesaurus or Concept Class database reference tool. A Thesaurus is typically a one-level or two-level expansion of a term to other terms that are similar in meaning. A Concept Class is a tree structure that expands each meaning of a word into potential concepts that are related to the initial term (e.g., in the TOPIC system). Concept classes are sometimes implemented as a network structure that links word stems (e.g., in the RetrievalWare system). An example of Thesaurus and Concept Class structures are shown in Figure 2.4 (Thesaurus-93) and Figure 2.5. Concept class representations assist a user who has minimal knowledge of a concept domain by allowing the user to expand upon a particular concept showing related concepts. A concept based database shows associations that are not normally found in a language based thesaurus. For example, "negative advertising" may be linked to "elections" in a concept database, but are hopefully not synonyms to be found in a thesaurus. Generalization is associated with the user viewing concepts logically higher in a hierarchy that have more general meaning. Specificity is going lower in the thesaurus looking at concepts that are more specific.  Thesauri are either semantic or based upon statistics. A semantic thesaurus is a listing of words and then other words that are semantically similar. Electronic versions of thesauri are commercially available and are language based (e.g., English, Spanish, etc.). Systems such as RetrievalWare and TOPIC provide them as part of the search system. In executing a query, a term can be expanded to all related terms in the thesaurus or concept tree. Optionally, the user may display the thesaurus or concept tree and indicate which related terms should be used in a query. This function is essential to eliminate synonyms which introduce meanings that are not in the user's search statement. For example, a user searching on "pasture lands" and "fields" would not want all of the terms associated with "magnetic fields" included in the expanded search statement.  The capability usually exists to browse the thesaurus or concept trees and add additional terms and term relationships in the case of concept trees. This allows users to enhance the thesaurus or concept tree with jargon specific to their area of interest. Information Retrieval System Capabilities  35     COMPUTER        7 7  \   DATA PROCESSOR  MAINFRAME   PC  MULTITASKING COMPUTER  "~ó-. CPU               MINICOMPUTER   Figure 2.4 Thesaurus for term "computer"   COMPUTER HARDWARE   /   \   PROCESSOR  PERIPHERAL  COMPUTER SOFTWARE     OPERATING SYSTEM APPLICATION  NETWORK  Figure 2.5 Hierarchical Concept Class Structure for "Computer"  The problem with thesauri is that they are generic to a language and can introduce many search terms that are not found in the document database. An alternative uses the database or a representative sample of it to create statistically related terms. It is conceptually a thesaurus in that words that are statistically related to other words by their frequently occurring together in the same items. This type of thesaurus is very dependent upon the database being searched and may not be portable to other databases. The statistical techniques for generating a thesaurus are discussed in detail in Chapter 6. Irs a statistical thesaurus it is very difficult to name a thesaurus class or understand by viewing it 36                                                                                                Chapter 2  what caused its creation (i.e., there is not a semantic basis defining the classes). As such, statistical thesauri are frequently used as automatic expansions of user's searches without the user directly interacting with the thesaurus.  Theoretically thesauri and concept trees could be used to either expand a search statement with additional terms or make it more specific but substituting more specific terms. From this perspective expanding the terms increases the recall of the search with a possible decrease in precision. Going to more specific terms increases precision and possibly reduce recall. In most cases the generalization process is used in expanding a search statement with more terms.
issr-0022	2.1.8 Natural Language Queries  Rather than having the user enter a specific Boolean query by specifying search terms and the logic between them, Natural Language Queries allow a user to enter a prose statement that describes the information that the user wants to find. The longer the prose, the more accurate the results returned. The most difficult logic case associated with Natural Language Queries is the ability to specify negation in the search statement and have the system recognize it as negation. The system searches and finds those items most like the query statement entered. The techniques for locating items similar to the search statement (described in Chapters 5 and 7) are suited for finding items like other items but do not have inherent techniques to exclude items that are like a certain portion of the search statement. For many users, this type of an interface provides a natural extension to asking someone to perform a search. In this case the discourse is with the computer. An example of a Natural Language Query is:  Find for me all the items that discuss oil reserves and current attempts to find new oil reserves. Include any items that discuss the international financial aspects of the oil production process. Do not include items about the oil industry in the United States.  The way a system uses this input for a search Is described in Chapter 7. The problem with many techniques and systems is to understand the negation concept of excluding items about the oil industry in the United States.  When this capability has been made available, users have a tendency to enter sentence fragments that reflect their search need rather than complete  sentences. This is predictable because the users want to minimize use of the human resource (their time). The likely input for the above example is:  oil reserves and attempts to find new oil reserves, international financial aspects of oil production, not United States oil industry Information Retrieval System Capabilities                                                    37  This usage pattern is important because sentence fragments make morphological analysis of the natural language query difficult and may limit the system's ability to perform term disambiguation (e.g., understand which meaning of a word is meant).  Using the same search statement, a Boolean query attempting to find the same information might appear:  ("locate" AND "new" and "oil reserves") OR ("international" AND "financ*" AND "oil production") NOT ("oil industry" AND "United States")  Associated with natural language queries is a function called relevance feedback. The natural language does not have to be input by the user but just identified by the user. This introduces the concept of finding items that "are like" other items. Thus a user could identify a particular item(s) in the database or text segments within item(s) and use that as the search statement. This is discussed in detail in Chapter 7.  To accommodate the negation function and provide users with a transition to the natural language systems, most commercial systems have a user interface that provides both a natural language and Boolean logic capability. Negation is handled by the Boolean portion of a search. The integration of these two search statement types has not been achieved in Information Retrieval Systems. Natural language interfaces improve the recall of systems with a decrease in precision when negation is required.
issr-0023	2.1.9      Multimedia Queries  The user interface becomes far more complex with the introduction of the availability of multimedia items. All of the previous discussions still apply for search of the textual portions of a multimedia database. But in addition, the user has to be able to specify search terms for the other modalities. The current systems only focus on specification of still images as another search criteria. The still image could be used to search images that are part of an item. They also could be used to locate a specific scene in a video product. As described later, in the video modality, scene changes are extracted to represent changes in the information presentation. The scene changes are represented as a series of images. Additionally, where there is static text in the video, the current technology allows for OCRing the text (e.g., in the latest release of the VIRAGE system). The ability to search for audio as a match makes less sense as a user specification. To adequately perform the search you would have to simulate the audio segment and then look for a match. Instead audio sources are converted to searchable text via audio transcription. This allows queries to be applied to the text. But, like Optical Character Reading (OCR) output, the transcribed audio will contain many errors 38                                                                                               Chapter 2  (accuracies of 85-90% are the best that can be achieved from news broadcasts, conversational speech is in the range of 60%). Thus the search algorithms must allow for errors in the data. The errors are very different compared to OCR. OCR errors will usually create a text string that is not a valid word. In automatic speech recognition (ASR), all errors are other valid words since ASR selects entries ONLY from a dictionary of words. Audio also allows the user to search on specific speakers, since speaker identification is relatively accurate against audio sources.  The correlation between different parts of a query against different modalities is usually based upon time or location. The most common example would be on time. For example if a video news program has been indexed, the user could have access to the scene changes, the transcribed audio, the closed captioning and the index terms that a user has assigned while displaying the video. The query could be "Find where Bill Clinton is discussing Cuban refugees and there is a picture of a boat". All of the separate tracks of information are correlated on a time basis. The system would return those locations where Bill Clinton is identified as the speaker (user the audio track and speaker identification), where in any of the text streams (OCRed text from the video, transcribed audio, closed captioning, or index terms) there is discussion of refugees and Cuba, and finally during that time segment there is at least one scene change that includes a boat.
issr-0024	2.2        Browse Capabilities  Once the search is complete, Browse capabilities provide the user with the capability to determine which items are of interest and select those to be displayed. There are two ways of displaying a summary of the items that are associated with a query: line item status and data visualization. From these summary displays, the user can select the specific items and zones within the items for display. The system also allows for easy transitioning between the summary displays and review of specific items. If searches resulted in high precision, then the importance of the browse capabilities would be lessened. Since searches return many items that are not relevant to the user's information need, browse capabilities can assist the user in focusing on items that have the highest likelihood in meeting his need.
issr-0025	2.2.1 Ranking  Under Boolean systems, the status display is a count of the number of items found by the query.  Every one of the items meet all aspects of the Boolean  query. The reasons why an item was selected can easily be traced to and displayed (e.g., via highlighting) in the retrieved items. Hits are retrieved in either a sorted order (e.g., sort by Title) or in time order from the newest to the oldest item. With the introduction of ranking based upon predicted relevance values, the status summary displays the relevance score associated with the item along with a brief descriptor of the item (usually both fit on one display screen line).  The relevance Information Retrieval System Capabilities                                                    39  score is an estimate of the search system on how closely the item satisfies the search statement. Typically relevance scores are normalized to a value between 0.0 and 1.0. The highest value of 1.0 is interpreted that the system is sure that the item is relevant to the search statement. This allows the user to determine at what point to stop reviewing items because of reduced likelihood of relevance. Theoretically every item in the system could be returned but many of the items will have a relevance value of 0.0 (not relevant). Practically, systems have a default minimum value which the user can modify that stops returning items that have a relevance value below the specified value. In addition to ranking based upon the characteristics of the item and the database, in many circumstances collaborative filtering is providing an option for selecting and ordering output. In this case, users when reviewing items provide feedback to the system on the relative value of the item being accessed. The system accumulates the various user rankings and uses this information to order the output for other user queries that are similar. Collaborative filtering has been very successful in sites such as AMAZON.COM MovieFinder.com, and CDNow.com in deciding what products to display to users based upon their queries (Herlocker-99)  Since one line is usually dedicated per item in a summary display, part of a zone truncated by allocated space on the display is typically displayed with the relevance weight of the item. This zone is frequently the Title and provides the user with additional information with the relevance weight to avoid selecting nonrelevant items for review. Presenting the actual relevance number seems to be more confusing to the user than presenting a category that the number falls in. For example, some systems create relevance categories and indicate, by displaying items in different colors, which category an item belongs to. Other systems uses a nomenclature such as High, Medium High, Medium, Low, and Non-relevant. The color technique removes the need for written indication of an item's relevance, thereby providing additional positions in a line to display more of the title but causes problems with users that suffer from partial or total color blindness.  Rather than limiting the number of items that can be assessed by the number of lines on a screen, other graphical visualization techniques showing the relevance relationships of the hit items can be used. For example, a two or three dimensional graph can be displayed where points on the graph represent items and the location of the points represent their relative relationship between each other and the user's query. In some cases color is also used in this representation. This technique allows a user to see the clustering of items by topics and browse through a cluster or move to another topical cluster. This has an analogy of moving through the stacks at a library. In a single image the user can see the effects of his search statement rather than displaying a few items at a time.  Information visualization is also being used in displaying individual items and the terms that contributed to the item's selection. This graphical display assists the user in determining how to reformulate his query to improve finding the information the user requires.   Chapter 8 describes information visualization.
issr-0026	40                                                                                                Chapter 2  2.2.2 Zoning  When the user displays a particular item, the objective of minimization of overhead still applies. The user wants to see the minimum information needed to determine if the item is relevant. Once the determination is made an item is possibly relevant, the user wants to display the complete item for detailed review. Limited display screen sizes require selectability of what portions of an item a user needs to see to make the relevance determination. For example, display of the Title and Abstract may be sufficient information for a user to predict the potential relevance of an item. Limiting the display of each item to these two zones allows multiple items to be displayed on a single display screen. This makes maximum use of the speed of the user's cognitive process in scanning the single image and understanding the potential relevance of the multiple items on the screen.  Related to zoning for use in minimizing what an end user needs to review from a hit item is the idea of locality and passage based search and retrieval. In this case the basic search unit is not the complete item, but an algorithmic defined subdivision of the item. This has been known as passage retrieval where the item is divided into uniform-sized passages that are indexed (Kaskiel-97, Knaus-95, Zobel-95) and locality based retrieval where the passage boundaries can be dynamic (Kretser-99.) In these cases the system can display the particular passage or locality that caused the item to be found rather than the complete item. The system would also provide an expand capability to retrieve the complete item as an option.
issr-0027	2.2.3 Highlighting  Another display aid is an indication of why an item was selected. This indication, frequently highlighting, lets the user quickly focus on the potentially  relevant parts of the text to scan for item relevance. Different strengths of highlighting indicates how strongly the highlighted word participated in the  selection of the item. Most systems allow the display of an item to begin with the first highlight within the item and allow subsequent jumping to the next highlight. Another capability, which is gaining strong acceptance, is for the system to determine the passage in the document most relevant to the query and position the browse to start at that passage. The DCARS system that acts as a user frontend to the RetrievalWare search system allows the user to browse an item in the order of the paragraphs or individual words that contributed most to the rank value associated with the item.  Highlighting has always been useful in Boolean systems to indicate the cause of the retrieval This is because of the direct mapping between the terms in the search and the terms in the item. Using Natural Language Processing, automatic expansion of terms via thesauri, and the similarity ranking algorithms Information Retrieval System Capabilities                                                    41  discussed in detail later in this book, highlighting loses some of its value The terms being highlighted that caused a particular item to be returned may not have direct or obvious mapping to any of the search terms entered. This causes frustration by the user trying to guess why a particular item was retrieved and how to use that information in reformulating the search statement to make it more exact. In a ranking system different terms can contribute to different degrees to the decision to retrieve an item. The highlighting may vary by introducing colors and intensities to indicate the relative importance of a particular word in the item in the decision to retrieve the item. Information visualization appears to be a better display process to assist in helping the user formulate his query than highlights in items.
issr-0028	2.3        Miscellaneous Capabilities  There are many additional functions that facilitate the user's ability to input queries, reducing the time it takes to generate the queries, and reducing a priori the probability of entering a poor query. Vocabulary browse provides knowledge on the processing tokens available in the searchable database and their distribution in terms of items within the database. Iterative searching and search history logs summarize previous search activities by the user allowing access to previous results from the current user session. Canned queries allow access to queries generated and saved in previous user sessions.
issr-0029	2.3.1 Vocabulary Browse  Vocabulary Browse provides the capability to display in alphabetical sorted order words from the document database. Logically, all unique words (processing tokens) in the database are kept in sorted order along with a count of the number of unique items in which the word is found. The user can enter a word or word fragment and the system will begin to display the dictionary around the entered text. Figure 2.6 shows what is seen in vocabulary browse if the user enters "comput." The system indicates what word fragement the user entered and then alphabetically displays other words found in the database in collating sequence on either side of the entered term. The user can continue scrolling in either direction reviewing additional terms in the database. Vocabulary browse provides information on the exact words in the database. It helps the user determine the impact of using a fixed or variable length mask on a search term and potential mis-spellings. The user can determine that entering the search term "compul*" in effect is searching for "compulsion" or "compulsive" or "compulsory." It also shows that someone probably entered the word "computen" when they really meant "computer." It provides insight on the impact of using terms in a search. By vocabulary browsing, a term may be seen to exist in a large 42                                                                                                Chapter 2  number of documents which could make it a poor candidate as an ORed term requiring additional ANDed terms to focus on items of interest. The search term "computer" would return an excessive number of hits if used as an "OR" term.  TERM                                         OCCURRENCES  compromise                                                           53  comptroller                                                            18 compulsion                                                              5  compulsive                                                             22 compulsory                                                              4  comput  computation                                                           265  compute                                                                 1245 com pu ten                                                                  1  computer                                                                10,800 computerize                                                             18  computes                                                                 29  Figure 2.6 Vocabulary Browse List with entered term "comput"
issr-0030	2.3.2 Iterative Search and Search History Log  Frequently a search returns a Hit file containing many more items than the user wants to review. Rather than typing in a complete new query, the results of the previous search can be used as a constraining list to create a new query that is applied against it.   This has the same effect as taking the original query and  adding additional search statement against it in an AND condition. This process of refining the results of a previous search to focus on relevant items is called iterative search. This also applies when a user uses relevance feedback to enhance a previous search.  During a login session, a user could execute many queries to locate the needed information. To facilitate locating previous searches as starting points for new searches, search history logs are available. The search history log is the capability to display all the previous searches that were executed during the current session. The query along with the search completion status showing number of hits is displayed.
issr-0031	Information Retrieval System Capabilities                                                    43  2.3.3  Canned Query  The capability to name a query and store it to be retrieved and executed during a later user session is called canned or stored queries. Users tend to have areas of interest within which they execute their searches on a regular basis. A canned query allows a user to create and refine a search that focuses on the user's general area of interest one time and then retrieve it to add additional search criteria to retrieve data that is currently needed. For example, a user may be responsible for European investments. Rather than always having to create a query that limits the search to European geographic search terms and then the specific question requiring resolution, a canned query can be created with all the needed geographic terms and used as a starting point for additional query specification. Significant effort can go into making the canned query into a comprehensive and focused search since it is created once but used many times with additional search terms for specific information needs. Queries that start with a canned query are significantly larger than ad hoc queries. Canned query features also allow for variables to be inserted into the query and bound to specific values at execution time.
issr-0032	2.3.4  Multimedia  Once a list of potential items that satisfy the query are discovered, the techniques for displaying them when they are multimedia introduces new challenges. The discussions under Ranking, above, suggest that typically hits are listed in the likely order of importance with one hit per line. The rationale is to present the most information possible to the user on the largest number of item to allow the user to select the items to be retrieved. To display more aggregate data, textual interfaces sometimes allow for clustering of the hits and then use of graphical display to show a higher level view of the information. Neither of these techniques lend themselves well when the information is multimodal. The textual aspect of the multimedia can be used to apply all of the techniques described above. But using the example in 2.1.9 above, how does the system also include the image of the boat that was part of the query. Typically a "thumbnail" of the image is displayed with the hit item. But this has the disadvantage of using more than one line per hit and reducing the number of hits that a user can select from on a single screen. If the source is audio, then other problems associated with the human linear processing of audio becomes major issues. In the textual domain, users can visually scan text very fast and via peripheral processing can maintain the context of what they are reading. If the output against audio searches was audio hit files, the user processing rate associated with listening drops dramatically. This is another reason why the transcribed audio (even if it is errorful) becomes a critical augmentation in users reviewing audio files. Thus in addition to listening to the audio, the user can visually be following the transcribed text. This provides a mechanism for the user to percieve the context and additionally provides a quick 44                                                                                                Chapter 2  scanning option to look ahead at upcoming information to be used in conjunction with the audio processing of the original source. In tests recently performed (May 2000 in paper to be published) it was shown that the combination of errorful transcribed audio used in conjunction with the having the original audio also available reduced the processing time of items by 50 per cent from audio only (this was the initial testing, experience would drop it even further). The transcribed text could also be used as a navigation technique through the audio (Whittaker-99.) They appropriately label this new paradigm What You See Is (Almost) What You Hear (WYSIAWYH). They also noted that even though the transcribed text could be used as an index into future retrieval of the audio source, most users also needed the ability to annotate the transcriptions (note taking) to allow them to include other audio information such as in tonal information provided by the original speech.  The second area of complexity comes from any attempt at ranking such an output. In the textual only domain, there has been significant research into algorithms and heuristics associated with weighting algorithms for textual items. But when the hit is composed of different modalities, (e.g., text, image, audio), how did you create an aggregate weight of each "hit region". Little research has gone into the derivation of the weight assigned to an item that combines the weights assigned to how well each modality satisfied its portion of the query.
issr-0033	2.4 Z39.50 and WAIS Standards  There are two potential sources for any standard, those agreed to by an official standards organization and a de facto standard based upon high usage across a large user population. The standards organization most involved in Information Systems standards in the United States is the American National Standards Institute/ National Information Standards Organization (ANSI/NISO) in generating its Z39.50, Information Retrieval Application Service Definition and Protocol Specification. NISO is the only organization accredited by ANSI to approve and maintain standards for information services, libraries and publishers. This standard is one of many standards they generated to support interconnection of computer systems. Its relationship to other standards can be seen in the Open Systems Interconnection (OSI) basic reference model (ISO 7498). In addition to the formal standard, a second de facto standard is the WAIS standard based upon its usage in the INTERNET.  In addition to standards associated with specific language interfaces of Information Retrieval Systems, there are attempts being made to standardize the architecture of information systems. The largest de facto information system is the Internet and the Internet Engineering Task Force is focusing on how the architecture of the Internet should be modified to allow future scalability and addressability  of items on  the  Internet.     This  architecture  directly  affects Information Retrieval System Capabilities                                                     45  commercial information systems usage on the Internet and the integratability of future research into this environment.  The Z39.50 standard does not specify an implementation, but the capabilities within an application (Application Service) and the protocol used to communicate between applications (Information Retrieval Application Protocol). It is a computer to computer communications standard for database searching and record retrieval. Its objective is to overcome different system incompatibilities associated with multiple database searching (e.g., unique user interfaces, command language, and basic search functions). The first version of Z39.50 was approved in 1992. An international version of Z39.50, called the Search and Retrieve Standard (SR), was approved by the International Organization for Standardization (ISO) in 1991. Z39.50-1995, the latest version of Z39.50, replaces SR as the international information retrieval standard.  The standard describes eight operation types: Init (initialization), Search, Present, Delete, Scan, Sort, Resource-report, and Extended Services. There are five types of queries (Types 0, 1,2, 100, 101, and 102). Type 101 extends previous types allowing for proximity and Type-102 is a Ranked List Query (yet to be defined). The Extended Services include saving query results, queries, and updating the database.  The client is identified as the "Origin" and performs the communications functions relating to initiating a search, translation of the query into a standardized format, sending a query, and requesting return records. The server is identified as the "Target" and interfaces to the database at the remote responding to requests from the Origin (e.g., pass query to database, return records in a standardized format and status). The end user does not have to be aware of the details of the standard since the Origin function performs the mapping from the user's query interface into Z39.50 format. This makes the dissimilarities of different database systems transparent to the user and facilitates issuing one query against multiple databases at different sites returning to the user a single integrated Hit file.  The communications between the Origin and Target utilize a dialogue known as Z39.50 association. Z39.50 not only standardizes the messages to be exchanged between the Origin and Target systems, but also the structure and the semantics of the search query, the sequence of message exchange, and the mechanism for returning records (Turner-95, Kunze-95). The 1992 version of Z39.50 was focused on library functions such as database searching, cataloguing and interlibrary loan (primarily MARC bibliographic record structure). Z39.50 version 3 (in Z39.50-1995) addresses additional functions to support nonbibliographic data such as fiill text documents and images. It also begins to address some of the functions being defined as necessary to information systems such as ranking values. Z39.50-1995 has just been approved by NISO and is published as the new standard (the latest information on Z39.5Q is available on the Worldwide Web at http://ds.internic.net/z3950/z3950.html).  Wide Area Information Service (WAIS) is the de facto standard for many search environments on the INTERNET.      WAIS was developed by a project 46                                                                                                Chapter 2  started in 1989 by three commercial companies (Apple, Thinking Machines, and Dow Jones). The original idea was to create a program that would act as a personal librarian. It would act like a personal agent keeping track of significant amounts of data and filtering it for the information most relevant to the user. The interface concept was user entered natural language statements of topics the user had interest. In addition it provided the capability to communicate to the computer that a particular item was of interest and have the computer automatically find similar items (i.e., relevance feedback). The original corporate interest was in Apple providing the user interface, Thinking Machines providing the processing power, and Dow Jones providing the data.  The developers of WAIS pursued a generalized system of information retrieval that could access data collections all around the world (Hahn-94, Levine94) on the Internet. Like other Internet services, free versions of WAIS were originally provided. Some of the initial products migrated to a commercial company that sells and supports a WAIS system. A free version of WAIS is still available via the Clearinghouse for Networked Information Discovery and Retrieval (CINDIR) called "FreeWAIS."  The original development of WAIS started with the 1988 Z39.50 protocol as a base following the client/server architecture concept. At that time Z39.50 was focused on a bibliographic MARC record structure against structured files. Numerous deficiencies were identified in the Z39.50 protocol forcing the developers of WAIS to vary from the Z39.50 standard (ORION-93). The developers incorporated the information retrieval concepts that allow for ranking, relevance feedback and natural language processing functions that apply to full text searchable databases. Since they were diverging from the standard, they decided to simplify their client server interface. In particular they decided to have a nonMarkovian process. Thus state information is not kept between requests between clients and servers. This is one of the major differences between WAIS and Z39.50 compliant systems.  The availability of FreeWAIS in the public domain made it the method of choice for implementing databases on the INTERNET. The architecture gained significant momentum from the mandate that all U.S. Government Agencies publish their material electronically and make it accessible to the general public. WAIS and the INTERNET became the standard approach for answering the mandate. Additionally, many organizations are using WAIS as the engine to archive and access large amounts of textual information. The appeal for WAIS is that, for public domain software, it represents a well tested and documented product that can be trusted with data. A substantial community continues to test, fix and enhance the basic system. The current trend is away from WAIS and to a standard Internet interface (WEB interface) and using one of the more powerful search systems.  Another activity being driven by the publishing and Digital Libraries efforts is the creation of a unique way of identifying, naming and controlling documents in an information retrieval environment.  It has the same paradigm as Information Retrieval System Capabilities                                                    47  the TIPSTER architecture of separating the indexes and surrogate search structures from storing the actual item. The leader in this effort is the Center for National Research Initiatives (CNRI) that is working with the Department of Defense and also the American Association of Publishers (AAP), focusing on an Internet implementation that allows for control of electronic published and copyright material. The CNRI concept is based upon using a "handle" system where the handle is the unique network identifier of a digital object. The AAP prefers the term Digital Object Identifier over the term handle (CNRI-97). Most computer systems identify an item by the location it is stored (e.g., Uniform Resource Locators (URLs) on the Internet - see Section 4.7 for more detail). From a library and large information system perspective it is far more efficient to refer to items by name rather than location. There is still significant debate over whether the name should be just a unique identifier (e.g., a number) or also have semantic content to the user. The term "handle" refers to this unique name. A Handle server (similar to document manager in TIPSTER) ensures persistence, location independence and multiple instance management. Persistence ensures that the handle is available to locate information potentially past the life of the organization that created the item to identify any locations of the item. Location independence allows for the movement of items with a mechanism for knowing their current location. Multiple instance allows keeping track of the location of duplicates of an item. Inherent in this architecture is a unique "Handle" naming authority(s) and servers to assign names and keep track of locations. This is similar to the Domain Name Servers used in networks, but is designed to handle orders of magnitude more objects in efficient fashion.  In addition to the Handle Server architecture, CNRI is also advocating a communications protocol to retrieve items from existing systems. This protocol call Repository Archive Protocol (RAP) defines the mechanisms for Clients to use the handles to retrieve items. It also includes other administrative functions such as privilege validation. The Handle system is designed to meet the Internet Engineering Task Force (IETF) requirements for naming Internet objects via Uniform Resource Names to replace URLs as defined in the Internet's RFC-1737 (IETF-96).
issr-0034	2.5 Summary  Chapter 2 provides an overview of the functions commonly associated  with Information Retrieval Systems. These functions define the user's view of the system versus the internal implementation that is described throughout the rest of this book. Until the early 1990s, the pressure on development of new user functions that assist the user in locating relevant items was driven by the academic community. The commercialization of information retrieval functions being driven by the growth of the Internet has changed the basis of development time from "academic years" (i.e., one academic year equals 18 months - the time to 48                                                                                                Chapter 2  define the research, perform it and publish the results) to "Web years" (i.e., one Web year equals three months - demand to get new products up very quickly to be first). The test environment and test databases are changing from small scale academic environments to millions of records with millions of potential users testing new ideas. Even IBM, one of the most traditional, conservative companies, has an "alpha" site available on the Internet which contains the latest visualization and search software that is still in the process of being developing.  The areas to expect Web year changes in capabilities is in functions to assist the user in expanding his query, application of the above functions into a multilingual environment (i.e., the Internet provides information in many languages), and most importantly tools to support information visualization capabilities. The short queries that the typical user enters return too much data. The research community continues to development algorithms that is used to improve the precision and recall of the user's search. Automatic learning from user's queries coupled with large thesauri and concept dictionaries performs the query expansion process. New visualization tools have the most significant impact by allowing use of human cognitive processing to interpret the results of a user's search statement and focus on the items that most likely are relevant. Visualization tools also assist the users in enhancing their queries to find needed information. The basic search capabilities described in this chapter will not change much, but significant improvements can be expected in the browse capabilities. The underlying reason for these advancements is the need to optimize the human resource in finding needed information.
issr-0035	3 Cataloging and Indexing  3.1    History and Objectives of Indexing  3.2    Indexing Process  3.3    Automatic Indexing  3.4    Information Extraction  3.5    Summary  The first two chapters of this book presented the architecture of a total Information Storage and Retrieval System and the basic functions that apply to it. One of the most critical aspects of an information system that determines its effectiveness is how it represents concepts in items. The transformation from the received item to the searchable data structure is called Indexing, This process can be manual or automatic, creating the basis for direct search of items in the Document Database or indirect search via Index Files. Rather than trying to create a searchable data structure that directly maps to the text in the input items, some systems transform the item into a completely different representation that is concept based and use this as the searchable data structure. The concept weighting schemes have demonstrated the capability to find items that the traditional weighted and non-weighted data structures have missed. Systems that use a specialized hardware text search processor do not require the searchable data structure, but search the original standardized documents (see Chapter 9).  Once the searchable data structure has been created, techniques must be defined that correlate the user-entered query statement to the set of items in the database to determine the items to be returned to the user. This process is called Search and is often different between searches applied to the document database (called ad hoc queries) and searches against incoming items to determine the Mail File(s) the item should be delivered to (called dissemination searches). In the newer systems a by-product of the search process is a relative value for each item 52                                                                                                Chapter 3  with respect to its correlation to the query constraints. This value is used in ranking the item. In some cases a simplified ranking algorithm is applied to the items found from the search process after it is completed.  Closely associated with the indexing process is the information extraction process. Its goal is to extract specific information to be normalized and entered into a structured database (DBMS). It is similar to the process of creating the search structure for an item in that both must locate concepts in the item. Information extraction differs because it focuses on very specific concepts and contains a transformation process that modifies the extracted information into a form compatible with the end structured database. This process was referred to in Chapter 2 as Automatic File Build. Another way information extraction can be used is in the generation of a summary of an item. The emphasis changes from extracting facts to go into index fields to extracting larger contextual constructs (e.g., sentences) that are combined to form a summary of an item.
issr-0036	3.1 History and Objectives of Indexing  To understand the system design associated with creation and manipulation of the searchable data structures, it is necessary to understand the objectives of the indexing process. Reviewing the history of indexing shows the dependency of information processing capabilities on manual and then automatic processing systems. Through most of the 1980's the goals of commercial Information Retrieval Systems were constrained to facilitating the manual indexing paradigm. In the I990's, exponential growth in computer processing capabilities with a continuing decrease in cost of computer systems has allowed Information Retrieval Systems to implement the previously theoretical functions introducing, a new information retrieval paradigm.
issr-0037	3.1.1 History  Indexing  (originally  called   Cataloging)   is  the  oldest  technique  for  identifying the contents of items to assist in their retrieval. The objective of cataloging is to give access points to a collection that are expected and most useful to the users of the information. The basic information required on an item, what is the item and what it is about, has not changed over the centuries. As early as the third-millennium, in Babylon, libraries of cuneiform tablets were arranged by subject (Hyman-89). Up to the 19th Century there was little advancement in cataloging, only changes in the methods used to represent the basic information (Norris-69). In the late 1800s subject indexing became hierarchical (e.g., Dewey Decimal System). In 1963 the Library of Congress initiated a study on the computerization of bibliographic surrogates. From 1966 - 1968 the Library of Congress ran its MARC I pilot project. MARC (MAchine Readable Cataloging) standardizes the structure, contents and coding of bibliographic records.    The Cataloging and Indexing                                                                              53  system became operational in 1969 (Avram-75). The earliest commercial cataloging system is DIALOG, which was developed by Lockheed Corporation in 1965 for NASA. It became commercial in 1978 with three government files of indexes to technical publications. By 1988, when it was sold to Knight-Ridder, DIALOG contained over 320 index databases used by over 91,000 subscribers in 86 countries (Harper-81).  Indexing (cataloging), until recently, was accomplished by creating a bibliographic citation in a structured file that references the original text. These files contain citation information about the item, keywording the subject(s) of the item and, in some systems a constrained length free text field used for an abstract/summary. The indexing process is typically performed by professional indexers associated with library organizations. Throughout the history of libraries, this has been the most important and most difficult processing step. Most items are retrieved based upon what the item is about. The user's ability to find items on a particular subject is limited by the indexer creating index terms for that subject. But libraries and library indexing have always assumed the availability of the library staff to act if needed as a human intermediary for users having problems in locating information. For users looking for well-defined data (e.g., people by name and titles) have good success by themselves. But when users are searching for topics they fail on 70% of single query requests and 45% of the time in ever finding the data they need. But when the users consult with a librarian the failure rates drop to 10% (Nordlie-99.) Thus library based indexing was never under significant pressure to invent user interfaces, support material and augmented search engines that would assure users could find the material they needed. They could rely on human interaction to resolve the more complex information needs.  The initial introduction of computers to assist the cataloguing function did not change its basic operation of a human indexer determining those terms to assign to a particular item. The standardization of data structures (e.g., MARC format) did allow sharing of the indexes between libraries. It reduced the manual overhead associated with maintaining a card catalog. By not having to make physical copies of the index card for every subject index term, it also encouraged inclusion of additional index terms. But the process still required the indexer to enter index terms that are redundant with the words in the referenced item. The user, instead of searching through physical cards in a card catalog, now performed a search on a computer and electronically displayed the card equivalents.  In the 1990s, the significant reduction in cost of processing power and memory in modem computers, along with access to the full text of an item from the publishing stages in electronic form, allow use of the full text of an item as an alternative to the indexer-generated subject index. The searchable availability of the text of items has changed the role of indexers and allowed introduction of new techniques to facilitate the user in locating information of interest. The indexer is no longer required to enter index terms that are redundant with words in the text of an item. The searcher is no longer presented a list of potential item of interest, but is additionally informed of the likelihood that each item satisfies his search goal.
issr-0038	54                                                                                                Chapter 3  3.1.2 Objectives  The objectives of indexing have changed with the evolution of Information Retrieval Systems. Availability of the full text of the item in searchable form alters the objectives historically used in determining guidelines for manual indexing. The full text searchable data structure for items in the Document File provides a new class of indexing called total document indexing. In this environment, all of the words within the item are potential index descriptors of the subject(s) of the item. Chapter 1 discusses the process of Item normalization that takes all possible words in an item and transforms them into processing tokens used in defining the searchable representation of an item. In addition to determining the processing tokens, current systems have the ability to automatically weight the processing tokens based upon their potential importance in defining the concepts in the item.  The first reaction of many people is to question the need for manual indexing at all, given that total document indexing is available for search. If one can search on any of the words in a document why does one need to add additional index terms? Previously, indexing defined the source and major concepts of an item and provided a mechanism for standardization of index terms (i.e., use of a controlled vocabulary). A controlled vocabulary is a finite set of index terms from which all index terms must be selected (the domain of the index). In a manual indexing environment, the use of a controlled vocabulary makes the indexing process slower, but potentially simplifies the search process. The extra processing time comes from the indexer trying to determine the appropriate index terms for concepts that are not specifically in the controlled vocabulary set. Controlled vocabularies aide the user in knowing the domain of terms that the indexer had to select from and thus which terms best describe the information needed. Uncontrolled vocabularies have the opposite effect, making indexing faster but the search process much more difficult.  The availability of items in electronic form changes the objectives of manual indexing. The source information (frequently called citation data) can automatically be extracted. There is still some utility to the use of indexes for index term standardization. Modern systems, with the automatic use of thesauri and other reference databases, can account for diversity of language/vocabulary use and thus reduce the need for controlled vocabularies. Most of the concepts discussed in the document is locatable via search of the total document index. The primary use of manual subject indexing now shifts to abstraction of concepts and judgments on the value of the information. The automatic text analysis algorithms can not consistently perform abstraction on all concepts that are in an item. They can not correlate the facts in an item in a cause/effect relationship to determine additional related concepts to be indexed. An item that is discussing the increase in water temperatures at factory discharge locations could be discussing "economic stability" of a country that has fishing as its major industry.    It requires the Cataloging and Indexing                                                                              55  associative capabilities of a human being to make the connection. A computer system would typically not be able to correlate the changes in temperature to economic stability. The additional index terms added under this process enhance the recall capability of the system. For certain queries it may also increase the precision. This processing deficiency indicates the potential for future enhancements of Information Retrieval Systems with Artificial Intelligence techniques.  The words used in an item do not always reflect the value of the concepts being presented. It is the combination of the words and their semantic implications that contain the value of the concepts being discussed. The utility of a concept is also determined by the user's need. The Public File indexer needs to consider the information needs of all users of the library system. Individual users of the system have their own domains of interest that bound the concepts in which they are interested. It takes a human being to evaluate the quality of the concepts being discussed in an item to determine if that concept should be indexed. The difference in "user need" between the library class of indexers and the individual users is why Private Index files are an essential part of any good information system. It allows the user to logically subset the total document file into folders of interest including only those documents that, in the user's judgment, have future value. It also allows the user to judge the utility of the concepts based upon his need versus the system need and perform concept abstraction. Selective indexing based upon the value of concepts increases the precision of searches.  Availability of full document indexing saves the indexer from entering index terms that are identical to words in the document. Users may use Public Index files as part of their search criteria to increase the recall. They may want to constrain the search by their Private Index file to increase the precision of the search. Figure 3.1 shows the potential relationship between use of the words in an item to define the concepts. Public Indexing of the concept adds additional index terms over the words in the item to achieve abstraction. The index file use fewer terms than found in the items because it only indexes the important concepts. Private Index files are even more focused, limiting the number of items indexed to those that have value to the user and within items only the concepts bounded by the specific user's interest domain. There is overlap between the Private and Public Index files, but the Private Index file is indexing fewer concepts in an item than the Public Index file and the file owner uses his specific vocabulary of index terms.  In addition to the primary objective of representing the concepts within an item to facilitate the user's finding relevant information, electronic indexes to items provide a basis for other applications to assist the user. The format of the index, in most cases, supports the ranking of the output to present the items most likely to be relevant to the user's information needs first (see Chapters 5 and 7). Also, the index can be used to cluster items by concept (see Chapter 6). The clustering of items has the effect of making an electronic system similar to a physical library.   The paradigm of going to the library and browsing the book 56  Chapter 3  shelves in a topical area is the same as electronically browsing through items clustered by concepts.
issr-0039	3.2        Indexing Process  When an organization with multiple indexers decides to create a public or private index some procedural decisions on how to create the index terms assist the indexers and end users in knowing what to expect in the index file. The first decision is the scope of the indexing to define what level of detail the subject index will contain. This is based upon usage scenarios of the end users. The other decision is the need to link index terms together in a single index for a particular concept.  PUBLIC INDEX FILE  PRIVATE INDEX FILE  DOCUMENT FILE  Figure 3.1  Items Overlap Between Full Item Indexing, Public File Indexing and Private File Indexing  Linking Index terms is needed when there are multiple independent concepts found within an item.
issr-0040	Cataloging and Indexing                                                                              57  3.2.1      Scope of Indexing  When performed manually, the process of reliably and consistently determining the bibliographic terms that represent the concepts in an item is extremely difficult. Problems arise from interaction of two sources: the author and the indexer. The vocabulary domain of the author may be different than that of the indexer, causing the indexer to misinterpret the emphasis and possibly even the concepts being presented. The indexer is not an expert on all areas and has different levels of knowledge in the different areas being presented in the item. This results in different quality levels of indexing. The indexer must determine when to stop the indexing process.  There are two factors involved in deciding on what level to index the concepts in an item: the exhaustivity and the specificity of indexing desired. Exhaustivity of indexing is the extent to which the different concepts in the item are indexed. For example, if two sentences of a 10-page item on microprocessors discusses on-board caches, should this concept be indexed? Specificity relates to the preciseness of the index terms used in indexing. For example, whether the term "processor" or the term "microcomputer" or the term "Pentium" should be used in the index of an item is based upon the specificity decision. Indexing an item only on the most important concept in it and using general index terms yields low exhaustivity and specificity. This approach requires a minimal number of index terms per item and reduces the cost of generating the index. For example, indexing this paragraph would only use the index term "indexing." High exhaustivity and specificity indexes almost every concept in the item using as many detailed terms as needed. Under these parameters this paragraph would have "indexing," "indexer knowledge," "exhaustivity" and "specificity" as index terms. Low exhaustivity has an adverse effect on both precision and recall. If the full text of the item is indexed, then low exhaustivity is used to index the abstract concepts not explicit in the item with the expectation that the typical query searches both the index and the full item index. Low specificity has an adverse effect on precision, but no effect to a potential increase in recall.  Another decision on indexing is what portions of an item should be indexed. The simplest case is to limit the indexing to the Title or Title and Abstract zones. This indexes the material that the author considers most important and reduces the costs associated with indexing an item. Unfortunately this leads to loss of both precision and recall.  Weighting of index terms is not common in manual indexing systems. Weighting is the process of assigning an importance to an index term's use in an item. The weight should represent the degree to which the concept associated with the index term is represented in the item. The weight should help in discriminating the extent to which the concept is discussed in items In the database. The manual process of assigning weights adds additional overhead on the indexer and requires a more complex data structure to store the weights.
issr-0041	58                                                                                                Chapter 3  3.2.2      Precoordination and Linkages  Another decision on the indexing process is whether linkages are available between index terms for an item. Linkages are used to correlate related attributes associated with concepts discussed in an item. This process of creating term linkages at index creation time is called precoordination. When index terms are not coordinated at index time, the coordination occurs at search time. This is called postcoordination, that is coordinating terms after (post) the indexing process. Postcoordination is implemented by "AND"ing index terms together, which only finds indexes that have ail of the search terms.  Factors that must be determined in the linkage process are the number of terms that can be related, any ordering constraints on the linked terms, and any additional descriptors are associated with the index terms (Vickery-70). The range of the number of index terms that can be linked is not a significant implementation issue and primarily affects the design of the indexer's user interface. When multiple terms are being used, the possibility exists to have relationships between the terms. For example, the capability to link the source of a problem, the problem and who is affected by the problem may be desired. Each term must be caveated with one of these three categories along with linking the terms together into an instance of the relationships describing one semantic concept. The order of the terms is one technique for providing additional role descriptor information on the index terms. Use of the order of the index terms to implicitly define additional term descriptor information limits the number of index terms that can have a role descriptor. If order is not used, modifiers may be associated with each term linked to define its role. This technique allows any number of terms to have the associated role descriptor. Figure 3.2 shows the different types of linkages. It assumes that an item discusses the drilling of oil wells in Mexico by CITGO and the introduction of oil refineries in Peru by the U.S. When the linked capability is added, the system does not erroneously relate Peru and Mexico since they are not in the same set of linked items. It still does not have the ability to discriminate between which country is introducing oil refineries into the other country. Introducing roles in the last two examples of Figure 3.2 removes this ambiguity. Positional roles treat the data as a vector allowing only one value per position. Thus if the example is expanded so that the U.S. was introducing oil refineries in Peru, Bolivia and Argentina, then the positional role technique would require three entries, where the only difference would be in the value in the "affected country" position. When modifiers are used, only one entry would be required and all three countries would be listed with three "MODIFIERS.
issr-0042	3.3 AUTOMATIC INDEXING  Automatic indexing is the capability for the system to automatically determine the index terms to be assigned to an item. The simplest case is when all words in the document are used as possible index terms (total document indexing). Cataloging and Indexing                                                                             59  More complex processing is required when the objective is to emulate a human indexer and determine a limited number of index terms for the major concepts in the item. As discussed, the advantages of human indexing are the ability to determine concept abstraction and judge the value of a concept. The disadvantages of human indexing over automatic indexing are cost, processing  INDEX TERMS                                         Methodology  oil, wells, Mexico, CITGO, refineries,                No linking of terms  Peru, BP, drilling  (oil wells, Mexico, drilling, CITGO)                     linked (Precoordination)  (U.S.,oil refineries, Peru, introduction)  (CITGO, drill, oil wells, Mexico)                          linked (Precoordination)  (U.S., introduction, oil refineries, Peru)                 with position indicating role  (SUBJECT: CITGO;                               linked (Pre-coordination)  ACTION: drilling;                                with modifier indicating role  OBJECT: oil,wells MODIFIER: in Mexico)  (SUBJECT:U.S.; ACTION:introduces; OBJECT: oil refineries; MODIFIER: in Peru)  Figure 3.2 Linkage of Index Terms  time and consistency.   Once the initial hardware cost is amortized, the costs of  automatic indexing are absorbed as part of the normal operations and maintenance costs of the computer system. There are no additional indexing costs versus the salaries and benefits regularly paid to human indexers.  Processing time of an item by a human Indexer varies significantly based upon the indexer's knowledge of the concepts being indexed, the exhaustivlty and specificity guidelines and the amount and accuracy of preprocessing via Automatic File Build. Even for relatively short items (e.g., 300 - 500 words) it normally takes at least five minutes per item. A significant portion of this time is caused by the human interaction with the computer (e.g., typing speeds, cursor positioning, correcting spelling errors, taking breaks between activities).   Automatic indexing 60                                                                                                Chapter 3  requires only a few seconds or less of computer time based upon the size of the processor and the complexity of the algorithms to generate the index.  Another advantage to automatic indexing is the predictably of algorithms. If the indexing is being performed automatically, by an algorithm, there is consistency in the index term selection process. Human indexers typically generate different indexing for the same document. In an experiment on consistency in TREC-2, there was, on the average, a 20 per cent difference in judgment of the same item's topics between the original and a second independent judge of over 400 items (Harman-95). Since the judgments on relevance are different, the selection of index terms and their weighting to reflect the topics is also different. In automatic indexing, a sophisticated researcher understands the automatic process and be able to predict its utility and deficiencies, allowing for compensation for system characteristics in a search strategy. Even the end user, after interacting with the system, understands for certain classes of information and certain sources, the ability of the system to find relevant items is worse than other classes and sources. For example, the user may determine that searching for economic issues is far less precise than political issues in a particular newspaper based information system. The user may also determine that it is easier to find economic data in a information database containing Business Weekly than the newspaper source.  Indexes resulting from automated indexing fall into two classes: weighted and unweighted. In an unweighted indexing system, the existence of an index term in a document and sometimes its word location(s) are kept as part of the searchable data structure. No attempt is made to discriminate between the value of the index terms in representing concepts in the item. Looking at the index, it is not possible to tell the difference between the main topics in the item and a casual reference to a concept. This architecture is typical of the commercial systems through the 1980s. Queries against unweighted systems are based upon Boolean logic and the items in the resultant Hit file are considered equal in value. The last item presented in the file is as likely as the first item to be relevant to the user's information need.  In a weighted indexing system, an attempt is made to place a value on the index term's representation of its associated concept in the document. An index term's weight is based upon a fiinction associated with the frequency of occurrence of the term in the item. Luhe, one of the pioneers in automatic indexing, introduced the concept of the "resolving power" of a term. Luhn postulated that the significance of a concept in an item is directly proportional to the frequency of use of the word associated with the concept in the document (Luhn-58, Salton-75). This is reinforced by the studies of Brookstein, Klein and Raita that show "content bearing" words are not randomly distributed (i.e., Poisson distributed), but that their occurrence "clump" within items (Brookstein-95). Typically, values for the index terms are normalized between zero and one. The higher the weight, the more the term represents a concept discussed in the item. The weight can be adjusted to account for other information such as the number of items in the database that contain the same concept (sec Chapter 5). Cataloging and Indexing                                                                               61  The query process uses the weights along with any weights assigned to terms in the query to determine a scalar value (rank value) used in predicting the likelihood that an item satisfies the query. Thresholds or a parameter specifying the maximum number of items to be returned are used to bound the number of items returned to a user (see Chapter 7). The results are presented to the user in order of the rank value from highest number to lowest number.  Automatic indexing can either try to preserve the original text of an item basing the final set of searchable index values on the original text or map the item into a completely different representation, called concept indexing, and use the concepts as a basis for the final set of index values. The automatic indexing techniques are introduced in this section and later described in detail in Chapter 5.
issr-0043	3.3.1      Indexing by Term  When the terms of the original item are used as a basis of the index process, there are two major techniques for creation of the index: statistical and natural language. Statistical techniques can be based upon vector models and probabilistic models with a special case being Bayesian models. They are classified as statistical because their calculation of weights use statistical information such as the frequency of occurrence of words and their distributions in the searchable database. Natural language techniques also use some statistical information, but perform more complex parsing to define the final set of index concepts.  Often weighted systems are discussed as vectorized information systems. This association comes from the SMART system at Cornell University created by Dr. Gerald Salton (Salton-73, Salton-83). The system emphasizes weights as a foundation for information detection and stores these weights in a vector form. Each vector represents a document and each position in a vector represents a different unique word (processing token) in the database. The value assigned to each position is the weight of that term in the document. A value of zero indicates that the word was not In the document. The system and its associated research results have been evolving for over 30 years. Queries can be translated into the vector form. Search is accomplished by calculating the distance between the query vector and the document vectors.  In addition to a vector model, the other dominant approach uses a probabilistic model. The model that has been most successful in this area is the Bayesian approach. This approach is natural to information systems and is based upon the theories of evidential reasoning (drawing conclusions from evidence). Bayesian approaches have long been applied to information systems (Maron-60). The Bayesian approach could be applied as part of index term weighting, but usually is applied as part of the retrieval process by calculating the relationship between an item and a specific query. A Bayesian network is a directed acyclic graph in which each node represents a random variable and the arcs between the nodes represent a probabilistic dependence between the node and  its parents 62                                                                                                Chapter 3  (Howard-81, Pearl-88).   Figure 3.3 shows the basic weighting approach for index terms or associations between query terms and index terms.  Figure 3.3 Two-level Bayesian network  The nodes C] and C2 represent "the item contains concept C," and the F nodes represent "the item has feature (e.g., words) Fy." The network could also be interpreted as C representing concepts in a query and F representing concepts in an item. The goal is to calculate the probability of Q given Fy. To perform that calculation two sets of probabilities are needed:  1.    The prior probability P(Q) that an item is relevant to concept C  2.    The conditional probability P(Fjj/Q) that the features Fy where] = 1, m are present in an item given that the item contains topic Q.  The automatic indexing task is to calculate the posterior probability P(C,/F,i, ... ,Fim), the probability that the item contains concept C, given the presence of features F,j. The Bayes inference formula that is used is:  P(C/Flh ..., Fta) = P(C,) P(F,,,..., FJCdnhn .ª , F!m).  If the goal is to provide ranking as the result of a search by the posteriors, the Bayes rule can be simplified to a linear decision rule:  where I(Flk) is an indicator variable that equals ! only if Flk is present in the item (equals zero otherwise) and w is a coefficient corresponding to a specific feature/concept pair. A careful choice of w produces a ranking in decreasing order that is equivalent to the order produced by the posterior probabilities. Interpreting the coefficients, w, as weights corresponding to each feature (e.g., index term) and the function g as the sum of the weights of the features, the result of applying the formula is a set of term weights (Fung-95). Cataloging and Indexing                                                                              63  Another approach to defining indexes to items is via use of natural language processing. The DR-LINK (Document Retrieval through Linguistic Knowledge) system processes items at the morphological, lexical, semantic, syntactic, and discourse levels (Liddy-93, Weiner-95). Each level uses information from the previous level to perform its additional analysis. The discourse level is abstracting information beyond the sentence level and can determine abstract concepts using pre-defined models of event relationships. This allows the indexing to include specific term as well as abstract concepts such as time (e.g., differentiates between a company was sold versus a company will be sold). Normal automatic indexing does a poor job at identifying and extracting "verbs" and relationships between objects based upon the verbs.
issr-0044	3.3.2 Indexing by Concept  The basis for concept indexing is that there are many ways to express the same idea and increased retrieval performance comes from using a single representation. Indexing by term treats each of these occurrences as a different index and then uses thesauri or other query expansion techniques to expand a query to find the different ways the same thing has been represented. Concept indexing determines a canonical set of concepts based upon a test set of terms and uses them as a basis for indexing all items This is also called Latent Semantic Indexing because it is indexing the latent semantic information in items. The determined set of concepts does not have a label associated with each concept (i.e., a word or set of words that can be used to describe it), but is a mathematical representation (e.g., a vector).  An example of a system that uses concept indexing is the MatchPlus system developed by HNC Inc. The MatchPlus system uses neural networks to facilitate machine learning of concept/word relationships and sensitivity to similarity of use (Caid-93). The systems goal is to be able to determine from the corpus of items, word relationships (e.g., synonyms) and the strength of these relationships and use that information in generating context vectors. Two neural networks are used. One neural network learning algorithm generates stem context vectors that are sensitive to similarity of use and another one performs query modification based upon user feedback.  Word stems, items and queries are represented by high dimensional (at least 300 dimensions) vectors called context vectors. Each dimension in a vector could be viewed as an abstract concept class. The approach is based upon cognitive science work by Waltz and Pollack (Waltx-85). To define context vectors, a set of n features are selected on an ad hoc basis (e.g., high frequency terms after removal of stop words). The selection of the initial features Is not critical since they evolve and expand to the abstract concept classes used in the indexing process. For any word stem kf its context vector V* is an /i-dimensional vector with each component./ interpreted as follows: 64                                                                                               Chapter 3  Yk positive if k is strongly associated with feature j \k´ 0 if word k is not associated with feature j Yk negative if word k contradicts feature y  The interpretation of components for concept vectors is exactly the same as weights in neural networks. Each of the n features is viewed as an abstract concept class. Then each word stem is mapped to how strongly it reflects each concept in the items in the corpus. There is overlap between the concept classes (features) providing a distributed representation and insulating against a small number of entries for context vectors that could have no representation for particular stems (Hinton-84). Once the context vectors for stems are determined, they are used to create the index for an item. A weighted sum of the context vectors for all the stems in the item is calculated and normalized to provide a vector representation of the item in terms of the n concept classes (features). Chapter 5 provides additional detail on the specific algorithms used. Queries (natural language only) go through the same analysis to determine vector representations. These vectors are then compared to the item vectors.
issr-0045	3.3.3 Multimedia Indexing  Indexing associated with multimedia differs from the previous discussions of indexing. The automated indexing takes place in multiple passes of the information versus just a direct conversion to the indexing structure. The first pass in most cases is a conversion from the analog input mode into a digital structure. Then algorithms are applied to the digital structure to extract the unit of processing of the different modalities that will be used to represent the item. In an abstract sense this could be considered the location of a processing token in the modality. This unit will then undergo the final processing that will extract the searchable features that represent the unit. Indexing video or images can be accomplished at the raw data level (e.g., the aggregation of raw pixels), the feature level distinguishing primitive attributes such as color and luminance, and at the semantic level where meaningful objects are recognized (e.g., an airplane in the image/video frame). An example is processing of video. The system (e.g., Virage) will periodically collect a frame of video input for processing. It might compare that frame to the last frame captured to determine the differences between the frames. If the difference is below a threshold it will discard the frame. For a frame requiring processing, it will define a vector that represents the different features associated with that frame. Each dimension of the vector represents a different feature level aspect of the frame. The vector then becomes the unit of processing in the search system. This is similar to processing an image. Semantic level indexing requires pattern recognition of objects within the images.  Examples can be found Cataloging and Indexing                                                                              65  in MITs Photobook (Pentland-94), IBM's QBIC (Niblack-93) and the MultiMedia Datablade from Informix/Virage (Bach-96.)  If you consider an analog audio input, the system will convert the audio to digital format and determine the phonemes associated with the utterances. The phonemes will be used as input to a Hidden Markov Search model (see Chapter 4 and Chapter 10), that will determine with a confidence level the words that were spoken. A single phoneme can be divided into four states for the Markov model. It is the textual words assocaited with the audio that becomes the searchable structure.  In addition to storing the extracted index searchable data, a multimedia item needs to also store some mechanism to correlate the different modalities during search. There are two main mechanisms that are used, positional and temporal. Positional is used when the modalities are interspersed in a linear sequential composition. For example a document that has images or audio inserted, can be considered a linear structure and the only relationship between the modalities will be the juxtaposition of each modality. This would allow for a query that would specify location of an image of a boat within one paragraph of "Cuba and refugees".  The second mechanism is based upon time because the modalities are executing concurrently. The typical video source off television is inherently a multimedia source. It contains video, audio, and potentially closed captioning. Also the creation of multimedia presentations are becoming more common using the Synchronized Multimedia Integration Language (SMIL). It is a mark-up language designed to support multimedia presentations that integrate text (e.g., from slides or free running text) with audio, images and video. In both of these examples, time is the mechanism that is used to synchronize the different modalities. Thus the indexing must include a time-offset parameter versus a physical displacement. It also suggests that the proximity to increase precision will be based upon time concurrency (or ranges) versus physical proximity.
issr-0046	3.4 Information Extraction  There are two processes associated with information extraction: determination of facts to go into structured fields in a database and extraction of  text that can be used to summarize an item. In the first case only a subset of the important facts in an item may be identified and extracted. In summarization all of the major concepts in the item should be represented in the summary.  The process of extracting facts to go into indexes is called Automatic File Build in Chapter 1. Its goal is to process incoming items and extract index terms that will go into a structured database. This differs from indexing in that its objective is to extract specific types of information versus understanding all of the t^xt of the document. An Information Retrieval System's goal is to provide an indepth   representation   of the  total   contents   of an   item   (Sundheim-92).   An 66                                                                                                Chapter 3  Information Extraction system only analyzes those portions of a document that potentially contain information relevant to the extraction criteria. The objective of the data extraction is in most cases to update a structured database with additional facts. The updates may be from a controlled vocabulary or substrings from the item as defined by the extraction rules. The term "slot" is used to define a particular category of information to be extracted. Slots are organized into templates or semantic frames. Information extraction requires multiple levels of analysis of the text of an item. It must understand the words and their context (discourse analysis). The processing is very similar to the natural language processing described under indexing.  In establishing metrics to compare information extraction, the previously defined measures of precision and recall are applied with slight modifications to their meaning. Recall refers to how much information was extracted from an item versus how much should have been extracted from the item. It shows the amount of correct and relevant data extracted versus the correct and relevant data in the item. Precision refers to how much information was extracted accurately versus the total information extracted.  Additional metrics used are overgeneration and fallout. Overgeneration measures the amount of irrelevant information that is extracted. This could be caused by templates filled on topics that are not intended to be extracted or slots that get filled with non-relevant data. Fallout measures how much a system assigns incorrect slot fillers as the number of potential incorrect slot fillers increases (Lehnert-91).  These measures are applicable to both human and automated extraction processes. Human beings fall short of perfection in data extraction as well as automated systems. The best source of analysis of data extraction is from the Message Understanding Conference Proceedings. Conferences (similar to TREC) were held in 1991, 1992, 1993 and 1995. The conferences are sponsored by the Advanced Research Project Agency/Software and Intelligent Systems Technology Office of the Department of Defense. Large test databases are made available to any organization interested in participating in evaluation of their algorithms. In MUC-5 (1993), four experienced human analysts performed detailed extraction against 120 documents and their performance was compared against the top three information extraction systems. The humans achieved a 79 per cent recall with 82 per cent precision. That is, they extracted 79 per cent of the data they could have found and 18 per cent of what they extracted was erroneous. The automated programs achieved 53 per cent recall and 57 per cent precision. The other mediating factor is the costs associated with information extraction. The humans required between 15 and 60 minutes to process a single item versus the 30 seconds to three minutes required by the computers. Thus the existing algorithms are not operating close to what a human can achieve, but they are significantly cheaper. A combination of the two in a computer-assisted information extraction system appears the most reasonable solution in the foreseeable future.  Another related information technology is document summarization. Rather than trying to determine specific facts, the goal of document summarization Cataloging and Indexing                                                                              67  is to extract a summary of an item maintaining the most important ideas while significantly reducing the size. Examples of summaries that are often part of any item are titles, table of contents, and abstracts with the abstract being the closest. The abstract can be used to represent the item for search purposes or as a way for a user to determine the utility of an item without having to read the complete item. It is not feasible to automatically generate a coherent narrative summary of an item with proper discourse, abstraction and language usage (Sparck Jones-93). Restricting the domain of the item can significantly improve the quality of the output (Paice-93, Reimer-88). The more restricted goals for much of the research is in finding subsets of the item that can be extracted and concatenated (usually extracting at the sentence level) and represents the most important concepts in the item. There is no guarantee of readability as a narrative abstract and it is seldom achieved. It has been shown that extracts of approximately 20 per cent of the complete item can represent the majority of significant concepts (Morris-92). Different algorithms produces different summaries. Just as different humans create different abstracts for the same item, automated techniques that generate different summaries does not intrinsically imply major deficiencies between the summaries. Most automated algorithms approach summarization by calculating a score for each sentence and then extracting the sentences with the highest scores. Some examples of the scoring techniques are use of rhetorical relations (e.g., reason, direction, contrast: see Miike-94 for experiments in Japanese), contextual inference and syntactic coherence using cue words (Rush-71), term location (Salton-83), and statistical weighting properties discussed in Chapter 5. There is no overall theoretic basis for the approaches leading to many heuristic algorithms. Kupiec et al. are pursuing statistical classification approach based upon a training set reducing the heuristics by focusing on a weighted combination of criteria to produce "optimal" scoring scheme (Kupiec-95). They selected the following five feature sets as a basis for their algorithm:  Sentence Length Feature that requires sentence to be over five words in  length  Fixed Phrase Feature that looks for the existence of phrase "cues" (e.g., "in conclusion)  Paragraph Feature that places emphasis on the first tee and last five paragraphs in an item and also the location of the sentences within the paragraph  Thematic Word Feature that uses word frequency  Uppercase Word Feature that places emphasis on proper names and acronvms. 68                                                                                               Chapter 3  As with previous experiments by Edmundson, Kupiec et al. discovered that location based heuristics gives better results than the frequency based features (Edmundson-69).  Although there is significant overlap in the algorithms and techniques for information extraction and indexing items for information retrieval, this text does not present more detail on information extraction. For additional information, the MUC proceedings from Morgan Kaufman Publishers, Inc. in San Francisco is one source of the latest detailed information on information extraction.
issr-0047	3.5 Summary  This chapter introduces the concepts behind indexing. Historically, term indexing was applied to a hum an-generated set of terms that could be used to locate an item. With the advent of computers and the availability of text in electronic form, alternatives to human indexing are available and essential. There is too much information in electronic form to make it feasible for human indexing of each item. Thus automated indexing techniques are absolutely essential. When humans performed the indexing, there were guidelines on the scope of the indexing process. They were needed to ensure that the human indexers achieved the objectives of a particular indexing effort. The guidelines defined the level of detail to which the indexing was to be applied (i.e., exhaustivity and specificity). In automated systems there is no reason not to index to the lowest level of detail. The strength in manual indexing was the associative powers of the human indexer in consolidating many similar ideas into a small number of representative index terms and knowing when certain concepts were of such low value as to not warrant indexing. Automated indexing systems try to achieve these by using weighted and natural language systems and by concept Indexing. The reliance of automated systems on statistical information alone never achieve totally accurate assignment of importance weights to the concepts being indexed. The power of language is not only in the use of words but also the elegance of their combinations.  The goal of automatic indexing is not to achieve equivalency to human processing, but to achieve sufficient interpretation of items to allow users to locate needed information with the minimum amount of wasted effort. Even the human indexing process has left much to be desired and caused significant energy by the user to locate all of the needed information.  As difficult as determining index terms is, text summarization encounters an even higher level of complexity. The focus of text summarization is still on just the location of text segments that adequately represent an item. The combining of these segments into a readable "abstract" is still an unachievable goal. In the near term, a summarization that may not be grammatically correct but adequately covers the concepts in an item can be used by user to determine if the complete item should be read in detail. Cataloging and Indexing                                                                              69  The importance of the algorithms being developed for automatic indexing can not be overstated. The original text of items is not being searched. The extracted index information is realistically the only way to find information. The weaker the theory and implementation of the indexing algorithms is, the greater the impact on the user in wasting energy to find needed information. The Global Information Infrastructure (e.g., the Internet) is touching every part of our lives from academic instruction to shopping and getting news. The indexing and search algorithms drives the success of this new aspect of everyday life.
issr-0048	4 Data Structure  4.1    Introduction to Data Structures  4.2    Stemming Algorithms  4.3    Inverted File Structure  4.4    N-Gram Data Structure  4.5    PAT Data Structure  4.6    Signature File Structure  4.7    Hypertext and XML Data Structures  4.8    Hidden Markov Models  4.9    Summary  Knowledge of data structures used in Information Retrieval Systems provides insights into the capabilities available to the systems that implement them. Each data structure has a set of associated capabilities that provide an insight into the objectives of the implementers by its selection. From an Information Retrieval System perspective, the two aspects of a data structure that are important are its ability to represent concepts and their relationships and how well it supports location of those concepts. This chapter discusses the major logical data structures that are used in information retrieval systems. The implementation of a data structure (e.g., as an object, linked list, array, hashed file) is discussed only as an example. A description of Hidden Markov Models (HMMs) is included in Section 8. HMMs are starting to be used as a new approach for searching for information.
issr-0049	72  Chapter 4  4.1 Introduction to Data Structures  There are usually two major data structures in any information system. One structure stores and manages the received items in their normalized form. The process supporting this structure is called the "document manager." The other major data structure contains the processing tokens and associated data to support search. Figure 4.1 expands the document file creation function in Figure 1.4 from Chapter 1, showing the document manager function. Details on the creation of processing tokens can be found in Section 1.3.1. The results of a search are references to the items that satisfy the search statement, which are passed to the document manager for retrieval. This chapter focuses on data structures used to support the search function. It does not address the document management function nor the data structures and other related theory associated with the parsing of queries. For that background the reader should pursue a text on finite automata and language (regular expressions).             ITEM    NORMALIZATION         [ DOCUMENT FILE     CREATION      '^*ó    DOCUMENT  DOCUMENT  MANAGER  SEARCH MANAGER       I     r-------^^ CO        T3       m     ZJ    o U  5gt; ój O    -n ^   33 O O Ot(T1     MNAI  HFIL    ^ó ó^    m lt; O   Figure 4.1  Major Data Structures Data Structure                                                                                              73  One of the first transformations often applied to data before placing it in the searchable data structure is stemming. Stemming reduces the diversity of representations of a concept (word) to a canonical morphological representation. The risk with stemming is that concept discrimination information may be lost in the process, causing a decrease in precision and the ability for ranking to be performed. On the positive side, stemming has the potential to improve recall.  The most common data structure encountered in both data base and information systems is the inverted file system (discussed in Section 4.3). It minimizes secondary storage access when multiple search terms are applied across the total database. All commercial and most academic systems use inversion as the searchable data structure. A variant of the searchable data structure is the N-gram structure that breaks processing tokens into smaller string units (which is why it is sometimes discussed under stemming) and uses the token fragments for search. Ngrams have demonstrated improved efficiencies and conceptual manipulations over full word inversion. PAT trees and arrays view the text of an item as a single long stream versus a juxtaposition of words. Around this paradigm search algorithms are defined based upon text strings. Signature files are based upon the idea of fast elimination of non-relevant items reducing the searchable items to a manageable subset. The subset can be returned to the user for review or other search algorithms may be applied to it to eliminate any false hits that passed the signature filter.  A special data structure that is becoming common place because of its use on the Internet is hypertext. This structure allows the creator of an item to manually or automatically create imbedded links within one item to a related item.
issr-0050	4.2         Stemming Algorithms  The concept of stemming has been applied to information systems from their initial automation in the 1960's. The original goal of stemming was to improve performance and require less system resources by reducing the number of unique words that a system has to contain. With the continued significant increase in storage and computing power, use of stemming for performance reasons is no longer as important. Stemming is now being reviewed for the potential improvements it can make In recall versus its associated decline in precision. A system designer can trade off the increased overhead of stemming in creating processing tokens versus reduced search time overhead of processing query terms with trailing "don't cares" (see Section 2.1.5 Term Masking) to include all of their variants. The stemming process creates one large index for the stem versus Term Masking which requires the merging (ORing) of the indexes for every term that matches the search term.
issr-0051	74                                                                                                 Chapter 4  4.2.1      Introduction to the Stemming Process  Stemming algorithms are used to improve the efficiency of the information system and to improve recall. Conflation is the term frequently used to refer to mapping multiple morphological variants to a single representation (stem). The premise is that the stem carries the meaning of the concept associated with the word and the affixes (endings) introduce subtle modifications to the concept or are used for syntactical purposes. Languages have precise grammars that define their usage, but also evolve based upon human usage. Thus exceptions and non-consistent variants are always present in languages that typically require exception look-up tables in addition to the normal reduction rules.  At first glance, the idea of equating multiple representations of a word as a single stem term would appear to provide significant compression, with associated savings in storage and processing. For example, the stem "comput" could associate "computable, computability, computation, computational, computed, computing, computer, computerese, computerize" to one compressed word. But upon closer examination, looking at an inverted file system implementation, the savings is only in the dictionary since weighted positional information is typically needed in the inversion lists. In an architecture with stemming, the information is in the one inversion list for the stem term versus distributed across multiple inversion lists for each unstemmed term. Since the size of the inversion lists are the major storage factor, the compression of stemming does not significantly reduce storage requirements. For small test databases such as the Cranfield collection, Lennon reported savings of 32 per cent (Lennon-81). But when applied to larger databases of 1.6 Megabytes and 50 Megabytes, the compression reduced respectively to 20 percent and 13.5 percent (Harman-91). Harman also points out that misspellings and proper names reduce the compression even more. In a large text corpus, such as the TREC database, over 15 per cent of the unique words are proper nouns or acronyms that should not be stemmed.  Another major use of stemming is to improve recall. As long as a semantically consistent stem can be identified for a set of words, the generalization process of stemming does help in not missing potentially relevant items. Stemming of the words "calculate, calculates, calculation, calculations, calculating" to a single stem ("calculat") insures whichever of those terms is entered by the user, it Is translated to the stem and finds all the variants in any items they exist. In contrast, stemming can not improve, but has the potential for decreasing precision. The precision value is not based on finding all relevant items but just minimizing the retrieval of non-relevant items. Any function that generalizes a user's search statement can only increase the likelihood of retrieving non-relevant items unless the expansion guarantees every item retrieved by the expansion is relevant. Data Structure                                                                                              75  It is important for a system to be able to categorize a word prior to making the decision to stem it. Certain categories such as proper names and acronyms should not have stemming applied because their morphological basis is not related to a common core concept. Stemming can also cause problems for Natural Language Processing (NLP) systems by causing the loss of information needed for aggregate levels of natural language processing (discourse analysis). The tenses of verbs may be lost in creating a stem, but they are needed to determine if a particular concept (e.g., economic support) being indexed occurred in the past or will be occurring in the future. Time is one example of the type of relationships that are defined in Natural Language Processing systems (see Chapter 5).  The most common stemming algorithm removes suffixes and prefixes, sometimes recursively, to derive the final stem. Other techniques such as table lookup and successor stemming provide alternatives that require additional overheads. Successor stemmers determine prefix overlap as the length of a stem is increased. This information can be used to determine the optimal length for each stem from a statistical versus a linguistic perspective. Table lookup requires a large data structure. A system such as RetrievalWare that is based upon a very large thesaurus/concept network has the data structure as part of its basic product and thus uses table look-up. The Kstem algorithm used in the INQUERY System combines a set of simple stemming rules with a dictionary to determine processing tokens.  The affix removal technique removes prefixes and suffixes from terms leaving the stem. Most stemmers are iterative and attempt to remove the longest prefixes and suffixes (Lovins-68, Salton-68, Dawson-74, Porter-80 and Paice-90). The Porter algorithm is the most commonly accepted algorithm, but it leads to loss of precision and introduces some anomalies that cause the user to question the integrity of the system. Stemming is applied to the user's query as well as to the incoming text. If the transformation moves the query term to a different semantic meaning, the user will not understand why a particular item is returned and may begin questioning the integrity of the system in general.
issr-0052	4.2.2 Porter Stemming Algorithm  The Porter Algorithm is based upon a set of conditions of the stem, suffix and prefix and associated actions given the condition. Some examples of stem conditions are:  1. The measure, m, of a stem is a function of sequences of vowels (a, e, i, o, u, y) followed by a consonant. If V is a sequence of vowels and C is a sequence of consonants, then m is:  C(VC)mV 76  Chapter 4  where the initial C and final V are optional and m is the number   VC repeats.  Measure                          Example  rn=0                                 free, why  m=l                                frees, whose  m=2                                prologue, compute  2.   *lt;Xgt;            - stem ends with letter X  3.   *v*                - stem contains a vowel  4.   *d                 - stem ends in double consonant  5.   *o                 - stem ends with consonant-vowel-consonant sequence  where the final consonant is not w, x, or y  Suffix conditions take the form current_suffix = = pattern Actions are in the form old_suffix -gt; new_suffix  Rules are divided into steps to define the order of applying the rules.    The following are some examples of the rules:  STEP    CONDITION     SUFFIX  la 1b  Ibl1  1c  2  3 4  5a 5b  NULL  *v*  NULL *v* mgt;0 mgt;0  mgt;l   and and *lt;Lgt;  *d  sses ing at  y  aliti  icate  able  e NULL  REPLACEMENT EXAMPLE  ss stresses-gt;stress  NULL making-gt;mak  ate inflat(ed)-gt;inflate  i happy-gt;happi  al formaliti-gt; formal  ic duplicate-gt;duplic  NULL adjustable-gt;adjust  NULL inflate-gt;inflat  single letter control l-gt;control  Given process  the word   "duplicatable,"   the following are the steps in the stemming  dupSicat rule 4  duplicate rule Ibi  duplic rule 3  1 Ibl rules are expansion rules to make correction to stems for proper conflation. For example stemming of skies drops the es, making it ski, which Is the wrong concept and the I should be changed to y. Data Structure                                                                                             77  The application of another rule in step 4, removing "ic," can not be applied since only one rule from each step is allowed be applied.
issr-0053	4.2.3 Dictionary Look-Up Stemmers  An alternative to solely relying on algorithms to determine a stem is to use a dictionary look-up mechanism. In this approach, simple stemming rules still may be applied. The rules are taken from those that have the fewest exceptions (e.g., removing pluralization from nouns). But even the most consistent rules have exceptions that need to be addressed. The original term or stemmed version of the term is looked up in a dictionary and replaced by the stem that best represents it. This technique has been implemented in the INQUERY and RetrievalWare Systems.  The INQUERY system uses a stemming technique called Kstem. Kstem is a morphological analyzer that conflates word variants to a root form (Kstem-95). It tries to avoid collapsing words with different meanings into the same root. For example, "memorial" and "memorize" reduce to "memory." But "memorial" and "memorize" are not synonyms and have very different meanings. Kstem, like other stemmers associated with Natural Language Processors and dictionaries, returns words instead of truncated word forms. Generally, Kstem requires a word to be in the dictionary before it reduces one word form to another. Some endings are always removed, even if the root form is not found in the dictionary (e.g., 4ness', 'ly'). If the word being processed is in the dictionary, it is assumed to be unrelated to the root after stemming and conflation is not performed (e.g., 'factorial' needs to be in the dictionary or it is stemmed to 'factory'). For irregular morphologies, it is necessary to explicitly map the word variant to the root desired (for example, "matrices" to "matrix").  The Kstem system uses the following six major data files to control and limit the stemming process:  Dictionary of words (lexicon)  Supplemental list of words for the dictionary  Exceptions list for those words that should retain an "e" at the end (e.g., "suites" to "suite" but "suited" to "suit")  DirectConflation - allows definition of direct conflation via  word pairs that override the stemming algorithm  Country_Nationality - conflations between nationalities and countries ("British" maps to "Britain") 78                                                                                               Chapter 4  Proper Nouns - a list of proper nouns that should not be stemmed.  The strength of the Retrieval Ware System lies in its Thesaurus/Semantic Network support data structure that contains over 400,000 words. The dictionaries that are used contain the morphological variants of words. New words that are not special forms (e.g., dates, phone numbers) are located in the dictionary to determine simpler forms by stripping off suffixes and respelling plurals as defined in the dictionary.
issr-0054	4.2.4 Successor Stemmers  Successor stemmers are based upon the length of prefixes that optimally stem expansions of additional suffixes. The algorithm is based upon an analogy in structural linguistics that investigated word and morpheme boundaries based upon the distribution of phonemes, the smallest unit of speech that distinguish one word from another (Hafer-74). The process determines the successor varieties for a word, uses this information to divide a word into segments and selects one of the segments as the stem.  The successor variety of a segment of a word in a set of words is the number of distinct letters that occupy the segment length plus one character. For example, the successor variety for the first three letters (i.e., word segment) of a five-letter word is the number of words that have the same first three letters but a different fourth letter plus one for the current word. A graphical representation of successor variety is shown in a symbol tree. Figure 4.2 shows the symbol tree for the terms bag, barn, bring, both, box, and bottle. The successor variety for any prefix of a word is the number of children that are associated with the node in the symbol tree representing that prefix. For example, the successor variety for the first letter "b" is three. The successor variety for the prefix uba" is two.  The successor varieties of a word are used to segment a word by applying one of the following four methods:  1.   Cutoff method:  a cutoff value is selected to define stem length.  The value varies for each possible set of words.  2.   Peak and Plateau:   a segment break is made after a character whose successor variety exceeds that of the character immediately preceding it and the character immediately following it.  3- Complete word method: break on boundaries of complete words. Data Structure  79  4. Entropy method: uses the distribution of successor variety letters. Let Dak| be the number of words beginning with the k length sequence of letters a. Let |Dakj| be the number of words in Dak with successor j. The   a   /   \  9  r  n  n  h  t            Figure 4.2 Symbol Tree for terms bag, barn, bring, box, bottle , both  probability that a member of D has the successor j is given by |Dak,|/|Daic|. The  entropy (Average Information as defined by Shannon-5 J) of |DtfJ is: 80                                                                                                Chapter 4  26  Hak   =      £   -(|Dakj|/|Dak|)(log2(|Dakj|/|Dak|))  P=\  Using this formula a set of entropy measures can be calculated for a word and its predecessors. A cutoff value is selected and a boundary is identified whenever the cutoff value is reached. Hafer and Weiss experimented with the techniques, discovering that combinations of the techniques performed best, which they used in defining their stemming process. Using the words in Figure 4.2 plus the additional word "boxer," the successor variety stemming is shown in Figure 4.3.  PREFIX                      Successor Variety               Branch Letters  b                                   3                                   a,r,o  bo                                  2                                    t,x  box                                 1                                       e  boxe                                 1                                       r  boxer                                1                                   blank  Figure 4.3 Successor Variety Stemming  If the cutoff method with value four was selected then the stem would be "boxe." The peak and plateau method can not apply because the successor variety monotonically decreases. Applying the complete word method, the stem is "box." The example given does not have enough values to apply the entropy method. The advantage of the peak and plateau and the complete word methods is that a cutoff value does not have to be selected (Frakes-92).  After a word has been segmented, the segment to be used as the stem must be selected. Hafer and Weiss used the following rule:  if (first segment occurs in lt;= 12 words in database)  first segment is stem else (second segment is stem)  The idea is that if a segment is found in more than 12 words in the text being analyzed, it is probably a prefix. Hafer and Weiss noted that multiple prefixes in the English language do not occur often and thus selecting the first or second  segment in general determines the appropriate stem.
issr-0055	4.2.5 Conclusions  Frakes summarized studies of various stemming studies (Frakes-92).   He cautions that some of the authors failed to report test statistics, especially sizes, Data Structure                                                                                            81  making interpretation difficult. Also some of the test sample sizes were so small as to make their results questionable. Frakes came to the following conclusions:  Stemming can affect retrieval (recall) and where effects were identified they were positive. There is little difference between retrieval effectiveness of different frill stemmers with the exception of the Hafer and Weiss stemmer.  Stemming is as effective as manual conflation. Stemming is dependent upon the nature of the vocabulary.  To quantify the impact of stemmers, Paice has defined a stemming performance measure called Error Rate Relative to Truncation (ERRT) that can be used to compare stemming algorithms (Paice-94). The approach depends upon the ability to partition terms semantically and morphologically related to each other into "concept groups." After applying a stemmer that is not perfect, concept groups may still contain multiple stems rather than one. This introduces an error reflected in the Understemming Index (UI). Also it is possible that the same stem is found in multiple groups. This error state is reflected in the Overstemming Index (01). The worst case stemming algorithm is where words are stemmed via truncation to a word length (words shorter than the length are not truncated). UI and 01 values can be calculated based upon truncated word lengths. The perfect case is where UI and 01 equal zero. ERRT is then calculated as the distance from the origin to the (UI, 01) coordinate of the stemmer being evaluated (OP) versus the distance from the origin to the worst case intersection of the line generated by pure truncation (OT) (see Figure 4-4).  The values calculated are biased by the initial grouping of the test terms. Larger ERRT values occur with looser grouping. For the particular test runs, the UI of the Porter Algorithm was greater than the UI of the Paice/Husk algorithms (Paice-90). The 01 was largest for the Paice and the least for Porter. Finally, the ERRT of the Porter was greater than the Paice algorithm. These results suggest that the Paice algorithm appeared significantly better than the Porter algorithm. But the differences in objectives between the stemmers (Porter being a light stemmer - tries to avoid overstemming leaving understemming errors and Paice being the opposite, a heavy stemmer) makes comparison less meaningful. While this approach to stemmer evaluation requires additional work to remove imprecisions and provide a common comparison framework, it provides a mechanism to develop a baseline to discuss future developments.  The comparisons by Frakes and Paice support the intuitive feeling that stemming as a generalization of processing tokens for a particular concept (word) can only help in recall. In experiments, stemming has never been proven to significantly improve recall (Harman-91). Stemming can potentially reduce 82                                                                                                Chapter 4  01  Ul  Figure 4.4 Computation of ERRT value  precision. The impact on precision can be minimized by the use of ranking items based upon all the terms in the query, categorization of terms and selective exclusion of some terms from stemming. Unless the user is very restrictive in the query , the impact of the other search terms and those expanded automatically by the system ameliorates the effects of generalization caused by stemming. Stemming in large databases should not be viewed as a significant compression technique to save on storage. Its major advantage is in the significant reduction of dictionary sizes and therefore a possible reduction in the processing time for each search term.
issr-0056	4.3         Inverted File Structure  The most common data structure used in both database management and Information Retrieval Systems is the inverted file structure. Inverted file structures are composed of three basic files: the document file, the inversion lists (sometimes called posting files) and the dictionary. The name 'inverted file" comes from its underlying methodology of storing an inversion of the documents: inversion of the document from the perspective that, for each word, a list of documents in which the word is found in is stored (the inversion list for that word). Each document in the system is given a unique numerical identifier. It is that identifier that is stored in the inversion list. The way to locate the inversion list for a particular word is via the Dictionary.   The Dictionary is typically a sorted list of all unique words Data Structure  83  (processing tokens) in the system and a pointer to the location of its inversion list (see Figure 4.5). Dictionaries can also store other information used in query optimization such as the length of inversion lists.  DOCUMENTS  DOC   #1,   computer, bit, byte  DOC    #2,    memory, byte  DOC   #3,   computer, bit, memory________  DOC       #4, computer  byte,  DICTIONARY  bit(2)  byte (3)  computer (3)  memory (2)  INVERSION LISTS -bit- 1,3 "byte- 1,2,4 ¶computer - 1, 3, 4 -memory- 2, 3  Figure 4.5 Inverted File Structure  Additional information may be used from the item to increase precision and provide a more optimum inversion list file structure. For example, if zoning is used, the dictionary may be partitioned by zone. There could be a dictionary and set of inversion lists for the "Abstract" zone in an item and another dictionary and set of inversion lists for the "Main Body" zone. This increases the overhead when a user wants to search the complete item versus restricting the search to a specific zone. Another typical optimization occurs when the inversion list only contains one or two entries. Those entries can be stored as part of the dictionary. The inversion list contains the document identifier for each document in which the word is found. To support proximity, contiguous word phrases and term weighting algorithms, all occurrences of a word are stored in the inversion list along with the word position. Thus if the word "bit" was the tenth, twelfth and eighteenth word in document #1, then the inversion list would appear:  bit-1(10), 1(12), 1(18)  Weights can also be stored in inversion lists. Words with special characteristics are frequently stored in their own dictionaries to allow for optimum internal representation and manipulation (e.g., dates which require date ranging and numbers).  When a search is performed, the inversion lists for the terms in the query are located and the appropriate logic is applied between inversion lists. The result is a final hit list of items that satisfy the query. For systems that support ranking, the list is reorganized into ranked order. The document numbers are used to retrieve the documents from the Document File. Using the inversion lists in Figure 84  Chapter 4  4-5, the query (bit AND computer) would use the Dictionary to find the inversion lists for "bit" and "computer." These two lists would be logically ANDed: (1,3) AND (1,3,4) resulting in the final Hit list containing (1,3).  Rather than using a dictionary to point to the inversion list, B-trees can be used. The inversion lists may be at the leaf level or referenced in higher level pointers. Figure 4.6 shows how the words in Figure 4.5 would appear. A B-tree of order m is defined as:  A root node with between 2 and 2m keys  All other internal nodes have between m and 2m keys  All keys are kept in order from smaller to larger  All leaves are at the same level or differ by at most one level.  bit - 1,3  byte - 1, 2, 4  computer -1,3,4  memory -2, 3  Figure 4-6 B-Tree Inversion Lists  Cutting and Pedersen described use of B-trees as an efficient inverted file storage mechanism for data that undergoes heavy updates (Cutting-90).  The nature of Information systems is that items are seldom if ever modified once they are produced. Most commercial systems take advantage of this fact by allowing document files and their associated inversion lists to grow to a certain maximum size and then to freeze them, starting a new structure. Each of these databases of document file, dictionary, inversion lists is archived and made available for a user's query. This has the advantage that for queries only interested in more recent information, only the latest databases need to be searched.   Since Data Structure                                                                                             85  older items are seldom deleted or modified, the archived databases may be permanently backed-up, thus saving on operations overhead. Starting a new inverted database has significant overhead in adding new words and inversion lists until the frequently found words are added to the dictionary and inversion lists. Previous knowledge of archived databases can be used to establish an existing dictionary and inversion structure at the start of a new database, thus saving significant overhead during the initial adding of new documents2.  Inversion lists structures are used because they provide optimum performance in searching large databases. The optimality comes from the minimization of data flow in resolving a query. Only data directly related to the query are retrieved from secondary storage. Also there are many techniques that can be used to optimize the resolution of the query based upon information maintained in the dictionary.  Inversion list file structures are well suited to store concepts and their relationships. Each inversion list can be thought of as representing a particular concept. The inversion list is then a concordance of all of the items that contain that concept. Finer resolution of concepts can additionally be maintained by storing locations with an item and weights of the item in the inversion lists. With this information, relationships between concepts can be determined as part of search algorithms (see Chapter 7). Location of concepts is made easy by their listing in the dictionary and inversion lists. For Natural Language Processing algorithms, other structures may be more appropriate or required in addition to inversion lists for maintaining the required semantic and syntactic information.
issr-0057	4.4 N-Gram Data Structures  N-Grams can be viewed as a special technique for conflation (stemming) and as a unique data structure in information systems. N-Grams are a fixed length consecutive series of "n" characters. Unlike stemming that generally tries to determine the stem of a word that represents the semantic meaning of the word, ngrams do not care about semantics. Instead they are algorithmically based upon a fixed number of characters. The searchable data structure is transformed into overlapping n-grams, which are then used to create the searchable database. Examples of bigrams, trigrams and pentagrams are given in Figure 4.7 for the word phrase "sea colony." For n-grams, with n greater than two, some systems allow interword symbols to be part of the n-gram set usually excluding the single character with interword symbol option. The symbol # is used to represent the interword symbol which is anyone of a set of symbols (e.g., blank, period, semicolon, colon, etc.). Each of the n-grams created becomes a separate processing tokens and are searchable. It is possible that the same n-gram can be created multiple times from a single word.  The INQUIRE DBMS provides this feature. 86                                                                                                Chapter 4  se ea co ol lo on ny                                            Digrams  (no interword symbols)  sea col olo Ion ony                                              Trigrams  (no interword symbols)  #se sea ea# #co col olo Ion ony ny#                   Trigrams  (with interword symbol #)  #sea# #colo colon olony Iony#                              Pentagrams  (with interword symbol #)  Figure 4.7   Bigrams, Trigrams and Pentagrams for "sea colony"
issr-0058	4.4.1 History  The first use of n-grams dates to World War II when it was used by cryptographers. Fletcher Pratt states that "with the backing of bigram and trigram tables any cryptographer can dismember an simple substitution cipher" (Pratt-42). Use of bigrams was described by Adamson as a method for conflating terms (Adamson-74). It does not follow the normal definition of stemming because what is produced by creating n-grams are word fragments versus semantically meaningful word stems. It is this characteristic of mapping longer words into shorter n-gram fragments that seems more appropriately classified as a data structure process than a stemming process.  Another major use of n-grams (in particular trigrams) is in spelling error detection and correction (AngeIl-83, McIllroy-82, Morris-75, Peterson-80, Thorelli-62, Wang-77, and Zamora-81). Most approaches look at the statistics on probability of occurrence of n-grams (trigrams in most approaches) in the English vocabulary and indicate any word that contains non-existent to seldom used 11gram s as a potential erroneous word. Damerau specified four categories of spelling errors (Damerau-64) as shown in Figure 4.8. Using the classification scheme, Zamora showed trigram analysis provided a viable data structure for identifying misspellings and transposed characters. This impacts information systems as a possible basis for identifying potential input errors for correction as a procedure within the normalization process (see Chapter 1). Frequency of occurrence of ngram patterns also can be used for identifying the language of an item (Damashek95, Cohen-95). Data Structure                                                                                          37  Error Category Single Character Insertion Single Character Deletion Single Character Substitution Transposition of two adjacent characters  Figure 4.8 Categories of Spelling Errors  In information retrieval, trigrams have been used for text compression (Wisn-87) and to manipulate the length of index terms (Will-79, Schek-78, Schuegraf-76). D'Amore and Mah (D'Amore-85) used a variety of different 11gram s as index elements for inverted file systems they implemented- They have also been the core data structure to encode profiles for the Logicon LMDS system (Yochum-95) used for Selective Dissemination of Information. For retrospective search, the Acquaintance System uses n-grams to store the searchable document file (Damashek-95, Huffman-95) for retrospective search of large textual databases.
issr-0059	4.4.2 N-Gram Data Structure  As shown in Figure 4.7, an n-gram is a data structure that ignores words and treats the input as a continuous data, optionally limiting its processing by interword symbols. The data structure consists of fixed length overlapping symbol  segments that define the searchable processing tokens. These tokens have logical linkages to all the items in which the tokens are found.   Inversion lists, document  vectors (described in Chapter 5) and other proprietary data structures are used to store the linkage data structure and are used in the search process. In some cases just the least frequently occurring n-gram is kept as part of a first pass search process (Yochum-85). Examples of these implementations are found in Chapter 5.  The choice of the fixed length word fragment size has been studied in many contexts. Yochum and D'Amore investigated the impacts of different values for "n/1 Fatah Comlekoglu (Comlekoglu-90) investigated n-gram data structures using an inverted file system for n=2 to n=26. Trigrams (n-grams of length 3) were determined to be the optimal length, trading off information versus size of data structure.    The Aquaintance System uses longer n-grams., ignoring word Chapter 4  boundaries. The advantage of n-grams is that they place a finite limit on the number of searchable tokens.  MaxSegn = (A,)n  The maximum number of unique n-grams that can be generated, MaxSeg, can be calculated as a function of n which is the length of the n-grams, and X which is the number of processable symbols from the alphabet (i.e., non-interword symbols).  Although there is a savings in the number of unique processing tokens and implementation techniques allow for fast processing on minimally sized machines, false hits can occur under some architectures. For example, a system that uses trigrams and does not include interword symbols or the character position of the n-gram in an item finds an item containing "retain detail" when searching for "retail" (i.e., all of the trigrams associated with "retail" are created in the processing of "retain detail"). Inclusion of interword symbols would not have helped in this example. Inclusion of character position of the n-gram would have discovered that the n-grams "ret," "eta," "tai," "ail" that define "retail" are not all consecutively starting within one character of each other. The longer the n-gram, the less likely this type error is to occur because of more information in the word fragment. But the longer the n-gram, the more it provides the same result as full word data structures since most words are included within a single n-gram. Another disadvantage of n-grams is the increased size of inversion lists (or other data structures) that store the linkage data structure. In effect, use of n-grams expands the number of processing tokens by a significant factor. The average word in the English language is between six and seven characters in length. Use of trigrams increases the number of processing tokens by a factor of five (see Figure 4.7) if interword symbols are not included. Thus the inversion lists increase by a factor of five.  Because of the processing token bounds of n-gram data structures, optimized performance techniques can be applied in mapping items to an n-gram searchable structure and in query processing. There is no semantic meaning in a particular n-gram since it is a fragment of processing token and may not represent a concept. Thus n-grams are a poor representation of concepts and their relationships. But the juxtaposition of n-grams can be used to equate to standard word indexing, achieving the same levels of recall and within 85 per cent precision levels with a significant improvement in performance (Adams-92). Vector representations of the n-grams from an item can be used to calculate the similarity between items.
issr-0060	4.5 PAT Data Structure  Using n-grams with interword symbols included between valid processing tokens equates to a continuous text input data structure that is being indexed in Data Structure                                                                                            89  contiguous "n" character tokens. A different view of addressing a continuous text input data structure comes from PAT trees and PAT arrays. The input stream is transformed into a searchable data structure consisting of substrings. The original concepts of PAT tree data structures were described as Patricia trees (Flajolet-86, Frakes-92, Gonnet-83, Knuth-73, and Morrison-68) and have gained new momentum as a possible structure for searching text and images (Gonnet-88) and applications in genetic databases (Manber-90). The name PAT is short for PAtricia Trees (PATRICIA stands for Practical Algorithm To Retrieve Information Coded In Alphanumerics.)  In creation of PAT trees each position in the input string is the anchor point for a sub-string that starts at that point and includes all new text up to the end of the input. All substrings are unique. This view of text lends itself to many different search processing structures. It fits within the general architectures of hardware text search machines and parallel processors (see Chapter 9). A substring can start at any point in the text and can be uniquely indexed by its starting location and length. If all strings are to the end of the input, only the starting location is needed since the length is the difference from the location and the total length of the item. It is possible to have a substring go beyond the length of the input stream by adding additional null characters. These substrings are called sistring (semi-infinite string). Figure 4.9 shows some possible sistrings for an input text.  A PAT tree is an unbalanced, binary digital tree defined by the sistrings. The individual bits of the sistrings decide the branching patterns with zeros branching left and ones branching right. PAT trees also allow each node in the tree to specify which bit is used to determine the branching via bit position or the  Text                   Economics for Warsaw is complex.  sistring 1             Economics for Warsaw is complex,  sistring 2             conomics for Warsaw is complex,  sistring 5             omics for Warsaw is complex,  sistring 10            for Warsaw is complex.  sistring 20           w is complex.  sistring 30           ex.  Figure 4.9 Examples of sistrings  number of bits to skip from the parent node. This is useful in skipping over levels that do not require branching.  The key values are stored at the leaf nodes (bottom nodes) in the PAT Tree. For a text input of size fcV there are "n" leaf nodes and "n-F at most higher level nodes. It is possible to place additional constraints on sistrings for the leaf nodes.   We may be interested in limiting our searches to word boundaries. 90                                                                                                Chapter 4  Thus we could limit our sistrings to those that are immediately after an interword symbol. Figure 4,10 gives an example of the sistrings used in generating a PAT  INPUT                                                      100110001101  sistring 1             1001....  sistring2               001100...  sistring 3                01100....  sistring 4                  11.......  sistring 5                     1000...  sistring 6                      000.....  sistring 7                        001101  sistring 8                          01101  Figure 4.10 Sistrings for input "100110001101"  tree. If the binary representations of "h" is (100), "o" is (110), "m" is (001) and ue" is (101) then the word "home" produces the input 100110001101.... Using the sistrings, the frill PAT binary tree is shown in Figure 4.11. A more compact tree where skip values are in the intermediate nodes is shown in Figure 4.12. In this version the value in the intermediate nodes (indicated by rectangles) is the number of bits to skip until the next bit to compare that causes differences between similar terms. This final version saves space, but requires comparing a search value to the leaf node (in an oval) contents to ensure the skipped bits match the search term (i.e., skipped bits are not compared).  The search terms are also represented by their binary representation and the PAT trees for the sistrings are compared to the search terms looking for matches.  As noted in Chapter 2, one of the most common classes of searches is prefix searches. PAT trees are ideally constructed for this purpose because each sub-tree contains all the sistrings for the prefix defined up to that node in the tree structure. Thus all the leaf nodes after the prefix node define the sistrings that satisfy the prefix search criteria. This logically sorted order of PAT trees also facilitates range searches since it is easy to determine the sub-trees constrained by the range values. If the total input stream is used in defining the PAT tree, then suffix, imbedded string, and fixed length masked searches (see Section 2.1.5) are all easy because the given characters uniquely define the path from the root node to where the existence of sistrings need to be validated. Fuzzy searches are very difficult because large number of possible sub-trees could match the search term.  A detailed discussion on searching PAT trees and their representation as an array is provided by Gonnet, Baeza-Yates and Snider (Gonnet-92). In their comparison to Signature and Inversion files, they concluded that PAT arrays have more accuracy than Signature files and provide the ability to string searches that Data Structure  91  1000  1001  001100  001101  Figure 4.11  PAT Binary Tree for input "100110001101' 92  Chapter 4  Figure 4. 12   PAT Tree skipping bits for "100110001101"  are inefficient in inverted files (e.g., suffix searches, approximate string searches,  longest repetition).  Pat Trees (and arrays) provide an alternative structure if string searching  is the goal.    They store the text in an alternative structure supporting string  manipulation. The structure does not have facilities to store more abstract concepts and their relationships associated with an item. The structure has interesting potential applications, but is not used in any major commercial products at this time.
issr-0061	Data Structure                                                                                             93  4.6 Signature File Structure  The goal of a signature file structure is to provide a fast test to eliminate the majority of items that are not related to a query. The items that satisfy the test can either be evaluated by another search algorithm to eliminate additional false hits or delivered to the user to review. The text of the items is represented in a highly compressed form that facilitates the fast test. Because file structure is highly compressed and unordered, it requires significantly less space than an inverted file structure and new items can be concatenated to the end of the structure versus the significant inversion list update. Since items are seldom deleted from information data bases, it is typical to leave deleted items in place and mark them as deleted. Signature file search is a linear scan of the compressed version of items producing a response time linear with respect to file size.  The surrogate signature search file is created via superimposed coding (Faloutsos-85, Moders-49). The coding is based upon words in the item. The words are mapped into a "word signature." A word signature is a fixed length code with a fixed number of bits set to "1." The bit positions that are set to one are determined via a hash function of the word. The word signatures are ORed together to create the signature of an item. To avoid signatures being too dense with "l"s, a maximum number of words is specified and an item is partitioned into blocks of that size. In Figure 4.13 the block size is set at five words, the code length is 16 bits and the number of bits that are allowed to be "1" for each word is five.  TEXT: Computer Science graduate students study (assume block size is  five words) WORD                                                       Signature  Computer                                      0001   0110 0000  0110  Science                                         1001   0000 1110  0000  graduate                                        1000  0101 0100  0010  students                                        0000  0111   1000  0100  study                                            0000  0110 0110  0100  Block Signature                                          1001   0111   1110 0110  Figure 4.13 Superimposed Coding  The words in a query are mapped to their signature.    Search is accomplished by template matching on the bit positions specified by the words in the query.  The signature file can be stored as a signature with each row representing a signature block. Associated with each row is a pointer to the original text block. A design objective of a signature file system is trading off the size of the data 94                                                                                               Chapter 4  structure versus the density of the final created signatures. Longer code lengths reduce the probability of collision in hashing the words (i.e., two different words hashing to the same value). Fewer bits per code reduce the effect of a code word pattern being in the final block signature even though the word is not in the item. For example, if the signature for the word "hard" is 1000 0111 0010 0000, it incorrectly matches the block signature in Figure 4.13 (false hit). In a study by Faloutous and Christodoulakis (Faloutous-87) it was shown that if compression is applied to the final data structure, the optimum number of bits per word is one. This then takes on the appearance of a binary coded vector for each item, where each position in the vector represents the existence of a word in the item. This approach requires the maximum code length but ensures that there are not any false hits unless two words hash to the same value.  Search of the signature matrix requires O(N) search time. To reduce the search time the signature matrix is partitioned horizontally. One of the earliest techniques hashes the block signature to a specific slot. If a query has less than the number of words in a block it maps to a number of possible slots rather than just one. The number of slots decreases exponentially as the number of terms increases (Gustafson-71). Another approach maps the signatures into an index sequential file, where, for example, the first un" bits of the signature is used as the index to the block of signatures that will be compared sequentially to the query (Lee-89). Other techniques are two level signatures (Sacks-Davis-83, Sacks-Davis-88) and use of B-tree structures with similar signatures clustered at leaf nodes (Deppisch86).  Another implementation approach takes advantage of the fact that searches are performed on the columns of the signature matrix, ignoring those columns that are not indicated by hashing of any of the search terms. Thus the signature matrix may be stored in column order versus row order (Faioutsos-88, Lin-88, Roberts-79), called vertical partitioning. This is in effect storing the signature matrix using an inverted file structure. The major overhead comes from updates, since new ul"s have to be added to each inverted column representing a signature in the new item.  Signature files provide a practical solution for storing and locating information in a number of different situations. Faloutsos summarizes the environments that signature files have been applied as medium size databases, databases with low frequency of terms, WORM devices, parallel processing machines, and distributed environments (Faloutsos-92).
issr-0062	4.7 Hypertext and XML Data Structures  The advent of the Internet and its exponential growth and wide acceptance as a new global information network has introduced new mechanisms for representing information. This structure is called hypertext and differs from traditional information storage data structures in format and use. The hypertext is Data Structure                                                                                             95  stored in Hypertext Markup Language (HTML) and extensible Markup Language (XML). HTML is an evolving standard as new requirements for display of items on the Internet are identified and implemented. Bot of these languages provide detailed descriptions for subsets of text similar to the zoning discussed previously. These subsets can be used the same way zoning is used to increase search accuracy and improve display of hit results.
issr-0063	4.7.1 Definition of Hypertext Structure  The Hypertext data structure is used extensively in the Internet environment and requires an electronic media storage for the item. Hypertext allows one item to reference another item via an imbedded pointer. Each separate item is called a node and the reference pointer is called a link. The referenced item can be of the same or a different data type than the original (e.g., a textual item references a photograph). Each node is displayed by a viewer that is defined for the file type associated with the node.  For example, Hypertext Markup Language (HTML) defines the internal structure for information exchange across the World Wide Web on the Internet. A document is composed of the text of the item along with HTML tags that describe how to display the document. Tags are formatting or structural keywords contained between less-than, greater than symbols (e.g., lt;titlegt;, lt;stronggt; meaning display prominently). The HTML tag associated with hypertext linkages is lt;a href= ..JNAME /agt; where "a" and "/a" are an anchor start tag and anchor end tag denoting the text that the user can activate, "href is the hypertext reference containing either a file name if the referenced item is on this node or an address (Uniform Resource Locator - URL) and a file name if it is on another node. "#NAME" defines a destination point other than the top of the item to go to. The URL has three components: the access method the client used to retrieve the item, the Internet address of the server where the item is stored, and the address of the item at the server (i.e., the file including the directory it is in). For example, the URL for the HTML specification appears:  http://info.cern.ch/hypertext/WWW/MarkUp/HTML.html  "HTTP" stands for the Hypertext Transfer Protocol which is the access protocol used to retrieve the item in HTML. Other Internet protocols are used for other activities such as file transfer (ftp://), a specific text search system (gopher://), remote logon (tenet://) and collaborative newsgroups (news://). The destination point is found in "info.cern.ch" which is the name of the "info" machine at CERN with uchf being Switzerland, and 47hypertext/WWW/MarkUP/HTML.html" defines where to find the file HTML.html. Figure 4.14 shows an example of a segment of a HTML document. Most of the formatting tags indicated by lt; gt; are not described, being out of the scope of this text, but detailed descriptions can be 96                                                                                                Chapter 4  found in the hundreds of books available on HTML. The lt;a href= ... gt; are the previously described hypertext linkages.  An item can have many hypertext linkages. Thus, from any item there are multiple paths that can be followed in addition to skipping over the linkages to continue sequential reading of the item. This is similar to the decision a reader makes upon reaching a footnote, whether to continue reading or skip to the footnote. Hypertext is sometimes called a "generalized footnote."  In a conventional item the physical and logical structure are closely related. The item is sequential with imbedded citations to other distinct items or  lt;CENTERgt;  lt;IMG SO'Vimages/homeJglo.jpg" WIDTH=468 HEIGHT=107  BORDER=0 ALT="WELCOME TO NETSCAPEgt;lt;BRgt;  lt;digital librarygt;  lt;AHREF=7comprod/mirror/index.htmrgt;  lt;DDgt;  The     beta    testing     is     over:     please     read     our     report     lt;A  HREF="http://www.charm.net/doc/charm/report/theme.html"gt; and your  can                  find                  more                  references                  at  HREF="http://www.charm.net/doc/charm/results/tests.html"gt;  Figure 4.14 Example of Segment of HTML  locations in the item. From the author's perspective, the substantive semantics lie in the sequential presentation of the information. Hypertext is a non-sequential directed graph structure, where each node contains its own information. The author assumes the reader can follow the linked data as easily as following the sequential presentation. A node may have several outgoing links, each of which is then associated with some smaller part of the node called an anchor. When an anchor is activated, the associated link is followed to the destination node, thus navigating the hypertext network. The organizational and reference structure of a conventional item is fixed at printing time while hypertext nodes and links can be changed dynamically. New linkages can be added and the information at a node can change without modification to the item referencing it.  Conventional items are read sequentially by a user. In a hypertext environment, the user "navigates" through the node network by following links. This is the defining capability that allows hypertext to manage loosely structured information. Each thread through different nodes could represent a different concept with additional detail. In a small and familiar network the navigation works well, but in a large information space, it is possible for the user to become disoriented. This issue is discussed in detail in Chapters 5, 7, and 8.  Quite often hypertext references are used to include information that is other than text (e.g., graphics, audio, photograph, video) in a text item.   The Data Structure                                                                                             97  multiple different uses for hypertext references are evolving as more experience is gained with them. When the hypertext is logically part of the item, such as in a graphic, the referenced file is usually resident at the same physical location. When other items created by other users are referenced, they frequently are located at other physical sites. When items are deleted or moved, there is no mechanism to update other items that reference them. Linkage integrity is a major issue in use of hypertext linkages.  Dynamic HTML became available with Navigator 4.0 and Internet Explorer 4.0. It is a collective term for a combination of the latest HTML tags and options, style sheets and programming that will let you create WEB pages that are more animated and responsive to user interaction. Some of the features supported are an object-oriented view of a WEB page and its elements, cascading style sheets, programming that can address most page elements add dynamic fonts. Object oriented views are defined by the Document Object Model - DOM (Micorsoft calls this the Dynamic HTML Object Model while Netscape calls it the HTML Object Model). For example every heading on a page can be named and given attributes of text style and color that can be manipulated by name in a small "program" or script included on the page. A style sheet describes the default style characteristics (page layout, font, text size, etc) of a document or portion of a document. Dynamic HTML allows the specification of style sheets in a cascading fashion (linking style sheets to predefined levels of precedence within the same set of pages. As a result of a user interaction, a new style sheet can be applied changing the appearance of the display. Layering is the use of alternative style sheets to vary the content of a page by providing content layers that overlay and superimpose existing content sections. The existing HTML programming capabilities are being expanded to address the additional data structures. Netscape is also allowing for dynamic fonts to be part of the WEB page thus eliminating the font choice being dependent upon what the browser provides. Since there is no International standard definition of Dynamic HTML, it is being defined in parallel by both Microsoft and Netscape thus precipitating differences in definition and function.
issr-0064	4.7.2 Hypertext History  Although information sciences is just starting to address the impact of the hypertext data structure, the concept of hypertext has been around for over 50 years.    In  1945 an article written by Vannevar Bush in  1933 was published  describing the Memex (memory extender) system (Bush-67). It was a microfilm based system that would allow the user to store much of the information from the scientific explosion of the 1940s on microfilm and retrieve it at multiple readers at the user's desk via individual links. The term "hypertext" came from Ted Nelson in 1965 (Nelson-74). Nelson's vision of all the world's literature being interlinked via hypertext references is part of his Xanadu System. The lack of cost effective computers with sufficient speed and memory to implement hypertext effectively 98                                                                                                Chapter 4  was one of the main inhibitors to its development. One of the first commercial uses of a hypertext system was the mainframe system, Hypertext Editing System, developed at Brown University by Andres van Dam and later sold to Houston Manned Spacecraft Center where it was used for Apollo mission documentation (van Dam-88). Other systems such as the Aspen system at MIT, the KMS system at Carnegie Mellon, the Hyperties system at the University of Maryland and the Notecards system developed at Xerox PARC advanced the hypertext concepts providing hypertext (and hypermedia) systems. HyperCard, delivered with Macintosh computers, was the first widespread hypertext production product. It had a simple metalanguage (HyperTalk) that facilitated authoring hypertext items. It also provided a large number of graphical user interface elements (e.g., buttons, hands,) that facilitated the production of sophisticated items.  Hypertext became more available in the early 1990's via its use in CDROMs for a variety of educational and entertainment products. Its current high level of popularity originated with it being part of the specification of the World Wide Web by the CERN (the European Center for Nuclear Physics Research) in Geneva, Switzerland. The Mosaic browser, freely available from CERN on the Internet, gave everyone who had access the ability to receive and display hypertext documents.
issr-0065	4.7.3 XML  The extensible Markup Language (XML) is starting to become a standard data structure on the WEB. Its first recommendation (1.0) was issued on February 10, 1998. It is a middle ground between the simplicities but lack of flexibility of HTML and the complexity but richness of SGML (ISO 8879). Its objective is extending HTML with semantic information. The logical data structure within XML is defined by a Data Type Description (DTD) and is not constrained to the 70 defined tags and 50 attributes in the single DTD for HTML. The user can create any tags needed to describe and manipulate their structure. The W3C (Worldwide Web Consortium) is redeveloping HTML as a suite of XML tags. The following is a simple example of XML tagging:  lt;companygt;Widgets Inc.lt;/companygt; lt;citygt;Bostonlt;/citygt;  lt;stategt;Masslt;/stategt; lt;prodoctgt;widgetslt;/productgt;  The W3C is also developing a Resource Description Format (RDF) for representing properties of WEB resources such as images, documents and relationships between them. This will include the Platform for Internet Content Selection (PICS) for attaching labels to material for filtering (e.g., unsuitable for children). Data Structure                                                                                            99  Hypertext links for XML are being defined in the Xlink (XML Linking Language) and Xpoint (XML Pointer language) specifications. This will allow for distinction for different types of links to locations within a document and external to the document. This will allow an application to know if a link is just a repositioning reference within an item or link to another document that is an extension of the existing document. This will help in determining what needs to be retrieved to define the total item to be indexed.  Finally XML will include an XML Style Sheet Linking definition to define how to display items on a particular style sheet and handle cascading style sheets. This will allow designers to limit what is displayed to the user (saving on display screen space) and allow expansion to the whole item if desired.
issr-0066	4.8 Hidden Markov Models  Hidden Markov Models (HMM) have been applied for the last 20 years to solving problems in speech recognition and to a lesser extent in the areas locating named entities (Bikel-97), optical character recognition (Bazzi-98) and topic identification (KubaIa-97). More recently HMMs have been applied more generally to information retrieval search with good results. One of the first comprehensive and practical descriptions of Hidden Markov Models was written by Dr. Lawrence Rabiner (Rabiner-89)  A HMM can best be understood by first defining a discrete Markov process. The easiest way to understand it is by an example. Lets take the example of a three state Markov Model of the Stock Market. The states will be one of the following that is observed at the closing of the market:  State 1 (SI): market decreased State 2 (S2): market did not change State 3 (S3): market increased in value  The movement between states can be defined by a state transition matrix with state transitions (this assumes you can go from any state to any other state):  .5           .3           .4  A={*u} =    .1           .6          .3  .6          .7          .5  Given that the market fell on one day (State 1), the matrix suggests that the  probability of the market not changing the next day is .1. This then allows questions such as the probability that the market will increase for the next 4 days then fall. This would be equivalent to the sequence of SEQ = {S3, S3, S3, S3, SI}. In order to simplify our model, lets assume that instead of the current state being dependent upon all the previous states, lets assume it is only dependent upon the 100                                                                                              Chapter 4  last state (discrete, first order, Markov chain.) This would then be calculated by the formula:  P(SEQ) =P[S3, S3, S3, S3.S1]  = P[S3] * P[S3/S3] * P[S3/S3} * P[S3/S3] * P[S1/S3] = S3(init)*a3,3* a3,3* a3,3* a1gt;3 = (1.0)* (.5)* (.5)* (.5)* (.4) = .05  In the equation we also assume the probability of the initial state of S3 is S3(init)=l. The following graph depicts the model. The directed lines indicate the state transition probabilities a,j. There is also an implicit loop from every state back to itself  In the example every state corresponded to an observable event (change in the market).  When trying to apply this model to less precise world problems such as in speech  recognition, this model was too restrictive to be applicable. To add more flexibility a probability function was allowed to be associated with the state. The result is called the Hidden Markov Model. It gets its name from the fact that there are two  stochastic processes with the underlying stochastic process not being observable (hidden), but can only be analyzed by observations which originate from another  stochastic process. Thus the system will have as input a series of results, but it will not know the Markov model and the number of states that were associated with generating the results. So part of the HMM process is in determining which model of states best explains the results that are being observed.  Amore formal definition of a discrete Hidden Markov Model is summarized by Mittendorf and Schauble (Mittendorf-94): as consisting of the following: Data Structure                                                                                            101  1.   S = { So, ,..., sn.i} as a finite set of states where s0 always denotes the initial state. Typically the states are interconnected such that any state can be reached from any other state.  2.     V = { v0,..., vm_j} is a finite set of output symbols.    This will correspond to the physical output from the system being modeled.  3.    A = S x S a transition probability matrix where ay   represents the  n-\  probability of transitioning from state i to state] such that   /\#i,j = 1 for  all i = 0, ... ,n - 1. Every value in the matrix is a positive value between 0 and 1. For the case where every state can be reached from every other state every value in the matrix will be non-zero.  4.    B = S X V is an output probability matrix where element bjtk is a  m-\  function determining the probability and J^b jtk = 1 for all  £=0  j = 0, ...,n-l.  5.   The initial state distribution.  The HMM will generate an output symbol at every state transition. The transition probability is the probability of the next state given the current state. The output probability is the probability that a given output is generated upon arriving at the next state.  Given the HMM definition, it can be used as both a generator of possible sequences of outputs and their probabilities (as shown in example above), or given a particular out sequence it can model its generation by an appropriate HMM model. The complete specification of a HMM requires specification of the states, the output symbols and three probability measures for the state transitions, output probability functions and the initial states. The distributions are frequently called A, B, and 7t, and the following notation is used to define the model:  I = (A, B, 7i).  One of the primary problems associated with HMM is how to efficiently calculate the probability of a sequence of observed outputs given the HMM model. This can best be looked at as how to score a particular model given a series of outputs. Or another way to approach it is how to determine which of a number of competing models should be selected given an observed set of outputs. This is in effect 102                                                                                              Chapter 4  uncovering the hidden part of the model. They typical approach is to apply an "optimality criterion" to select the states. But there are many such algorithms to choose from. Once you have selected the model that you expect corresponds to the output, then there is the issue of determining which set of state sequences best explains the output. The final issue is how best to tune the X model to maximize the probability of the output sequence given X. This is called the training sequence and is crucial to allow the models to adapt to the particular problem being solved. More details can be found in Rabiner's paper (Rabiner-89).
issr-0067	4.9 Summary  Data structures provide the implementation basis of search techniques in Information Retrieval Systems. They may be searching the text directly, as in use of signature and possibly PAT trees, or providing the structure to hold the searchable data structure created by processing the text in items. The most important data structure to understand is the inverted file system. It has the greatest applicability in information systems. The use of n-grams has also found successes in a limited number of commercial systems. Even though n-grams have demonstrated successes in finding information, it is not a structure that lends itself to representing the concepts in an item. There is no association of an n-gram with a semantic unit (e.g., a word or word stem). Judging the relative importance (ranking) of items is much harder to accomplish under this data structure and the algorithmic options are very limited.  PAT and Signature data file structures have found successful implementations in certain bounded search domains. Both of these techniques encounter significant problems in handling very large databases of textual items. The Hypertext data structure is the newest structure to be considered from an Information Retrieval System perspective. It certainly can be mathematically mapped to linked lists and networks. But the model of how dependencies between items as hyperlinks are resolved is just being considered. The future high usage of this structure in information systems make its understanding important in finding relevant information on the Internet. Marchionini and Shneiderman believe that hypertext will be used in conjunction with full text search tools (Marchionini-88).  The stemming discussed in this chapter has the greatest effect on the human resources it takes to find relevant information. Stemming can increase the ability of the system to find relevant item by generalizing many words to a single representation. But this process reduces precision. Enough information has not been gathered to practically trade off the value of the increase in recall versus the decrease in precision for different degrees of stemming. Data Structure                                                                                            103
issr-0068	5   Automatic Indexing  5.1    Classes of Automatic Indexing  5.2    Statistical Indexing  5.3    Natural Language  5.4    Concept Indexing  5.5    Hypertext Linkages  5.6    Summary  Chapter 3 introduced the concept and objectives of indexing along with its history. This chapter focuses on the process and algorithms to perform indexing. The indexing process is a transformation of an item that extracts the semantics of the topics discussed in the item. The extracted information is used to create the processing tokens and the searchable data structure. The semantics of the item not only refers to the subjects discussed in the item but also in weighted systems, the depth to which the subject is discussed. The index can be based on the full text of the item, automatic or manual generation of a subset of terms/phrases to represent the item, natural language representation of the item or abstraction to concepts in the item. The results of this process are stored in one of the data structures (typically inverted data structure) described in Chapter 4. Distinctions, where appropriate, are made between what is logically kept in an index versus what is physically stored.  This text includes chapters on Automatic Indexing and User Search techniques. There is a major dependency between the search techniques to be implemented and the indexing process that stores the information required to execute the search. This text categorizes the indexing techniques into statistical, natural language, concept, and hypertext linkages. Insight into the rationale for this classification is presented in Section 5.1.
issr-0069	5.1 Classes of Automatic Indexing  Automatic indexing is the process of analyzing an item to extract the information to be permanently kept in an index.   This process is associated with 106                                                                                              Chapters  the generation of the searchable data structures associated with an item. Figure 1.5 Data Flow in an Information Processing System is reproduced here as Figure 5.1 to show where the indexing process is in the overall processing of an item. The figure is expanded to show where the search process relates to the indexing process. The left side of the figure including Identify Processing Tokens, Apply Stop Lists, Characterize tokens, Apply Stemming and Create Searchable Data Structure is all part of the indexing process. All systems go through an initial stage of zoning (described in Section 1.3.1) and identifying the processing tokens used to create the index. Some systems automatically divide the document up into fixed length passages or localities, which become the item unit that is indexed (Kretser-99.) Filters, such as stop lists and stemming algorithms, are frequently applied to reduce the number of tokens to be processed. The next step depends upon the search strategy of a particular system. Search strategies can be classified as statistical, natural language, and concept. An index is the data structure created to support the search strategy.  Statistical strategies cover the broadest range of indexing techniques and are the most prevalent in commercial systems. The basis for a statistical approach is use of frequency of occurrence of events. The events usually are related to occurrences of processing tokens (words/phrases) within documents and within the database. The words/phrases are the domain of searchable values. The statistics that are applied to the event data are probabilistic, Bayesian, vector space, neural net. The static approach stores a single statistic, such as how often each word occurs in an item, that is used in generating relevance scores after a standard Boolean search. Probabilistic indexing stores the information that are used in calculating a probability that a particular item satisfies (i.e., is relevant to) a particular query. Bayesian and vector approaches store information used in generating a relative confidence level of an item's relevance to a query. It can be argued that the Bayesian approach is probabilistic, but to date the developers of this approach are more focused on a good relative relevance value than producing and absolute probability. Neural networks are dynamic learning structures that are discussed under concept indexing where they are used to determine concept classes.  Natural Language approaches perform the similar processing token identification as in statistical techniques, but then additionally perform varying levels of natural language parsing of the item. This parsing disambiguates the context of the processing tokens and generalizes to more abstract concepts within an item (e.g., present, past, future actions). This additional information is stored within the index to be used to enhance the search precision.  Concept indexing uses the words within an item to correlate to concepts discussed in the item. This is a generalization of the specific words to values used to index the item. When generating the concept classes automatically, there may not be a name applicable to the concept but just a statistical significance. Automatic Indexing  107  STANDARDIZE INPUT  LOGICAL  SUBSETTING  (ZONING)  IDENTIFY PROCESSING TOKENS \   r   APPLY STOPLISTS (STOP ALGORITHMS)       CHARACTERIZE TOKENS       APPLY STEMMING   lt;    CREATE SEARCHABLE DATA STRUCTURE SEARCH RESULTS    UPDATE DOCUMENT FILE  QUERY  DISPLAY  USER COMMAND  Figure 5.1  Data Flow in Information Processing System 108                                                                                               Chapter 5  Finally, a special class of indexing can be defined by creation of hypertext linkages. These linkages provide virtual threads of concepts between items versus directly defining the concept within an item.  Each technique has its own strengths and weaknesses. Current evaluations from TREC conferences (see Chapter 11) show that to maximize location of relevant items, applying several different algorithms to the same corpus provides the optimum results, but the storage and processing overhead is significant.
issr-0070	5.2 Statistical Indexing  Statistical indexing uses frequency of occurrence of events to calculate a number that is used to indicate the potential relevance of an item. One approach used in search of older systems does not use the statistics to aid in the initial selection, but uses them to assist in calculating a relevance value of each item for ranking. The documents are found by a normal Boolean search and then statistical calculations are performed on the Hit file, ranking the output (e.g., term frequency algorithms). Since the index does not contain any special data, these techniques are discussed in Chapter 7 under ranking.  Probabilistic systems attempt to calculate a probability value that should be invariant to both calculation method and text corpora. This allows easy integration of the final results when searches are performed across multiple databases and use different search algorithms. A probability of 50 per cent would mean that if enough items are reviewed, on the average one half of the reviewed items are relevant. The Bayesian and Vector approaches calculate a relative relevance value (e.g., confidence level) that a particular item is relevant. Quite often term distributions across the searchable database are used in the calculations. An issue that continues to be researched is how to merge results, even from the same search algorithm, from multiple databases. The problem is compounded when an attempt is made to merge the results from different search algorithms. This would not be a problem if true probabilities were calculated.
issr-0071	5.2.1 Probabilistic Weighting  The probabilistic approach is based upon direct application of the theory of probability to information retrieval systems. This has the advantage of being able to use the developed formal theory of probability to direct the algorithmic development. It also leads to an invariant result that facilitates integration of results from different databases. The use of probability theory is a natural choice because it is the basis of evidential reasoning (i.e., drawing conclusions from evidence). This is summarized by the Probability Ranking Principle (PRP) and its Plausible Corollary (Cooper-94): Automatic Indexing                                                                                    109  HYPOTHESIS: If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of usefulness to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data is available for this purpose, then the overall effectiveness of the system to its users is the best obtainable on the basis of that data.  PLAUSIBLE COROLLARY: The most promising source of techniques for estimating the probabilities of usefulness for output ranking in IR is standard probability theory and statistics.  There are several factors that make this hypothesis and its corollary difficult (Gordon-92, Gordon-91, Robertson-77). Probabilities are usually based upon a binary condition; an item is relevant or not. But in information systems the relevance of an item is a continuous function from non-relevant to absolutely useful. A more complex theory of expected utility (Cooper-78) is needed to address this characteristic. Additionally, the output ordering by rank of items based upon probabilities, even if accurately calculated, may not be as optimal as that defined by some domain specific heuristic (Stirling-77). The domains in which probabilistic ranking are suboptimal are so narrowly focused as to make this a minor issue. But these issues mentioned are not as compelling as the benefit of a good probability value for ranking that would allow integration of results from multiple sources.  The source of the problems that arise in application of probability theory come from a lack of accurate data and simplifying assumptions that are applied to the mathematical model. If nothing else, these simplifying assumptions cause the results of probabilistic approaches in ranking items to be less accurate than other approaches. The advantage of the probabilistic approach is that it can accurately identify its weak assumptions and work to strengthen them. In many other approaches, the underlying weaknesses in assumptions are less obvious and harder to identify and correct. Even with the simplifying assumption, results from comparisons of approaches in the TREC conferences have shown that the probabilistic approaches, while not scoring highest, are competitive against all other approaches.  There are many different areas in which the probabilistic approach may be applied. The method of logistic regression is described as an example of how a probabilistic approach is applied to information retrieval (Gey-94). The approach starts by defining a "Model 0" system which exists before specific probabilistic models are applied. In a retrieval system there exist query terms q, and document terms djª which have a set of attributes (vi,. . ., vn) from the query (e.g., counts of term frequency in the query), from the document (e.g., counts of term frequency in the document ) and from the database (e.g., total number of documents in the database divided by the number of documents indexed by the term).  The logistic reference model uses a random sample of query-documcutterm triples for which binary relevance judgments have been made from a training 110                                                                                              Chapters  sample.   Log 0 is the logarithm of the odds (logodds) of relevance for term tk which is present in document Dj and query Q,:  log(0(R | Qi, Dj, t0) = Co + c,v, + ... + cnvn  The logarithm that the ith Query is relevant to the jth Document is the sum of the logodds for all terms:  log(0(R | Q,, Dj)) = J   tlog(∞(R I Q^ djgt; W) - log(O(R))]  k=\  where O(R) is the odds that a document chosen at random from the database is relevant to query Qx. The coefficients c are derived using logistic regression which fits an equation to predict a dichotomous independent variable as a function of independent variables that show statistical variation (Hosmer-89). The inverse logistic transformation is applied to obtain the probability of relevance of a document to a query:  P(R|QisDj)= i\(i+e-log(O(RiQi'Di)))  The coefficients of the equation for logodds is derived for a particular database using a random sample of query-document-term-relevance quadruples and used to predict odds of relevance for other query-document pairs.  Gey applied this methodology to the Cranfield Collection (Gey-94). The collection has 1400 items and 225 queries with known results. Additional attributes of relative frequency in the query (QRF), relative frequency in the document (DRF) and relative frequency of the term in all the documents (RFAD) were included, producing the following logodds formula:  Z} , log(O(R | tj)) = cQ+ Cilog(QAF) + c2log(QRF) + c3log(DAF) +  c4log(DRF)  + c5Iog(IDF) + c6log(RFAD)  where QAF, DAF, and IDF were previously defined, QRF = QAF\ (total number of terms in the query), DRF = DAF\(total number of words in the document) and RFAD = (total number of term occurrences in the c!atabase)\ (total number of all words in the database). Logs are used to reduce the impact of frequency information; then smooth out skewed distributions. A higher maximum likelihood is attained for logged attributes.  The coefficients and log (O(R)) were calculated creating the final formula  for ranking for query vector Q, which contains q terms: Automatic Indexing                                                                                   111  log(O(R |0)) = -5.138+ 2,   (Zj + 5.138)  k=\  The logistic inference method was applied to the test database along with the Cornell SMART vector system which uses traditional term frequency, inverse document frequency and cosine relevance weighting formulas (see Section 5.2.2). The logistic inference method outperformed the vector method.  Thus the index that supports the calculations for the logistic reference model contains the O(R) constant value (e.g., -5.138) along with the coefficients c0 through c6. Additionally, it needs to maintain the data to support DAF, DRF, IDF and RFAD. The values for QAF and QRF are derived from the query.  Attempts have been made to combine the results of different probabilistic techniques to get a more accurate value. The objective is to have the strong points of different techniques compensate for weaknesses. To date this combination of probabilities using averages of Log-Odds has not produced better results and in many cases produced worse results (Hull-96).
issr-0072	5.2.2 Vector Weighting  One of the earliest systems that investigated statistical approaches to information retrieval was the SMART system at Cornell University (Buckley-95, Salton-83). The system is based upon a vector model. The semantics of every item are represented as a vector. A vector is a one-dimensional set of values, where the order/position of each value in the set is fixed and represents a particular domain. In information retrieval, each position in the vector typically represents a processing token. There are two approaches to the domain of values in the vector: binary and weighted. Under the binary approach, the domain contains the value of one or zero, with one representing the existence of the processing token in the item. In the weighted approach, the domain is typically the set of all real positive numbers. The value for each processing token represents the relative importance of that processing token in representing the semantics of the item. Figure 5.2 shows how an item that discusses petroleum refineries in Mexico would be represented . In the example, the major topics discussed are indicated by the index terms for each column (i.e., Petroleum, Mexico, Oil, Taxes, Refineries and Shipping).  Binary vectors require a decision process to determine if the degree that a particular processing token represents the semantics of an item is sufficient to include it in the vector. In the example for Figure 5.2, a five-page item may have had only one sentence like "Standard taxation of the shipment of the oil to refineries is enforced." For the binary vector, the concepts of i4Tax" and "Shipment* are below the threshold of importance (e.g., assume threshold is 1.0) 112  Chapter 5  Petroleum Mexico   Oil    Taxes Refineries Shipping Binary                (       1,1,1,0,       1        ,    0      )  Weighted            (      2.8    ,     1.6   , 3.5,   .3   ,       3.1    ,     .1    )  Figure 5.2 Binary and Vector Representation of an Item  and they not are included in the vector. A weighted vector acts the same as a binary vector but it provides a range of values that accommodates a variance in the value of the relative importance of a processing token in representing the semantics of the item. The use of weights also provides a basis for determining the rank of an item.  The vector approach allows for a mathematical and a physical representation using a vector space model. Each processing token can be considered another dimension in an item representation space. In Chapter 7 it is shown that a query can be represented as one more vector in the same ndimensional space. Figure 5.3 shows a three-dimensional vector representation assuming there were only three processing tokens, Petroleum Mexico and Oil.  Oil  Figure 53 Vector Representation Automatic Indexing                                                                                    113  The original document vector has been extended by additional information such as citations/references to add more information for search and clustering purposes. There have not been significant improvements in retrieval using these techniques. Introduction of text generated from multimedia sources introduces a new rationale behind extending the vocabulary associated with an item. In the case where the text is not generated directly by an author but is the result of audio transcription, the text will contain a significant number of word errors. Audio transcription maps the phonemes that are in an audio item to the words most closely approximating those phonemes in a dictionary. Good audio transcription of broadcast news still has 15% of the words in error and conversational speech still has 40% or more of the words in error. These will be valid words but the wrong word. One mechanism to reduce the impact of the missing words is to use the existing database to expand the document. This is accomplished by using the transcribed document as a query against the existing database, selecting a small number of the highest ranked results, determining the most important (highest frequency) words across those items and adding those words to the original document. The new document will then be normalized and reweighted based upon the added words (Singhal-99). This technique reduced the losses in retrieval effectiveness from 15-27% to 7-13% when the audio transcriptions had high errors (40% or more). It has marginal benefit when the transcription has errors in the 15% range.  There are many algorithms that can be used in calculating the weights used to represent a processing token. Part of the art in information retrieval is deriving changes to the basic algorithms to account for normalization (e.g., accounting for variances in number of words in items). The following subsections present the major algorithms starting with the most simple term frequency algorithm.
issr-0073	5.2.2.1  Simple Term Frequency Algorithm  In both the unweighted and weighted approaches, an automatic indexing process implements an algorithm to determine the weight to be assigned to a processing token for a particular item. In a statistical system, the data that are potentially available for calculating a weight are the frequency of occurrence of the processing token in an existing item (i.e., term frequency - TF), the frequency of occurrence of the processing token in the existing database (i.e., total frequency TOTF) and the number of unique items in the database that contain the processing token (i.e., item frequency - IF, frequently labeled in other publications as document frequency - DF). As discussed in Chapter 3, the premises by Luhn and later Brookstein that the resolving power of content-bearing words is directly proportional to the frequency of occurrence of the word in the item is used as the basis for most automatic weighting techniques. Weighting techniques usually are based upon positive weight values. 114                                                                                               Chapters  The simplest approach is to have the weight equal to the term frequency. This approach emphasizes the use of a particular processing token within an item. Thus if the word "computer" occurs 15 times within an item it has a weight of 15. The simplicity of this technique encounters problems of normalization between items and use of the processing token within the database. The longer an item is, the more often a processing token may occur within the item. Use of the absolute value biases weights toward longer items, where a term is more likely to occur with a higher frequency. Thus, one normalization typically used in weighting algorithms compensates for the number of words in an item.  An example of this normalization in calculating term-frequency is the algorithm used in the SMART System at Cornell (Buckley-96). The term frequency weighting formula used in TREC 4 was:  ______________(1 + logfTFWl + log(average (TF))  (1 - slope) * pivot + slope * number of unique terms  where slope was set at .2 and the pivot was set to the average number of unique terms occurring in the collection (Singhal-95). In addition to compensating for document length, they also want the formula to be insensitive to anomalies introduced by stemming or misspellings.  Although initially conceived of as too simple, recent experiments by the SMART system using the large databases in TREC demonstrated that use of the simpler algorithm with proper normalization factors is far more efficient in processing queries and return hits similar to more complex algorithms.  There are many approaches to account for different document lengths when determining the value of Term Frequency to use (e.g., an items that is only 50 words may have a much smaller term frequency then and item that is 1000 words on the same topic). In the first technique, the term frequency for each word is divided by the maximum frequency of the word in any item. This normalizes the term frequency values to a value between zero and one. This technique is called maximum term frequency. The problem with this technique is that the maximum term frequency can be so large that it decreases the value of term frequency in short items to too small a value and loses significance.  Another option is to use logaritmetic term frequency. In this technique the log of the term frequency plus a constant is used to replace the term frequency. The log function will perform the normalization when the term frequencies vary significantly due to size of documents. Along this line the COSINE function used as a similarity measure (see Chapter 7) can be used to normalize values in a document. This is accomplished by treating the index of a document as a vector and divide the weights of all terms by the length of the vector. This will normalize to a vector of maximum length one. This uses all of the data in a particular item to perform the normalization and will not be distorted by any particular term. The problem occurs when there are multiple topics within an item. The COSINE technique will normalize all values based upon the total length of the vector that Automatic Indexing                                                                                   115  represents all of topics. If a particular topic is important but briefly discussed, its normalized value could significantly reduce its overall importance in comparison to another document that only discusses the topic.  Another approach recognizes that the normalization process may be over penalizing long documents (Singhal-95). Singhal did experiments that showed longer documents in general are more likely to be relevant to topics then short documents. Yet normalization was making all documents appear to be the same length. To compensate, a correction factor was defined that is based upon document length that maps the Cosine function into an adjusted normalization function. The function determines the document length crossover point for longer documents where the probability of relevance equals the probability of retrieval, (given a query set). This value called the "pivot point" is used to apply an adjustment to the normalization process. The theory is based upon straight lines so it is a matter of determining slope of the lines.  New normalization = (slope)*(old normalization) + K  K is generated by the rotation of the pivot point to generate the new line and the old normalization = the new normalization at that point. The slope for all higher values will be different. Substituting pivot for both old and new value in the above formula we can solve for K at that point. Then using the resulting formula for K and substituting in the above formula produces the following formula:  Pivoted function = slope)*(old normalization) + (1.0 - slope)*(pivot)  Slope and pivot are constants for any document/query set. Another problem is that the Cosine function favors short documents over long documents and also favors documents with a large number of terms. This favoring is increased by using the pivot technique. If log(TF) is used instead of the normal frequency then TF is not a significant factor, in documents with large number of terms the Cosine factor is approximated by the square root of the number of terms. This suggests that using the ratio of the logs of term frequencies would work best for longer items in the calculations:  (1 + log(TF))/(I + log(average(TF))  This leads to the final algorithm that weights each term by the above formula  divided by the pivoted normalization:  ((1 + log(TF))/(l + log(average(TF))/(slope)(No. unique terms) + (l-slope)*(pivot)  Singhal demonstrated the above formula works better against TREC data then TF/IVIAX(TF) or vector length normalization. The effect of a document with a high term frequency is reduced by the normalization function by dividing the TF by the average TF and by use of the log function.  The use of pivot normalization 16                                                                                              Chapter 5  adjusts for the bias towards shorter documents increasing the weights of longer documents.
issr-0074	5.2.2.2 Inverse Document Frequency  The basic algorithm is improved by taking into consideration the frequency of occurrence of the processing token in the database. One of the objectives of indexing an item is to discriminate the semantics of that item from other items in the database. If the token "computer" occurs in every item in the database, its value representing the semantics of an item may be less useful compared to a processing token that occurs in only a subset of the items in the database. The term "computer" represents a concept used in an item, but it does not help a user find the specific information being sought since it returns the complete database. This leads to the general statement enhancing weighting algorithms that the weight assigned to an item should be inversely proportional to the frequency of occurrence of an item in the database. This algorithm is called inverse document frequency (IDF). The un-normalized weighting formula is:  WEIGHT, = TFy * [Log2(n) - Log2(IFj) + 1]  where WEIGHTtj is the vector weight that is assigned to term "j" in item "i," TFy (term frequency) is the frequency of term "j" in item "i" , "n" is the number of items in the database and IFj (item frequency or document frequency) is the number of items in the database that have term "j" in them. A negative log is the same as dividing by the log value, thus the basis for the name of the algorithm. Figure 5.4 demonstrates the impact of using this weighting algorithm. The term "refinery'1 has the highest frequency in the new item (10 occurrences). But it has a normalized weight of 20 which is less than the normalized weight of "Mexico." This change in relative importance between "Mexico" and "refinery" from the unnormalized to normalized vectors is due to an adjustment caused by "refinery" already existing in 50 per cent of the database versus "Mexico" which is found in 6.25 per cent of the items.  The major factor of the formula for a particular term is (Log2(n) Log2(IFj)). The value for IF can vary from UP to wn." At X" the term is found in every item in the database and the factor becomes (Log2(n) - Loga(n)) = I. As the number of items a term is found in decreases, the value of the denominator decreases eventually approaching the value Log2(l) which is close to I. The weight assigned to the term in the item varies from Tf^ * (! + I) to Tf^ * (~ Log2(n)). The effect of this factor can be too great as the number of items that a term is found in becomes small. To compensate for this, the INQUERY system at the University of Massachusetts normalizes this factor by taking an additional log value. Automatic Indexing                                                                                    117  Assume that the term "oil" is found in 128 items, "Mexico" is found in 16 items and "refinery" is found in 1024 items. If a new item arrives with all three terms in it, "oil" found 4 times, "Mexico" found 8 times, and "refinery found 10 times and there are 2048 items in the total database, Figure 5.4 shows the weight calculations using inverse document frequency.  Using a simple unnormalized term frequency, the item vector is (4, 8, 10) Using inverse document frequency the following calculations apply:  Weighty = 4 * (Log2(2048) - Log2(128) +l) = 4*(ll-7+l) = 20 WeightMex.cc = 8 * (Log2(2048) - Log2(16) +l)=8*(ll-4+l)= 64  ight*^ = 10 * (Log2(2048) - Log2(1024) + 1) = 10*(ll - 10+ 1 ) = 20  with the resultant inverse document frequency item vector = (20, 64, 20) Figure 5.4 Example of Inverse Document Frequency  The value of "n" and IF, vary as items are added and deleted from the database. To implement this algorithm in a dynamically changing system, the physical index only stores the frequency of occurrence of the terms in an item (usually with their word location) and the IDF factor is calculated dynamically at retrieval time. The required information can easily be determined from an inversion list for a search term that is retrieved and a global variable on the number of items in the database.
issr-0075	5.2.2.3 Signal Weighting  Inverse document frequency adjusts the weight of a processing token for an item based upon the number of items that contain the term in the existing database. What it does not account for is the term frequency distribution of the processing token in the items that contain the term. The distribution of the frequency of processing tokens within an item can affect the ability to rank items. For example, assume the terms "SAW" and "DRILL" are found in 5 items with the following frequencies defined in Figure 5.5.  Both terms are found a total of 50 times in the five items. The term "SAW" does not give any insight into which item is more likely to be relevant to a search of "SAW". If precision is a goal (maximizing relevant items shown first), then the weighting algorithm could take into consideration  the non-uniforrn 118                                                                                              Chapter 5  distribution of term "DRILL" in the items that the term is found,   applying even higher weights to it than "SAW." The theoretical basis for the algorithm  Item Distribution SAW DRII  A 10 2  B 10 2  C 10 18  D 10 10  E 10 18  Figure 5.5 Item Distribution for SAW and DRILL  to emphasize precision is Shannon's work on Information Theory (Shannon-51).  In Information Theory, the information content value of an object is inversely proportional to the probability of occurrence of the item. An instance of an event that occurs all the time has less information value than an instance of a seldom occurring event. This is typically represented as INFORMATION = -Log2 (p), where p is the probability of occurrence of event "p." The information value for an event that occurs .5 per cent of the time is:  INFORMATION = - Log2(.0005) = -(-10) = 10 The information value for an event that occurs 50 per cent of the time is:  INFORMATION = - Log2 (.50)  = -(-1) = 1  If there are many independent occurring events then the calculation for the average information value across the events is:  AVEJNFO = - Yj    P* Lo^ (Pk)  The value of AVE JNFO takes its maximum value when the values for every pk is the same. Its value decreases proportionally to Increases in variances in the values of p^ The value of pk can be defined as TFji/rQTFk, the ratio of the frequency of occurrence of the term in an item to the total number of occurrences of the item in the data base. Using the AVEJNFQ formula, the terms that have the most uniform distribution in the items that contain the term have the maximum value. To use this information in calculating a weight, the formula needs the inverse of AVE INFO, where the minimum value is associated with uniform distributions Automatic Indexing                                                                                   119  and the maximum value is for terms that have large variances in distribution in the items containing the term. The following formula for calculating the weighting factor called Signal (Dennis-67) can be used:  Signalk = Log2 (TOTF) - A VE JNFO  producing a final formula of:  Weight* = TFlk * Signalk  n  Weight* = TF,k * [Log2(TOTFk) - ]T     TF,k/TOTFk Log2 (TFik/TOTFk)]  An example of use of the weighting factor formula is given for the values in Figure  5.5:  SignalsAw = LOG2(50) - [5 * (10/50LOG2(I0/50)} ]  SignalDRILL = LOG2 (50) - [2/50LOG2(2/50) + 2/50LOG2(2/50) +  18/50LOG2( 18/50)+ 10/50LOG2(10/50) + I8/50LOG2(18/50)  The weighting factor for term "DRILL" that does not have a uniform distribution is larger than that for term "SAW" and gives it a higher weight.  This technique could be used by itself or in combination with inverse document frequency or other algorithms. The overhead of the additional data needed in an index and the calculations required to get the values have not been demonstrated to produce better results than other techniques and are not used in any systems at this time. It is a good example of use of Information Theory in developing information retrieval algorithms. Effectiveness of use of this formula can be found in results from Harman and also from Lockbaum and Streeter (Harman-86, Lochbaum-89).
issr-0076	5.2.2.4 Discrimination Value  Another approach to creating a weighting algorithm is to base it upon the discrimination value of a term. To achieve the objective of finding relevant items, it is important that the index discriminates among items. The more all items appear the same, the harder it is to identify those that are needed. Salton and Yang (Salton-73) proposed a weighting algorithm that takes into consideration the ability for a search term to discriminate among items. They proposed use of a discrimination value for each term *i":  DISCRIM, = AVESIM, - AVESiM 120                                                                                              Chapter 5  where AVESIM is the average similarity between every item in the database and AVESIM, is the same calculation except that term "i" is removed from all items. There are three possibilities with the DISCRIMi value being positive, close to zero or negative. A positive value indicates that removal of term "i" has increased the similarity between items. In this case, leaving the term in the database assists in discriminating between items and is of value. A value close to zero implies that the term's removal or inclusion does not change the similarity between items. If the value of DISCRIM, is negative, the term's effect on the database is to make the items appear more similar since their average similarity decreased with its removal Once the value of DISCRMj is normalized as a positive number, it can be used in the standard weighting formula as:  Weight* = TFlk * DISCRIMk
issr-0077	5.2.2.5 Problems With Weighting Schemes  Often weighting schemes use information that is based upon processing token distributions across the database. The two weighting schemes, inverse document frequency and signal, use total frequency and item frequency factors which makes them dependent upon distributions of processing tokens within the database. Information databases tend to be dynamic with new items always being added and to a lesser degree old items being changed or deleted. Thus these factors are changing dynamically. There are a number of approaches to compensate for the constant changing values.  a.   Ignore the variances and calculate weights based upon current values, with the factors changing over time.   Periodically rebuild the complete search database.  b.   Use a fixed value while monitoring changes in the factors.  When the changes reach a certain threshold, start using the new value and update all existing vectors with the new value.  c.   Store the invariant variables (e.g., term frequency within an item) and at search time calculate the latest weights for processing tokens in items needed for search terms.  In the first approach the assumption minimizes the system overhead of maintaining currency on changing values, with the effect that term weights for the same term vary from item to item as the aggregate variables used In calculating the weights based upon changes in the database vary over time. Periodically the database and all term weights are recalculated based upon the most recent updates to the database.   For large databases in the millions of items, the overhead of Automatic Indexing                                                                                    121  rebuilding the database can be significant. In the second approach, there is a recognition that for the most frequently occurring items, the aggregate values are large. As such, minor changes in the values have negligible effect on the final weight calculation. Thus, on a term basis, updates to the aggregate values are only made when sufficient changes not using the current value will have an effect on the final weights and the search/ranking process. This process also distributes the update process over time by only updating a subset of terms at any instance in time. The third approach is the most accurate. The weighted values in the database only matter when they are being used to determine items to return from a query or the rank order to return the items. This has more overhead in that database vector term weights must be calculated dynamically for every query term. If the system is using an inverted file search structure, this overhead is very minor.  An interesting side effect of maintaining currency in the database for term weights is that the same query over time returns a different ordering of items. A new word in the database undergoes significant changes in its weight structure from initial introduction until its frequency in the database reaches a level where small changes do not have significant impact on changes in weight values.  Another issue is the desire to partition an information database based upon time. The value of many sources of information vary exponentially based upon the age of an item (older items have less value). This leads to physically partitioning the database by time (e.g., starting a new database each year), allowing the user to specify the time period to search. There are issues then of how to address the aggregate variables that are different for the same processing token in each database and how to merge the results from the different databases into a single Hit file.  The best environment would allow a user to run a query against multiple different time periods and different databases that potentially use different weighting algorithms, and have the system integrate the results into a single ranked Hit file. This issue is discussed in Chapter 7.
issr-0078	5.2.2.6 Problems With the Vector Model  In addition to the general problem of dynamically changing databases and the effect on weighting factors, there are problems with the vector model on assignment of a weight for a particular processing token to an item. Each processing token can be viewed as a new semantic topic. A major problem comes in the vector model when there are multiple topics being discussed in a particular item. For example, assume that an item has an in-depth discussion of "oil" in "Mexico" and also "coal" in "Pennsylvania." The vector model does not have a mechanism to associate each energy source with its particular geographic area. There is no way to associate correlation factors between terms (i.e., precoordination discussed in Chapter 3) since each dimension in a vector is independent of the other dimensions. Thus the item results in a high value in a search for 4"coal in Mexico." 122  Chapter 5  Another major limitation of a vector space is in associating positional information with a processing term. The concept of proximity searching (e.g., term "a" within 10 words of term "b") requires the logical structure to contain storage of positional information of a processing term. The concept of a vector space allows only one scalar value to be associated with each processing term for each item. Restricting searches to subsets of an item has been shown to provide increased precision (see Chapter 7). In effect this capability overcomes the multitopical item problem by looking at subsets of an item and thus increasing the probability that the subset is discussing a particular semantic topic.
issr-0079	5.2.3 Bayesian Model  One way of overcoming the restrictions inherent in a vector model is to use a Bayesian approach to maintaining information on processing tokens. The Bayesian model provides a conceptually simple yet complete model for information systems. In its most general definition, the Bayesian approach is based upon conditional probabilities (e.g., Probability of Event 1 given Event 2 occurred). This general concept can be applied to the search function as well as to creating the index to the database. The objective of information systems is to return relevant items. Thus the general case, using the Bayesian formula, is P(REL/DOQ , Query,) which is interpreted as the probability of relevance (REL) to a search statement given a particular document and query. Interpretation of this process is discussed in detail in Chapter 7. In addition to search, Bayesian formulas can be used in determining the weights associated with a particular processing token in an item. The objective of creating the index to an item is to represent the semantic information in the item. A Bayesian network can be used to determine the final set of processing tokens (called topics) and their weights. Figure 5.6 shows a simple view of the process where Tj represents the relevance of topic "i" in a particular item and Pj represents a statistic associated with the event of processing token "j" being present in the item.  Figure 5.6 Bayesian Term Weighting Automatic Indexing                                                                                   123  The "m" topics would be stored as the final index to the item. The statistics associated with the processing token are typically frequency of occurrence. But they can also incorporate proximity factors that are useful in items that discuss multiple topics. There is one major assumption made in this model:  Assumption of Binary Independence : the topics and the processing token statistics are independent of each other. The existence of one topic is not related to the existence of the other topics. The existence of one processing token is not related to the existence of other processing tokens.  In most cases this assumption is not true. Some topics are related to other topics and some processing tokens related to other processing tokens. For example, the topics of "Politics" and "Economics" are in some instances related to each other (e.g., an item discussing Congress debating laws associated with balance of trade) and in many other instances totally unrelated. The same type of example would apply to processing tokens. There are two approaches to handling this problem. The first is to assume that there are dependencies, but that the errors introduced by assuming the mutual independence do not noticeably effect the determination of relevance of an item nor its relative rank associated with other retrieved items. This is the most common approach used in system implementations. A second approach can extend the network to additional layers to handle interdependencies. Thus an additional layer of Independent Topics (ITs) can be placed above the Topic layer and a layer of Independent Processing Tokens (IPs) can be placed above the processing token layer. Figure 5.7 shows the extended Bayesian network. Extending the network creates new processing tokens for those cases where there are dependencies between processing tokens. The new set of Independent Processing Tokens can then be used to define the attributes associated with the set of topics selected to represent the semantics of an item. To compensate for dependencies between topics the final layer of Independent Topics is created. The degree to which each layer is created depends upon the error that could be introduced by allowing for dependencies between Topics or Processing Tokens. Although this approach is the most mathematically correct, it suffers from losing a level of precision by reducing the number of concepts available to define the semantics of an item.
issr-0080	53 Natural Language  The goal of natural language processing is to use the semantic information in addition to the statistical information to enhance the indexing of the item. This improves the precision of searches, reducing the number of false hits a user reviews. The semantic information is extracted as a result of processing the 124  Chapter 5  language rather than treating each word as an independent entity. The simplest output of this process results in generation of phrases that become indexes to an item. More complex analysis generates thematic representation of events rather  Figure 5.7 Extended Bayesian Network  than phrases. Statistical approaches use proximity as the basis behind determining the strength of word relationships in generating phrases. For example, with a proximity constraint of adjacency,    the phrases 'Venetian blind" and "blind  Venetian" may appear related and map to the same phrase.  But syntactically and  semantically those phrases are very different concepts. Word phrases generated by natural language processing algorithms enhance indexing specification and provide another level of disambiguation. Natural language processing can also combine the concepts into higher level concepts sometimes referred to as thematic representations. One example represents them as concept-relationship-concept triples   (Liddy-93).
issr-0081	Automatic Indexing                                                                                   125  5.3.1 Index Phrase Generation  The goal of indexing is to represent the semantic concepts of an item in the information system to support finding relevant information. Single words have conceptual context, but frequently they are too general to help the user find the desired information. Term phrases allow additional specification and focusing of the concept to provide better precision and reduce the user's overhead of retrieving non-relevant items. Having the modifier "grass" or "magnetic" associated with the term "field" clearly disambiguates between very different concepts. One of the earliest statistical approaches to determining term phrases proposed by Salton was use of a COHESION factor between terms (Salton-83):  COHESION^ = SIZE-FACTOR * (PAIR-FREQk,h / TOTFk * TOTFH)  where SIZE-FACTOR is a normalization factor based upon the size of the vocabulary and PAIR-FREQkj, is the total frequency of co-occurrence of the pair Teraik Ternih in the item collection. Co-occurrence may be defined in terms of adjacency, word proximity, sentence proximity, etc. This initial algorithm has been modified in the SMART system to be based on the following guidelines (BUCKLEY-95):  any pair of adjacent non-stop words is a potential phrase any pair must exist in 25 or more items  phrase weighting uses a modified version of the SMART system single term algorithm  normalization is achieved by dividing by the length of the single-term subvector.  Natural language processing can reduce errors in determining phrases by determining inter-item dependencies and using that information to create the term  phrases used in the indexing process. Statistical approaches tend to focus on two term phrases. A major advantage of natural language approaches is their ability to  produce multiple-term phrases to denote a single concept. If a phrase such as "industrious intelligent students" was used often, a statistical approach would create phrases such as "industrious intelligent" and "intelligent student." A natural language approach would create phrases such as "industrious student," "intelligent student" and "industrious intelligent student."  The first step in a natural language determination of phrases is a lexical analysis of the input. In its simplest form this is a part of speech tagger that, for example, identifies noun phrases by recognizing adjectives and nouns. Precise part 126                                                                                               Chapter5  of speech taggers exist that are accurate to the 99 per cent range. Additionally, proper noun identification tools exist that allow for accurate identification of names, locations and organizations since these values should be indexed as phrases and not undergo stemming. Greater gains come from identifying syntactic and semantic level dependencies creating a hierarchy of semantic concepts. For example, "nuclear reactor fusion" could produce term phrases of "nuclear reactor" and "nuclear fusion." In the ideal case all variations of a phrase would be reduced to a single canonical form that represents the semantics for a phrase. Thus, where possible the phrase detection process should output a normalized form. For example, "blind Venetian" and "Venetian who is blind" should map to the same phrase. This not only increases the precision of searches, but also increases the frequency of occurrence of the common phrase. This, in turn, improves the likelihood that the frequency of occurrence of the common phrase is above the threshold required to index the phrase. Once the phrase is indexed, it is available for search, thus participating in an item's selection for a search and the rank associated with an item in the Hit file. One solution to finding a common form is to transform the phrases into a operator-argument form or a header-modifier form. There is always a category of semantic phrases that comes from inferring concepts from an item that is non-determinable. This comes from the natural ambiguity inherent in languages that is discussed in Chapter 1.  A good example of application of natural language to phrase creation is in the natural language information retrieval system at New York University developed in collaboration with GE Corporate Research and Development (Carballo-95). The text of the item is processed by a fast syntactical process and extracted phrases are added to the index in addition to the single word terms. Statistical analysis is used to determine similarity links between phrases and identification of subphrases. Once the phrases are statistically noted as similar, a filtering process categorizes the link onto a semantic relationship (generality, specialization, antonymy, complementation, synonymy, etc.).  The Tagged Text Parser (TTP), based upon the Linguistic String Grammar (Sager-81), produces a regularized parse tree representation of each sentence reflecting the predicate-argument structure (Strzalkowski-93). The tagged text parser contains over 400 grammar production rules. Some examples of the part of speech tagger identification are given in Figure 5.8.  CLASS                                         EXAMPLES  determiners                                   a, the  singular nouns                              paper, notation, structure, language  plural nouns                                  operations, data, processes  preposition                                    in, by, of, for  adjective                                       high, concurrent  present tense verb                          presents, associates  present participal                           multiprogramming  5.8 Part of Speech Tags Automatic Indexing                                                                                   127  The TTP parse trees are header-modifier pairs where the header is the main concept and the modifiers are the additional descriptors that form the concept and eliminate ambiguities. Figure 5.9 gives an example of a regularized parse tree structure generated for the independent clause:  The former Soviet President has been a local hero   ever since a Russian tank invaded Wisconsin  |assert  perf[HAVE]  verb[BE]  subject  np  noun[President] t_pos[The] adj [former] adj [Soviet] object np  noun [hero] t__pos[a] adj [local] adv[ever] sub_ord [since] verb[invade] subject np  nounftank] t__pos[a] adj[Russian] object np noun[Wisconsin]  Figure 5.9 TTP Parse Tree  This structure allows for identification of potential term phrases usually based upon noun identification. To determine if a header-modifier pair warrants indexing, Strzalkowski calculates a value for Informational Contribution (IC) for each element in the pair. Higher values of IC indicate a potentially stronger semantic relationship between terms. The basis behind the IC formula is a conditional probability between the terms. The formula for IC between two terms (x,y) is; 128                                                                                                Chapter 5  where fxy is the frequency of (x,y) in the database, nx is the number of pairs in which "x" occurs at the same position as in (x,y) and D(x) is the dispersion parameter which is the number of distinct words with which x is paired. When IC=1, x occurs only with y (fx,y=nx and dx = 1).  Nominal compounds are the source of many inaccurate identifications in creating header-modifier pairs. Use of statistical information on frequency of occurrence of phrases can eliminate some combinations that occur infrequently and are not meaningful.  The next challenge is to assign weights to term phrases. The most popular term weighting scheme uses term frequencies and inverse document frequencies with normalization based upon item length to calculate weights assigned to terms (see Section 5.2.2.2). Term phrases have lower frequency occurrences than the individual terms. Using natural language processing, the focus is on semantic relationships versus frequency relationships. Thus weighting schemes such as inverse document frequency require adjustments so that the weights are not overly diminished by the potential lower frequency of the phrases. For example, the weighting scheme used in the New York University system uses the following formula for weighting phrases:  weight(PhraseO = (Ci*log(termf) + C2*oc(N,i))*IDF  where a(N,i) is 1 for ilt;N and 0 otherwise and Ci and C2 are normalizing factors.  The N assumes the phrases are sorted by IDF value and allows the top "N" highest  IDF (inverse document frequency) scores to have a greater effect on the overall  weight than other terms.
issr-0082	5.3.2 Natural Language Processing  Section 5.3.1 discussed generation of term phrases as indexes. Lexical analysis determining verb tense, plurality and part of speech is assumed to have been completed prior to the following additional processing. Natural language processing not only produces more accurate term phrases, but can provide higher level semantic information identifying relationships between concepts.  The DR-LINK system (Liddy-93) and Its commercial Implementation via Textwise System adds the functional processes Relationship Concept Detectors, Conceptual Graph Generators and Conceptual Graph Matchers that generate higher level linguistic relationships including semantic and discourse level relationships. This system is representative of natural language based processing systems. During the first phase of this approach, the processing tokens in the document are mapped to Subject Codes as defined by the codes in the Longman's Automatic Indexing                                                                                   129  Dictionary of Common English (LDOCE). Disambiguation uses a priori statistical term relationships and the ordering of the subject codes in the LDOCE, which indicates most likely assignment of a term to a code. These codes equate to index term assignment and have some similarities to the concept-based systems discussed in Section 5.4.  The next phase is called the Text Structurer, which attempts to identify general discourse level areas within an item. Thus a news story may be subdivided into areas associated with EVALUATION (opinions), Main event (basic facts), and Expectations (Predictions). These have been updated to include Analytical Information, Cause/Effect Dimension and Attributed Quotations in the more recent versions of DR-LINK (see http://199.100.962 on the Internet). These areas can then be assigned higher weighting if the user includes "Preference" in a search statement. The system also attempts to determine TOPIC statement identifiers. Natural language processing is not just determining the topic statement(s) but also assigning semantic attributes to the topic such as time frame (past, present, future). To perform this type analysis, a general model of the predicted text is needed. For example, news items likely follow a model proposed by van Dijk (Dijk-88). Liddy reorganized this structure into a News Schema Components consisting of Circumstance, Consequence, Credentials, Definition, Error, Evaluation, Expectation, History, Lead, Main Event, No Comment, Previous Event, References and Verbal reaction. Each sentence is evaluated and assigned weights associated with its possible inclusion in the different components. Thus, if a query is oriented toward a future activity, then, in addition to the subject code vector mapping, it would weight higher terms associated with the Expectation component.  The next level of semantic processing is the assignment of terms to components, classifying the intent of the terms in the text and identifying the topical statements. The next level of natural language processing identifies interrelationships between the concepts. For example, there may be two topics within an item "national elections" and "guerrilla warfare." The relationship "as a result of is critical to link the order of these two concepts. This process clarifies if the elections were caused by the warfare or the warfare caused by the elections. Significant information is lost by not including the connector relationships. These types of linkages are generated by general linguistic cues (words in text) that are fairly general and domain independent.  The final step is to assign final weights to the established relationships. The relationships are typically envisioned as triples with two concepts and a relationship between them. Although all relationships are possible, constructing a system requires the selection of a subset of possible relationships and the rules to locate the relationships. The weights are based upon a combination of statistical information and values assigned to the actual words used in establishing the linkages. Passive verbs would receive less weight than active verbs.  The additional information beyond the indexing is kept in additional data structures associated with each item. This information is used whenever it is implicitly included in a search statement that is natural language based or explicitly requested by the user.
issr-0083	130                                                                                              Chapter 5  5.4 Concept Indexing  Natural language processing starts with a basis of the terms within an item and extends the information kept on an item to phrases and higher level concepts such as the relationships between concepts. In the DR-LINK system, terms within an item are replaced by an associated Subject Code. Use of subject codes or some other controlled vocabulary is one way to map from specific terms to more general terms. Often the controlled vocabulary is defined by an organization to be representative of the concepts they consider important representations of their data. Concept indexing takes the abstraction a level further. Its goal is to gain the implementation advantages of an index term system but use concepts instead of terms as the basis for the index, producing a reduced dimension vector space.  Rather than a priori defining a set of concepts that the terms in an item are mapped to, concept indexing can start with a number of unlabeled concept classes and let the information in the items define the concepts classes created. The process of automatic creation of concept classes is similar to the automatic generation of thesaurus classes described in Chapter 6. The process of mapping from a specific term to a concept that the term represents is complex because a term may represent multiple different concepts to different degrees. A term such as "automobile" could be associated with concepts such as "vehicle," "transportation," "mechanical device," "fuel," and "environment." The term "automobile" is strongly related to "vehicle," lesser to "transportation" and much lesser the other terms. Thus a term in an item needs to be represented by many concept codes with different weights for a particular item.  An example of applying a concept approach is the Convectis System from HNC Software Inc. (Caid-93, Carleton-95). The basis behind the generation of the concept approach is a neural network model (Waltz-85). Context vector representation and its application to textual items is described by Gallant (Gallant91a, Gallant-91b). If a vector approach is envisioned, then there is a finite number of concepts that provide coverage over all of the significant concepts required to index a database of items. The goal of the indexing is to allow the user to find required information, minimizing the reviewing of items that are non-relevant. In an ideal environment there would be enough vectors to account for all possible concepts and thus they would be orthogonal in an "N" dimensional vector-space model. It is difficult to find a set of concepts that are orthogonal with no aspects in common. Additionally, implementation trade offs naturally limit the number of concept classes that are practical. These limitations increase the number of classes to which a processing token is mapped.  The Convectis system uses neural network algorithms and terms in a similar context (proximity) of other terms as a basis for determining which terms are related and defining a particular concept. A term can have different weights associated with different concepts as described. The definition of a similar context is typically defined by the number of non-stop words separating the terms.   The Automatic Indexing                                                                                   131  farther apart terms are, the less coupled the terms are associated within a particular concept class. Existing terms already have a mapping to concept classes. New terms can be mapped to existing classes by applying the context rules to the classes that terms near the new term are mapped. Special rules must be applied to create a new concept class. Example 5.9 demonstrates how the process would work for the term "automobile."  TERM: automobile  Weights for associated concepts:  Vehicle                                                              .65  Transportation                                                   .60  Environment                                                      .35  Fuel                                                                   .33  Mechanical Device                                             . 15  Vector Representation Automobile: (.65,..., .60,..., .35, .33,..., .15) Figure 5.10 Concept Vector for Automobile  Using the concept representation of a particular term, phrases and complete items can be represented as a weighted average of the concept vectors of the terms in them. The algorithms associated with vectors (e.g., inverse document frequency) can be used to perform the merging of concepts.  Another example of this process is Latent Semantic Indexing (LSI). Its assumption is that there is an underlying or "latent" structure represented by interrelationships between words (Deerwester-90, Dempster-77, Dumais-95, Gildea-99, Hofmann-99). The index contains representations of the "latent semantics" of the item. Like Convectis, the large term-document matrix is decomposed into a small set (e.g., 100-300) of orthogonal factors which use linear combinations of the factors (concepts) to approximate the original matrix. Latent Semantic Indexing uses singular-value decomposition to model the associative relationships between terms similar to eigenvector decomposition and factor analysis (see Cullum-85).  Any rectangular matrix can be decomposed into the product of three matrices. Let X be a mxn matrix such that:  where To and Do have orthogonal columns and are m x r and r x n matrices, So is anrxr diagonal matrix and r is the rank of matrix X.   This is the singular value 132                                                                                              Chapter 5  decomposition ofX   The k largest singular values of So are kept along with their corresponding columns in To and Do matrices, the resulting matrix:  is the unique matrix of rank k that is closest in least squares sense to X.   The  matrix X, containing the first k independent linear components of the original X represents the major associations with noise eliminated.  If you consider X to be the term-document matrix (e.g., all possible terms being represented by columns and each item being represented by a row), then truncated singular value decomposition can be applied to reduce the dimmensionality caused by all terms to a significantly smaller dimensionality that is an approximation of the original X:  where u} ... uk and v1 ... vk are left and right singular vectors and svj ... svk are singualr values. A threshold is used against the full SV diagnonal matrix to determine the cutoff on values to be used for query and document representation (i.e., the dimensionality reduction). Hofmann has modified the standard LSI approach using addional formalism via Probabilistic Latent Semantic Analysis (Hofmann-99).  With so much reduction in the number of words, closeness is determined by patterns of word usage versus specific co-locations of terms. This has the effect of a thesaurus in equating many terms to the same concept. Both terms and documents (as collections of terms) can be represented as weighted vectors in the k dimensional space. The selection of k is critical to the success of this procedure. If k is too small, then there is not enough discrimination between vectors and too many false hits are returned on a search. If k is too large, the value of Latent Semantic Indexing is lost and the system equates to a standard vector model.
issr-0084	5.5 Hypertext Linkages  A new class of information representation, described in Chapter 4 as the hypertext data structure, is evolving on the Internet. Hypertext data structures must be generated manually although user interface tools may simplify the process. Very little research has been done on the information retrieval aspects of hypertext linkages and automatic mechanisms to use the information of item pointers in creating additional search structures. In effect, hypertext linkages are creating an additional information retrieval dimension. Traditional items can be viewed as two dimensional constructs. The text of the items is one dimension representing the Automatic Indexing                                                                                   133  information in the items. Imbedded references are a logical second dimension that has had minimal use in information search techniques. The major use of the citations has been in trying to determine the concepts within an item and clustering items (Salton-83). Hypertext, with its linkages to additional electronic items, can be viewed as networking between items that extends the contents. To understand the total subject of an item it is necessary to follow these additional information concept paths. The imbedding of the linkage allows the user to go immediately to the linked item for additional information. The issue is how to use this additional dimension to locate relevant information.  The easiest approach is to do nothing and let the user follow these paths to view items. But this is avoiding one of the challenges in information systems on creating techniques to assist the user in finding relevant information. Looking at the Internet at the current time there are three classes of mechanisms to help find information: manually generated indexes, automatically generated indexes and web crawlers (intelligent agents). YAHOO (http://www.yahoo.com) is an example of the first case where information sources (home pages) are indexed manually into a hyperlinked hierarchy. The user can navigate through the hierarchy by expanding the hyperlink on a particular topic to see the more detailed subtopics. At some point the user starts to see the end items. LYCOS (http://www.lycos.com) and Altavista (http://www.altavista.digital.com) automatically go out to other Internet sites and return the text at the sites for automatic indexing. Lycos returns home pages from each site for automatic indexing while Altavista indexes all of the text at a site. None of these approaches use the linkages in items to enhance their indexing.  Webcrawlers (e.g., WebCrawler, OpenText, Pathfinder) and intelligent agents (Coriolis Groups' NetSeekerô) are tools that allow a user to define items of interest and they automatically go to various sites on the Internet searching for the desired information. They are better described as a search tool than an indexing tool that a priori analyzes items to assist in finding them via a search.  What is needed is an index algorithm for items that looks at the hypertext linkages as an extension of the concepts being presented in the item where the link exists. Some links that are for references to multi-media imbedded objects would not be part of the indexing process. The Universal Reference Locator (URL) hypertext links can map to another item or to a specific location within an item. The current concept is defined by the information within proximity of the location of the link. The concepts in the linked item, or with a stronger weight the concepts in the proximity of the location included in the link, need to be included in the index of the current item. If the current item is discussing the financial state of Louisiana and a hyperlink is included to a discussion on crop damage due to draughts in the southern states, the index should allow for a t4hif on a search statement including "droughts in Louisiana."  One approach is to view the hyperlink as an extension of the text of the item in another dimension. The index values of the hyperlieked item has a reduced weighted value from contiguous IqxI biased by the type of linkage. The weight of processing tokens appears: 134                                                                                              Chapters  Weighty = (a * Weighty +p*Weightk,,) * (y *Linki,k)  where Weighty is the Weight associated with processing token "j" in item"i" and processing token "1" in item "k" that are related via a hyperlink. Link^ is the weight associated with strength of the link. It could be a one-level link that is weak or strong, or it could be a multilevel transitive link, a, (3 and y are weighting/normalization factors. The values could be stored in an expanded index structure or calculated dynamically if only the hyperlink relationships between items are available.  Taking another perspective, the system could automatically generate hyperlinks between items. Attempts have been made to achieve this capability, but they suffer from working with static versus dynamic growing databases or ignoring the efficiency needed for an operational environment (Allan-95, Furuta-89, Rearick-91). Kellog and Subhas have proposed a new solution based upon document segmentation and clustering (Kellog-96). They link at both the document and document sub-part level using the cover-coefficient based incremental clustering method (C2ICM) to generate links between the document (document sub-parts) pairs for each cluster. (Can-95). The automatic link generation phase is performed in parallel with the clustering phase. Item pairs in the same cluster are candidates for hyperlinking (link-similarity) if they have a similarity above a given threshold. The process is completed in two phases. In the first phase the document seeds and an estimate of the number of clusters is calculated. In the second phase the items are clustered and the links are created. Rather than storing the link information within the item or storing a persistent link ID within the item and the link information externally, they store all of the link information externally. They create HTML items on demand. When analyzing links missed by their algorithm, three common problems were discovered:  misspellings or multiple word representations (e.g., cabinet maker and cabinetmaker)  parser problems with document segmentation caused by punctuation  errors (lines were treated as paragraphs and sentences)  problems occurred when the definition of sobparts (smaller sentences) of items was attempted  A significant portion of errors came from parsing rather than algorithmic problems. This technique has maximum effectiveness for referential links which naturally have higher similarity measures.
issr-0085	Automatic Indexing                                                                                    135  5.6 Summary  Automatic indexing is the preprocessing stage allowing search of items in an Information Retrieval System. Its role is critical to the success of searches in finding relevant items. If the concepts within an item are not located and represented in the index during this stage, the item is not found during search. Some techniques allow for the combinations of data at search time to equate to particular concepts (i.e. postcoordination). But if the words are not properly identified at indexing time and placed in the searchable data structure, the system can not combine them to determine the concept at search time. If an inefficient data structure is selected to hold the index, the system does not scale to accommodate large numbers of items.  The steps in the identification of the processing tokens used in the index process were generally discussed in Chapter 3. Chapter 5 focuses on the specific characteristics of the processing tokens to support the different search techniques. There are many ways of defining the techniques. All of the techniques have statistical algorithmic properties. But looking at the techniques from a conceptual level, the approaches are classified as statistical, natural language and concept indexing. Hypertext linkages are placed in a separate class because an algorithm to search items that include linkages has to address dependencies between items. Normally the processing for processing tokens is restricted to an item. The next item may use some corpus statistics that changed by previous items, but does not consider a tight coupling between items. In effect, one item may be considered an extension of another, which should effect the concept identification and representation process.  Of all the statistical techniques, an accurate probabilistic technique would have the greatest benefit in the search process. Unfortunately, identification of consistent statistical values used in the probabilistic formulas has proven to be a formidable task. The assumptions that must be made significantly reduce the accuracy of the search process. Vector techniques have very powerful representations and have been shown to be successful. But they lack the flexibility to represent items that contain many distinct but overlapping concepts. Bayesian techniques are a way to relax some of the constraints inherent in a pure vector approach, allowing dependencies between concepts within the same item to be represented. Most commercial systems do not try to calculate weighted values at index time. It is easier and more flexible to store the basic word data for each item and calculate the statistics at search time. This allows tuning the algorithms without having to re-index the database. It also allows the combination of statistical and traditional Boolean techniques within the same system.  Natural language systems attempt to introduce a higher level of abstraction indexing on top of the statistical processes. Making use of rules associated with language assist in the disambiguation of terms and provide an additional layer of concepts that are not found in purely statistical systems. Use of natural language processing provides the additional data that could focus searches, 136                                                                                               Chapter 5  reducing the retrieval of non-relevant items. The tendency of users to enter short queries may reduce the benefits of this approach.  Concept indexing is a statistical technique whose goal is to determine a canonical representation of the concepts. It has been shown to find relevant items that other techniques miss. In its transformation process, some level of precision is lost. The analysis of enhanced recall over potential reduced precision is still under investigation.
issr-0086	6 Document and Term Clustering  6.1     Introduction to Clustering  6.2    Thesaurus Generation  6.3    Item Clustering  6.4    Hierarchy of Clusters  6.5     Summary  Chapter 5 introduced indexing associated with representation of the semantics of an item. In all of the techniques discussed in Chapter 5, our information database can be viewed as being composed of a number of independent items indexed by a series of index terms. This model lends itself to two types of clustering: clustering index terms to create a statistical thesaurus and clustering items to create document clusters. In the first case clustering is used to increase recall by expanding searches with related terms. In document clustering the search can retrieve items similar to an item of interest, even if the query would not have retrieved the item. The clustering process is not precise and care must be taken on use of clustering techniques to minimize the negative impact misuse can have. These issues are discussed in Section 6.1 along with some general guidelines of clustering.  Section 6.2 discusses a variety of specific techniques to create thesaurus clusters. The techniques can be categorized as those that use the complete database to perform the clustering and those that start with some initial structure. Section 6.3 looks at the same techniques as they apply to item (document) clustering. A class of clustering algorithms creates a hierarchical output. The hierarchy of clusters usually reflects more abstract concepts in the higher levels and more detailed specific items in the lower levels. Given the large data sets in information retrieval systems, it is essential to optimize the clustering process in terms of time and required processing power. Hierarchical clustering and its associated performance improvements are described in Section 6.4.
issr-0087	140                                                                                               Chapter 6  6.1  Introduction to Clustering  The concept of clustering has been around as long as there have been libraries. One of the first uses of clustering was an attempt to cluster items discussing the same subject. The goal of the clustering was to assist in the location of information. This eventually lead to indexing schemes used in organization of items in libraries and standards associated with use of electronic indexes. Clustering of words originated with the generation of thesauri. Thesaurus, coming from the Latin word meaning "treasure," is similar to a dictionary in that it stores words. Instead of definitions, it provides the synonyms and antonyms for the words. Its primary purpose is to assist authors in selection of vocabulary. The goal of clustering is to provide a grouping of similar objects (e.g., terms or items) into a "class" under a more general title. Clustering also allows linkages between clusters to be specified. The term class is frequently used as a synonym for the term cluster. They are used interchangeably in this chapter.  The process of clustering follows the following steps:  a.    Define the domain for the clustering effort.   If a thesaurus is being created, this equates to determining the scope of the thesaurus such as "medical terms."     If document clustering is being performed,  it  is determination of the set of items to be clustered.  This can be a subset of the database or the complete database.    Defining the domain for the clustering identifies those objects to be used in the clustering process and reduce the potential for erroneous data that could induce errors in the clustering process.  b.   Once the domain is determined, determine the attributes of the objects to be clustered.  If a thesaurus is being generated, determine the specific words in the objects to be used in the clustering process.   Similarly, if documents are being clustered, the clustering process may focus on  specific zones within the items (e.g., Title and abstract only, main body of the item but not the references, etc.) that are to be used to determine similarity.  The objective, as with the first step (a.) is to reduce erroneous  associations.  c.    Determine the strength of the relationships between the attributes  whose co-occunrence in objects suggest those objects should be in the same class. For thesauri this is determining which words are synonyms and the strength of their term relationships. For documents it may be defining a similarity function based upon word co-occurrences that determine the similarity between two items.  d.     At this point, the total set of objects and the strengths of the relationships between the objects have been determined. The final step is Document and Term Clustering                                                                  141  applying some algorithm to determine the class(s) to which each item will be assigned.  There are guidelines (not hard constraints) on the characteristics of the classes:  A well-defined semantic definition should exist for each class. There is a risk that the name assigned to the semantic definition of the class could also be misleading. In some systems numbers are assigned to classes to reduce the misinterpretation that a name attached to each class could have. A clustering of items into a class called "computer" could mislead a user into thinking that it includes items on main memory that may actually reside in another class called "hardware."  The size of the classes should be within the same order of magnitude. One of the primary uses of the classes is to expand queries or expand the resultant set of retrieved items. If a particular class contains 90 per cent of the objects, that class is not useful for either purpose. It also places in question the utility of the other classes that are distributed across 10 per cent of the remaining objects.  Within a class, one object should not dominate the class. For example, assume a thesaurus class called "computer" exists and it contains the objects (words/word phrases) "microprocessor," "286-processor," "386processor" and "pentium." If the term "microprocessor" is found 85 per cent of the time and the other terms are used 5 per cent each, there is a strong possibility that using "microprocessor" as a synonym for "286processor" will introduce too many errors. It may be better to place "microprocessor" into its own class.  Whether an object can be assigned to multiple classes or just one must be decided at creation time. This is a tradeoff based upon the specificity and partitioning capability of the semantics of the objects. Given the ambiguity of language in general, it is better to allow an object to be in  multiple classes rather than constrained to one. This added flexibility comes at a cost of additional complexity in creating and maintaining the classes.  There are additional important decisions associated with the generation of thesauri that are not part of item clustering (Aitchison-72):  Word coordination approach: specifies if phrases as well as individual terms are to be clustered (see discussion on precoordinatioe and postcoordination in Chapter 3). 142                                                                                               Chapter 6  Word relationships: when the generation of a thesaurus includes a human interface (versus being totally automated), a variety of relationships between words are possible. Aitchison and Gilchrist (Aitchison-72) specified three types of relationships: equivalence, hierarchical and nonhierarchical. Equivalence relationships are the most common and represent synonyms. The definition of a synonym allows for some discretion in the thesaurus creation, allowing for terms that have significant overlap but differences. Thus the terms photograph and print may be defined as synonyms even though prints also include lithography. The definition can even be expanded to include words that have the same "role" but not necessarily the same meaning. Thus the words "genius" and "moron" may be synonyms in a class called "intellectual capability." A very common technique is hierarchical relationships where the class name is a general term and the entries are specific examples of the general term. The previous example of "computer" class name and "microprocessor," "pentium," etc. is an example of this case. Nonhierarchical relationships cover other types of relationships such as "object"-"attribute" that would contain "employee" and "job title."  A more recent word relationship scheme (Wang-85) classified relationships as Parts-Wholes, Collocation, Paradigmatic, Taxonomy and Synonymy, and Antonymy. The only two of these classes that require further amplification are collocation and paradigmatic. Collocation is a statistical measure that relates words that co-occur in the same proximity (sentence, phrase, paragraph). Paradigmatic relates words with the same semantic base such as "formula" and "equation."  In the expansion to semantic networks other relationships are included such as contrasted words, child-of (sphere is a child-of geometric volume), parent-of, part-of (foundation is part of a building), and contains part-of (bicycle contains parts-of wheel, handlebars) (RetrievalWare-95).  Homograph resolution:    a homograph  is a word that has multiple,  completely different meanings. For example, the term "field" could mean a electronic field, a field of grass, etc.    It is difficult to eliminate  homographs by supplying a unique meaning for every homograph (limiting the thesaurus domain helps). Typically the system allows for homographs and requires that the user interact with the system to select the desired meaning. It is possible to determine the correct meaning of the homograph when a user enters multiple search terms by analyzing the other terms entered (hay, crops, and field suggest the agricultural meaning for field).  Vocabulary constraints: this includes guidelines on the normalization and specificity of the vocabulary. Normalization may constrain the thesaurus to stems versos complete words. Specificity may eliminate specific words Document and Term Clustering                                                                 143  or use general terms for class identifiers.    The previous discussion in Chapter 3 on these topics applies to their use in the thesauri.  As is evident in these guidelines, clustering is as much an arcane art as it is a science. Good clustering of terms or items assists the user by improving recall. But typically an increase in recall has an associated decrease in precision. Automatic clustering has the imprecision of information retrieval algorithms, compounding the natural ambiguities that come from language. Care must be taken to ensure that the increases in recall are not associated with such decreases in precision as to make the human processing (reading) of the retrieved items unmanageable. The key to successful clustering lies in steps c. and d., selection of a good measure of similarity and selection of a good algorithm for placing items in the same class. When hierarchical item clustering is used, there is a possibility of a decrease in recall discussed in Section 6.4. The only solution to this problem is to make minimal use of the hierarchy.
issr-0088	6.2 Thesaurus Generation  Manual generation of clusters usually focuses on generating a thesaurus (i.e., clustering terms versus items) and has been used for hundreds of years. As items became available in electronic form, automated term statistical clustering techniques became available. Automatically generated thesauri contain classes that reflect the use of words in the corpora. The classes do not naturally have a name, but are just a groups of statistically similar terms. The optimum technique for generating the classes requires intensive computation. Other techniques starting with existing clusters can reduce the computations required but may not produce optimum classes.  There are three basic methods for generation of a thesaurus; hand crafted, co-occurrence, and header-modifier based. Using manually made thesauri only helps in query expansion if the thesauri is domain specific for the domain being searched. General thesaurus (e.g., WordNet) does not help as much because of the many different meanings for the same word (Voorhees-93, Voorhees-94). Techniques for co-occurrence creation of thesauri are described in detail below. In header-modifier based thesauri term relationships are found based upon linguistic relationships. Words appearing in similar grammatical contexts are assumed to be similar (Hindle-90, Grafenstette-94, Jing-94, Ruge-92). The linguistic parsing of the document discovers the following syntactical structures: Subject-Verb, VerbObject, Adjective-Noun, and Noun-Noun. Each noun has a set of verbs, adjectives and nouns that it co-occurs with, and a mutual information value is calculated for each using typically a log function (see Mandala-99). Then a final similarity between words is calculated using the mutual information to classify the terms. 144                                                                                               Chapter 6  6.2.1  Manual Clustering  The manual clustering process follows the steps described in Section 6.1 in the generation of a thesaurus. The first step is to determine the domain for the clustering. Defining the domain assists in reducing ambiguities caused by homographs and helps focus the creator. Usually existing thesauri, concordances from items that cover the domain and dictionaries are used as starting points for generating the set of potential words to be included in the new thesaurus. A concordance is an alphabetical listing of words from a set of items along with their frequency of occurrence and references of which items in which they are found. The art of manual thesaurus construction resides in the selection of the set of words to be included. Care is taken to not include words that are unrelated to the domain of the thesaurus or those that have very high frequency of occurrence and thus hold no information value (e.g., the term Computer in a thesaurus focused on data processing machines). If a concordance is used, other tools such as KWOC, KWIC or KWAC may help in determining useful words. A Key Word Out of Context (KWOC) is another name for a concordance. Key Word In Context (KWIC) displays a possible term in its phrase context. It is structured to identify easily the location of the term under consideration in the sentence. Key Word And Context (KWAC) displays the keywords followed by their context. Figure 6.1 shows the various displays for "computer design contains memory chips" (NOTE: the phrase is assumed to be from doc4; the other frequency and document ids for KWOC were  KWOC  TERM FREQ                    ITEM Ids  chips 2                       doc2, doc4  computer 3                       docl, doc4, doclO  design 1                       doc4  memory 3                       doc3, doc4, doc8, docl2  KWIC   chips/ computer design contains memory  computer design contains memory chips/  design contains memory chips/ computer  memory chips/ computer design contains  KWAC   chips computer design contains memory chips  computer computer design contains memory chips  design computer design contains memory chips  memory computer design contains memory chips  Figure 6.1 Example of KWOC, KWIC and KWAC Document and Term Clustering                                                                  145  created for this example.) In the Figure 6.1 the character "/" is used in KWIC to indicate the end of the phrase. The KWiC and KWAC are useful in determining the meaning of homographs. The term "chips" could be wood chips or memory chips. In both the KWIC and KWAC displays, the editor of the thesaurus can read the sentence fragment associated with the term and determine its meaning. The KWOC does not present any information that would help in resolving this ambiguity.  Once the terms are selected they are clustered based upon the word relationship guidelines and the interpretation of the strength of the relationship. This is also part of the art of manual creation of the thesaurus, using the judgment of the human analyst. The resultant thesaurus undergoes many quality assurance reviews by additional editors using some of the guidelines already suggested before it is finalized.
issr-0089	6.2.2 Automatic Term Clustering  There are many techniques for the automatic generation of term clusters to create statistical thesauri. They all use as their basis the concept that the more frequently two terms co-occur in the same items, the more likely they are about the same concept. They differ by the completeness with which terms are correlated. The more complete the correlation, the higher the time and computational overhead to create the clusters. The most complete process computes the strength of the relationships between all combinations of the "n" unique words with an overhead of O(n2). Other techniques start with an arbitrary set of clusters and iterate on the assignment of terms to these clusters. The simplest case employs one pass of the data in creation of the clusters. When the number of clusters created is very large, the initial clusters may be used as a starting point to generate more abstract clusters creating a hierarchy.  The steps described in Section 6.1 apply to the automatic generation of thesauri. The basis for automatic generation of a thesaurus is a set of items that represents the vocabulary to be included in the thesaurus. Selection of this set of items is the first step of determining the domain for the thesaurus. The processing tokens (words) in the set of items are the attributes to be used to create the clusters. Implementation of the other steps differs based upon the algorithms being applied. In the following sections a term is usually restricted to be included in only one class. It is also possible to use a threshold instead of choosing the highest value, allowing a term to be assigned to all of the classes that it could be included in above the threshold. The automated method of clustering documents is based upon the polythetic clustering (Van Rijsbergen-79) where each cluster is defined by a set of words and phrases. Inclusion of an item in a cluster is based upon the similarity of the item's words and phrases to those of other items in the cluster.
issr-0090	146                                                                                               Chapter 6  6.2.2.1  Complete Term Relation Method  In the complete term relation method, the similarity between every term pair is calculated as a basis for determining the clusters. The easiest way to understand this approach is to consider the vector model. The vector model is represented by a matrix where the rows are individual items and the columns are the unique words (processing tokens) in the items. The values in the matrix represent how strongly that particular word represents concepts in the item. Figure 6.2 provides an example of a database with 5 items and 8 terms.  To determine the relationship between terms, a similarity measure is required. The measure calculates the similarity between two terms. In Chapter 7 a number of similarity measures are presented. The similarity measure is not critical   Terml Term2 Term3 Term4 Term5 Term6 Term7 Term8  Item 1 0 4 0 0 0 2 1 3  Item 2 3 1 4 3 1 2 0 1  Item 3 3 0 0 0 3 0 3 0  Item 4 0 1 0 3 0 0 2 0  ItemS 2 2 2 3 1 4 0 2  Figure 6.2 Vector Example  in understanding the methodology so the following simple measure is used: SIM(Term1, Termj) = E (Termk,j) (Termkj)  where "k" is summed across the set of all items. In effect the formula takes the two columns of the two terms being analyzed, multiplying and accumulating the values in each row. The results can be paced in a resultant "m" by "m" matrix, called a  Term-Term Matrix (Salton-83), where "m" is the number of columns (terms) in the original matrix. This simple formula is reflexive so that the matrix that is generated is symmetric. Other similarity formulas could produce a non-symmetric matrix. Using the data in Figure 6.2, the Term-Term matrix produced is shown in Figure 6.3. There are no values on the diagonal since that represents the autocorrelation of a word to itself. The next step is to select a threshold that determines if two terms are considered similar enough to each other to be in the same class. In this example, the threshold value of SO is used. Thus two terms are considered similar if the similarity value between them is 10 or greater. This produces a new binary matrix called the Term Relationship matrix (Figure 6.4) that defines which terms are similar. A one in the matrix indicates that the terms specified by the column and the row are similar enough to be in the same class. Term 7 demonstrates that a term may exist on its own with no other similar terms Document and Term Clustering  147  identified. In any of the clustering processes described below this term will always migrate to a class by itself.  The final step in creating clusters is to determine when two objects (words) are in the same cluster. There are many different algorithms available. The following algorithms are the most common: cliques, single link, stars and connected components.   Terml Term2 Term3 Term4 Term5 Term6 Term 7 Term8  Term 1  7 16 15 14 14 9 7  Term 2 7  8 12 3 18 6 17  Term 3 16 8  18 6 16 0 8  Term 4 15 12 18  6 18 6 9  Term 5 14 3 6 6  6 9 3  Term 6 14 18 16 18 6  2 16  Term 7 9 6 0 6 9 2  3  Term 8 7 17 8 9 3 16 3   Figure 6.3 Term-Term Matrix   Terml Term2 Term3 Term4 TermS Term6 Term7 Term8  Term 1  0 1 1 1 1 0 0  Term 2 0  0 1 0 1 0 1  Term 3 1 0  1 0 1 0 0  Term 4 1 1 1  0 1 0 0  TermS 1 0 0 0  0 0 0  Term 6 1 1 1 1 0  0 1  Term 7 0 0 0 0 0 0  0  Term 8 0 1 0 0 0 1 0   Figure 6.4 Term Relationship Matrix  Cliques require all items in a cluster to be within the threshold of all other items. The methodology to create the clusters using cliques is:  0.   Let i = 1  1.   Select term, and place it in a new class  2.   Start with term^ where r = k = i + 1  3.   Validate if termk is within the threshold of all terms within the current  class  4.   Ifnotjetk = k+ 1  5.   If k gt; m (number of words) 148                                                                                               Chapter 6  then r = r + 1  if r = m then go to 6 else  k = r  create a new class with termj in it  goto 3 else go to 3  6.   If current class only has term; in it and there are other classes  with term, in them  then delete current class else i = i + 1  7.   Ifi = m+ 1 then go to 8  else go to 1  8.   Eliminate any classes that duplicate or are subsets of other classes.  Applying the algorithm to Figure 6.4, the following classes are created:  Class 1   (Term 1, Term 3, Term 4, Term 6)  Class 2   (Term 1, Term 5)  Class 3   (Term 2, Term 4, Term 6)  Class 4   (Term 2, Term 6, Term 8)  Class 5  (Term 7)  Notice that Term 1 and Term 6 are in more than one class. A characteristic of this approach is that terms can be found in multiple classes.  In single link clustering the strong constraint that every term in a class is  similar to every other term is relaxed. The rule to generate single link clusters is that any term that is similar to any term in the cluster can be added to the cluster. It is impossible for a term to be in two different clusters.  This in effect partitions  the set of terms into the clusters. The algorithm is:  1.   Select a term that is not in a class and place it in a new class  2.   Place in that class all other terms that are related to it  3.   For each term entered into the class, perform step 2  4.   When no new terms can be identified in step 2, go to step 1.  Applying the algorithm for creating clusters using single link to the Term Relationship Matrix, Figure 6.4, the following classes are created:  Class 1 (Term 1, Term 3, Term 4, Term 5, Term 6, Term 2, Term8) Class 2 (Term 7)  There are many other conditions that can be placed on the selection of terms to be clustered. The Star technique selects a term and then places in the class all terms that are related to that term (i.e., in effect a star with the selected Document and Term Clustering  149  term as the core). Terms not yet in classes are selected as new seeds until all terms are assigned to a class. There are many different classes that can be created using the Star technique. If we always choose as the starting point for a class the lowest numbered term not already in a class, using Figure 6.4, the following classes are created:  Class 1 (Term 1, Term 3, Term 4, Term 5, Term 6) Class 2 (Term 2, Term 4, Term 8, Term6) Class 3 (Term 7)  This technique allows terms to be in multiple clusters (e.g., Term 4). This could be eliminated by expanding the constraints to exclude any term that has already been selected for a previous cluster  The String technique starts with a term and includes in the class one additional term that is similar to the term selected and not already in a class. The new term is then used as the new node and the process is repeated until no new terms can be added because the term being analyzed does not have another term related to it or the terms related to it are already in the class. A new class is started with any term not currently in any existing class. Using the additional guidelines to select the lowest number term similar to the current term and not to select any term already in an existing class produces the following classes:  {   T5   )  Figure 6ª5 Network Diagram of Term Similarities 150                                                                                               Chapter 6  Class 1  (Term 1, Term 3, Term 4, Term 2, Term 6, Term 8) Class 2 (Term 5) Class 3 (Term 7)  A technique to understand these different algorithms for generating classes is based upon a network diagram of the terms. Each term is considered a node and arcs between the nodes indicate terms that are similar. A network diagram for Figure 6.4 is given in Figure 6. 5. To determine cliques, sub-networks are identified where all of the items are connected by arcs. From this diagram it is obvious that Term 7 (T7) is in a class by itself and Term 5 (T5) is in a class with Term 1 (Tl). Other common structures to look for are triangles and four sided polygons with diagonals. To find all classes for an item, it is necessary to find all subnetworks, where each subnetwork has the maximum number of nodes, that the term is contained. For Term 1 (Tl), it is the subnetwork Tl, T3, T4, and T6. Term 2 (T2) has two subnetworks: T2, T4, T6 and the subnetwork T2, T6, T8. The network diagram provides a simple visual tool when there are a small number of nodes to identify classes using any of the other techniques.  The clique technique produces classes that have the strongest relationships between all of the words in the class. This suggests that the class is more likely to be describing a particular concept. The clique algorithm produces more classes than the other techniques because the requirement for all terms to be similar to all other terms will reduce the number of terms in a class. This will require more classes to include all the terms. The single link technique partitions the terms into classes. It produces the fewest number of classes and the weakest relationship between terms (Salton-72, Jones-71, SaIton-75). It is possible using the single link algorithm that two terms that have a similarity value of zero will be in the same class. Classes will not be associated with a concept but cover a diversity of concepts. The other techniques lie between these two extremes.  The selection of the technique is also governed by the density of the term relationship matrix and objectives of the thesaurus. When the Term Relationship Matrix is sparse (i.e., contains a few number of ones), then the constraint dependencies between terms need to be relaxed such as in single link to create classes with a reasonable number of Items. If the matrix is dense (i.e., lots of ones implying relationships between many terms), then the tighter constraints of the clique are needed so the number of items In a class does not become too large.  Cliques provide the highest precision when the statistical thesaurus Is used for query term expansion. The single link algorithm maximizes recall but can cause selection of many non-relevant items. The single link assignment process has the least overhead in assignment of terms to classes, requiring O(rT) comparisons (Croft-77)
issr-0091	Document and Term Clustering  151  6.2.2.2 Clustering Using Existing Clusters  An alternative methodology for creating clusters is to start with a set of existing clusters. This methodology reduces the number of similarity calculations required to determine the clusters. The initial assignment of terms to the clusters is revised by revalidating every term assignment to a cluster. The process stops when minimal movement between clusters is detected. To minimize calculations, centroids are calculated for each cluster. A centroid is viewed in Physics as the center of mass of a set of objects. In the context of vectors, it will equate to the average of all of the vectors in a cluster.  One way to understand this process is to view the centroids of the clusters as another point in the N-dimensional space where N is the number of items. The first assignment of terms to clusters produces centroids that are not related to the final clustering of terms. The similarity between all existing terms and the centroids of the clusters can be calculated. The term is reallocated to the cluster(s) that has the highest similarity. This process is iterated until it stabilizes. Calculations using this process are of the order O(n). The initial assignment of terms to clusters is not critical in that the iterative process changes the assignment of terms to clusters.  A graphical representation of terms and centroids illustrates how the   V V   ^^Nj I      H   /  VV VV V ^ /"""ó^ yvvv v\    V \ f  VVVVN J VV VV    \  iVV V V    VV    V    i fl    V     V ] vv  V  vv vvv   J \VV      Vy A vvv    1  Figure 6.6b. Initial Centroids for Clusters   V     V        \^    / vv vvv \ X"ó"\ yvvv v \    \f    VVVVN ivv^v   \  vv v v   vv  v   i f      IV Jvv  V vv vvv   J \VV      Vy A7VV         /  Figure 6.6a Centroids after Reassigning Terms 152  Chapter 6  classes move after the initial assignment. The solid black box represents the centroid for each of the classes. In Figure 6.6b. the centroids for the first three arbitrary class are shown. The ovals in Figure 6.6b. show the ideal cluster assignments for each term. During the next iteration the similarity between every term and the clusters is performed reassigning terms as needed. The resulting new centroid for the new clusters are again shown as black squares in Figure 6.6a. The new centroids are not yet perfectly associated with the ideal clusters, but they are much closer. The process continues until it stabilizes.  The following example of this technique uses Figure 6.2 as our weighted environment, and assumes we arbitrarily placed Class 1 = (Terml and Term2), Class 2 = (Term3 and Term 4) and Class 3 = (Term5 and Term 6). This would produce the following centroids for each class:  Class 1 = (0 +4)/2, (3 + l)/2, (3 + 0)12, (0 +l)/2, (2 + 2)/2  = 4/2, 4/2, 3/2, 1/2, 4/2  Class 2 = 0/2, 7/2, 0/2, 3/2, 5/2 Class 3 = 2/2 3/2, 3/2, 0/2, 5/2  Each value in the centroid is the average of the weights of the terms in the cluster for each item in the database. For example in Class 1 the first value is calculated by averaging the weights of Terml and Term2 in Item 1. For Class 2 and 3 the numerator is already the sum of the weights of each term. For the next step, calculating similarity values, it is often easier to leave the values in fraction form.  Applying the simple similarity measure defined in Section 6.2.2.1 between each of the 8 terms and 3 centroids just calculated comes up with the following assignment of similarity weights and new assignment of terms to classes in the row Assign shown in Figure 6.7:   Terml Term2 Term3 Term4 Tenn5 Term6 Term? Term8  Class 1 29/2 29/2 24/2 27/2 17/2 32/2 15/2 24/2  Class 2 31/2 20/2 38/2 45/2 12/2 34/2 6/2 17/2  Class 3 28/2 21/2 22/2 24/2 17/2 30/2 11/2 19/2  Assign C!ass2 Class 1 Class2 Class2 Class3 Class2 Class 1 Classl  Figure 6.7   Iterated Class Assignments  In the case of Term 5, where there is tie for the highest similarity, either class could be assigned. One technique for breaking ties is to look at the similarity weights of the other items in the class and assign it to the class that has the most similar weights. The majority of terms in Class 1 have weights in the high 20's/2, thus Term 5 was assigned to Class 3.  Term 7 is assigned to Class 1 even though Document and Term Clustering  153  its similarity weights are not in alignment with the other terms in that class. Figure 6.8 shows the new centroids and results of similarity comparisons for the next iteration.  Class 1 = 8/3, 2/3, 3/3, 3/3, 4/3 Class 2 = 2/4, 12/4, 3/4, 3/4, 11/4 Class 3 = 0/1, 1/1,3/1,0/1, 1/1   Terml Term2 Term3 Term4 Term5 Term6 Term? Term8  Class 1 23/3 45/3 16/3 27/3 15/3 36/3 23/3 34/3  Class 2 67/4 45/4 70/4 78/4 33/4 72/4 17/4 40/4  Class 3 12/1 3/1 6/1 6/1 11/1 6/1 9/1 3/1  Assign Class2 Classl Class2 Class2 Class3 Class2 Class3 Classl  Figure 6.8 New Centroids and Cluster Assignments  In this iteration of the process,, the only change is Term 7 moves from Class 1 to Class 3. This is reasonable, given it was not that strongly related to the other terms in Class 1.  Although the process requires fewer calculations than the complete term relationship method, it has inherent limitations. The primary problem is that the number of classes is defined at the start of the process and can not grow. It is possible for there to be fewer classes at the end of the process. Since all terms must be assigned to a class, it forces terms to be allocated to classes, even if their similarity to the class is very weak compared to other terms assigned.
issr-0092	6.2.2.3 One Pass Assignments  This technique has the minimum overhead in that only one pass of all of the terms is used to assign terms to classes. The first term is assigned to the first class. Each additional term is compared to the centroids of the existing classes. A threshold is chosen. If the item is greater than the threshold, it is assigned to the class with the highest similarity. A new centroid has to be calculated for the modified class. If the similarity to all of the existing centroids is less than the threshold, the term is the first item in a new class. This process continues until all items are assigned to classes. Using the system defined in Figure 6.3, with a threshold of 10 the following classes would be generated:  Class 1 = Term 1, Term 3, Term 4 Class 2 = Term 2, Term 6, Term 8 154                                                                                              Chapter 6  Class 3 = Term 5 Class 4 = Term 7  NOTE: the centroid values used during the one-pass process:  Class 1 (TermI, Term3) = 0, 7/2, 3/2, 0, 4/2  Classl (Terml, Term3, Term4) = 0, 10/3, 3/3, 3/3, 7/3  Class2 (Term2, Term6) = 6/2, 3/2, 0/2, 1/2, 6/2  Although this process has minimal computation on the order of O(n), it does not produce optimum clustered classes. The different classes can be produced if the order in which the items are analyzed changes. Items that would have been in the same cluster could appear in different clusters due to the averaging nature of centroids.
issr-0093	6.3 Item Clustering  Clustering of items is very similar to term clustering for the generation of thesauri. Manual item clustering is inherent in any library or filing system. In this case someone reads the item and determines the category or categories to which it belongs. When physical clustering occurs, each item is usually assigned to one category. With the advent of indexing, an item is physically stored in a primary category, but it can be found in other categories as defined by the index terms assigned to the item.  With the advent of electronic holdings of items, it is possible to perform automatic clustering of the items. The techniques described for the clustering of terms in Sections 6.2.2.1 through 6.2.2.3 also apply to item clustering. Similarity between documents is based upon two items that have terms in common versus terms with items in common. Thus, the similarity function is performed between rows of the item matrix. Using Figure 6.2 as the set of items and their terms and similarity equation:  SIM(Item,, Itemj) = I (Term^ ) (Teraijj£)  as k goes from 1 to 8 for the eight terms, an Item-Item matrix is created (Figure 6.9). Using a threshold of 10 produces the Item Relationship matrix shown in Figure 6.10. Document and Term Clustering  155   Item 1 Item 2 Item 3 Item 4 Item 5  Item 1  11 3 6 22  Item 2 11  12 10 36  Item 3 3 12  6 9  Item 4 6 10 6  11  Item 5 22 36 9 11   Figure 6.9 Item/Item Matrix   Iteml Item 2 Item3 Item 4 ItemS  Iteml  1 0 0 1  Item2 1  1 1 1  Item 3 0 1  0 0  Item4 0 1 0  3  Item5 1 1 0 1   Figure 6.10 Item Relationship Matrix  Using the Clique algorithm for assigning items to classes produces the following classes based upon Figure 6.10:  Class 1 = Item 1, Item 2, Item 5  Class 2 = Item 2, Item 3  Class 3 = Item 2, Item 4, Item 5  Application of the single link technique produces:  Class 1 = Item 1, Item 2, Item 5, Item 3, Item 4  All the items are in this one cluster, with Item 3 and Item 4 added because of their similarity to Item 2. The Star technique (i.e., always selecting the lowest nonassigned item) produces:  Class 1 - Item 1, Item 2, Item 5 Class 2 - Item 3, Item 2 Class 3 - Item4, Itera2, ItemS  Using the String technique and stopping when all items are assigned to classes produces the following:  Class 1 - Item 1, Item 2, Item 3  Class 2 - Item 4, Item 5 156                                                                                               Chapter 6  In the vocabulary domain homographs introduce ambiguities and erroneous hits. In the item domain multiple topics in an item may cause similar problems. This is especially true when the decision is made to partition the document space. Without precoordination of semantic concepts, an item that discusses "Politics" in "America" and "Economics" in "Mexico" could get clustered with a class that is focused around "Politics" in "Mexico."  Clustering by starting with existing clusters can be performed in a manner similar to the term model. Lets start with item 1 and item 3 in Class 1, and item 2 and item 4 in Class 2. The centroids are:  Class 1 = 3/2, 4/2, 0/2, 0/2, 3/2, 2/2, 4/2, 3/2 Class 2 = 3/2, 2/2, 4/2, 6/2, 1/2, 2/2, 2/2, 1/2  The results of recalculating the similarities of each item to each centroid and reassigning terms is shown in Figure 6.11.  Class 1         Class 2                          Assign  Class 1 Class 2 Class 2 Class 2 Class 2  Figure 6.11 Item Clustering with Initial Clusters  Finding the centroid for Class 2, which now contains four items, and recalculating the similarities does not result in reassignment for any of the items.  Instead of using words as a basis for clustering items, the Acquaintance system uses n-grams (Damashek-95, Cohen-95). Not only does their algorithm cluster items, but when items can be from more than one language, it will also recognize the different languages.
issr-0094	6.4 Hierarchy of Clusters  Hierarchical clustering in Information Retrieval focuses on the area of hierarchical agglomerative clustering methods (HACM) (Willet-88).    The term  agglomerative means the clustering process starts with unclustered items and performs pairwise similarity measures to determine the clusters. Divisive is the term applied to starting with a cluster and breaking it down into smaller clusters. The objectives of creating a hierarchy of clusters are to:  Item I 33/2 17/2  Item 2 23/2 51/2  Item 3 30/2 18/2  Item 4 8/2 24/2  Item 5 31/2 47/2  Document and Term Clustering  157  Reduce the overhead of search  Provide for a visual representation of the information space  Expand the retrieval of relevant items.  Search overhead is reduced by performing top-down searches of the centroids of the clusters in the hierarchy and trimming those branches that are not relevant (discussed in greater depth in Chapter 7). It is difficult to create a visual display of the total item space. Use of dendograms along with visual cues on the size of clusters (e.g., size of the ellipse) and strengths of the linkages between clusters (e.g., dashed lines indicate reduced similarities) allows a user to determine alternate paths of browsing the database (see Figure 6.12). The dendogram allows the user to determine which clusters to be reviewed are likely to have items of interest. Even without the visual display of the hierarchy, a user can use the  Figure 6.12 Dendogram  logical hierarchy to browse items of interest. A user, once having identified an item of interest, can request to see other items in the cluster. The user can increase the specificity of items by going to children clusters or by increasing the generality of items being reviewed by going to a parent cluster.  Most of the existing HACM approaches can be defined in terms of the Lance-Williams dissimilarity update formula (Lance-66). It defines a general formula for calculating the dissimilarity D between any existing cluster Ck and a new cluster Cy created by combining clusters C, and Cr 158                                                                                                 Chapter 6  D(CijsCk) = aiD(Ci,Ck) + ajD(CJ9Ck) + PD(CISCj) + y| D(C,,Ck) - D(Cj?Ck)|  By proper selection of a, p, and y, the current techniques for HACM can be represented (Frakes-92). In comparing the various methods of creating hierarchical clusters Voorhees and later El-Hamdouchi and Willet determined that the group average method produced the best results on document collections (Voorhees-86, El-Hamdouchi-89).  The similarity between two clusters can be treated as the similarity between all objects in one cluster and all objects in the other cluster. Voorhees showed that the similarity between a cluster centroid and any item is equal to the mean similarity between the item and all items in the cluster. Since the centroid is the average of all items in the cluster, this means that similarities between centroids can be used to calculate the similarities between clusters.  Ward's Method (Ward-63) chooses the minimum square Euclidean distance between points (e.g., centroids in this case) normalized by the number of objects in each cluster. He uses the formula for the variance I, choosing the minimum variance:  I,j = ((m1mJ)/(m1 + m }))dh}2 di/ = Zk=l (xI(k - xhkf  where m, is the number of objects in Classs and dj?J2 is the squared Euclidean distance. The process of selection of centroids can be improved by using the reciprocal nearest neighbor algorithm (Murtaugh-83, Murtaugh-85).  The techniques discribed in Section 6.2 created independent sets of classes. The automatic clustering techniques can also be used to create a hierarchy of objects (items or terms). The automatic approach has been applied to creating  item hierarchies more than in hierarchical statistical thesaurus generation.  In the  manual creation of thesauri, network relationships are frequently allowed between terms and classes creating an expanded thesaurus called semantic networks (e.g., in TOPIC and RetrievalWare). Hierarchies have also been created going from general categories to more specific classes of terms. The human creator ensures that the generalization or specification as the hierarchy is created makes semantic sense. Automatic creation of a hierarchy for a statistical thesaurus introduces too many errors to be productive.  But for item hierarchies the algorithms can also be applied. Centroids were used to reduce computation required for adjustments in term assignments to classes. For both terms and items, the centroid has the same structure as any of the items or terms when viewed as a vector from the Item/Term matrix (see Figure 6.2). A term is a vector composed of a column whereas an item is a vector composed of a row. The Scatter/Gather system (Hearst-96) is an example of this technique. In the Scatter/Gather system an initial set of clusters was generated. Document and Term Clustering                                                                 159  Each of these clusters was re-clustered to produce a second level. This process iterated until individual items were left at the lowest level.  When the creation of the classes is complete, a centroid can be calculated for each class. When there are a large number of classes, the next higher level in the hierarchy can be created by using the same algorithms used in the initial clustering to cluster the centroids. The only change required may be in the thresholds used. When this process is complete, if there are still too many of these higher level clusters, an additional iteration of clustering can be applied to their centroids. This process will continue until the desired number of clusters at the highest level is achieved.  A cluster can be represented by a category if the clusters were monolithic (membership is based upon a specific attribute). If the cluster is polythetic, generated by allowing for multiple attributes (e.g., words/concepts), then it can best be represented by using a list of the most significant words in the cluster. An alternative is to show a two or three-dimensional space where the clusters are represented by clusters of points. Monolithic clusters have two advantages over polythetic (Sanderson-99): how easy it is for a user to understand the topic of the cluster and the confidence that every item within the cluster will have a significant focus on the topic. For example, YAHOO is a good example of a monolithic cluster environment.  Sanderson and Croft proposed the following methodology to building a concept hierarchy. Rather than just focusing the construction of the hierarchy, they looked at ways of extracting terms from the documents to represent the hierarchy. The terms had the following characteristics:  Terms had to best reflect the topics  A parent term would refer to a more general concept then its child  A child would cover a related subtopic of the parent  A directed acyclic graph would represent relationships versus a pure hierarchy.  Ambiguous terms would have separate entries in the hierarchy for each meaning.  As a concept hierarchy, it should be represented similar to WordNet (Miller-95)  which uses synonyms, antonyms, hyponyrn/hypernym (is-a/is-a-type-of)* an^ meronym/holonym (has-part/is-a-part-of). Some techniques for generating hierarchies are Grefenstette's use of the similarity of contexts for locating synonyms (Grefenstette-94), use of key phrases (e.g., "such as", "and other") as an indicator of hyponym/hypernym relationships (Hearst-98), use of head and modifier noun and verb phrases to determine hierarchies (Woods-97) and use of a cohesion statistic to measure the degree of association between terms (Forsyth-86). 160                                                                                                Chapter 6  Sanderson and Croft used a test based upon subsumption. It is defined given two terms X and Y, X subsumes Y if:  P(X/Y)gt;.8,P(Y/X)lt;1.  X subsumes Y if the documents which Y occurs in are almost (.8) a subset of the documents that X occurs in. The factor of .8 was heuristically used because an absolute condition was eliminating too many useful relationships. X is thus a parent of Y.  The set of documents to be clustered was determined by a query and the query terms were used as the initial set of terms for the monolithic cluster. This set was expanded by adding more terms via query expansion using peudorelevance feedback (Blind feedback, Local Context Analysis) which is described in Chapter 7. They then used the terms and the formula above to create the hierarchies.
issr-0095	6.5 Summary  Thesauri, semantic nets and item clusters are essential tools in Information Retrieval Systems, assisting the user in locating relevant items. They provide more benefit to the recall process than in improving precision. Thesauri, either humanly generated or statistical, and semantic nets are used to expand search statements, providing a mapping between the users vocabulary and that of the authors. The number of false hits on non-relevant items retrieved is determined by how tightly coupled the terms are in the classes. When automatic techniques are used to create a statistical thesaurus, techniques such as cliques produce classes where the items are more likely to be related to the same concept than any of the other approaches. When a manually created thesaurus is used, human intervention is required to eliminate homonyms that produce false hits. A homonym is when a term has multiple, different meanings (e.g., the term field meaning an area of grass or an electromagnetic field). The longer (more terms) in the search statement, the less important the human intervention to eliminate homonyms. This is because items identified by the wrong interpretation of the homonym should have a low weight because the other search terms are not likely to be found in the item. When search statements are short, significant decreases in precision will occur if homonym pruning is not applied.  Item clustering also assists the user in identifying relevant items. It is used in two ways: to directly find additional items that may not have been found by the query and to serve as a basis for visualization of the Hit file. Each item cluster has a common semantic basis containing similar terms and thus similar concepts. To assist the user in understanding the major topics resulting from a search, the items retrieved can be clustered and used to create a visual (e.g., graphical) representation of the clusters and their topics (see Chapter 8 for examples). This allows a user to navigate between topics, potentially showing topics the user had not considered. The topics are not defined by the query but by the text of the items retrieved. Document and Term Clustering                                                                 161  When items in the database have been clustered, it is possible to retrieve all of the items in a cluster, even if they were not identified by the search statement. When the user retrieves a strongly relevant item, the user can look at other items like it without issuing another search. When relevant items are used to create a new query (i.e., relevance feedback discussed in Section 7.3), the retrieved hits are similar to what might be produced by a clustering algorithm. As with the term clustering, item clustering assists in mapping between a user's vocabulary and the vocabulary of the authors.  From another perspective term clustering and item clustering achieve the same objective even though they are the inverse of each other. The objective of both is to determine additional relevant items by a co-ocurrence process. A statistical thesaurus creates a cluster of terms that co-occur in the same set of items. For all of the terms within the same cluster (assuming they are tightly coupled) there will be significant overlap of the set of items they are found in. Item clustering is based upon the same terms being found in the other items in the cluster. Thus the set of items that caused a term clustering has a strong possibility of being in the same item cluster based upon the terms. For example, if a term cluster has 10 terms in it (assuming they are tightly related), then there will be a set of items where each item contains major subsets of the terms. From the item perspective, the set of items that has the commonality of terms, has a strong possibility to be placed in the same item cluster.  Hierarchical clustering of items is of theoretical interest, but has minimal practical application. The major rationale for using hierarchical clustering is to improve performance in search of clusters. The complexity of maintaining the clusters as new items are added to the system and the possibility of reduced recall (discussed in Chapter 7) are examples of why this is not used in commercial systems. Hierarchical thesauri are used in operational systems because there is additional knowledge in the human generated hierarchy. They have been historically used as a means to select index terms when indexing items. It provides a controlled vocabulary and standards between indexers.
issr-0096	7 User Search Techniques  7.1     Search Statements and Binding  7.2    Similarity Measures and Ranking  7.3    Relevance Feedback  7.4    Selective Dissemination of Information Search  7.5    Weighted Searches of Boolean Systems  7.6    Searching the INTERNET and Hypertext  7.7    Summary  Previous chapters defined the concept of indexing and the data structures most commonly associated with Information Retrieval Systems. Chapter 5 described different weighting algorithms associated with processing tokens. Applying these algorithms creates a data structure that can be used in search. Chapter 6 describes how clustering can be used to enhance retrieval and reduce the overhead of search. Chapter 7 focuses on how search is performed. To understand the search process, it is first necessary to look at the different binding levels of the search statement entered by the user to the database being searched. The selection and ranking of items is accomplished via similarity measures that calculate the similarity between the user's search statement and the weighted stored representation of the semantics in an item. Relevance feedback can help a user enhance search by making use of results from previous searches. This technique uses information from items judged as relevant and non-relevant to determine an expanded search statement. Chapter 6 introduces the concept of representing multiple items via an single averaged representation called a "centroid." Searching centroids can reduce search computation, but there is an associated risk of missing relevant items because of the averaging nature of a centroid. Hyperlinked items introduce new concepts in search originating from the dynamic nature of the linkages between items.
issr-0097	166                                                                                               Chapter 7  7.1 Search Statements and Binding  Search statements are the statements of an information need generated by users to specify the concepts they are trying to locate in items. As discussed in Chapter 2, the search statement uses traditional Boolean logic and/or Natural Language. In generation of the search statement, the user may have the ability to weight (assign an importance) to different concepts in the statement. At this point the binding is to the vocabulary and past experiences of the user. Binding in this sense is when a more abstract form is redefined into a more specific form. The search statement is the user's attempt to specify the conditions needed to subset logically the total item space to that cluster of items that contains the information needed by the user.  The next level of binding comes when the search statement is parsed for use by a specific search system. The search system translates the query to its own metalanguage. This process is similar to the indexing of item processes described in Chapter 5. For example, statistical systems determine the processing tokens of interest and the weights assigned to each processing token based upon frequency of occurrence from the search statement. Natural language systems determine the syntactical and discourse semantics using algorithms similar to those used in indexing. Concept systems map the search statement to the set of concepts used to index items.  The final level of binding comes as the search is applied to a specific database. This binding is based upon the statistics of the processing tokens in the database and the semantics used in the database. This is especially true in statistical and concept indexing systems. Some of the statistics used in weighting are based upon the current contents of the database. Some examples are Document Frequency and Total Frequency for a specific term. Frequently in a concept indexing system, the concepts that are used as the basis for indexing are determined by applying a statistical algorithm against a representative sample of the database versus being generic across all databases (see Chapter 5). Natural Language indexing techniques tend to use the most corpora-independent algorithms. Figure 7.1 illustrates the three potential different levels of binding. Parenthesis are used in the second binding step to indicate expansion by a thesaurus.  The length of search statements directly affect the ability of Information Retrieval Systems to find relevant items. The longer the search query, the easier it is for the system to find items. Profiles used as search statements for Selective Dissemination of Information systems are usually very long, typically 75 to 100 terms. In large systems used by research specialists and analysts, the typical ad hoc search statement is approximately 7 terms. In a paper to be published in SIGIR-97, Fox et al. at Virginia Tech have noted that the typical search statement on the Internet is one or two words. These extremely short search statements for ad hoc queries significantly reduce the effectiveness of many of the techniques User Search Techniques                                                                             167  INPUT                                                     Binding  "Find   me   information    on    the           User     search     statement     using  impact of the oil spills in Alaska on           vocabulary of user the price of oil"  impact,    oil    (petroleum),    spills           Statistical system binding extracts  (accidents),   Alaska,   price   (cost,           processing tokens  value)  impact (.308), oil (.606), petroleum           Weights assigned to search   terms  (.65), spills (.12), accidents (.23),           based    upon    inverse    document  Alaska (.45), price (.16), cost (.25),           frequency algorithm and database value (.10)  Figure 7.1 Examples of Query Binding  whose performance is discussed in Chapter 10 and are requiring investigation into new automatic search expansion algorithms.
issr-0098	7.2 Similarity Measures and Ranking  Searching in general is concerned with calculating the similarity between a user's search statement and the items in the database. Although many of the older systems are unweighted, the newer classes of Information Retrieval Systems have logically stored weighted values for the indexes to an item. The similarity may be applied to the total item or constrained to logical passages in the item. For example, every paragraph may be defined as a passage or every 100 words. The PIRCS system from Queen's College, CUNY, applies its algorithms to subdocuments defined as 550 word chunks (Kwok-96, Kwok-95). In this case, the similarity will be to the passages versus the total item. Rather limiting the definition of a passage to a fixed length size, locality based similarity allows variable length passages (neighborhoods) based upon similarity of content (Kretser-99). This then leads to the ability to define locality based searching and retrieval of the precise locations of information that satisfies the query. The highest similarity for any of the passages is used as the similarity measure for the item. Restricting the similarity measure to passages gains significant precision with minimal impact on recall. In results presented at TREC-4, it was discovered that passage retrieval makes a significant difference when search statements are long (hundreds of terms) but does not make a major difference for short queries. The lack of a large number of terms makes it harder to find shorter passages that contain the search terms expanded from the shorter queries. 168                                                                                                Chapter 7  Once items are identified as possibly relevant to the user's query, it is best to present the most likely relevant items first. This process is called "ranking." Usually the output of the use of a similarity measure in the search process is a scalar number that represents how similar an item is to the query.
issr-0099	7.2.1 Similarity Measures  A variety of different similarity measures can be used to calculate the similarity between the item and the search statement. A characteristic of a similarity formula is that the results of the formula increase as the items become more similar. The value is zero if the items are totally dissimilar. An example of a simple "sum of the products" similarity measure from the examples in Chapter 6 to determine the similarity between documents for clustering purposes is:  SIM(Itemi, Itemj) = I (Termigt;k) (Termjtk)  This formula uses the summation of the product of the various terms of two items when treating the index as a vector. If Itertij is replaced with Queryj then the same formula generates the similarity between every Item and Query,. The problem with this simple measure is in the normalization needed to account for variances in the length of items. Additional normalization is also used to have the final results come between zero and +1 (some formulas use the range ó1 to +1).  One of the originators of the theory behind statistical indexing and similarity functions was Robertson and Spark Jones (Robertson-76). Their model suggests that knowledge of terms in relevant items retrieved from a query should adjust the weights of those terms in the weighting process. They used the number of relevant documents versus the number of non-relevant documents in the database and the number of relevant documents having a specific query term versus the number of non-relevant documents having that term to devise four formulas for weighting. This assumption of the availability of relevance information in the weighting process was later relaxed by Croft and Harper (Croft-79). Croft expanded this original concept, taking into account the frequency of occurrence of terms within an item producing the following similarity formula (Croft-83):  SIM(DOQ, QUERYJ = J (C + IDF.) * /, j)  where C is a constant used in tuning, IDFj is the inverse document frequency for term "i" in the collection and  /j = K + (K- l)TFl/maxfreqJ User Search Techniques                                                                             169  where K is a tuning constant, TFjtJ is the frequency of term! "i" item j and maxfrecjj is the maximum frequency of any term in item "j." The best values for K seemed to range between 0.3 and 0.5.  Another early similarity formula was used by Salton in the SMART system (Saiton-83). Salton treated the index and the search query as ndimensional vectors (see Chapter 5). To determine the "weight" an item has with respect to the search statement, the Cosine formula is used to calculate the distance between the vector for the item and the vector for the query:  (DOC,,k * QTERMj,k) SIM(DOCi, QUERYj) =  __________________________________  1 k=\                            k=\  where DOQk is the kth term in the weighted vector for Item "i" and QTERM^ Is the kth term in query "j." The Cosine formula calculates the Cosine of the angle between the two vectors. As the Cosine approaches "1," the two vectors become coincident (i.e., the term and the query represent the same concept). If the two are totally unrelated, then they will be orthogonal and the value of the Cosine is "0." What is not taken into account is the length of the vectors. For example, if the following vectors are in a three dimensional (three term) system:  Item = (4 ,8,0)  Query 1 = (1, 2,0)  Query 2 = (3, 6,0)  then the Cosine value is identical for both queries even though Query 2 has significantly higher weights in the terms in common. To improve the formula, Salton and Buckley (Salton-88) changed the term factors in the query to:  QTERMlJt = (0.5 + (0.5 TFi.k/maxfreqk)) * IDFj  where TFj?k is the frequency of term "i" in query "k," maxfreqk is the maximum  frequency of any term in query "k* and IDF; is the inverse document frequency for term "i" (see Chapter 5 for the formula). In the most recent evolution of the formula, the IDF factor has been dropped (Buckley-96).  Two other commonly used measures are the Jaccard and the Dice similarity measures (Rijsbergen-79). Both change the normalizing factor in the denominator to account for different characteristics of the data. The denominator in the Cosine formula is invariant to the number of terms in common and produces very small numbers when the vectors are large and the number of common 170                                                                                               Chapter 7  elements is small. In the Jaccard similarity measure, the denominator becomes dependent upon the number of terms in common. As the common elements increase, the similarity value quickly decreases, but is always in the range -1 to +1. The Jaccard formula is :  (DOClik * QTERMhk)  k=l  SIM(DOC,, QUERYj) =   ------------------------------------------------- k=\                    k=\                           k=\  The Dice measure simplifies the denominator from the Jaccard measure and introduces a factor of 2 in the numerator. The normalization in the Dice formula is also invariant to the number of terms in common.  2*   2.   (DOClik * QTERM,k)  k=l  SIM(DOC,, QUERYj) =                ______________________________  *=1                                                   *=I  Figure 7.2 shows how the normalizing denominator results vary with the commonality of terms. For the Dice value, the numerator factor of 2 is divided into  the denominator. Notice that as long as the vector values are same, independent of their order, the Cosine and Dice normalization factors do not change. Also notice  that when there are a number of terms in common between the query and the document, that the Jaccard formula can produce a negative normalization factor.  It might appear that similarity measures only apply to statistical systems where the formulas directly apply to the stored indexes.  In the implementation of  Natural Language systems, also weighted values come from statistical data in conjunction with the natural language processing stored as indexes. Similarity algorithms are applied to these values in a similar fashion to statistical systems. But in addition to the similarity measures, constructs are used at the discourse level to perform additional filtering of the items.  Use of a similarity algorithm returns the complete data base as search results. Many of the items have a similarity close or equal to zero (or minimum value the similarity measure produces). For this reason, thresholds are usually associated with the search process. The threshold defines the items in the resultant Hit file from the query. Thresholds are either a value that the similarity measure must equal or exceed or a number that limits the number of items in the Hit file. A User Search Techniques                                                                             171  QUERY = (2, 2, 0, 0, 4) DOC1 = (0,2,6,4,0) DOC2    = (2, 6, 0, 0, 4)  Cosine                      Jaccard                     Dice  DOC1                       36.66                        16                             20  DOC2                      36.66                        -12                           20  Figure 7.2 Normalizing Factors for Similarity Measures  default is always the case where the similarity is greater than zero. Figure 7.3 illustrates the threshold process. The simple "sum of the products" similarity formula is used to calculate similarity between the query and each document. If no threshold is specified, all three documents are considered hits. If a threshold of 4 is selected, then only DOC1 is returned.  Vector:                American, geography, lake, Mexico, painter, oil,  reserve, subject  DOC1                 geography o/Mexico suggests oil reserves are available  vector (0, 1,0,2,0,3, 1,0)  DOC2                 American geography has lakes available everywhere  vector (1,3,2,0,0,0,0,0)  DOC3                 painters suggest Mexico lakes as subjects  vector (0,0, 1,3,3,0,0,2)  QUERY              oil reserves in Mexico  vector (0, 0, 0, 1,0, 1, 1,0)  SIM (Q, DOC!) = 6, SIM (Q, DOC2) = 0, SIM(Q, DOC3) = 3 Figure 7.3 Query Threshold Process 172  Chapter 7  Figure 7.4 Item Cluster Hierarchy  One special area of concern arises from search of clusters of terms that are stored in a hierarchical scheme (see Chapter 6). The items are stored in clusters that are represented by the centroid for each cluster. Figure 7.4 shows a cluster representation of an item space. In Figure 7.4, each letter at the leaf (bottom nodes) represent an item (i.e., K, L, M, N, D, E, F, G, H, P, Q, R, J). The letters at the higher nodes (A, C, B, I) represent the centroid of their immediate children nodes. The hierarchy is used in search by performing a top-down process. The query is compared to the centroids "A" and "B." If the results of the similarity measure are above the threshold, the query is then applied to the nodes' children. If not, then that part of the tree is pruned and not searched. This continues until the actual leaf nodes that are not pruned are compared. The problem comes from the nature of a centroid which is an average of a collection of items (in Physics, the center of gravity). The risk is that the average may not be similar enough to the query for continued search, but specific items used to calculate the centroid may be close enough to satisfy the search. The risks of missing items and thus reducing recall increases as the standard deviation increases. Use of centroids reduces the similarity computations but could cause a decrease in recall. It should have no effect on precision since that is based upon the similarity calculations at the leaf (item) level.  In Figure 7.5 the filled circle represents the query and the filled boxes represent the centroids for the three clusters represented by the ovals. In this case, the query may only be similar enough to the end two circles for additional analysis. But there are specific items in the right cluster that are much closer to the query than the cluster centroid and could satisfy the query. These items cannot be returned because when their centroid is eliminated they are no longer considered.  As part of investigating improved techniques to present Hits to users, Hearst and Pederscn from XEROX Palo Alto Research Center (PARC) are User Search Techniques  173   V V      V\  AX  / VV vvv        \ ï       ! / vvv v\   i         1 /fvvK /   \     VV VV    VV    V  V    VV  V VV VVV / I     VV     V/\ vvv   I  Figure 7.5 Centroid Comparisons  pursuing topical clustering as an alternative to similarity search ranking (Hearst96). In their experiments they applied the clustering to the entire corpora. Although the clustering conveyed some of the content and structure of the corpora, it was shown to be less effective in retrieval than a standard similarity query (Pirolli-96). Constraining the search to the hierarchy retrieved fewer relevant items than a similarity query that focused the results on an indexed logical subset of the corpus.
issr-0100	7.2.2 Hidden Markov Models Techniques  Use of Hidden Markov Models for searching textual corpora has introduced a new paradigm for search. In most of the previous search techniques, the query is thought of as another "document" and the system tries to find other documents similar to it. In HMMs the documents are considered unknown statistical processes that can generate output that is equivalent to the set of queries that would consider the document relevant. Another way to look at it is by taking the general definition that a HMM is defined by output that is produced by passing some unknown key via state transitions through a noisy channel. The observed output is the query, and the unknown keys are the relevant documents. The noisy channel is the mismatch between the author's way of expressing ideas and the user's ability to specify his query. Leek, Miller and Schwartz (Leek-99) computed for each document the probability that D was the relevant document in the users mind given that Q was the query produced, i.e., P(D is R/Q).  The development for a HMM approach begins with applying Bayes rule to the conditional probability:  P(D is R/Q) = P(Q/D is R) * P(D is R) / P(Q)  Since we are performing the analysis from the document's perspective, the P(Q)  will be the same for every document and thus can be ignored. P(D is R) is also almost an impossible task in a large diverse corpora.    Relevant documents sets 174                                                                                                Chapter 7  seem to be so sensitive to the specific queries, that trying to estimate P(D is R) does not return any noticeable improvements in query resolution. Thus the probability that a document is relevant given a specific query can be estimated by calculating the probability of the query given the document is Relevant, i.e., P(Q/D is R).  As described in Chapter 4, a Hidden Markov Model is defined by a set of states, a transition matrix defining the probability of moving between states, a set of output symbols and the probability of the output symbols given a particular state. The set of all possible queries is the output symbol set and the Document file defines the states. States could for example be any of the words or stems of the words in the documents. Thus the HMM process traces itself through the states of a document (e.g., the words in the document) and at each state transition has an output of query terms associated with the new state. State transitions are associated with ways that words are combined to make documents. Given the query, it is possible to calculate the probability that any particular document generated the query.  The biggest problem in using this approach is to estimate the transition probability matrix and the output (queries that could cause hits) for every document in the corpus. If there was a large training database of queries and the relevant documents they were associated with that included adequate coverage, then the problem could be solved using Estimation-Maximization algorithms (Dempster-77, Bryne-93.) But given the lack of data, Leek et. al. recommend making the transition matrix independent of specific document sets and applying simple unigram estimation for output distributions (Leek-99).
issr-0101	7.2.3 Ranking Algorithms  A by-product of use of similarity measures for selecting Hit items is a value that can be used in ranking the output. Ranking the output implies ordering the output from most likely items that satisfy the query to least likely items. This reduces the user overhead by allowing the user to display the most likely relevant  items first.   The original Boolean systems returned items ordered by date of entry into the system versus by likelihood of relevance to the user's search statement.  With the inclusion of statistical similarity techniques into commercial systems and the large number of hits that originate from searching diverse corpora, such as the  Internet,  ranking has become a common feature of modern systems.  A summary of ranking algorithms from the research community is found in an article written  by Belkin and Croft (Belkin-87).  In most of the commercial systems, heuristic rules are used to assist in the ranking of items. Generally/systems do not want to use factors that require knowledge across the corpus (e.g., inverse document frequency) as a basis for their similarity or ranking functions because it is too difficult to maintain current values as the database changes and the added complexity has not been shown to significantly improve the overall weighting process. A good example of how a commercial   product   integrates   efficiency   with   theoretical   concepts   is   the User Search Techniques                                                                             175  RetrievalWare system's approach to queries and ranking (RETR1EVALWARE95).  RetrievalWare first uses indexes (inversion lists) to identify potential relevant items. It then applies coarse grain and fine grain ranking. The coarse grain ranking is based on the presence of query terms within items. In the fine grain ranking, the exact rank of the item is calculated. The coarse grain ranking is a weighted formula that can be adjusted based on completeness, contextual evidence or variety, and semantic distance. Completeness is the proportion of the number of query terms (or related terms if a query term is expanded using the RetrievalWare semantic network/thesaurus) found in the item versus the number in the query. It sets an upper limit on the rank value for the item. If weights are assigned to query terms, the weights are factored into the value. Contextual evidence occurs when related words from the semantic network are also in the item. Thus if the user has indicated that the query term "charge" has the context of "paying for an object" then finding words such as "buy," "purchase," "debt" suggests that the term "charge" in the item has the meaning the user desires and that more weight should be placed in ranking the item. Semantic distance evaluates how close the additional words are to the query term. Synonyms add additional weight; antonyms decrease weight. The coarse grain process provides an initial rank to the item based upon existence of words within the item. Since physical proximity is not considered in coarse grain ranking, the ranking value can be easily calculated.  Fine grain ranking considers the physical location of query terms and related words using factors of proximity in addition to the other three factors in coarse grain evaluation. If the related terms and query terms occur in close proximity (same sentence or paragraph) the item is judged more relevant. A factor is calculated that maximizes at adjacency and decreases as the physical separation increases. If the query terms are widely distributed throughout a long item, it is possible for the item to have a fine grain rank of zero even though it contains the query terms.  Although ranking creates a ranking score, most systems try to use other ways of indicating the rank value to the user as Hit lists are displayed. The scores have a tendency to be misleading and confusing to the user. The differences between the values may be very close or very large. It has been found to be better to indicate the general relevance of items than to be over specific (see Chapter S).
issr-0102	7.3 Relevance Feedback  As discussed in the early chapters in this text, one of the major problems in finding relevant items lies in the difference in vocabulary between the authors 176                                                                                               Chapter 7  and the user. Thesuari and semantic networks provide utility in generally expanding a user's search statement to include potential related search terms. But this still does not correlate to the vocabulary used by the authors that contributes to a particular database. There is also a significant risk that the thesaurus does not include the latest jargon being used, acronyms or proper nouns. In an interactive system, users can manually modify an inefficient query or have the system automatically expand the query via a thesaurus. The user can also use relevant items that have been found by the system (irrespective of their ranking) to improve future searches, which is the basis behind relevance feedback. Relevant items (or portions of relevant items) are used to reweight the existing query terms and possibly expand the user's search statement with new terms.  The first major work on relevance feedback was published in 1965 by Rocchio (republished in 1971: Rocchio-71). Rocchio was documenting experiments on reweighting query terms and query expansion based upon a vector representation of queries and items. The concepts are also found in the probabilistic model presented by Robertson and Sparck Jones (Robertson-76). The relevance feedback concept was that the new query should be based on the old query modified to increase the weight of terms in relevant items and decrease the weight of terms that are in non-relevant items. This technique not only modified the terms in the original query but also allowed expansion of new terms from the relevant items. The formula used is:  r ,.i                nr ^  where  Qn          = the revised vector for the new query  Qo          = the original query  r             = number of relevant items  DR,        = the vectors for the relevant items  nr           = number of non-relevant items  DNRj      = the vectors for the non-relevant items.  The factors r and nr were later modified to be constants that account for the number of items along with the importance of that particular factor in the equation.  Additionally a constant was added to Qo to allow adjustments to the importance of the weight assigned to the original query.   This led to the revised version of the  formula: User Search Techniques  177  where a, P, and y are the constants associated with each factor (usually \ln or Mnr  r  times a constant). The factor P 2^ DR i is referred to as positive feedback because it is using the user judgments on relevant items to increase the values of terms for  nr  the next iteration of searching.  The factor y^T/)JV/?j  is referred to as negative  feedback since it decreases the values of terms in the query vector. Positive feedback is weighted significantly greater than negative feedback. Many times only positive feedback is used in a relevance feedback environment. Positive feedback is more likely to move a query closer to a user's information needs. Negative feedback may help, but in some cases it actually reduces the effectiveness of a query. Figure 7.6 gives an example of the impacts of positive and negative feedback. The filled circles represent non-relevant items; the other circles represent relevant items. The oval represents the items that are returned from the query. The solid box is logically where the query is initially. The hollow box is the query modified by relevance feedback (positive only or negative only in the Figure).   o∞ o      / o o \  o ^ o o    o  o  o∞o   on oo  oo p          1   o     o  o     o   o  /\   o o      /o o \  o o∞ o L o)  oo∞ o    o   o o   o o    o     o 1 cL      o  Positive Feedback                                        Negative Feedback  Figure 7.6 Impact of Relevance Feedback  Positive feedback moves the query to retrieve items similar to the items retrieved and thus in the direction of more relevant items. Negative feedback moves the query away from the non-relevant items retrieved, but not necessarily closer to more relevant items.  Figure 7.7 shows how the formula is applied to three items (two relevant and one non-relevant). If we use the factors a = 1, p = % (14 times a constant s/i), y 178  Chapter 7  = % (1/1 times a constant XA) in the foregoing formula we get the following revised query (NOTE: negative values are changed to a zero value in the revised Query vector):  Qn = (3, 0,0,2, 0) + Va (2+1, 4+3,0+0,0+0, 2+0) - V4 (0, 0, 4, 3, 2) = (33/4,l3/4,0{-lKl1/4,0)   Term 1 Term 2 Term 3 Term 4 Term 5  Qo 3 0 0 2 0  DOClr 2 4 0 0 2  DOC2r 1 3 0 0 0  DOC3nr 0 0 4 3 3  Qn 33/4 VA 0 VA 0  Figure 7.7 Query Modification via Relevance Feedback  5  Using the unnormalized similarity formula SIM(Qk,DOCj) =   /j TERM ^ TERMu produces the results shown in Figure 7.8:  /=!   DOCl DOC2 DOC3  Qo 6 3 6  Qn 14V4 9.0 3.75  Figure 7.8 Effect of Relevance Feedback  In addition to showing the benefits of relevance feedback, this example illustrates the problems of identifying information. Although DOC3 is not relevant to the user, the initial query produced one of the highest similarity measures for it. This was caused by a query term (Term 4) of interest to the user that has a significant weight in DOC3. The fewer the number of terms in a user query, the more likely a specific term to cause non-relevant items to be returned. The modification to the query by the relevance feedback process significantly increased the similarity measure values for the two relevant items (DOCl and DOC2) while decreasing the value of the non-relevant item. It is also of interest to note that the new query added a weight to Term 2 that was not in the original query. One reason that the user might not have initially had a value to Term 2 is that it might not have been in the user's vocabulary. For example, the user may have been searching on "PC" and '"word processor1" and not been aware that many authors use the specific term "Macintosh" rather than "PC."  Relevance feedback, in particular positive feedback, has been proven to be of significant value in producing better queries. Some of the early experiments on User Search Techniques                                                                             179  the SMART system (Ide-69, Ide-71, Salton-83) indicated the possible improvements that would be gained by the process. But the small collection sizes and evaluation techniques put into question the actual gains by using relevance feedback. One of the early problems addressed in relevance feedback is how to treat query terms that are not found in any retrieved relevant items. Just applying the algorithm would have the effect of reducing the relative weight of those terms with respect to other query terms. From the user's perspective, this may not be desired because the term may still have significant value to the user if found in the future iterations of the search process. Harper and van Rijisbergen addressed this issue in their proposed EMIM weighting scheme (Harper-785 Harper-80). Relevance feedback has become a common feature in most information systems. When the original query is modified based upon relevance feedback, the systems ensure that the original query terms are in the modified query, even if negative feedback would have eliminated them. In some systems the modified query is presented to the user to allow the user to readjust the weights and review the new terms added.  Recent experiments with relevance feedback during the TREC sessions have shown conclusively the advantages of relevance feedback. Queries using relevance feedback produce significantly better results than those being manually enhanced. When users enter queries with a few number of terms, automatic relevance feedback based upon just the rank values of items has been used. This concept in information systems called pseudo-relevance feedback, blind feedback or local context analysis (Xu-96) does not require human relevance judgments. The highest ranked items from a query are automatically assumed to be relevant and applying relevance feedback (positive only) used to create and execute an expanded query. The system returns to the user a Hit file based upon the expanded query. This technique also showed improved performance over not using the automatic relevance feedback process. In the automatic query processing tests from TREC (see Chapter 10) most systems use the highest ranked hits from the first pass to generate the relevance feedback for the second pass.
issr-0103	7.4 Selective Dissemination of Information Search  Selective Dissemination of Information, frequently called dissemination systems, are becoming more prevalent with the growth of the Internet. A dissemination system is sometimes labeled a "push" system while a search system  is called a "pull" system. The differences are that in a search system the user proactively makes a decision that he needs information and directs the query to the information system to search. In a dissemination system, the user defines a profile (similar to a stored query) and as new information is added to the system it is automatically compared to the user's profile. If it is considered a match, it is asynchronously sent to the user's "mail" file (see Chapter 1). 180                                                                                               Chapter 7  One concept that ties together the two search statements (query and profile) is the introduction of a time parameter associated with a search statement. As long as the time is in the future, the search statement can be considered active and disseminating as items arrive. Once the time parameter is past, the user's need for the information is no longer exists except upon demand (i.e., issuing the search statement as an ad hoc query).  The differences between the two functions lie in the dynamic nature of the profiling process, the size and diversity of the search statements and number of simultaneous searches per item. In the search system, an existing database exists. As such, corpora statistics exist on term frequency within and between terms. These can be used for weighting factors in the indexing process and the similarity comparison (e.g., inverse document frequency algorithms). A dissemination system does not necessarily have a retrospective database associated with it. Thus its algorithms need to avoid dependency upon previous data or develop a technique to estimate terms for their formula. This class of system is also discussed as a binary classification system because there is no possibility for real time feedback from the user to assist in search statement refinement. The system makes a binary decision to reject or file the item (Lewis-95).  Profiles are relatively static search statements that cover a diversity of topics. Rather than specifying a particular information need, they usually generalize all of the potential information needs of a user. They are focused on current information needs of the user. Thus profiles have a tendency to contain significantly more terms than an ad hoc query (hundreds of terms versus a small number). The size tends to make them more complex and discourages users from wanting to change them without expert advice.  One of the first commercial search techniques for dissemination was the Logicon Message Dissemination System (LMDS). The system originated from a system created by Chase, Rosen and Wallace (CRW Inc.). It was designed for speed to support the search of thousands of profiles with items arriving every 20 seconds. It demonstrated one approach to the problem where the profiles were treated as the static database and the new item acted like the query. It uses the terms in the item to search the profile structure to identify those profiles whose logic could be satisfied by the item. The system uses a least frequently occurring trigraph (three character) algorithm that quickly identifies which profiles are not satisfied by the item. The potential profiles are analyzed in detail to confirm if the item is a hit.  Another example of a dissemination approach is the Personal Library Software (PLS) system. It uses the approach of accumulating newly received items into the database and periodically running user's profiles against the database. This makes maximum use of the retrospective search software but loses near real time delivery of items. More recent examples of a similar approach are the Retrievalware and the InRoute software systems. In these systems the item is processed into the searchable form. Since the Profiles are relatively static, some use is made in identifying all the terms used in all the profiles. Any words in the items that are members of this list can not contribute to the similarity process and User Search Techn iques                                                                             181  thus are eliminated from the search structure. Every profile is then compared to the item. Retrievalware uses a statistical algorithm but it does not include any corpora data. Thus not having a database does not affect its similarity measure. InRoute, like the INQUERY system used against retrospective database, uses inverse document frequency information. It creates this information as it processes items, storing and modifying it for use as future items arrive. This would suggest that the values would be continually changing as items arrive until sufficient items have arrived to stabilize the inverse document frequency weights. Relevance feedback has been proven to enhance the search capabilities of ad hoc queries against retrospective databases. Relevance feedback can also be applied to dissemination systems. Unlike an ad hoc query situation, the dissemination process is continuous, and the issue is the practicality of archiving all of the previous relevance judgments to be used in the relevance feedback process. Allan performed experiments on the number of items that have to arrive and be judged before the effects of relevance feedback stabilize (Allan-96). Previous work has been done on the number of documents needed to generate a new query and the amount of training needed (Buckley-94, Aalbersberg-92, Lewis-94). The two major choices are to save relevant items or relevance statistics for words. By saving dissimilar items, Allan demonstrated that the system sees a 2-3 per cent loss in effectiveness by archiving 10 per cent of the relevance judgments. This still requires significant storage space. He was able to achieve high effectiveness by storing information on as few as 250 terms.  Another approach to dissemination uses a statistical classification technique and explicit error minimization to determine the decision criteria for selecting items for a particular profile (Schutze-95). In this case, the classification process is related to assignment for each item into one of two classes: relevant to a user's profile or non-relevant. Error minimization encounters problems in high dimension spaces. The dimensionality of an information space is defined by the number of unique terms where each term is another dimension. This is caused by there being too many dimensions for a realistic training set to establish the error minimization parameters. To reduce the dimensionality, a version of latent semantic indexing (LSI) can be used. The process requires a training data set along with its associated profiles. Relevance feedback is an example of a simple case of a learning algorithm that does not use error minimization. Other examples of algorithms used in linear classifiers that perform explicit error minimization are linear discriminant analysis, logistic regression and linear neural networks.  Schutze et al. used two approaches to reduce the dimensionality: selecting a set of existing features to use or creating a new much smaller set of features that the original features are mapped into. A x2 measure was used to determine the most important features. The test was applied to a table that contained the number of relevant (Nr) and non-relevant (Nnr) items in which a term occurs plus the number of relevant and non-relevant items in which the term does not occur (Nr., Nnr. respectively). The formula used was: 182                                                                                               Chapter 7  2    ___________N(NrNnr_-Nr_Nnr)2___________  X      (Nr + Nr_){Nnr + NnrJ(Nr + Nnr)(Nr_ + Nnr_)  To focus the analysis, only items in the local region defined by a profile were analyzed. The chi-squared technique provides a more effective mechanism than frequency of occurrence of terms. A high x2 score indicates a feature whose frequency has a significant dependence on occurrence in a relevant or non-relevant item.  An alternative technique to identify the reduced feature (vector) set is to use a modified latent semantic index (LSI) technique to determine a new reduced set of concept vectors. The technique varies from the LSI technique described in Chapter 5 by creating a separate representation of terms and items by each profile to create the "local" space of items likely to be relevant (i.e., Local LSI). The results of the analysis go into a learning algorithm associated with the classification technique (Hull-94). The use of the profile to define a local region is essential when working with large databases. Otherwise the number of LSI factors is in the hundreds and the ability to process them is currently unrealistic. Rather than keeping the LSI factors separate per profile, another approach is to merge the results from all of the queries into a single LSI analysis (Dumais-93). This increases the number of factors with associated increase in computational complexity.  Once the reduced vector set has been identified, then learning algorithms can be used for the classification process. Linear discriminate analysis, logistic regression and neural networks are three possible techniques that were compared by Schutze et al. Other possible techniques are classification trees (Tong-94, Lewis-94a), Bayesian networks (Croft-94), Bayesian classifiers (Lewis-92), rules induction (Apte-94), nearest neighbor techniques (Masand-92, Yang-94), and least square methods (Fuhr-89). Linear discrimination analysis uses the covariance class for each document class to detect feature dependence (Gnanadesikan-79). Assuming a sample of data from two groups with ns and ni members, mean vectors  jc j and x2 and covariance matrices d and C2 respectively, the objective is to maximize the separation between the two groups. This can be achieved by maximizing the distance between the vector means, scaling to reflect the structure in the pooled covariance matrix. Thus choose a such that:  a  a = arga max User Search Techniques                                                                             183  is maximized where T is the transpose and (ª, + n2 - 2)C = (wj - l)Ci + 0?2 1)C2. Since C is positive, the Cholesky decomposition of C = RT. Let b =Ra; then the formula becomes;  bTRT-lCXl-x2) a  = arg bmax ----------j==-------- which is maximized by choosing b oc RT~l( xr x 2). This means:  a* = Rô16 = C-1(x1- x2)  The one dimensional space defined by y = aTx should cause the group means to be well separated- To produce a non-linear classifier, a pair of shrinkage parameters is used to create a very general family of estimators for the group covariance matrix (Freidman-89). This process called Regularized Discriminant Analysis looks at a weighted combination of the pooled and unpooled covariance matrices. The optimal values of the shrinkage parameters are selected based upon the cross validation over the training set. The non-linear classifier produced by this technique has not been shown to make major improvements in the classification process (Hull-95).  A second approach is to use logistic regression (Cooper-94a). It models a binary response variable by a linear combination of one or more predictor variables, using a logit link fiinction:  g(7l)=l0g(7C/(l-7t))  and modeling variance with a binomial random variable. This is achieved by modeling the dependent variable log(7i/(l - 7c)) as a linear combination of independent variables using a form g{%) = x$gt;. In this formula n is the estimated response probability (probability of relevance), x, is the feature vector (reduced  vector) for document /, and (3 is the weight vector which is estimated from the matrix of feature vectors.   The optimal value of p can be calculated   using the  maximum likelihood and the Newton-Raphson method of numerical optimization (McCulIagh-89). The major difference from previous experiments using logistic regression is that Schutze et al. do not use information from all the profiles but  restrict the analysis for each profile.  A third technique is to use neural networks for the learning function. A neural network is a network of input and output cells (based upon neuron functions in the brain) originating with the work of McCulloch and Pitts (McCulloch-43). Each input pattern is propagated forward through the network. When an error is detected it is propagated backward adjusting the cell parameters to reduce the 184  Chapter 7  error, thus achieving learning. This technique is very flexible and can accommodate a wide range of distributions. A major risk of neural networks is that they can overfit by learning the characteristics of the training data set and not be generalized enough for the normal input of items. In applying training to a neural network approach, a validation set of items is used in addition to the training items to ensure that overfitting has not occurred. As each iteration of parameter adjustment occurs on the training set, the validation set is retested. Whenever the errors on the validation set increase, it indicates that overfitting is occurring and establishes the number of iterations on training that improve the parameter values while not harming generalization.  The linear and non-linear architectures for an implementation of neural nets is shown in Figure 7.9.  OUTPUT UNIT  OUTPUT UNIT  HIDDEN  UNIT BLOCK FOR LSI  HIDDEN  UNIT  BLOCK FOR TERMS  LSI REPRESENTATION TERM REPRESENTATION  LSI REPRESENTATION  TERM REPRESENTATION  Linear Neural network                               Non-linear Neural network  Figure 7.9 Linear and Non-linear networks  In the non-linear network, each of the hidden blocks consists of three hidden units. A hidden unit can be interpreted as feature detectors that estimate the  probability of a feature being present in the input. Propagating this to the output unit can improve the overall estimation of relevance in the output unit. The networks show input of both terms and the LSI representation (reduced feature set). In both architectures, all input units are directly connected to the output units. Relevance is computed by setting the activations of the input units to the document's representation and propagating the activation through the network to User Search Techniques                                                                            185  the output unit, then propagating the error back through the network using a gradient descent algorithm (Rumelhart-95). A sigmoid was chosen as:  as the activation function for the units of the network (Schutze-95). In this case backpropagation minimizes the same error as logistic regression (Rumelhart-95a). The cross-entropy error is:  til0gCTj+ 1   -tj)l0g(l   -lt;Ji)  where t[ is the relevance for document / and a\ is the estimated relevance (or activation of the output unit) for document /. The definition of the sigmoid is equivalent to:  which is the same as the logit link function.  Schutze et al. performed experiments with the Tipster test database to compare the three algorithms. They show that the linear classification schemes perform 10-15 per cent better than the traditional relevance feedback. To use the learning algorithms based upon error minimization and numerical computation one must use some technique of dimensionality reduction. Their experiments show that local latent semantic indexing is best for linear discrimination analysis and logistic regression since they have no mechanism for protecting against overfitting. When there are mechanisms to avoid overfitting such as in neural networks, other less precise techniques of dimension reduction can be used. This work suggests that there are alternatives to the statistical classification scheme associated with profiles and dissemination.  An issue with Mail files is the logical reorganization associated with display of items. In a retrospective query, the search is issued once and the hit list is a static file that does not change in size or order of presentation. The dissemination function is always adding items that satisfy a user's profile to the user's Mail file. If the items are stored sorted by rank, then the relative order of items can always be changing as new items are inserted in their position based upon the rank value. This constant reordering can be confusing to the user who remembers items by spatial relationships as well as naming. Thus the user may remember an item next to another item is of significant interest. But in trying to 186                                                                                               Chapter 7  retrieve it at a later time, the reordering process can make it significantly harder to find.
issr-0104	7.5 Weighted Searches of Boolean Systems  The two major approaches to generating queries are Boolean and natural language. Natural language queries are easily represented within statistical models and are usable by the similarity measures discussed. Issues arise when Boolean queries are associated with weighted index systems. Some of the issues are associated with how the logic (AND, OR, NOT) operators function with weighted values and how weights are associated with the query terms. If the operators are interpreted in their normal interpretation, thay act too restrictive or too general (i.e., AND and OR operators respectively). Salton, Fox and Wu showed that using the strict definition of the operators will suboptimize the retrieval expected by the user (Salton-83a). Closely related to the strict definition problem is the lack of ranking that is missing from a pure Boolean process. Some of the early work addressing this problem recognized the fuzziness associated with mixing Boolean and weighted systems (Brookstein-78, Brookstein-80)  To integrate the Boolean and weighted systems model, Fox and Sharat proposed a fuzzy set approach (Fox-86). Fuzzy sets introduce the concept of degree of membership to a set (Zadeh-65). The degree of membership for AND and OR operations are defined as:  DEGAnB = min(DEGA, DEGB) DEGaub = max(DEGA, DEGB)  where A and B are terms in an item. DEG is the degree of membership. The Mixed Min and Max (MMM) model considers the similarity between query and document to be a linear combination of the minimum and maximum item weights. Fox proposed the following similarity formula:  SIM(QUERYOr, DOC) = C0R ,* max(DOCl,, DGC2....,DOCn) +  C0R2*min(D0Ci,DOC2,..., DOCn)  S!M(QUERYAND, DOC) = CAND1*min(DOC,,DOC2, ...,DOCn) + CAnd2* max(DOCl,, DOC2,...,DOCn)  where Cqri and C0R2 are weighting coefficients for the OR operation and Candi and CAkd2 are the weighting coefficients for the AND operation. Lee and Fox found in their experiments that the best performance comes when Candi is between 0.5 to 0.8 and COri is greater than 0.2. User Search Techniques                                                                             187  The MMM technique was expanded by Paice (Paice-84) considering all item weights versus the maximum/minimum approach. The similarity measure is calculated as:  SIM(QUERY DOC) =  where the d^s are inspected in ascending order for AND queries and descending order for OR queries. The r terms are weighting coefficients. Lee and Fox showed that the best values for r are 1.0 for AND queries and 0.7 for OR queries (Lee-88). This technique requires more computation since the values need to be stored in ascending or descending order and thus must be sorted.  An alternative approach is using the P-norm model which allows terms within the query to have weights in addition to the terms in the items. Similar to the Cosine similarity technique, it considers the membership values (dAJ ,..., dAn) to be coordinates in an "n" dimensional space. For an OR query, the origin (all values equal zero) is the worst possibility. For an AND query the ideal point is the unit vector where all the D; values equal 1. Thus the best ranked documents will have maximum distance from the origin in an OR query and minimal distance from the unit vector point. The generalized queries are:  Qor = (A,t,a0 OR (A2,a2) OR ...OR(An,an)  Qand = (A,,,a,) AND (A2,a2) AND ... AND (An,an)  The operators (AND and OR) will have a strictness value assigned that varies from 1 to infinity where infinity is the strict definition of the Boolean operator. The a, values are the query term weights. If we assign the strictness value to a parameter labeled "S" then the similarity formulas between queries and items are:  sds)/(af + a\ + ï ï ï + as  SIM(QOr,DOC) = $](a?dsAl + ï ï ï + asndsAn)/(af + a\ + ï ï ï + asn  SIM(QAND,DOC) = i - sJ  SIM(Qnot,DOC) = I - SIM(Q, DOC)  Another approach suggested by Salton provides additional insight into the issues of merging the Boolean queries and weighted query terms under the assumption that there are no weights available in the indexes (SaIton-83).   The  objective is to perform the normal Boolean operations and then refine the results using weighting techniques.   The following procedure is a modification to his 188                                                                                               Chapter 7  approach for defining search results. The normal Boolean operations produce the following results:  "A OR B" retrieves those items that contain the term A or the term B or both  "A AND B" retrieves those items that contain both terms A and B  "A NOT B"  retrieves those items that contain term A and not contain term B.  If weights are then assigned to the terms between the values 0.0 to 1.0, they may be interpreted as the significance that users are placing on each term. The value 1.0 is assumed to be the strict interpretation of a Boolean query. The value 0.0 is interpreted to mean that the user places little value on the term. Under these assumptions, a term assigned a value of 0.0 should have no effect on the retrieved set. Thus  "Aj OR Bo" should return the set of items that contain A as a term "Ai AND Bo" will also return the set of items that contain term A "Aj NOT Bo" also return set A.  This suggests that as the weight for term B goes from 0.0 to 1.0 the resultant set changes from the set of all items that contains term A to the set normally generated from the Boolean operation. The process can be visualized by use of the VENN diagrams shown in Figure 7.10. Under the strict interpretation "A] OR Bj" would include all items that are in all the areas in the VENN diagram. "Ai OR Bo" would be only those items in A (i.e., the whie and black dotted areas) which is everything except items in "B NOT A" (the grey area.) Thus as the value of query term B goes from 0.0 to 1.0, items from "B NOT A" are proportionally added until at 1.0 all of the items will be added.  Similarly, under the strict interpretation "Aj AND Bj" would include all of the items that are in the black dotted area. "A, AND Bo" will be all of the items in A as described above. Thus, as the value of query term B goes from 1.0 to 0.0 items will be proportionally added from "A NOT B" (white area) until at 0.0 all of the items will be added.  Finally, the strict interpretation of "As NOT Bt" is grey area while "Ai NOT Bo" is all of A. Thus as the value of B goes from 0.0 to 1.0, items are proportionally added from "A AND B" (black dotted area) until at 1.0 all of the items have been added.  The final issue is the determination of which items are to be added or dropped in interpreting the weighted values. Inspecting the items in the totally User Search Techniques  189  Figure 7.10 VENN Diagram  strict case (both terms having weight 1.0) and the case where the value is 0.0 there is a set of items that are in both solutions (invariant set). In adding items they should be the items most similar to the set of items that does not change in either situation. In dropping items, they should be the items least similar to those that are in both situations.  Thus the algorithm follows the following steps:  1.  Determine the items that are satisfied by applying strict interpretation of the Boolean functions  2.    Determine the items that are part of the set that is invariant  3.    Determine the Centroid of the invariant set  4.     Determine the number of items to be added or deleted by multiplying the term weight times the number of items outside of the invariant set and rounding up to the nearest whole number  5.    Determine  the  similarity  between   items  outside  of the invariant set and the Centroid  6.   Select the items to be included or removed from the final set  Figure 7.11 gives an example of solving a weighted Boolean query.  QUERY* ends up with a set containing all of the items that contain the  term "Computer" and two items from the set "computer" NOT "program."   The symbol \ ] stands for rounding up to the next integer. In QUERY2 the final set 190                                                                                               Chapter 7   Computer program cost sale  Dl 0 4 0 8  D2 0 2 0 0  D3 4 0 2 4  D4 0 6 4 6  D5 0 4 6 4  D6 6 0 4 0  D7 0 0 0 0  r_)C 4 2 0 2  Ql = QUERY] = Computer10 OR program 333 Q2 = QUERY, = cost 75 AND sale10  Ql strict interpretation = (Dl, D2, D3, D4, D5, D6, D8)  Q2stnctmterpretat1on =  Qluivariant=(D8) Q2invanant = (D3, D4, D5)  Qlopnonai = (Dl, D2, D3, D4, D5, D6) thus f.333 times 6 items] = 2 items Q2optonai = (Dl, D8) which means [ (1 - .75) times 2 items] = 1 item  Figure 7.11  Example of Weighted Boolean Query  contains all of set "cost" AND "sale" plus .25 of the set of "sale" NOT "cost." Using the simple similarity measure:  SIM(Item,, Itemj) = I (Term,,k) (Termjgt;k)  leads to the following set of similarity values based upon the centroids:  CENTROID (Ql) = (D8) = (4, 2, 0, 2)  CENTROID (Q2) = (D3, D4, D5) = 1/3(4+0+0, 0+6+4, 2+4+6, 4+6+4)  SIM(CENTROID0,,D1) = (0 + 8 + 0+16) =24 SIM(CENTROIDOi,D2)-(0 + 4 + 0 + 0)     =   4  SIM(CENTROIDQi,D3) = (16 + 0 + 0 + 8)   =24  SIM(CENTROIDQ|9D4) = (0 + 12 + 0 + 12) = 24 SIM(CENTROIDQ,,D5) = (0 + 8 + 0 + 8) =16 SIM(CENTROID0i,D6) = (24 + 0 + 0 + 0)   =24  SIM(CENTRO!Do:,Dl)= 1/3(0 + 40 + 0+ 112)       =1/3(152) SIM(CENTRGIDq2,D8) = 1/3(16 + 20 +0 +28)         = 1/3(64) User Search Techniques                                                                             191  For Ql, two additional items are added to the invariant set (D8) u (Dl, D3), by choosing the lowest number items because of the tie at 24, giving the answer of (Dl, D3, D8). For Q2, one additional item is added to the invariant set (D3, D4, D5) u (Dl) giving the answer (Dl, D3, D4, D5).
issr-0105	7.6 Searching the INTERNET and Hypertext  The Internet has multiple different mechanisms that are the basis for search of items. The primary techniques are associated with servers on the Internet that create indexes of items on the Internet and allow search of them. Some of the most commonly used nodes are YAHOO, AltaVista and Lycos. In all of these systems there are active processes that visit a large number of Internet sites and retrieve textual data which they index. The primary design decisions are on the level to which they retrieve data and their general philosophy on user access. LYCOS (http://www.lycos.com) and AltaVista automatically go out to other Internet sites and return the text at the sites for automatic indexing (http://www.altavista.digital.com). Lycos returns home pages from each site for automatic indexing while Altavista indexes all of the text at a site. The retrieved text is then used to create an index to the source items storing the Universal Resource Locator (URL) to provide to the user to retrieve an item. All of the systems use some form of ranking algorithm to assist in display of the retrieved items. The algorithm is kept relatively simple using statistical information on the occurrence of words within the retrieved text.  Closely associated with the creation of the indexes is the technique for accessing nodes on the Internet to locate text to be indexed. This search process is also directly available to users via Intelligent Agents. Intelligent Agents provide the capability for a user to specify an information need which will be used by the Intelligent Agent as it independently. moves between Internet sites locating information of interest. There are six key characteristics of intelligent agents (Heilmann-96):  1.     Autonomy -    the search agent must be able to operate without interaction with a human agent.    It must have control over its own internal states and make independent decisions.   This implies a search capability to traverse information sites based upon pre-established criteria collecting potentially relevant information.  2.   Communications Ability - the agent must be able to communicate with the information sites as it traverses them.    This implies a universally accepted language defining the external interfaces (e.g., Z39.50). 192                                                                                               Chapter 7  3.   Capacity for Cooperation - this concept suggests that intelligent agents need to cooperate to perform mutually beneficial tasks.  4.   Capacity for Reasoning - There are three types of reasoning scenarios (Roseler-94):  Rule-based - where user has defined a set of conditions and actions to be taken  Knowledge-based - where the intelligent agents have stored previous conditions and actions taken which are used to deduce future actions  Artificial evolution based - where intelligent agents spawn new agents with higher logic capability to perform its objectives.  5.   Adaptive Behavior - closely tied to 1 and 4 , adaptive behavior permits the intelligent agent to assess its current state and make decisions on the actions it should take  6.   Trustworthiness - the user must trust that the intelligent agent will act on the user's behalf to locate information that the user has access to and is relevant to the user.  There are many implementation aspects of Intelligent Agents. They include communications to traverse the Internet, how to wrap the agent in an appropriate interface shell to work within an Internet server, and security and protection for both the agent and the servers. Although these are critical for the implementation of the agents, the major focus for information storage and retrieval is how to optimize the location of relevant items as the agent performs its task. This requires expansion of search capabilities into conditional and learning feedback mechanisms that are becoming major topics in information retrieval.  Automatic relevance feedback is being used in a two-step process to enhance user's queries to include corpora-specific terminology. As an intelligent agent moves from site to site, it is necessary for it to use similar techniques to learn the language of the authors and correlate it to the search need of the user. How much information gained from relevance feedback from one site should be carried to the next site has yet to be resolved. Some basic groundwork is being laid by the work on incremental relevance feedback discussed earlier. It will also need capabilities to normalize ranking values across multiple systems. The quantity of possible information being returned necessitates a merged ranking to allow the user to focus on the most likely relevant items first.  Finally, there is the process of searching for information on the Internet by following Hyperlinks. A Hyperlink is an embedded link to another item that can be instantiated by clicking on the item reference. Frequently hidden to the user User Search Techniques                                                                            193  is a URL associated with the text being displayed. As discussed in Chapter 5, inserting hyperlinks in an item is a method of indexing related information. One of the issues of the existing Hyperlink process is the inability for the link to have attributes. In particular, a link may be a pointer to another object that is an integral aspect of the item being displayed (e.g. an embedded image or quoted text in another item). But the reference could also be to another item that generally supports the current text. It could also be to another related topic that the author feels may be of interest to the reader. There are many other interpretations of the rationale behind the link that are author specific.  Understanding the context of the link in the item being viewed determines the utility of following the associated path. Thus the Hyperlinks create a static network of linked items based upon an item being viewed. The user can manually move through this network space by following links. The search in this sense is the ability to start with an item and create the network of associated items (i.e., following the links). The results of the search is a network diagram that defines the interrelated items which can be displayed to the user to assist in identification of where the user is in the network and to facilitate movement to other nodes (items) within the network (Gershon-95, Hasan-95, Mukherjea-95, Munzner-95). The information retrieval aspect of this problem is how to automatically follow the hyperlinks and how the additional information as each link is instantiated impacts the resolution of the user's search need. One approach is to use the function described in Section 5.5 as a mechanism for assigning weights to the terms in original and linked items to use with the search statement to determine hits.  New search capabilities are continually becoming available on the Internet. Dissemination systems are proliferating to provide individual users with items they are potentially interested in for personal or business reasons. Some examples are the PointCast system, Fish Wrap newspaper service at MIT and SFGATE (San Francisco Examiner and San Francisco Chronicle) that allow users to define specific areas of interest. Items will be e-mailed as found or stored in a file for later retrieval. The systems will continually update your screen if you are on the Internet with new items as they are found (http://fishwrapdocs.www.media.mit.edu/docs/, http:/www.sfgate.com, http:/www.pointcast.com). There are also many search sites that collect relevance information from user interaction and use relevance feedback algorithms and proprietary heuristics and provide modifications on information being delivered. Firefly interacts with a user, learning the user's preferences for record albums and movies. It provides recommendations on potential products of interest. The Firefly system also compares the user's continually changing interest profile with other users and informs users of others with similar interests for possible collaboration (http:/www.ffly.com). Another system that uses feedback across multiple users to categorize and classify interests is the Empirical Media system (http:/www.empiracal.com). Based upon an Individual user's relevance ranking of what is being displayed the system learns a user's preference. It also judges from other user's rankings of items the likelihood that an item will be of interest to other 194                                                                                               Chapter 7  users that show the same pattern of interest. Thus it uses this "Collaborative Intelligence" in addition to its internal ranking algorithms to provide a final ranking of items to individual users. Early research attempts at using queries across multiple users to classify document systems did not show much promise (Salton-83). But the orders of magnitude increase (million times greater or more) in user interaction from the Internet provides a basis for realistic clustering and learning.
issr-0106	7.7 Summary  Creating the index to an Information Retrieval System defines the searchable concepts that represent the items received by a system. The user search process is the mechanism that correlates the user's search statement with the index via a similarity function. There are a number of techniques to define the indexes to an item. It is typically more efficient to incur system overhead at index creation time than search time. An item is processed once at index time, but there will be millions of searches against the index. Also, the user is directly affected by the response time of a search but, in general, is not aware of how long it takes from receipt of an item to its being available in the index. The selection and implementation of similarity algorithms for search must be optimized for performance and scaleable to accommodate very large databases.  It is typical during search parsing that the user's initial search statement is expanded via a thesaurus or semantic net to account for vocabulary differences between the user and the authors. But excessive expansion takes significantly more processing and increases the response time due to the number of terms that have to be processed. Most systems have default limits on the number of new terms added to a search statement. Chapter 7 describes some of the basic algorithms that can be used as similarity measures. These algorithms are still in a state of evolution and are continually being modified to improve their performance. The search algorithms in a probabilistic indexing and search system are much more complex than the similarity measures described. For systems based upon natural language processing, once the initial similarity comparisons are completed, there is an additional search processing step to make use of discourse level information, adding additional precision to the final results.  Relevance feedback is an alternative to thesaurus expansion to assist the user in creating a search statement that will return the needed information. Thesaurus and semantic net expansions are dependent upon the user's ability to use the appropriate vocabulary in the search statement that represents the required information. If the user selects poor terms, they will be expanded with many more poor terms. Thesaurus expansion does not introduce new concepts that are relevant to the users information need, it just expands the description of existing concepts. Relevance feedback starts with the text of an item that the user has identified as meeting his information need; incorporating it into a revised search User Search Techniques                                                                             195  statement.     The vocabulary  in  the relevant  item  text  has the potential  for introducing new concepts that better reflect the user's information need along with adding additional terms related to existing search terms and adjusting the weights (importance) of existing terms.  Selective Dissemination of Information search is different from searches against the persistent information database in that it is assumed there is no information from a large corpus available to determine parameters in determining a temporary index for the item to use in the similarity comparison process (e.g., inverse document frequency factors.) An aspect of dissemination systems that helps in the search process is the tendency for the profiles to have significantly more terms than ad hoc queries. The additional information helps to identify relevant items and increase the precision of the search process. Relevance feedback can also be used with profiles with some constraints. Relevance feedback used with ad hoc queries against an existing database tends to move the terminology defining the search concepts towards the information need of the user that is available in the current database. Concepts in the initial search statement will eventually lose importance in the revised queries if they are not in the database. The goal of profiles is to define the coverage of concepts that the user cares about if they are ever found in new items. Relevance feedback applied to profiles aides the user by enhancing the search profile with new terminology about areas of interest. But, even though a concept has not been found in any items received, that area may still be of critical importance to the user if it ever is found in any new items. Thus weighting of original terms takes on added significance over the ad hoc situation.  Searching the Internet for information has brought into focus the deficiencies in the search algorithms developed to date. The ad hoc queries are extremely short (usually less than three terms) and most users do not know how to use the advanced features associated with most search sites. Until recently research had focused on a larger more sophisticated query. With the Internet being the largest most available information system supporting information retrieval search, algorithms are in the process of being modified to account for the lack of information provided by the users in their queries. Intelligent Agents are being proposed as a potential mechanism to assist users in locating the information they require. The requirements for autonomy and the need for reasoning in the agents will lead to the merging of information retrieval algorithms and the learning processes associated with Artificial Intelligence. The use of hyperlinks is adding another level of ambiguity in what should be defined as an item. When similarity measures are being applied to identify the relevance weight, how much of the hyperlinked information should be considered part of the item? The impacts on the definition of information retrieval boundaries are just starting to be analyzed while experimental products are being developed in Web years and immediately being made available.
issr-0107	8 Information Visualization  8.1     Introduction to Information Visualization  8.2    Cognitive and Perception  8.3    Information Visualization Technologies  8.4    Summary  The primary focus on Information Retrieval Systems has been in the areas of indexing, searching and clustering versus information display. This has been due to the inability of technology to provide the technical platforms needed for sophisticated display, academic's focusing on the more interesting algorithmic based search aspects of information retrieval, and the multi-disciplinary nature of the hum an-computer interface (HCI). The core technologies needed to address sophisticated information visualization have matured, supporting productive research and implementation into commercial products. The commercial demand for these technologies is growing with availability of the "information highway." System designers need to treat the display of data as visual computing instead of treating the monitor as a replica of paper. Functions that are available with electronic display and visualization of data that were not previously provided are (Brown-96):  modify representations of data and information or the display condition  (e.g., changing color scales)  use the same representation while showing changes in data (e.g., moving between clusters of items showing new linkages)  animate the display to show changes in space and time  enable interactive input from the user to allow dynamic movement between information spaces and allow the user to modify data presentation to optimize personal preferences for understanding the data. 200                                                                                               Chapter 8  Create hyperlinks under user control to establish relationships between data  If information retrieval had achieved development of the perfect search algorithm providing close to one hundred per cent precision and recall, the need for advances in information visualization would not be so great. But reality has demonstrated in TREC and other information fora that advancements are not even close to achieving this goal. Thus, any technique that can reduce the user overhead of finding the needed information will supplement algorithmic achievements in finding potential relevant items. Information Visualization addresses how the results of a search may be optimally displayed to the users to facilitate their understanding of what the search has provided and their selection of most likely items of interest to read. Visual displays can consolidate the search results into a form easily processed by the user's cognitive abilities, but in general they do not answer the specific retrieval needs of the user other than suggesting database coverage of the concept and related concepts.  The theoretical disciplines of cognitive engineering and perception provide a theoretical base for information visualization. Cognitive engineering derives design principles for visualization techniques from what we know about the neural processes involved with attention, memory, imagery and information processing of the human visual system. By 1989 research had determined that mental depiction plays a role in cognition that is different from mental description. Thus, the visual representation of an item plays as important a role as its symbolic definition in cognition.  Cognitive engineering results can be applied to methods of reviewing the concepts contained in items selected by search of an information system. Visualization can be divided into two broad classes: link visualization and attribute (concept) visualization. Link visualization displays relationships among items. Attribute visualization reveals content relationships across large numbers of items. Related to attribute visualization is the capability to provide visual cues on how search terms affected the search results. This assists a user in determining changes required to search statements that will return more relevant items.
issr-0108	8.1 Introduction to Information Visualization  The beginnings of the theory of visualization began over 2400 years ago. The philosopher Plato discerned that we perceive objects through the senses, using the mind. Our perception of the real world is a translation from physical energy from our environment into encoded neural signals. The mind is continually interpreting and categorizing our perception of our surroundings. Use of a computer is another source of input to the mind's processing functions. Text-only interfaces reduce the complexity of the interface but also restrict use of the more powerful information processing functions the mind has developed since birth. Information Visualization                                                                          201  Information visualization is a relatively new discipline growing out of the debates in the 1970s on the way the brain processes and uses mental images. It required significant advancements in technology and information retrieval techniques to become a possibility. One of the earliest researchers in information visualization was Doyle, who in 1962 discussed the concept of "semantic road maps" that could provide a user a view of the whole database (DoyIe-62). The road maps show the items that are related to a specific semantic theme. The user could use this view to focus his query on a specific semantic portion of the database. The concept was extended in the late 1960s, emphasizing a spatial organization that maps to the information in the database (Miller-68). Sammon implemented a non-linear mapping algorithm that could reveal document associations providing the information required to create a road map or spatial organization (Sammons-69).  In the 1990s technical advancements along with exponential growth of available information moved the discipline into practical research and commercialization. Information visualization techniques have the potential to significantly enhance the user's ability to minimize resources expended to locate needed information. The way users interact with computers changed with the introduction of user interfaces based upon Windows, Icons, Menus, and Pointing devices (WIMPs). Although movement in the right direction to provide a more natural human interface, the technologies still required humans to perform activities optimized for the computer to understand. A better approach was stated by Donald A. Norman (Rose-96):  ... people are required to conform to technology.  It is time to reverse this trend, time to make technology conform to people  Norman stresses that to optimize the user's ability to find information, the focus should be on understanding the aspects of the user's interface and processing of information which then can be migrated to a computer interface (Norman-90).  Although using text to present an overview of a significant amount of information makes it difficult for the user to understand the information, it is essential in presenting the details. In information retrieval, the process of getting to the relevant details starts with filtering many items via a search process. The results of this process is still a large number of potentially relevant items. In most systems the results of the search are presented as a textual list of each item perhaps ordered by rank. The user has to read all of the pages of lists of the items to see what is in the Hit list. Understanding the human cognitive process associated with visual data suggests alternative ways of presenting and manipulating information to focus on the likely relevant items. There are many areas that information visualization and presentation can help the user:  a.   reduce the amount of time to understand the results of a search and likely clusters of relevant information 202                                                                                               Chapter 8  b.    yield information that comes from the relationships between items versus treating each item as independent  c.    perform simple actions that produce sophisticated information search functions  A study was performed by Fox et al. using interviews and user task analysis on professionals in human factors engineering, library science, and computer science to determine the requirements to optimize their work with documents (Fox-93a). Once past the initial requirement for easy access from their office, the researchers' primary objective was the capability to locate and explore patterns in document databases. They wanted visual representations of the patterns and items of interest. There was a consistent theme that the tools should allow the users to view and search documents with the system sensitive to their view of the information space. The users wanted to be able to focus on particular areas of their interest (not generic system interest definitions) and then easily see new topical areas of potential interest to investigate. They sought an interface that permits easy identification of trends, interest in various topics and newly emerging topics. Representing information in a visual mode allows for cognitive parallel processing of multiple facts and data relationships satisfying many of these requirements.  The exponential growth in available information produces large Hit files from most searches. To understand issues with the search statement and retrieved items, the user has to review a significant number of status screens. Even with the review, it is hard to generalize if the search can be improved. Information visualization provides an intuitive interface to the user to aggregate the results of the search into a display that provides a high-level summary and facilitates focusing on likely centers of relevant items. The query logically extracts a virtual workspace (information space) of potential relevant items which can be viewed and manipulated by the user. By representing the aggregate semantics of the workspace, relationships between items become visible. It is impossible for the user to perceive these relationships by viewing the items individually. The aggregate presentation allows the user to manipulate the aggregates to refine the items in the workspace. For example, if the workspace is represented by a set of named clusters (name based upon major semantic content), the user may select a set of clusters that defines the next iteration of the search.  An alternative use of aggregates is to correlate the search terms with items retrieved. Inspecting relevant and non-relevant items in a form that highlights the effect of the expanded search terms provides insights on what terms were the major causes for the results. A user may have thought a particular term was very important. A visual display could show that the term in fact had a minimal effect on the item selection process, suggesting a need to substitute other search terms.  Using a textual display on the results of a search provides no mechanism to display inter-relationships between items. For example, if the user is interested in the development of a polio vaccine, there is no way for a textual listing of found items to show "date" and "researcher" relationships based upon published items. Information Visualization                                                                         203  The textual summary list of the Hit file can only be sorted via one attribute, typically relevance rank.  Aspects of human cognition are the technical basis for understanding the details of information visualization systems. Many techniques are being developed heuristically with the correlation to human cognition and perception analyzed after the techniques are in test. The commercial pressures to provide visualization in delivered systems places the creativity under the intuitive concepts of the developer.
issr-0109	8.2 Cognition and Perception  The user-machine interface has primarily focused on a paradigm of a typewriter. As computers displays became ubiquitous, man-machine interfaces focused on treating the display as an extension of paper with the focus on consistency of operations. The advent of WIMP interfaces and simultaneous parallel tasks in the user work environment expanded the complexity of the interface to manipulate the multiple tasks. The evolution of the interface focused on how to represent to the user what is taking place in the computer environment. The advancements in computer technology, information sciences and understanding human information processing are providing the basis for extending the human computer interface to improve the information flow, thus reducing wasted user overhead in locating needed information. Although the major focus is on enhanced visualization of information, other senses are also being looked at for future interfaces. The audio sense has always been part of simple alerts in computers. Illegal inputs are usually associated with a beep, and more recently users have a spectrum of audio sounds to associate with everything from start-up to shut down. The sounds are now being replaced by speech in both input and output interfaces. Still in the research arena is the value of using audio to encapsulate information (e.g., higher pitch as you move through an information space plus increased relevance). The tactile (touch) sense is being addressed in the experiments using Virtual Realty (VR). For example, VR is used as a training environment for areas such as medical procedures where tactile feedback plays an increasing role. Olfactory and taste are two areas where practical use for information processing or computer interfaces in general has yet to be identified. For Information Retrieval Systems, the primary area of interest is in information visualization.
issr-0110	8.2.1  Background  A significant portion of the brain is devoted to vision and supports the maximum information transfer function from the environment to a human being.  The center of debates in the 1970s was whether vision should be considered data collection or also has aspects of information processing.     In   1969 Arnheim  questioned the then current psychological division of cognitive operations of 204                                                                                               Chapter 8  perception and thinking as separate processes (Arnheim-69). Until then perception was considered a data collection task and thinking as a higher level function using the data. He contended that visual perception includes the process of understanding the information, providing an ongoing feedback mechanism between the perception and thinking. He further expanded his views arguing that treating perception and thinking as separate functions treats the mind as a serial automata (Arnheim-86). Under this paradigm, the two mental functions exclude each other, with perception dealing with individual instances versus generalizations. Visualization is the transformation of information into a visual form which enables the user to observe and understand the information. This concept can be extended where the visual images provide a fundamentally different way to understand information that treats the visual input not as discrete facts but as an understanding process. The Gestalt psychologists postulate that the mind follows a set of rules to combine the input stimuli to a mental representation that differs from the sum of the individual inputs (Rock-90):  Proximity - nearby figures are grouped together Similarity - similar figures are grouped together  Continuity - figures are interpreted as smooth continuous patterns rather than discontinuous concatenations of shapes (e.g., a circle with its diameter drawn is perceived as two continuous shapes, a circle and a line, versus two half circles concatenated together)  Closure - gaps within a figure are filled in to create a whole (e.g., using  dashed lines to represent a square does not prevent understanding it as a square)  Connectedness - uniform and linked spots, lines or areas are perceived as a single unit  Shifting the information processing load from slower cognitive processes to faster perceptual systems significantly improves the information-carrying interfaces between humans and computers (Card-96). There are many ways to present information in the visual space. An understanding of the way the cognitive processes work provides insights for the decisions on which of the presentations will maximize the information passing and understanding. There is not a single correct answer on the best way to present information.
issr-0111	8.2.2 Aspects of the Visualization Process  One of the first-level cognitive processes is preattention, that is, taking the significant visual information from the photoreceptors and forming primitives. Information Visualization  205  Primitives are part of the preconscious processes that consist of involuntary lower order information processing (Friedhoff-89). An example of this is the ease with which our visual systems detect borders between changes in orientation of the same object. In Figure 8.1 the visual system detects the difference in orientations between the left and middle portion of the figure and determines the logical border between them. An example of using the conscious processing capabilities of the brain is the detection of the different shaped objects and the border between them shown between the left side and middle of the Figure 8.1. The reader can likely detect the differences in the time it takes to visualize the two different boundaries.  ft         ft  ft     ftc:  lt;=    lt;=  ft  ft     ft   ft       ft       lt;=lt;=     CC=  ftft   ft  ft    ft ft   ftftft     ft  ftft ftftftftftft    ft  Figure 8.1 Preattentive Detection Mechanism  This suggests that if information semantics are placed in orientations, the mind's clustering aggregate function enables detection of groupings easier than using different objects (assuming the orientations are significant). This approach makes maximum use of the feature detectors in the retina.  The preattentive process can detect the boundaries between orientation groups of the same object. A harder process is to identify the equivalence of rotated objects. For example, a rotated square requires more effort to recognize it as a square. As we migrate into characters, the problem of identification of the character is affected by rotating the character in a direction not normally encountered. It is easier to detect the symmetry when the axis is vertical. Figure 8.2 demonstrates these effects.  R 3 V L      REAL Figure 8.2 Rotating a Square and Reversing Letters in "REAL" 206                                                                                                Chapter 8  Another visual factor is the optical illusion that makes a light object on a dark background to appear larger than if the item is dark and the background is light. Making use of this factor suggests that a visual display of small objects should use bright colors. An even more complex area is the use of colors. Colors have many attributes that can be modified such as hue, saturation and lightness. Hue is the physiological attribute of color sensation. Saturation is the degree to which a hue is different from a gray line with the same lightness, while lightness is the sensation of the amount of white or black. Complementary colors are two colors that form white or gray when combined (red/green, yellow/blue). Color is one of the most frequently used visualization techniques to organize, classify, and enhance features (Thorell-90). Humans have an innate attraction to the primary colors (red, blue, green and yellow), and their retention of images associated with these colors is longer. But colors also affect emotion, and some people have strong aversion to certain colors. The negative side of use of colors is that some people are color blind to some or many colors. Thus any display that uses colors should have other options available.  Depth, like color, is frequently used for representing visual information. Classified as monocular cues, changes in shading, blurring (proportional to distance), perspective, motion, stereoscopic vision, occlusion and texture depict depth. Most of the cues are affected more by lightness than contrast. Thus, choice of colors that maximizes brightness in contrast to the background can assist in presenting depth as a mechanism for representing information. Depth has the advantage that depth/size recognition are learned early in life and used all of the time. Gibson and Walk showed that six-month-old children already understand depth suggesting that depth may be an innate concept (Gibson-60). The cognitive processes are well developed, and the use of this information in classifying objects is ubiquitous to daily life. The visual information processing system is attuned to processing information using depth and correlating it to real world paradigms.  Another higher level processing technique is the use of configural aspects of a display (Rose-95). A configural effect occurs when arrangements of objects are presented to the user allowing for easy recognition of a high-level abstract condition. Configural clues substitute a lower level visual process for a higher level one that requires more concentration (see preattentive above). These clues are frequently used to detect changes from a normal operating environment such as in monitoring an operational system. An example is shown in Figure 8.3 where the sides of a regular polygon (e.g., a square in this example) are modified. The visual processing system quickly detects deviations from normally equally sized objects.  Another visual cue that can be used is spatial frequency. The human visual and cognitive system tends towards order and builds an coherent visual image whenever possible. The multiple spatial channel theory proposes that a complex image is constructed from the external inputs, not received as a single image. The final image is constructed from multiple receptors that detect changes Information Visualization  207  Figure 8.3 Distortions of a Regular Polygon  in spatial frequency, orientation, contrast, and spatial phase. Spatial frequency is an acuity measure relative to regular light-dark changes that are in the visual field or similar channels. A cycle is one complete light-dark change. The spatial frequency is the number of cycles per one degree of visual field. Our visual systems are less sensitive to spatial frequencies of about 5-6 cycles per degree of visual field (NOTE: one degree of visual field is approximately the viewing angle subtended by the width of a finger at arms length). Other animals have significantly more sensitive systems that allow them to detect outlines of camouflaged prey not detected by humans until we focus on the area. Associated with not processing the higher spatial frequencies is a reduction in the cognitive processing time, allowing animals (e.g. cats) to react faster to motion. When looking at a distinct, well defined image versus a blurred image, our visual system will detect motion/changes in the distinct image easier than the blurred image. If motion is being used as a way of aggregating and displaying information, certain spatial frequencies facilitate extraction of patterns of interest. Dr. Mary Kaiser of NASA-AMES is experimenting with perceptually derived displays for aircraft. She is interested in applying the human vision filters such as limits of spatial and temporal resolution, mechanisms of stereopsis, and attentional focus to aircraft (Kaiser-96).  The human sensory systems learn from usage. In deciding upon visual information techniques, parallels need to be made between what is being used to represent information and encountering those techniques in the real world environment. The human system is adept at working with horizontal and vertical references. They are easily detected and processed. Using other orientations requires additional cognitive processes to understand the changes from the expected inputs. The typical color environment is subdued without large areas of bright colors. Thus using an analogous situation, bright colors represent items to be focused on correlating to normal processing (i.e., noticing brightly colored flowers in a garden). Another example of taking advantage of sensory information that the brain is use to processing is terrain and depth information. Using a graphical representation that uses depth of rectangular objects to represent information is an image that the visual system is used to processing. Movement in that space is more easily interpreted and understood by the cognitive processes than 208                                                                                                Chapter 8  if, for example, a three-dimensional image of a sphere represented a visual information space.  In using cognitive engineering in designing information visualization techniques, a hidden risk is that "understanding is in the eye of the beholder." The integration of the visual cues into an interpretation of what is being seen is also based upon the user's background and context of the information. The human mind uses the latest information to assist in interpreting new information. If a particular shape has been representing important information, the mind has a predisposition to interpret new inputs as the same shape. For example, if users have been focusing on clusters of items, they may see clusters in a new presentation that do not exist. This leads to the question of changing visualization presentations to minimize legacy dispositions. Another issue is that our past experiences can affect our interpretation of a graphic. Users may interpret figures according to what is most common in their life experiences rather than what the designer intended.
issr-0112	8.3 Information Visualization Technologies  The theories associated with information visualization are being applied in commercial and experimental systems to determine the best way to improve the user interface, facilitating the localization of information. They have been applied to many different situations and environments (e.g., weather forecasting to architectural design). The ones focused on Information Retrieval Systems are investigating how best to display the results of searches, structured data from DBMSs and the results of link analysis correlating data. The goals for displaying the result from searches fall into two major classes: document clustering and search statement analysis. The goal of document clustering is to present the user with a visual representation of the document space constrained by the search criteria. Within this constrained space there exist clusters of documents defined by the document content. Visualization tools in this area attempt to display the clusters, with an indication of their size and topic, as a basis for users to navigate to items of interest. This is equivalent to searching the index at a library and then pursuing all the books on the different shelf locations that are retrieved by the search. The second goal is to assist the user in understanding why items were retrieved, thereby providing information needed to refine the query. Unlike the traditional Boolean systems where the user can easily correlate the query to the retrieved set of items, modern search algorithms and their associated ranking techniques make it difficult to understand the impacts of the expanded words in the search statement. Visualization techniques approach this problem by displaying the total set of terms, including additional terms from relevance feedback or thesaurus expansion, along with documents retrieved and indicate the importance of the term to the retrieval and ranking process.  Structured databases are important to information retrieval because structured files are the best implementation to hold certain citation and semantic Information Visualization                                                                          209  data that describe documents. Link analysis is also important because it provides aggregate-level information within an information system. Rather than treating each item as independent, link analysis considers information flowing between documents with value in the correlation between multiple documents. For example, a time/event link analysis correlates multiple documents discussing a oil spill caused by a tanker. Even if all of the items retrieved on the topic are relevant, displaying the documents correlated by time may show dependencies of events that are of information importance and are not described in any specific document. This section summarizes some of the major techniques being applied. This can assist in correlating the theory of visual perception to the practice of implementing systems.  One way of organizing information is hierarchical. A tree structure is useful in representing information that ranges over time (e.g., genealogical lineage), constituents of a larger unit (e.g., organization structures, mechanical device definitions) and aggregates from the higher to lower level (e.g., hierarchical clustering of documents). A two-dimensional representation becomes difficult for a user to understand as the hierarchy becomes large. One of the earliest experiments in information visualization was the Information Visualizer developed by XEROX PARC. It incorporates various visualization formats such as DataMap, InfoGrid, ConeTree, and the Perspective wall. The Cone-Tree is a 3-Dimensional representation of data, where one node of the tree is represented at the apex and all the information subordinate to it is arranged in a circular structure at its base. Any child node may also be the parent of another cone. Selecting a particular node, rotates it to the front of the display. Compared to other hierarchical representations (e.g., node and link trees) the cone makes the maximum information available to the user providing a perspective on size of each of the subtrees (Gershon-95a, Robertson-93). An example of a Cone-Tree is shown in Figure 8.4. The squares at the leaf nodes in tree are the actual documents. Higher level nodes can be considered centroids representing the semantic of the child nodes. Where the database is large, the boxes may represent a cluster of related items versus a single item. These clusters could be expanded to lower levels of the tree. The perspective wall divides the information into three visual areas with the area being focused on in the front and other areas out of focus to each side (see Figure 8.5). This allows the user to keep all of the information in perspective while focusing on a perticular area.  Another technique used in display of hierarchical information is tree maps (Johnson-91). This technique makes maximum use of the display screen space by using rectangular boxes that are recursively subdivided based upon parent-child relationships between the data. A particular information work space focused on articles on computers may appear as shown in Figure 8.6. The size of the boxes can represent the number of items on a particular topic. The location of the boxes can indicate a relationship between the topics. In Figure 8.6, the CPU, OS, Memory, and Network management articles are all related to a general category of computer operating systems versus computer applications which are shown in the rest of the figure. 210  Chapter 8  ?  D  Figure 8.4   Cone Tree  Figure 8.5 Perspective Wai! From InXight web site - wwwJnxIght.com Information Visualization  211  CPU articles  Memory articles  Network management articles  Word Processing software articles  PowerPoint software articles  Drawing software articles  Figure 8.6   Tree Map  When the information has network-type relationships, an approach using clusters can be shown via a semantic scatterplot. Both the Vineta and Bead systems display clustering patterns using a three-dimensional scatterplot (Krohn95, Chalmers-92 respectively). Battelle Pacific Northwest National Laboratory correlates documents and performs multidimensional scaling to plot each as a point in Euclidean vector space. The difficulty of representing all of the axis is overcome by projecting the space onto a plane and using elevation to indicate the frequency of occurrence and importance of a theme (concept) creating a semantic landscape (Wise-95 Card-96). The detailed relationships between items and their composite themes can be seen by the valleys, cliffs and ranges shown on the terrain map. One way of overcoming the multidimensional space representation in a hierarchical environment is to create embedded coordinate spaces. In this technique the larger coordinate space is redefined with coordinates inside of other coordinates. Thus a six-dimensional coordinate space may have three of the coordinates defined as a subspace within the other three coordinate spaces. This has been called Feiner's "worlds within worlds" approach (Feiner-90). Other techniques suggested to solve this representation problem can be found in semantic regions suggested by Kohonen, linked trees or graphs in the Narcissus system, or a non-Euclidean landscape suggested by Lamping and Rao (Munzner-95, Lin-91 and Lin-92, Hendley-95, Lamping-95 respectively). When searches are used to define the user's infospace of interest and provide additional focusing of semantic interest, the "information crystal" (similar to a VENN diagram) assists the user in detecting patterns of term relationships in the constrained Hit file (Spoerri-93). The CyberWorld system constrains its clustering visualization to a threedimensional sphere (Hemmje-94). Another clustering system that uses statistical information for a small number of items (50 - 120) to show term relationships via spatial positioning is the VIBE system (OIsen-93). The VIBE system allows users to associate query terms with different locations in the visual display. Documents are distributed to show their relevance to the different terms.   Lin has taken the 212  Chapter 8  self-organization concept further by using Kohonen's algorithm to automatically determine a table of contents (TOC) and display the results in a map display (Lin96).  The goal of many visualization techniques is to show the semantic relationships between individual items to assist the user in locating those groups of items of interest. Another objective of visualization is in assisting the users in refining their search statements. It is difficult for users in systems using similarity measures to determine what are the primary causes for the selection and ranking of items in a Hit file. The automatic expansion of terms and intricacies of the similarity algorithms can make it difficult to determine the effects that the various words in the search statement are having on creating the Hit file. Visualization tools need to assist the user in understanding the effects of his search statement even to the level of identifying important terms that are not contributing to the  File     Edit    Query  Edit     Results  New Ouªry[ [  Query History:                     Do Search 1  CM    Found    Query-Short form  ¶¶]['-.......""ffl'oó 2         100 T.docuaent retrieval ´     Authors: Qu´ry#3                   f  Sal ton ti;  Word* In Trtte:   l^ctoc docu processing ¶ent text                        B i rotrtaval                        i    ;  Content Words   1   Bast 50 Item* Found  Find Icon     [           Icon Label: Relevance Rank *l                      Icon Size:  Icon Color.   E*L Relevwuie   y|                  lconSh´pe:  CatorUgend                       User Rating  Wl.                       u-i    py^   mm            ¶   Uaeful___  Author      r.        "*""               "*""*              Not Useful  vBest25 ?BestSO  OBesUOOvBe*  ¶f  BooVstein, A     5,6   Lesk, H.   E   22     Karo, H.  E.     e   Ozkaraban, E o       Salton, C CD 37,39 o 32 lt;£gt; 13,14 9  ©  Sal ton, Garacd  o 33      Sal ton Gacald    CD 11,12    Salton teracd  CDÆ 3435 27,28  CD 7,8  ´   Laast     Most     .  X-Atds:   Est Relevance  Envision item Stunirary; Query *3  l´on#  Useful Est.Rel    Author/Edttor  405        sal tea,  Gaiard  ,  and Rotrioval  of  I  Figure 8.7 Envision Interface (from SIGIR 96, page 68)  search process.    One solution is a graphical display of the characteristics of the  retrieved items which contributed to their selection.    This is effected in the Envision system when index terms are selected as an axis.  The Envision system not only displays the relevance rank and estimated relevance of each item found by a query, but also simultaneously presents other  query information.   The design is intentionally graphical and simple using twoInformation Visualization  213  dimensional visualization. This allows a larger variety of user computer platforms to have access to their system (Nowell-96). Figure 8.7 shows Envision's three interactive windows to display search results: Query window, Graphic View window, and Item Summary window. The Query window provides an editable version of the query. The Item Summary window provides bibliographic citation information on items selected in the Graphic View window. The Graphic View window is similar to scatterplot graphs. Each item in the Hit file is represented by an icon in the window. Selecting an item in the window provides bibliographic information on the same display. Circles represent single items with the relevance weights displayed below them. Ellipses represent clusters of multiple items that are located at the same point in the scatterplot with the number of items in the  *    ||f|  affecr  llillfll It!  construct*           li*BlillillH  project*  i ll   II I      I       I I    I. I   I I   t  till! t  IBs it  ¶  ft      ttlf  1 l itli   i  Hill HI llllili lllllliill I III II I ll SlS  fiiiiifiilni´isiiiiiiiiiiiiiitftiiii  ïïïtsftlflft* ¶Kflaalttsflii tlKtlsilItltfci !!¶ï¶¶ï  ª   '   i    Figure 8.8 Visualization of Results (from SIGIR 96, page 88)  ellipse and their weights below the ellipse. In this example, estimated relevance is on the X-axis and author's name is on the Y-axis. This type of interface provides a  very user friendly environment but encounters problems when the number of relevant items and entries for an axis becomes very large. Envision plans to address this issue by a "zoom" feature that will allow seeing larger areas of the scatterplot at lesser detail. 214  Chapter 8  A similar technique is used by Veerasamy and Belkin (Veerasamy-96). They use a series of vertical columns of bars. The columns of bars represent documents, and the rows represent index terms. The height of the bar corresponds to the weight of the corresponding term (row) in the corresponding item (column). In addition to the query terms, the system shows the additional words added to the system by relevance feedback. Figure 8.8 provides an example for a search statement of "How affirmative action affected the construction industry." This approach quickly allows a user to determine which terms had the most effect on retrieving a specific item (i.e. by scanning down the column). It also allows the user to determine how the various terms contributed to the retrieval process (i.e. by scanning a row). This latter process is very important because it allows a user to determine if what he considers to be an important search term is not contributing strongly or not found at all in the items being retrieved. It also shows search terms that are causing items to be retrieved allowing their removal or reduction in query weight if they are causing false hits. In the Boolean environment this function was accomplished by vocabulary browsing (see Chapter 2) that allows for a user to see the number of items a particular term is in prior to including it in a search.  DCARS Expanded Query Histogram  Title/Expansion Hits Histogram 1.3  Help  EXPANDED QUERY HITS FORTHE RETRIEVED DOCUMENTS 1  PARLIAMENT PASSES LAW ON  RUSSIAN STRATEGIC ROCKET  KOZYREV IN WASHINGTON.  SERBIAN RADICALS RATTLE S  GRACHEV DISCUSSES BOSMIAN  RUSSIA WANTS TO CHANGE CF  BOSNIAN MUSLIMS ACCEPT VA  LATVIA PROTESTS NAVAL MAN  MOROZOV, GRACHEV ON NUCLE  MOROZOV CONSIDERS US PLAN  RUSSIA ON NORTH KOREAN NU  RUSSIA / DISMISSES / UKRA  KOZYREV / ON STRATEGIC WE  REACTIONS TO BELGRADE'S C  1       2       3      4      5       6 TotaS Number of Hits  ARMS                     1  POWER                 6  POV/ERS              p  STRENGTH          |  MISSILE                 |  MISSILES            P  PROLIFERATE        " PROLIFERATION "  WARFARE CONFLICT  CONFLICTS          ï  VIOLENCE            I  WAR                      ft  WEAPONS               |  Figure 8.9 Example of DCARS Query Histogram (from briefing by CALSPAN) Information Visualization  215  Figure 8.10 CityScape Example  A slightly different commercial version having properties similar to the systems above is the Document Content Analysis and Retrieval System (DCARS) being developed by Calspan Advanced Technology Center. Their system is designed to augment the RetrievalWare search product. They display the query results as a histogram with the items as rows and each term's contribution to the selection indicated by the width of a tile bar on the row (see Figure 8.9). DCARS provides a friendly user interface that indicates why a particular item was found, but it is much harder to use the information in determining how to modiiy search statements to improve them.  Another representation that is widely used for both hierarchical and network related information is the "cityscape" which uses the metaphor of movement within a city. In lieu of using hills, as in the terrain approach, skyscrapers represent the theme (concept) area as shown in Figure 8.10. This is similar to extending bar charts to three dimensions. Buildings can be connected by lines which can vary in representation to describe interrelationships between themes. Colors or fill designs can be used for the visualization presenting another layer of information (e.g., the building having the same color may be members of a higher concept). Movement within the cityscape (or terrain) of the viewer perspective allows zooming in on specific information areas that will bring into view additional structures that might have been hidden by the previous viewpoint. 216                                                                                               Chapter 8  An easily understood metaphor for users is that of a library. Information content can be represented as areas within a library that the user can navigate through. Once in a particular "information room" the user can view the virtual "books" available within that space as if they are sitting on a bookshelf. Once the book is accessed, the user can scan a group of related items with each item represented as a page within the book. The user can fan the pages out. This is exemplified by the WebBook (Card-96a).  Correlating items or words within items was described in detail in Chapter 6 to cluster items or create statistical thesauri. When the complete term relationship method is used, a very large matrix is created. Each cell in the matrix defines the similarity between two terms (or items). Meaningful display of the table is not possible in table form. Mitre Corporation has developed an interface that displays the complete matrix using clusters of dots to represent correlation's. Once the user zooms in on a particular area of correlation, the specific words become visible along with clusters showing their correlation to other words (Gershon-96). Anther approach to representing thesaurus and contents is being tested by Zizi and Pediotakis (Zizi-96). They build a thesaurus automatically from the abstracts of the items extracting both single and two-word expressions. They create a presentation view and a document view. They divide the display space, based upon thesaurus classes, into regions. Each area is sized proportionally to the importance of the class for the collection. Once the presentation view is defined, the document view is created. The documents are placed on ellipses corresponding to the presentation view, and the weight of the document is reflected by the radius of the ellipse.  Another task in information systems is the visualization of specific text within an item versus between items. In some situations, items are allowed to change over time via editing. Thus, there is both the static representation and a time varying representation. Text changing representations are very important when the text being represented is a software program of millions of lines of code. ATT Bell laboratories created the SeeSoft system which uses columns and color codes to show when different lines of code have been changed. This technique was used as a basis for a similar code visualization tool, DEC FUSE/SoftVis (Zaremba95). They created small pictures of files that represent the code in the file with the size of the picture scaled to the number of lines of code in the file. Color coding indicates different characteristics of the code (e.g., green is comments). A user can quickly see the relative structure of all of the code files composing a system along with the complexity of each of the modules. The TileBars tool from Xerox PARC provides the user with a visualization of the distribution of query terms within each item in a Hit file. Using this tool, the user can quickly locate the section of the item that is most likely to be of interest.  Although information retrieval focuses on the unstructured text, another aspect of informational items is the citation data and structured aspects of indexing items. This data structure can be manipulated via structured databases as well as traditional information systems. Visualization tools have also been constructed for databases.   The first visualization tool was the Query By Example user interface Information Visualization                                                                         217  developed by IBM (Zloof-75). The interface presented the user with twodimensional tables on the display screen and based upon the user is defining values of interest on the tables, the system would complete the search. Current visual query languages use visual representations of the database structure and contents (Catarci-96). The Information Visualization and Exploration Environment (IVEE) makes use of the three dimensional representation of the structured database as constrained by the user's search statement (Ahlberg-95). In one representation a three-dimensional box represents a larger space and smaller boxes within the space represent realizations of specific values (e.g., the box represents a department and the smaller boxes represent employees in the department). It has additional visualizations of data as maps and starfields. The user is provided with sliders and toggles to manipulate the search. Another specialized tool for displaying homes for sale in a database is the HomeFinder system. It presents a starfield display of homes for sale and overlays it with a city map showing the geographic location for each icon that represents a home (Ahlberg-94).  When hyperlinks are used as the information retrieval basis for locating relevant items, the user encounters orientation problems associated with the path the user followed to get to the current location. This is effect getting "lost in cyberspace." One solution is providing the user with a view of the information space. The user can user a pointing device to indicate the item the user would like to navigate to. MITRE Corporation has developed a tool used with web browsers that enables a user to see a tree structure visual representation of the information space they have navigated through (Gershon-96).  Another area in information visualization is the representation of pattern and linkage analysis. A system that incorporates many information visualization techniques including those used to represent linkage analysis is the Pathfinder Project sponsored by the Army (Rose-96). It contains the Document Browser, CAMEO, Counts, CrossField Matrix, OILSTOCK and SPIRE tools. The Document Browser uses different colors and their density for words in the text of items to indicate the relative importance of the item to their profile of interest. CAMEO models an analytic process by creating nodes and links to represent a problem. Queries are associated with the nodes. The color of the nodes change based on how well the found items satisfy the query. Counts uses statistical information on words and phrases and plots them over time. Time is used as a parameter to show trends in development of events. The display uses a threedimensional cityscape representation of the data. The Cross Field Matrix creates a two-dimensional matrix of two fields in a dataset. Selected values in each of the datasets will be in each row and column for the two fields. Colors are used to represent time span. For example, countries could be on one axis and products on the other axis. The colors would indicate how long the country has been producing a particular product. Intersection can be used to access all the items that supported the particular product in a particular country. OILSTOCK allows the placement of data on a geographic mapping tool. The relationship of data to maps is a different use of information visualization. Discussion of this area is left to the many sources on  Geographic  Information   Systems  (GIS).     The  SPIRE  tool   is  a  type of 218                                                                                                Chapter 8  scattergraph of information. Items are clustered and displayed in a star chart configuration. Distance between two points is indicative of their similarity based upon concurrence of terms.
issr-0113	8.4 Summary  Information visualization is not a new concept. The well known saying that ua picture is worth a thousand words" is part of our daily life. Everything from advertisements to briefings make use of visual aides to significantly increase the amount of information presented and provide maximum impact on the audience. The significant amount of "noise" (non-relevant items) in interactions with information systems requires use of user interface aides to maximize the information being presented to the user. Pure textual interfaces provide no capabilities for aggregation of data, allowing a user to see an overview of the results of a search. Viewing the results of a search using a hierarchical paradigm allows higher levels of abstraction showing overall results of searches before the details consume the display.  Visualization techniques attempt to represent aggregate information using a metaphor (e.g., peaks, valleys, cityscapes) to highlight the major concepts of the aggregation and relationships between them. This allows the user to put into perspective the total information before pursuing the details. It also allows major pruning of areas of non-relevant information. A close analogy is when searching on "fields" in the index at a library, the book shelves on horticulture would be ignored if magnetic fields was the information need. By having the data visualized constrained by the users search, the display is focused on the user's areas of interest. Relationships between data and effectiveness of the search become obvious to the user before the details of individual items hide the higher level relationships.  Cognitive engineering suggests that alternative representations are needed to take maximum advantage of different physilogical and cultural experiences of the user. Colors are useless to a color blind user. A person who grew up on a farm living in the country may have more trouble understanding a "city-scape" than a New York city resident. Using visual cues that a person has developed over his life-experience can facilitate the mapping of the visual metphor to the information it is representing.  As the algorthms and automatic search expansion techniques become more complex, use of visualization will take on additional responsibilities in clarifying to the user, not only what information is being retrieved, but the relationship between the search statement and the items. Showng relationships between items has had limited use in systems and been focused on data mining type efforts. The growth of hypertext linkages will require new visualization tools to present the network relationships between linked items and assist the user in navigating this new structure. Information Visualization                                                                          219  The technical limiter to the use of information visualization is no longer the understanding of useful visual techniques nor the ability of computers to display the techniques. The issue is the computational overhead in calculating the relationships between items based upon a dynamically created subset of an information space. To collect the information to display a "city-scape" display from the results of a search requires:  identifying the sbset of items that is relevant to the search statement applying a threshold to determine the subset to process for visualization  calculating the pairwise similarity between all of the indicated items and clustering the results  determining the theme or subject of the clusters  determining the strength of the relationships between the clusters  creating the information visualization for the results.  Not only does the user expect to see the aggregate level clustered, but the user expects to be able to expand upon any particular cluster and see the results also displayed using a visualization technique. Thus at each level, the precision of the clustering process increases. The user expects to interact with the system and see the results of the search in near real time processing (e.g., anthing more than 20 seconds is delayed response).  There are two major processing issues in developing the information visualization display. The first is in the third step of calculating the pairwise similarities of all of the items. The second major issue is in the increased precision expected as the user moves from the higher to lower levels of information visualization. This requires an additional level of precision that will likely need natural language processing to achieve. The indexes for items and the algorithms proposed to determine similarity between a query and the items may need adjustments to optimize their performance in locating and organizing the results from a search for the information visualization process.
issr-0114	9 Text Search Algorithms  9.1    Introduction to Text Search Techniques  9.2    Software Text Search Algorithms  9.3    Hardware Text Search Systems  Three classical text retrieval techniques have been defined for organizing items in a textual database, for rapidly identifying the relevant items and for eliminating items that do not satisfy the search. The techniques are full text scanning (streaming), word inversion and multiattribute retrieval (Faloutsos-85, Salton-83). In addition to using the indexes as a mechanism for searching text in information systems, streaming of text was frequently found in the systems as an additional search mechanism. In addition to completing a query, it is frequently used to highlight the search terms in the retrieved item prior to display. In the earlier history of information systems, where the hardware (CPU, memory and disk systems) were limiters in performance, specialized hardware text search systems were created. The were used to offload the search process from the main computer leaving the user interface, access and display. If there is a requirement for a system to be able to accurately search for the complete set of search terms, then streaming will be required as a final step. The need for hardware text search streamers has been declining with the increases in CPU power, disk access and memory. But there is still a market for it in the areas of genetic research and many existing legacy systems in use.
issr-0115	9.1 Introduction to Text Search Techniques  The basic concept of a text scanning system is the ability for one or more users to enter queries, and the text to be searched is accessed and compared to the query terms. When all of the text has been accessed, the query is complete. One advantage of this type architecture is that as soon as an item is identified as satisfying a query, the results can be presented to the user for retrieval. Figure 9.1 provides a diagram of a text streaming search system. The database contains the full text of the items. The term detector is the special hardware/software that 222  Chapter 9  contains all of the terms being searched for and in some systems the logic between the items. It will input the text and detect the existence of the search terms. It will output to the query resolver the detected terms to allow for final logical processing of a query against an item. The query resolver performs two functions. It will accept search statements from the users, extract the logic and search terms and pass the search terms to the detector. It also accepts results from the detector and determines which queries are satisfied by the             gt; 0 gt; 0 n   \ r  Term Detector Query Resolver  User In   r              Figure 9.1  Text Streaming Architecture  item and possibily the weight associated with hit. The Query Resolver will pass information to the user interface that will be continually updating search status to the user and on request retrieve any items that satisfy the user search statement. The process is focused on finding at least one or all occurrences of a pattern of text (query term) in a text stream. It is assumed that the same alhabet is used in both situations (although in foreign language streamers different encodings may have to be available for items from the same language such as in cryllic). The worst case search for a pattern of m characters in a string of n characters is at least n - m + 1 or a magnitude of O(ri) (Rivest-77). Some of the original brute force methods could require O(n*m) symbol comparisons (Sedgewick-88). More recent improvements have reduced the time to O(´ + m).  In the case of hardware search machines, multiple parallel search machines (term detectors) may work against the same data stream allowing for more queries or against different data streams reducing the time to access the complete database. In software systems, multiple detectors may execute at the same time.  There are two approaches to the data stream. In the first approach the complete database is being sent to the detector(s) functioning as a search of the database. In the second approach random retrieved items are being passed to the detectors. In this second case the idea is to perform an index search of the database and let the text streamer perform additional search logic that is not satisfied by the index search (Bird-78, Hollar-79). Examples of limits of index searches are: Text Search Algorithms                                                                            223  search for stop words  search for exact matches when steming is performed  search for terms that contain both leading and trailing "don't cares"  search for symbols that are on the interword symbol list (e.g., ",;)  The major disadvantage of basing the search on streaming the text is the dependency of the search on the slowest module in the computer (the I/O module). Inversions/indexes gain their speed by minimizing the amount of data to be retrieved and provide the best ratio between the total number of items delivered to the user versus the total number of items retrieved in response to a query. But unlike inversion systems that can require storage overheads of 50% to 300%, of the original databases (BIRD-78), the full text search function does not require any additional storage overhead. There is also the advantage where hits may be returned to the user as soon as found. Typically in an index system, the complete query must be processed before any hits are determined or available. Streaming systems also provide a very accurate estmate of current search status and time to complete the query. Inversions/indexes also encounter problems in fuzzy searches (m of n characters) and imbedded string query terms (i.e., leading and trailing "don't care", see Chapter 2). It is difficult to locate all the possibe index values short of searching the complete dictionary of possible terms. Most streaming algorithms will locate imbedded query terms and some algorithms and hardware search units will also perform fuzzy searches. Use of special hardware text serarch units insures a scalable environment where performance bottlenecks can be overcome by adding additional search units to work in parallel of of the data being streamed.  Many of the hardware and software text searchers use finite state automata as a basis for their algorithms. A finite state automata is a logical machine that is composed of five elements:  I - a set of input symbols from the aphabet supported by the automata S - a set of possible states  P - a set of productions that define the next state based upon the current state and  input symbol  So - a special state called the initial state SF - a set of one or more final states from the set S  A finite state automata is represented by a directed graph consisting of a series of nodes (states) and edges between nodes represented as transitions defined by the set of productions.   The symbol(s) associated with each edge defines the inputs that 224  Chapter 9  allow a transition from one node Si to another node Sj. Figure 9.2a shows a finite state automata that will identify the character string CPU in any input stream. The automata is defined by the the automata definition in Figure 9.2b  I = set of all alphabetic characters S = set {So, Si S2 S3) P = set{S0-ªS, if I = C  50 -gt; So if I * C S, -gt; S2 if I = P S,-gt;SoifI*{P, C}  51  -gt;Si if I = C S2-gt;S3ifI = U S2-gt;S! if I = C S2-ªSoin*{C, U}  }  So = { So } SF = { S3 }  Figure 9.2b Automata Definition /C, U  = C  Figure 9.2a Finite State Automata  The automata remains in the initial state until it has an input symbol of "C" which moves it to state Sj. It will remain in that state as long as it receives "C"s as input. If it receives a "P" it will move to S2. If it receives anything else it falls back to the initial state. Once in state S2 it will either go to the final state if "U" is the next symbol, go to Si if a "C" is received or go back to the initial state So if anything else is received.  It is possible to represent the productions by a table with the states as the rows and the input symbols that cause state transitions as each column. The states are representing the current state and the values in the table are the next state given the particular input symbol.
issr-0116	Text Search Algorithms                                                                           225  9.2         Software Text Search Algorithms  In software streaming techniques, the item to be searched is read into memory and then the algorithm is applied. Although nothing in the architecture described above prohibits software streaming from being applied to many simultaneous searches against the same item, it is more frequently used to resolve a particular search against a particular item. There are four major algorithms associated with software text search: the brute force approach, Knuth-MorrisPratt, Boyer-Moore, Shift-OR algorithm, and Rabin-Karp. Of all of the algrithms, Boyer-Moore has been the fastest requiring at most O(n + m) comparisons (Smit82). Knuth-Pratt-Morris and Boyer-Moore both require O(n) preprocessing of search strings (Knuth-77, Boyer-77, Rytter-80).  The Brute force approach is the simplest string matching algorithm. The idea is to try and match the search string against the input text. If as soon as a mismatch is detected in the cmparison process, shift the nput text one position and start the comparison process over. The expectednumber of comparisons when searching an input text string of n characters for a pattern of m characters is (Baeza-Yates-89):  Nc = -----(1-   ym)(n-m+\) + 0(1)  có 1          / c  where Nc is the expected number of comparisons and c is the size of the alphabet for the text.  The Knuth-Pratt-Morris algorithm made a major imprvement in previous algorithms in that even in the worst case it does not depend upon the length of the text pattern being searched for. The basic concept behind the algorithm is that whenever a mismatch is detected, the previousmatched characters define the number of characters that can be skipped in the input stream prior to starting the comparison process again. For example given:  Position                   12 3 4 5 6 7 8  Input Stream       = a b d ad   e f g Search Pattern     = a b d f  When the mismatch occurs in position 4 with a "f in the pattern and a "b" in the input stream, a brute force approach may shift just one position in the input text and restart the comparison. But since the first three positions of the pattern 226                                                                                                Chapter 9  matched (a b d), then shifting one position can not find an "a" because it has already been identified as a "b". The algorithm allows the comparison to jump at least the three positions associated with the recognized "a b d" . Since the mismatch on the position could be the beginning of the search string, four positions can not be skipped. To know the number of positions to jump based upon a mismatch in the search pattern, the search pattern is pre-processed to define a number of characters to be jumped for each position. The Shift Table that specifies the number of places to jump given a mismatch is shown in Figure 9.3. In the table it should be noted that the alignment is primarily based on aligning over the repeats of the letters "a" and "ab". Figure 9.4 provides an example application of the algorithm (Salton-89) where S is the search pattern and I is the input text stream.  Boyer-Moore recognized that the string algorithm could be significantly enhanced if the comparison process started at the end of the search pattern processing right to left versus the start of the search pattern. The advantage is that large jumps are possible when the mismatched character in the input stream does not exist in the search pattern which occurs frequently. This leads to two possible sources of determining how many input characters to be jumped. As in the KnuthMorris-Pratt technique any characters that have been matched in the search pattern will require an alignment with that substring. Additionally the character in the input stream that was mismatched also requires alignment with its next occurrence  Search Pattern = abcabcacab  Position in pattern pattern character length previous number of input    repeating substring characters to jump  1 a 0 1  2 b 0 1  3 c 0 2  4 a 0 4  5 b 1 5  6 c 2 6  7 a 3 3  8 c 4 3  9 a 0 8  10 b 1 8  Figure 9.3   Shift Characters Table Text Search Algorithms                                                                            227  p       1        2       3        4        5        6        7       8       9        10      11      12      13      14      15      16 Sabcabcacab  I  babcbabcabcaabca t  mismatch in position 1 shift one position  p       1       2       3       4       5       6       7       8       9        10      11      12      13      14      15      16  S    abcabcacab  1  babcbabcabcaabca  t  mismatch in position 5, no repeat pattern, skip 3 places  p       1        2       3       4        5        6        7       8        9        10      11       12      13      14      15      16  S                                  abcabcacab  I  babcbabcabcaabca  t mismatch in position 5, shift one position  p       1        2       3        4        5        6       7       8       9        10      11      12      13      14      15      16  S                                         abcabcacab  I  babcbabcabcaabca  t  mismatch in position 13, longest repeating pattern is "a b c a" thus skip 3  p       1        2        3        4        5       6       7       8       9        10      11      12      13      14      15      16  S                                                              abcabcab  I  babcbabcabcaabca  alignment after last shift  Figure 9.4 Example of Knuth-Morris-Pratt Algorithm  in the search pattern or the complete pattern can be moved. This can be defined as:  ALGOi - on a mismatch, the character in the input stream is compared to the search pattern to determine the shifting of the search pattern (number of characters in input stream to be skipped) to align the input character to a character in the search pattern.   If the character does not exist in the 228                                                                                                Chapter 9  search pattern then it is possible to shift the length of the search pattern matched to that position.  ALGO2 - on a mismatch occurs with previous matching on a substring in the input text, the matching process can jump to the repeating ocurrence in the pattern of the initially matched subpattern - thus aligning that portion of the search pattern that is in the input text.  Upon a mismatch, the comparison process can skip the MAXIMUM (ALGOi, ALGO2). Figure 9.5 gives an example of this process. In this example the search pattern is (a b d a a b) and the alphabet is (a, b, c, d, e, f) with m = 6 and c = 6.  Position 1 2 3 4 5 6 7 8 9 10 11 12 13  Input Stream f a b f a a b b d a b a b  Search Pattern  a b d a a b            t           a.    mismatch in position 4: ALGO] = 3, ALGO2 = 4, thus skip 4 places  Position 1 2 3 4 5 6 7 8 9 10 11 12 13  Input Stream f a b f a a b b d a b a b  Search Pattern      a b d a a b             t      b.   mismatch in position 9:   ALGOi = 1, ALGO2   =   4 thus skip four places  Position 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Input Stream f a b f a a b b d a b d a a b  Search Pattern          a b d a a b  c.   new aligned search continues with a match  Figure 9.5 Boyer-Moore Algorithm  The comparison starts at the right end of the search pattern and works towards the start of the search pattern.    In the first comparison (Figure 9.5 a.) the mismatch Text Search Algorithms                                                                            229  occurs in position 4 after matching on positions 7, 6, and 5.. ALGOi wants to align the next occurrence of the input text stream mismatch character "f* which does not exist in the search pattern thus allowing for a skip of three positions. ALGO2 recognizes that the mismatch occurred after 3 previous search pattern characters had matched. Based upon the pattern stream it knows that the subpattern consisting of the first three characters (a b) repeats in the first two positions of the search pattern. Thus given a mismatch in position 4, the search pattern can be moved four places to align the subpattern consisting of the first two characters (a b) over their known occurrence in positions 6, and 7 in the input text. In the next comparison (Figure 9.5 b.) there is a mismatch in position 9. The input character that mismatched is a "d" and the fewest positions to shift to align the next occurrence of a "d" in the search pattern over it is one position. The analysis for ALGO2 is the same as before. With the next jump of four positions, the two patterns will match.  The original Boyer-Moore algorithm has been the basis for additional text search techniques. It was originally designed to support scanning for a single search string. It was expanded to handle multiple search strings on a single pass (Kowalski-83). Enhanced and simplified versions of the Boyer-Moore algorithm have been developed by may researchers (Mollier-Nielsen-84, Iyengar-80, Commentz-Walter-79, Baeza-Yates-90, Galil-79, Horspol-80).  A different approach that has similarity to n-grams and signature files defined in Chapter 4 is to divide the text into /w-character substrings, calculate a hash function (signature) value for each of the strings (Harrison-71). A hash value is calculated for the search pattern and compared to that of the text. Karp and Rabin discovered an efficient signature function to calculate these values; h(k) = k mod q, where q is a large prime number (Karp-87). The signature value for each location in the text which is based upon the value calculated for the previous location. Hashing functions do not gauranttee uniqueness. Their algorithm wll find those positions in the text of an item that have the same hash value as the search pattern. But the actual text must then be compared to ensure there is a match. Detailed implementation of the Karp-Rabin algorithm is presented by Baeza-Yates (Baeza-Yates-92). In his comparison of all of the algorithms on a search of 1000 random patterns in random text, the Horspool simplification of the Boyer-Moore algorithm showed the best execution time for patterns of any length. The major drawback of the Boyer-Moore class of algorithms is the significant preprocessing time to set up the tables. Many of these algorithms are also implemented with hardware.  Another approach based upon Knuth-Pratt-Morris uses a finite state machine to process multiple query terms (Aho-75). The pattern matching machine consists of a set of states. The machine processes the input text by successively reading in the next symbol and based upon the current state, make the state transitions while indicating matches when they occur. The machines operation is based upon three functions; GOTO (i.e., state transition), a failure function and an output function. Figure 9.6 shows the functions for the set of words HE, SHE, HIS, 230  Chapter 9  and HER. The initial state is labeled state 0. The GOTO function is a directed graph where the letter(s) on the connecting line between states (circles) specify the transition for that input given the current state. For example in Figure 9.6, if the current state is 1 and a E or I are received, then the machine will go to steates 2 and 6 respectively. The absence of an arrow or current input character that is not on a line leading from the current nore represents a failure condition. When a failure occurs, the failure function maps a state into another state (it could be to itself) to continue the search process. Certain states are defined as output states. Whenever they are reached it means one or more query terms have been matched.  (a) GOTO Function  1 2 3 4 5 6 7 8 9  0 0 0 1 2 0 3 0 3  (b) Failure Function  state              2  OUTPUT       HE  5                   7  SHE, HE        HIS  HERS  (c) OUTPUT Function Figure 9.6 Tables for Aho-Corasick Algorithm  Thus if an H has been received and the system is in state 1. If the next input symbol is an E the system moves to state 2, if an I is received then it moves to state 6, if any other letter is received, it will be an error and Failure Function (the third column in 9.6(b)) specifies the system should move to state 0 and the same input character is applied to this state. Text Search Algorithms  231  The number of characters compared is the same for both the AhoCorasick and the KMP algorithms. In the new algorithm the number of state transitions required to process a string is independent of the number of search terms and the operation to perfom the search is linear with respect to the number of characters in the input stream. The order of magnitude of the number of characters compared is equal to w* O(T) where w is a constant greater than 1 and T is the number of characters in the input string. This is a major enhancement over both Knuth-Morris-Pratt which is proportional to the number of characters in the query and Boyer-Moore which can only handle one query term.  These concepts were expanded by Baeza-Yates and Gonnet and can handle "don't care" symbols and compliment symbols (Baeza-Yates-92a). The search also handles the cases of up to k mismatches. Their approach uses a vector of m differnet states, where m is the length of the search pattern and state i gives the state of the search between the positions 1, .. . , i of the pattern and positions (j - / + 1), . . . ,y of the text where y is the current position in the text. This n effect expands the process to act like it has m simultaneous comparators working. If sy is the set of states (1 lt; i lt; m) after reading the jth character of the text. It represents the number of characters that are different in the corresponding positions between pat\9 . . . , pah and textj.i+u - ï ï , textj where pat is the search pattern and text is the text being searched. If Sg =0 then there is a perfect match. Otherwise it provides a fiizzy search capability where the search term length and the found term length are the same and the value is for the number of mismatches. For example let the search pattern be ababc (m = 5) and a segment of input text to be cbbabababcaba then figure 9.7 Shows the value for Sy_i vector. For example  Vector value     Vector Position  a b  aba  aba  a b a b  ...c   bbababal  babe  a b c  b c  b c  c a b a ...       Input Text Stream ...  1 1  0 2  3 3  0 4  Figure 9.7 Vector for Position j - I  the vector value for vector position 3 is 0 because the three pat characters aba have no matches with the corresponding three characters bab from the input text stream.  When one position in the text is advanced, the new vector is shown in Figure 9.8. 232  Chapter 9  a  a b aba aba a b a b c   bbababa  Vector value      Vector Position  babe                          0                         1  a b c                             2                         2  be                                  0                         3  be                                   4                         4  1 b c a b a ...     Input Text Stream ...  Figure 9.8 Vector for Position 7  If T(x) is a table such that Tj(x) = 0 if x = patv, otherwise T,(x) =1.   Thus everywhere that the current vector value is zero (i.e., the apttern matches), the T(x) value will be zero. Eery other location will have a T(x) value of one. Thus for example 9.7 above the T5(x) will appear (1,0,1,0,1) and for Figure 9.8 it will be (0,1,0,1,1) which is called T(new) below. It is then possible to define:  s(ij) = s(i, j-I) + Ti(textj)  If s(0, j) = 0 then the following shows the effect of moving the one position from Figure 9.7 (call old) to Figure 8 (call new):  s(l, new) = s(0, old) + T^new) = 0 + 0 = 0 s(2, new) = s(l, old) + T2(new) =1 + 1=2 s(3, new) = s(2, old) + T3(new) = 0 + 0 = 0 s(4, new) = s(3, old) + T4(new) = 3+1=4 s(5, new) = s(4, old) + T5(new) = 0+1 = 1  Because of these operations, they called the algorithm the Shift-add algorithm. To extend the technique to allow for "don't care" symbols, compliments of a character o class (i.e., matches a character that does not belong to the class), or any finite set of symbols, three possibilities will exist for any position in the pattern:  a character from the alphabet a "don't care" character (*)  a compliment of a chracter or class of characters (C) Text Search Algorithms                                                                            233  Letting m' be the total of the number of elements in each class with * assigned a value of 1 and compliments not cosidered. Let m be the size of the pattern. The pattern:  [Pp]a[a"eiou]* a [p . . .tx ... z] values each class   2   1     5    11         5 + 3  has m = 6 and m' = 18. The Shift-add algorithm is extended by modifying the table T, such that, for each position every character in the class is processed. Thus if the alhabet equals (a, b, c, d) and the pattern is:  aZgt;[ab]b[abc]  with m =5 and m" = 8. If b=l (as for string interring), the entries for th table T are:  T(a)= 11000 T(b)= 10011 T(c)= 11101 T(d) = 01101  Baeza-Yates and Gonnet describe the details of the implementation of this algorithm in the referenced paper. One advantage to this algorithm is that it can easily be implemented in a hardware solution. The Shift-add algorithm is extended by Wu and Manber to handle insertions and deletions as well as positional mismatches (Wu-92).
issr-0117	9.3 Hardware Text Search Systems  Software text search is applicable to many circumstances but has encountered restrictions on the ability to handle many search terms simultaneously against the same text and limits due to I/O speeds. One approach that off loaded the resource intensive searching from the main processors was to have a specialized hardware machine to perform the searches and pass the results to the main computer which supported the user interface and retrieval of hits. Since the searcher is hardware based, scalability is achieved by increasing the number of hardware search devices. The only limit on speed is the time it takes to flow the text off of secondary storage (i.e., disk drives) to the searchers. By having one search machine per disk, the maximum time it takes to search a database of any size will be the time to search one disk. In some systems, the disks were formated to optimize the data flow off of the drives. Another major advantage of using a hardware text search unit is in the elimination of the index that represents the document database.   Typically the indexes are 70% the size of the actual items. 234  Chapter 9  Other advantages are that new items can be searched as soon as received by the system rather than waiting for the index to be created and the search speed is deterministic. Even though it may be slower than using an index, the predictability of how long it will take to stream the data provides the user with an exact search time. As hits as discovered they can immediately be made available to the user versus waiting for the total search to complete as in index searches.  Figure 9.1 represents hardware as well as software text search solutions. The algrithmetic part of the system is focused on the term detector. There has been three approaches to implementing term detectors: parallel comparators orassociative memory, a cellular structure, and a universal finite state automata (Hollar-79).  When the term cmparator is implemented with parallel comparators, each term in the query is assgned to an individual comparison element and input data are serially streamed into the detector. When a match occurs, the term comparator informs the external query resolver (usually in the main computer) by setting status flags. In some systems, some of the Boolean logic between terms is resolved in the term detector hardware (e.g., in the GESCAN machine). Instead of using specially designed comparators  Specialized hardware that interfaces with computers and is used to search secondary storage devices was developed from the early 1970s with the most recent product being the Parasel Searcher (previously the Fast Data Finder). The need for this hardware was driven by the limits in computer resources. The typical hardware configuration is shown in Figure 9.9 in the dashed box. The speed of search is then based on the speed of the I/O.  1        J gt; D gt; 0 n 1    !   \ 1 Term Detector  Query Resolver 1 User In   0 n !  S   ^ó ^ó ó\ ! ! Hardware Text Search Unit 1    Figure 9.9 Hardware Text Search Unit  One of the earliest hardware text string search units was the Rapid Search Machine developed by General Electric (Roberts-78). The machine consisted of a special purpose search unit where a single query was passed against a magnetic tape containing the documents.   A more sophisticated search unit was developed by  Operating Systems Inc. called the Associative File Processor (AFP) (Bird-77). It is capable of searching against multiple queries at the same time. Following that initial development, OSS, using a different approach, developed the High Speed Text Search (HSTS) machine. It uses an algorithm similar to the Aho-Corasick Text Search Algorithms  235  software finite state machine algorithm except that it runs three parallel state machines. One state machine is dedicated to contiguous word phrases (see chapter 2), another for imbedded term match and the final for exact word match. In parallel with that development effort, GE redesigned thier Rapid Search Machine into the GESCAN unit. TRW, based upon analysis of the HSTS, decided to develop their own text search unit. This became the Fast Data Finder which is now being marketed by Parasal. All of these machines were based upon state machines that input the text string and compared them to the query terms.  The GESCAN system uses a text array processor (TAP) that simultaneously matches many terms and conditions against a given text stream the TAP receives the query information from the users computer and directly access the textual data from secondary storage. The TAP consists of a large cache memory and an array of four to 128 query processors. The text is loaded into the cahche and searched by the query processors (Figure 9.10). Each query processor is independent and can be loaded at any time. A complete query is handled by each query processor. Queries support exact term matches, fixed length don't cares, variable length don't cares, terms may be restricted to specified zones, Boolean logic, and proximity.  A query processor works two operations in parallel; matching query terms to input text and boolean logic resolution. Term matching is performed by a series of character cells each containing one character of the query. A string of character cells is implemented on the same LSI chip and the chips can be connected in series for longer strings. When a word or phrase of the query is matched, a signal is sent  Query resolvers  Term matchers  Query processors  1  Q    Q-Q-Q P  P-P-P-----   -P  ---- -g  N  p-q-q---- -q  Figure 9.10 GESCAN Text Array Processor  to the resolution sub-process on the LSI chip. The resolution chip is responsible for resolving the Boolean logic between terms and proximity requirements. If the item satisfies the query, the information is transmitted to the users computer.  The 236                                                                                                Chapter 9  text array processor uses these chips in a matrix arrangement as shown in Figure 9.10. Each row of the matrix is a query processor in which the first chip performs the query resolution while the remaining chips match query terms.  The maximum number of characters in a query is restricted by the length of a row while the number of rows limit the number of simultaneous queries that can be processed.  Another approach for hardware searchers is to augment disc storage. The augmentation is a generalized associative search element placed between the read and write heads on the disk. The content addressable segment sequential memory (CASSM) system (Roberts-78) uses these search elements in parallel to obtain structured data from a database. The CASSM system was developed at the University of Florida as a general purpose search device (CopeIand-73). It can be used to perform string searching across the database. Another special search machine is the relational associative processor (RAP) developed at the University of Toronto (Schuster-79). Like CASSM performs search across a secondary storage device using a series of cells comparing data in parallel.  The Fast Data Finder (FDF) is the most recent specialized hardware text search unit still in use in many organizations. It was developed to search text and has been used to search English and foreign languages. The early Fast Data Finders consisted of an array of programmable text processing cells connected in series forming a pipeline hardware search processor (Mettler-93). The cells are implemented using a VSLI chip. In the TREC tests each chip contained 24 processor cells with a typical system containing 3600 cells (the FDF-3 has a rack mount configuration with 10,800 cells). Each cell will be a comparator for a single character limiting the total number of characters in a query to the nuber of cells. The cells are interconnected with an 8-bit data path and approximately 20-bit control path. The text to be searched passes through each cell in a pipeline fashion until the complete database has been searched. As data is analyzed at each cell, the 20 control lines states are modified dependning upon their current state and the results from the comparator. An example of a Fast Data Finder system is shown in Figure 9.11. A cell is composed of both a register cell (Rs) and a comparator (Cs). The input from the Document database is controlled and buffered by the microprocess/memory and feed through the comapators. The search characters are stored in the registers. The connection between the registers reflect the control lines that are also passing state information.  Groups of cells are used to detect query terms, along with logic between the terms, by appropriate programming of the control lines. When a pattern match is detected, a hit is passed to the internal microprocessor that passes it back to the host processor, allowing immediate access by the user to the Hit item. The functions supported by the Fast data Finder are:  Boolean Logic including negation Proximity on an arbitrary pattern Variable length "don't cares" Term counting and thresholds fuzzy matching Text Search Algorithms  237  term weights numeric ranges  The expense and requirement that the complete database be streamed to complete a search has discuraged general use of hardware text search units. Paracel, who now markets the Fast Data Finder, is modifying its application to the area of genetic analysis. Comparing sequence homology (linear sequence of genes as another chromosone) to known familys of proteins can provide insights about functions of newly sequenced genes. Parcel has combined the search capability of the FDF with their Biology Tool Kit (BTK). The major function that is applied is the fuzzy match capability that can be applied to chromosones. Searches can be applied to DNA against DNA, protein against protein, or DNA against protein  C1 -*      C2 -*      C3  J       R1      ó +*      R2      ó +gt;      R3     .  Cn  Rn  M icroprocessor/memory  i  5  CO  m  Host Workstation  Figure 9.11  Fast Data Finder Architecture  searches. The FDF is configured to implement linear Smith-Waterman (S-W) and sequence-profile algorithms. The Smith-Waterman dynamic programming algorithm is optimal for finding local sequence similarities. The General Profile algorithm allows search for regions of nucleic acids or proteins that have been conserved during evolution (Paracel-96).   The Fast Data Finder is loaded with a 238                                                                                               Chapter 9  sequence and will report back those sequences in the database whose local similarity score exceed a threshold that most closely resemble the query sequence. The BTK software then completes the analysis process in software.
issr-0118	9.4 Summary  Text search techniques using text scanning have played an important role in the development of Information Retrieval Systems. In the 1970s and 1980s they were essential tools for compensating for the insufficient computer power and for handling some of the more difficult search capabilities such as imbedded character strings and fuzzy searches. They currently play an important role in word processor systems (e.g., the Find function) and in Information Retrieval Systems for locating offensive terms (e.g., imbedded character strings) in the dictionary. The need for specialized hardware text search units to directly search the data on secondary storage has diminished with the growth of processing power of computers.
issr-0119	10   Multimedia Information Retrieval  10.1   Spoken Language Audio Retrieval  10.2  Non-Speech Audio Retrieval  10.3  Graph Retrieval  10.4  Imagery Retrieval  10.5  Video Retrieval  10.6  Summary  While the book up to this point has described techniques for indexing and retrieving text, increasing volumes of non-text artifacts such as graphics, imagery, audio (speech, music, sound), and video are available in personal collections, online services and the web. When indexing text, the elements used as the basis of indexing include characters, word stems, words, and phrases. However, when dealing with imagery, audio, or video, we must utilize techniques that process different elements. In audio, this might mean phonemes (or basic units of sound) and their properties (e.g., loudness, pitch), in imagery this might include principle components such as color, shape, texture, and location, and in video this might include camera position and movement in addition to imagery and audio elements.  Definition: Multimedia information retrieval is the process of satisfying a user's stated information need by identifying all relevant text, graphics, audio (speech and non-speech audio), imagery, or video documents or portions of documents from a document collection.  With approximately 10 million sites on the World Wide Web, increasingly users are demanding content-based access to materials. This is evident by the advent of  question answering services (e.g., www.ask.com) as well as the success of spoken 242                                                                                              Chapter 10  language                understanding                (e.g.,                www.sls.lcs.mit.edu/sls,  www.ibm.com/software/speech, www.speech.sri.com, www.nuance.com, cslu.cse.ogi.edu) and tools to support content-based access to speech (e.g., www.speech.cs.cmu.edu/speech). In addition, innovations are appearing in the areas of content-based access to non-speech audio (e.g., www.musclefish.com), imagery (www.qbic.almaden.ibm.com) and video (e.g., www.virage.com, www.broadcast.com, www.necn.com). A separated but related body of research addresses the use of multimedia and intelligent processing to enhance the human computer interface (Maybury and Wahlster 1998). Other research focuses on the automation of the extraction, transformation (to another media), and summarization of media content, however, here we focus primarily on retrieval.  In the remainder of this chapter we discuss retrieval of a range of classes of media including spoken language, non-speech audio, graphics, imagery, and video, the latter of which depends upon processing of the previous media types. We illustrate information access for each of these media types by describing specific systems in order to give the reader a concrete sense of the nature and capabilities of current systems in these areas.
issr-0120	10.1 Spoken Language Audio Retrieval  Just as a user may wish to search the archives of a large text collection, the ability to search the content of audio sources such as speeches, radio broadcasts, and conversations would be valuable for a range of applications. An assortment of techniques have been developed to support the automated recognition of speech (Waibel and Lee 1990). These have applicability for a range of application areas such as speaker verification, transcription, and command and control. For example, Jones et al. (1997) report a comparative evaluation of speech and text retrieval in the context of the Video Mail Retrieval (VMR) project. While speech transcription word error rates may be high (as much as 50% or more depending upon the source, speaker, dictation vs. conversation, environmental factors and so on), redundancy in the source material helps offset these error rates and still support effective retrieval. In Jones et al.'s speech/text comparative experiments, using standard information retrieval evaluation techniques, speaker-dependent techniques retain approximately 95% of the performance of retrieval of text transcripts, speaker independent techniques about 75%. However, system scalability remains a significant challenge. For example, whereas even the best speech recognition systems have on the order of 100,000 words in an electronic lexicon, text lexicons include upwards of 500,0000 vocabulary words. Another challenge is the need expend significant time and effort to develop an annotated video mail corpus to support machine learning and evaluation.  Some recent efforts have focused on the automated transcription of broadcast news. For example, Figure 10.1 illustrates BBN's Rough 'n 'Ready prototype that aims to provide information access to spoken language from audio and video sources (Kubala et al. 2000). Rough'n'Ready "creates a Rough summarization of speech that is Ready? for browsing."   Figure 10.1 illustrates a Multimedia Information Retrieval  243  January 31, 1998 sample from ABC's World News Tonight in which the left hand column indicates the speaker, the center column shows the translation with highlighted named entities (i.e., people, organizations, locations) and the rightmost column lists the topic of discussion. Rough'n'Ready's transcription is created by the BYBLOSô large vocabulary speech recognition system, a continuous-density Hidden Markov Model (HMM) system that has been competitively tested in annual formal evaluations for the past 12 years (Kubala, F. et al, 1997). BYBLOS runs at 3 times real-time, uses a 60,000 word dictionary, and most recently reported word error rates of 18.8% for the broadcast news transcription task.  ij   £fc    |lt;ft    Vfcw,  Nlp7*aol/RUMarCte*digital libraryhbn  World Sews Tonight 01/31/98  it's a strategy to pressure on council making deals and it's known each day in Southern California latest danger from hell.  From ABC news World headquarters in Mow York January thirty first nineteen ninety ... this is world news tonight Saturday here's  Elizabeth Vargas.  Good evening and defense secretary William Cohens said today that a military strike against a rock would be quota substantial in size and impact but Cohen stressed that the strike would not be able to remove Saddam Hussein from power or eliminate his deadly arsenal the defense secretary also had strong words today for the United Nations Security Council ABC's John Mcwethy reports.  With more american firepower being considered for the Persian; Guif defense secretary Cohen today issued by are the administration's toughest criticism of the UN security council without mentioning Russia or China buying named Cohen took dead aim at their reluctance to get tough with Sraq.  Frankly 1 find it... incredibly hard to accept the proposition but in the face oi Saddam's actions and that of members of the Security Council cannot bring themselves to to clear that this is a fundamental or material breach ... of old conduct on his part I think it challenges the credibility of Security Council.  Figure 10.1. BBN's Rough and Ready  Additional research in broadcast news processing is addressing multilingual information access. For example, Gauvain (2000) at LIMSI reports a North American broadcast news transcription system that performs with a 13.6% word error rate and reports spoken document retrieval performance using the SDR'98 TREC-7 data. Current work is investigating broadcast transcription of German and French broadcasts. Joint research between the Tokyo Institute of Technology and NHK broadcasting (Furui et al. 2000) is addressing transcription and topic extraction from Japanese broadcast news. The focus of Furui et al. is on improving processing by modeling filled pauses, performing on-line incremental speaker adaptation and by using a context dependent language model that models readings of words. The language model includes Chinese characters (Kanji) and two kinds of Japanese characters (Hira-gana and Kata-kana).  Foreign relations with the United States  Inspections  United Nations  Iraq  Politics and government
issr-0121	244  Chapter 10  10.2 Non-Speech Audio Retrieval  In addition to content-based access to speech audio, noise/sound retrieval is also important in such fields as music and movie/video production. Thorn Blum et al (1997) describe a user-extensible sound classification and retrieval system, called SoundFisher (www.musclefish.com), that draws from several disciplines, including signal processing, psychoacoustics, speech recognition, computer music, and multimedia databases. Just as image indexing algorithms use visual feature vectors to index and match images, Blum et al. use a vector of directly measurable acoustic features (e.g., duration, loudness, pitch, brightness) to index sounds. This enables users to search for sounds within specified feature ranges. For example, Figure 10.2a illustrates the analysis of male laughter on several dimensions including amplitude, brightness, bandwidth, and pitch. Figure 10.2b shows an enduser content-based retrieval application that enables a user to browse and/or query a sound database by acoustic (e.g., pitch, duration) and/or perceptual properties (e.g., "scratchy") and/or query by example. For example, SoundFisher supports such complex content queries as "Find all AIFF encoded files with animal or human vocal sounds that are similar to barking sounds without regard to duration or amplitude." The user can also perform a weighted query-by-value (e.g., foreground and transition with gt;.8 metallic and gt;.7 plucked aural properties and 2000 hz lt; average pitch lt; 300 hz and duration ...). The system can also be trained by example, so that perceptual properties (e.g., "scratchiness" or "buzziness") that are more indirectly related to acoustic features can be specified and retrieved.  ï l. \\  ¶\.-'.'l  OSO     0.30      I.CO     COO      130  Liufl iteiYuu-h MmI feb kd A \      MVMlid  Figure 10.2a. Analysis of Male Laugher. Figure 10.2b. Content based access to  audio. Multimedia Information Retrieval  245  Figure 10.2. Content-based Retrieval of Non-speech Audio  Performance of the SoundFisher system was evaluated using a database of 400 widely ranging sound files (e.g., captured from nature, animals, instruments, speech). Additional requirements identified by this research include the need for sound displays, sound synthesis (a kind of query formulation/refinement tool), sound separation, and matching of trajectories of features over time.
issr-0122	10.3 Graph Retrieval  Another important media class is graphics, to include tables and charts (e.g., column, bar, line, pie, scatter). Graphs are constructed from more primitive data elements such as points, lines, and labels. An innovative example of a graph retrieval system is Sagebook (Chuah, Roth, and Kerpedjiev 1997) created at Carnegie Mellon University (see www.cs.cmu.edu/Groups/sage/sage.html). SageBook, enables both search and customization of stored data graphics. Just as we may require an audio query during audio retrieval, Sagebook supports datagraphic query, representation (i.e., content description), indexing, search, and adaptation capabilities.  Figure 10.3 shows an example of a graphical query and the data-graphics returned for that query. As illustrated in the bottom left hand side of Figure 10.3, queries are formulated via a graphical direct-manipulation interface (called  v-,er FaU Picagfl_________|  ¶----SV     - V  ,- r  l|(l|,,,,lW.......,l,',´lW||M,n'., ,ml,!........Ñ......Ñ......,¶.ÑÑ´,,*ª, Ñ´,      Figure 10.3. SageBrush Query Interface and SageBook display of retrieved relevant  graphics 246                                                                                               Chapter 10  SageBrush) by selecting and arranging spaces (e.g., charts, tables), objects contained within those spaces (e.g., marks, bars), and object properties (e.g., color, size, shape, position). The right hand side of Figure 10.3 displays the relevant graphics retrieved by matching the underlying content and/or properties of the graphical query at the bottom left of Figure 10.3 with those of graphics stored in a library. Both exact matching and similarity based matching is performed on both graphical elements (or graphemes) as well as on the underlying data represented by the graphic. For example, in the query and responses in Figure 10.3, for two graphemes to match, they must be of the same class (i.e. bars, lines, marks) as well as use the same properties (i.e. color, shape, size, width) to encode data. The matches returned are sorted according to their degree of similarity to the query based on the match criteria. In Figure 10.3, all the data-graphics returned by a "close graphics matching strategy" (i.e., they are all of type "chart", have exactly one space in the graphic, and contain graphemes of type horizontal interval bar) are highlighted in the Figure.  In addition, retrieved data-graphics can be manually adapted. SageBook maintains an internal representation of the syntax and semantics of data-graphics, which includes spatial relationships between objects, relationships between datadomains (e.g., interval, 2D coordinate), and the various graphic and data attributes. Search is performed both on graphical and data properties, with three and four alternative search strategies, respectively, to enable varying degrees of match relaxation. Just as in large text and imagery collections, several data-graphic grouping techniques based on data and graphical properties were designed to enable clustering for browsing large collections. Finally, SageBook provides automatic adaptation techniques that can modify the retrieved graphic (e.g., eliminating graphical elements) that do not match the specified query.  The ability to retrieve graphics by content may enable new capabilities in a broad range of domains beyond business graphics. For example, graphics play a predominant role in domains such as cartography (terrain, elevation, features), architecture (blueprints), communications and networking (routers and links), systems engineering (components and connections) and military campaign planning (e.g., forces and defenses overlayed on maps). In each of these cases graphical elements, their properties, relations, and structure, can be analyzed for retrieval purposes.
issr-0123	10.4 Imagery Retrieval  Increasing volumes of imagery ó from web page images to personal collections from digital cameras ó have escalated the need for more effective and efficient imagery access. Researchers have identified needs for indexing and search of not only the metadata associated with the imagery (e.g., captions, annotations) but also retrieval directly on the content of the imagery. Initial algorithm development has focused on the automatic indexing of visual features of imagery (e.g., color, texture, shape) which can be used as a means for retrieving similar Multimedia Information Retrieval  247  images without the burden of manual indexing (Niblack and Jain, 1993, 1994, 1995). However, the ultimate objective is semantic based access to imagery.  Flicker et al.'s (1997) Query By Image Content (QBIC) system (www.qbic.almadenJbm.com, Seybold 1994) and its commercial version, Ultimedia Manager (www.ibm.com/software/data/umm/umm.html), exemplifies this imagery attribute indexing approach. QBIC supports access to imagery collections on the basis of visual properties such as color, shape, texture, and sketches (viewing from the Internet will show colors described in the text.). In their approach, query facilities for specifying color parameters, drawing desired shapes, or selecting textures replace the traditional keyword query found in text retrieval. For example, Figure 10.4a illustrates a query to a database of all US stamps prior to 1995 in which QBIC is asked to retrieve red images. The "red stamps" results are displayed in Figure 10.4b. If there are text captions associated with the imagery these of course can be exploited. For example, if we further refine this search by adding the keyword "president" we obtain the results shown in Figure 10.4c in which all stamps are both red in color and are related to "president". For the careful reader, the female stamp in the bottom right hand corner of Figure 10.4c is of Martha Washington from the presidential stamp collection.  QBIC  Stamps database courtesy of W  First select tpwemtopr by didnag´aw p´nªjng´i*to Hack cffamctaagSe of the desirec mat (ftaximtfigs of toe ´jS ´eo) M new ´gkms m pessted white, jwtMj Tbm sslact ft cete Ssxm toe este jacket to ptditi year kicUbs^b. Yam am ´tfa´fctci:ttgt;. the cote ªg´ttªlt;w Mtieryei-jfdessrsdRGBvTauftisit; She Uxt areas Repeattfaa far each desaowi cote and m pilt;± up to 5 cdat/pettenittgs pan  e it (ai%usl ths color percentage^. Select  rectange with  You r´pant etty selection, e  Figure 10.4a. QBIC Query by Color red 248  Chapter 10  Bt  £lt;St Vfew  So   frnrnrieatar  Figure 10.4b. Retrieved red stamps  Figure 10.4c. Red stamps of Presidents Multimedia Information Retrieval                                                              249  Using QBIC the user can also specify queries such as "find images with a coarsely textured, red round object and a green square". Because robust, domain independent object identification remains difficult and manual image annotation is tedious, the authors have developed automated and semiautomated object outlining tools (e.g., foreground/background models to extract objects) to facilitate database population.  More recently researchers have explored the application of content based imagery access to video retrieval. For example, Flicker et al. (1997) perform shot detection, extract a representative frame (r-frame, sometimes called keyframe) for each shot, and derive a layered representation of moving objects. This enables queries such as "find me all shots panning left to right" which yield a list of relevancy ranked r-frames (which acts as a thumbnail), selection of which retrieves the associated video shot.  Additional research in image processing has addressed specific kinds of content-based retrieval problems. Consider face processing, where we distinguish face detection (i.e., identifying a face or faces in a scene), face recognition (authenticating that a given face is of a particular person), and face retrieval (find the closest matching face in a repository given an example or some search criteria). For example, for the past few years the US Immigration and Naturalization Service has been using a face recognition system (www.faceit.com) to "watch" and approve registered "fast lane" drivers crossing the Otay Mesa port of entry at the US/Mexico border. Using a radiofrequency (RF) tag on the automobile, the system retrieves a picture of the driver registered with the automobile from a database, which is then matched to an image taken in real-time of the actual driver. If the verification is successful, the car is permitted to proceed without delay; if not, the vehicle is routed to an inspection station. Since FaceltÆ can find the head anywhere in the field of view of the camera, it works on any kind of vehicle (car, van, or sports utility). System performance can be assessed using measurements analogous to those used in text retrieval, such as precision and recall.  Researchers have also developed systems to track human movement (e.g., heads, hands, feet) and to differentiate human expressions (Pentland, 1997) such as a smile, surprise, anger, or disgust. This expression recognition is related to research in emotion recognition (Picard, 1997) in the context of human computer interaction. Face recognition is also important in video retrieval. For example, Wactlar et al's (2000) Informedia Digital Video Library system extracts information from audio and video and supports full content search over digitized video sources. Among other capabilities, Informedia provides a facility called named face which automatically associates a name with a face and enables the user to search for a face given a name and vice versa.
issr-0124	10.5 Video Retrieval  The ability to support content based access to video promises access to video mail (Jones et al., 1997), video taped meetings (Kubala et al. 1999), surveillance video, and broadcast television.  For example, Maybury, Merlino, and 250  Chapter 10  Morey (1997) report on the ability to create "personalcasts" from news broadcasts via the Broadcast News Navigator (BNN) system. BNN is a web-based tool that automatically captures, annotates, segments, summarizes and visualizes stones from broadcast news video. What QBIC is to static stock imagery, BNN is to broadcast news video. BNN integrates text, speech, and image processing technologies to perform multistream analysis of video to support content-based search and retrieval. BNN addresses the problem of time-consuming, manual video acquisition/annotation techniques that frequently result in inconsistent, error-full or incomplete video catalogues.  Figure 10.5 illustrates BNN's video query page. From this web page, the user can select to search among thirty national or local news sources, specify an absolute or relative date range, search closed captions or speech transcriptions, run a pre-specified profile, search on text keywords, or search on concepts that express topics or so-called named entities such as people, organizations, and locations. In Figure 10.5a the user has selected to search all news video sources for a 2 week period (27 February to 12 March, 2000) using free text as well as person and location tags. As displayed in Figure 10.5b, BNN automatically generates a custom query web page which includes menus of people and location names from content extracted over the relevant time period to ease query formulation by the user. In Figure 10.5b, the user has selected "George Bush" and "George W. Bush" from the people menu, "New York" and "New York City" from the location menu, and the key words "presidential primary". Because BNN incorporates the Alembic natural language                      information                       extraction                       system  (www.mitre.org/resources/centers/it/g063/nl-index.htmn the retrieved results include only stories that contain the person "Bush" as opposed to stories about brush or shrub and the state or city "New York".  Broadcast Neira r^vigatar Stoiy Sean* Tool  Ssltctthe type ofMew jauw^thajMsaodof time, and the types cf Ugsyou ars kttneUtAm, searching Figure 10.5a. Initial Query Page Multimedia Information Retrieval  251  Closed Caption Detailed Text Search Tool  Please enter or stdocttha words tint you ´rs mlertstad tn jwarcfoag cm.  ï  To*atortimi%Uit*in*iatSstonftrchoWd(Hmtiª´eMl^  ï  ToselecLimi^teiemamtEitoaiLMsctr^^holddowiilheCmtmimdkRywb^makit^yDWBe^^  Figure 10.5b. Detailed Content-based Query Figure 10.5. Broadcast News Navigator Query by Content  Figure 10.6 illustrates the kind of results BNN returns. Figures 6a and 6b display a "story skim" in which a keyframe together with the three most frequently occurring named entities are displayed for each relevant story. Figure 10.6a shows skims of those stories related to "Bush" whereas 6b shows "Gore" stories. When the user selects the first 5 March story (2nd row, far right) in Figure 10.6b, the display shown in Figure 10.6c is generated. In Figure 10.6c, when the user selects the closed caption button next to this story, the closed caption/transcribed text displays as shown detailing the number of delegates each candidate has won to date. In this manner the user can directly access dozens of news stations with a single query and can rapidly perform detailed and comparative analyses.  In addition to this direct access method, BNN also supports simple browsing of stories during particular time intervals or from particular sources. A useful facility in this regard is the ability to display a graph of named entity frequency over time. In addition, the user can automatically data mine the named entities in the analyzed stories using the "search for correlations" link shown on the left panel in Figure 3 0.6.  To evaluate the effectiveness of BNN, Merlino and Maybury (1999) report an empirical study of the optimal presentation of multimedia summaries of broadcast news with the belief that different mixes can improve user precision, recall, timeliness, and perceived quality of interaction. In that study, the authors empirically investigated the most effective presentations for both retrieval (i.e., finding relevant documents) and comprehension (i.e., answer a question) using twenty users who 252  Chapter 10  Figure 10.6a: "George Bush" Stories  Figure 10.6b. "Al Gore" Stories Multimedia Information Retrieval  253  Lei%:0a;0022  Lª SO^WHKRE JUU£ THE CAKKOATES IftHLYW THE tZUOCBXlXG PARTV HCOffittTJOH JJftOCESS!  {Real Video  [SiwdLar Stories        pnt^tStori^T  {geoebew. bush  £*, y´w go   Communicator K´!p  Leiigik:0O.-OOJ3  Stories           |lntemet  fBOSTQH  JNQRTH  Broadcast News Navigator  dossed Caption/Transcribed Text  Closed Caption/Transcribed Text  SO WHEREARETHECANDIDArES EABLYM THEDEMOCaiATIC PAK1Y HOMftfATTOHPROCESS7AL eOHELEADSBILt BRAdigital libraryEY 42 DEIK3ATES TO ^.ONTHEREPUBUCANSro^O^RGEW. BUSH GOES INTO SUPER nJESDAY WITH AHEAiTHY LEAD. HE HOLDS 208 DELEGATES. JOHK   ' McCAENF HAS 104. STEVEFORBES, WHO HAS DRDPPED OUT OF THE CONTEST. HAS TWO. AND ALAN KEYES HAS OKE. HOV7IMP0ETANT15 "SUPER                                                                                           ;  Figure 10.6c: BNN Story Detail Retrieval of "presidential primary" stories Figure 10.6: BNN Retrieval of "presidential primary" stories  were individually asked two sets of ten questions. The authors found that using BNN users could improve their retrieval performance by looking at only the 3 most frequent named entities (i.e., people, organizations, and locations) in the story (as in Figure 10.6b) rather than looking at the story details (as in Figure 10.6c). BNN enabled users to find video content about six times as fast as they could if searching with simple keywords. A key reason for this enhanced performance is BBN's automated segmentation of news programs into individual stories using cross media cues such as visual changes, speaker changes, and topic changes. Evaluation revealed the importance of quality source material and the need to enhance the selection of keyframes.  The topic detection and tracking initiative (TDT) for broadcast news and newswire sources (Wayne 1998) aims to explore algorithms that perform story segmentation (detection of story boundaries), topic tracking (detection of stories that discuss a topic, for each given target topic) and topic detection (detection of stories that discuss an arbitrary topic, for all topics). Whereas BNN focuses on story segmentation, subsequent research on the Geospatial News on Demand Environment (GeoNODE) addresses topic detection and tracking. GeoNODE (Hyland et al. 1999) presents news in a geospatial and temporal context. An analyst cam navigate the information space through indexed access into multiple types of information sources ´ from broadcast video, on-line newspapers, to specialist 254  Chapter 10  archives. GeoNODE incorporates information extraction, data mining/correlation and visualization components. Figure 10.7 illustrates the ability of GeoNODE to automatically nominate and animate topics from sources such as North American broadcast news and a Spanish web newspaper thereby directing analysis to the relevant documents having the right topic, the right time, and the right place. Figure  10.7  displays the GeoNODE time line of stories relevant to a given query across primary sources such as major CNN and MS-NBC programs. In contrast, Figure  10.8  cartographically encodes frequencies of documents that mention a location using color coded countries (higher color saturation of a country means more mentions) and yellow circles plotted on specific locations (the more documents that mention that location the larger the circle). For example, in the map in Figure 10.8 in the zoomed in portion on the left, North and South America are dark brown indicating high reporting frequency across sources whereas Africa has fewer reports and thus lighter color. Concentrations of larger yellow circles are situated in major cities in North and South America (e.g. note that large circles appear in most South American country capitals).    This enables the user to retrieve documents with specific geospatial mentions directly.   In preliminary evaluations with a corpus of 65,000 documents with 100 manually identified topics (covering 6941  of the documents), GeoNODE identified over 80% of the human defined topics and detected 83% of stories within topics with a misclassification error of .2%, results comparable to TDT initiative results.       .1..  i.i.          Ñ'___ ....      ' Ñ ._l,i Ñ_!,._ L-i IliiJ uj     .. -i.ei... ..      ,1 ...   ..........   ill..      .    ....     .....   L    ...    11,     Figure 10.7: GeoNODE Time-line  The ability to provide sophisticated analytic environments such as GeoNODE will in the future increasingly rely on an ability to extract information from a broad range of sources including text, audio, and video. Progress will depend upon the establishment of multimedia corpora, common evaluation tasks, and machine learning strategies to create high performance extraction and analysis systems. Multimedia Information Retrieval  255  jdj Emergency Hep o J Monterey ^ Cfly Names ^f Country Mamee  /j Named Locaiions  o 1-4 0 5-12  Figure 10.8: GeoNODE Map Display
issr-0125	10.6 Summary  This chapter addresses content-based access to multimedia. Multimedia retrieval encompasses a range of media types including text, speech and non-speech audio, and video. This chapter illustrates multiple systems that provide contentbased retrieval of media, some of which have become commercial software. This chapter summarizes, where results have been reported, empirical evaluation of these advances.  Impressive as these initial systems may be, they serve as pathfinders in new subfields of multimedia retrieval research. Many unanswered research questions remain which offer fertile ground for research. For example, imagery retrieval is presently based on shallow image feature analyses (e.g., color, shape, texture) and so integration of these results with those from deeper semantic analysis of text documents remains a challenge. Higher level intentional models of media (e.g., what is the purpose of a clip of non-speech audio or a graphic?) require even more sophisticated analytic methods but promise even deeper representations of media and correspondingly more powerful retrieval. In addition, enhanced interfaces promise to enhance retrieval performance and user satisfaction. Finally, techniques for rapidly creating multimedia corpora so scientists can create machine learned systems as well as develop metrics, measures, and methods for effective 256                                                                                              Chapter 10  community evaluations will remain important for fostering continued progress in the field.
issr-0126	11 Information System Evaluation  11.1   Introduction to Information System Evaluation  11.2  Measures Used in System Evaluations  11.3  Measurement Example -TREOResults  11.4  Summary  Interest in the evaluation techniques for Information Retrieval Systems has significantly increased with the commercial use of information retrieval technologies in the everyday life of the millions of users of the Internet. Until 1993 the evaluations were done primarily by academicians using a few small, well known corpora of test documents or even smaller test databases created within academia. The evaluations focused primarily on the effectiveness of search algorithms. The creation of the annual Text Retrieval Evaluation Conference (TREC) sponsored by the Defense Advanced Research Projects Agency (DARPA) and the National Institute of Standards and Technology (NIST) changed the standard process of evaluating information systems. Conferences have been held every year, starting from 1992, usually in the Fall months. The conference provides a standard database consisting of gigabytes of test data, search statements and the expected results from the searches to academic researchers and commercial companies for testing of their systems. This has placed a standard baseline into comparisons of algorithms. Although there is now a standard database, there is still debate on the accuracy and utility of the results from use of the test corpus. Section 11.2 introduces the measures that are available for evaluating information systems. The techniques are compared stressing their utility from an academic as well as a commercial perspective. Section 11.3 gives examples of results from major comparisons of information systems and algorithms.
issr-0127	11.1 Introduction to Information System Evaluation  In recent years the evaluation of Information Retrieval Systems and techniques for indexing, sorting, searching and retrieving information have become increasingly important (Saracevic-95).  This growth in interest is due to two major  reasons: the growing number of retrieval systems being used and additional focus on 258                                                                                               Chapter 11  evaluation methods themselves. The Internet is an example of an information space (infospace) whose text content is growing exponentially along with products to find information for value. Information retrieval technologies are the basis behind the search of information on the Internet. In parallel with the commercial interest, the introduction of a large standardized test database and a forum for yearly analysis via TREC has provided a methodology for evaluating the performance of algorithms and systems. There are many reasons to evaluate the effectiveness of an Information Retrieval System (Belkin-93, Callan-93):  To aid in the selection of a system to procure  To monitor and evaluate system effectiveness  To evaluate query generation process for improvements  To provide inputs to cost-benefit analysis of an information system  To determine the effects of changes made to an existing information  system.  From an academic perspective, measurements are focused on the specific effectiveness of a system and usually are applied to determining the effects of changing a system's algorithms or comparing algorithms among systems. From a commercial perspective, measurements are also focused on availability and reliability. In an operational system there is less concern over 55 per cent versus 65 per cent precision than 90 per cent versus 80 per cent availability. For academic purposes, controlled environments can be created that minimize errors in data. In operational systems, there is no control over the users and care must be taken to ensure the data collected are meaningful.  The most important evaluation metrics of information systems will always be biased by human subjectivity. This problem arises from the specific data collected to measure the user resources in locating relevant information. Metrics to accurately measure user resources expended in information retrieval are inherently inaccurate. A factor in most metrics in determining how well a system is working is the relevancy of items. Relevancy of an item, however, is not a binary evaluation, but a continuous function between an item's being exactly what is being looked for and its being totally unrelated. To discuss relevancy, it is necessary to define the context under which the concept is used. From a human judgment standpoint, relevancy can be considered:  t  Subjective                        - depends upon a specific user's judgment  Situational                        - relates to a user's requirements  Cognitive                         - depends on human perception and behavior  Temporal                         - changes over time  Measurable                      - observable at a points in time  The subjective nature of relevance judgments has been documented by Saracevic  and was shown in TREC-experiments (Harman-95, Saracevic-91). In TREC-2 and Information System Evaluation                                                                  259  TREC-3, two or three different users were given the same search statement and the same set of possible hits to judge as relevant or not. In general, there was a unanimous agreement on 70-80 per cent of the items judged by the human. Even in this environment (i.e., where the judges are not the creators of the query and are making every effort to be unbiased) there is still significant subjective disagreement on the relevancy of items. In a dynamic environment, each user has his own understanding of the requirement and the threshold on what is acceptable (see Chapter 1). Based upon his cognitive model of the information space and the problem, the user judges a particular item. Some users consider information they already know to be non-relevant to their information need. For example, a user being presented with an article that the user wrote does not provide "new" relevant information to answer the user's query, although the article may be very relevant to the search statement. Also the judgment of relevance can vary over time. Retrieving information on an "XT" class of PCs is not of significant relevance to personal computers in 1996, but would have been valuable in 1992. Thus, relevance judgment is measurable at a point in time constrained by the particular users and their thresholds on acceptability of information.  Another way of specifying relevance is from information, system and situational views. The information view is subjective in nature and pertains to human judgment of the conceptual relatedness between an item and the search. It involves the user's personal judgment of the relevancy (aboutness) of the item to the user's information need. When reference experts (librarians, researchers, subject specialists, indexers) assist the user, it is assumed they can reasonably predict whether certain information will satisfy the user's needs. Ingwersen categorizes the information view into four types of "aboutness" (Ingwersen-92):  Author Aboutness - determined by the author's language as matched by the system in natural language retrieval  Indexer Aboutness - determined by the indexer's transformation of the  author's natural language into a controlled vocabulary  Request Aboutness - determined by the user's or intermediary's processing of a seafth statement into a query  User Aboutness - determined by the indexer's attempt to represent the  document according to presupposition about what the user will want to knowIn this context, the system view relates to a match between query terms and terms within an item. It can be objectively observed, manipulated and tested without relying on human judgment because it uses metrics associated with the matching of the query to the item (Barry-94, Schamber-90).   The semantic relatedness between queries and items is assumed to be inherited via the index terms that represent the 260  Chapter 11  semantic content of the item in a consistent and accurate fashion.  Other aspects of the system view are presented in Section 11.2.  The situation view pertains to the relationship between information and the user's information problem situation. It assumes that only users can make valid judgments regarding the suitability of information to solve their information need. Lancaster and Warner refer to information and situation views as relevance and pertinence respectively (Lancaster-93). Pertinence can be defined as those items that satisfy the user's information need at the time of retrieval. The TRECevaluation process uses relevance versus pertinence as its criteria for judging items because pertinence is too variable to attempt to measure in meaningful items (i.e., it depends on each situation).
issr-0128	11.2 Measures Used in System Evaluations  To define the measures that can be used in evaluating Information Retrieval Systems, it is useful to define the major functions associated with identifying relevant items in an information system (see Figure 11.1). Items arrive  ITEMS ARE INDEXED INTO SYSTEM  QUERY HITS RETURNED  SYSTEM EXECUTES  QUERY AGAINST  INDEX  Search Statement  it File  USER CREATES  SEARCH STATEMENT  USER REVIEWS HITS FROM QUERY  Figure 11.1 Identifying Relevant Items  in the system and are automatically or manually transformed by "indexing" into searchable data structures. The user determines what his information need is and creates a search statement. The system processes the search statement, returning potential hits. The user selects those hits to review and accesses them.  Measurements can be made from two perspectives: user perspective and system perspective. The user perspective was described in Section 11.1. The Author's Aboutness occurs as part of the system executing the query against the Information System Evaluation                                                                   261  index. The Indexer Aboutness and User Aboutness occur when the items are indexed into items are indexed into the system. The Request Aboutness occurs when the user creates the search statement. The ambiguities in the definition of what is relevant occurs when the user is reviewing the hits from the query.  Typically, the system perspective is based upon aggregate functions, whereas the user takes a more personal view. If a user's PC is not connecting to the system, then, from that user's view the system is not operational. From the system operations perspective, one user not having access out of 100 users still results in a 99 per cent availability rate. Another example of how averaging distorts communications between the system and user perspective is the case where there are 150 students taking six courses. Assume there are 5 students in three of the courses and 45 students in the other three courses. From the system perspective there is an average of 25 students per instructor/course. For 10 per cent of the students (15 students) there is a great ratio of 10 students per instructor. But, 90 per cent of the users (students) have a ratio of 45 students to one instructor. Thus most of the users may complain of the poor ratio (45 to one) to a system person who claims it is really good (25 to one). Techniques for collecting measurements can also be objective or subjective. An objective measure is one that is well-defined and based upon numeric values derived from the system operation. A subjective measure can produce a number, but is based upon an individual users judgments.  Measurements with automatic indexing of items arriving at a system are derived from standard performance monitoring associated with any program in a computer (e.g., resources used such as memory and processing cycles) and time to process an item from arrival to availability to a search process. When manual indexing is required, the measures are then associated with the indexing process. The focus of the metrics is on the resources required to perform the indexing function since this is the major system overhead cost. The measure is usually defined in terms of time to index an item. The value is normalized by the exhaustivity and specificity (see Chapter 3) requirements. Ajiother measure in both the automatic and manual indexing process is the completeness and accuracy of the indexes created. These are evaluated by random sampling of indexes by quality assurance personnel.  A more complex area of measurements is associated with the search process. This is associated with a user creating a new search or modifying an existing query. In creating a search, an example of an objective measure is the time required to create the query, measured from when the user enters into a function allowing query input to when the query is complete. Completeness is defined as when the query is executed. Although of value, the possibilities for erroneous data (except in controlled environments) are so great that data of this nature are not collected in this area in operational systems. The erroneous data comes from the user performing other activities in the middle of creating the search such as going to get a cup of coffee.  Response time is a metric frequently collected to determine the efficiency of the search execution. Response time is defined as the time it takes to execute the search.  The ambiguity in response time originates from the possible definitions of 262                                                                                               Chapter 11  the end time. The beginning is always correlated to when the user tells the system to begin searching. The end time is affected by the difference between the user's view and a system view. From a user's perspective, a search could be considered complete when the first result is available for the user to review, especially if the system has new items available whenever a user needs to see the next item. From a system perspective, system resources are being used until the search has determined all hits. To ensure consistency, response time is usually associated with the completion of the search. This is one of the most important measurements in a production system. Determining how well a system is working answers the typical concern of a user: "the system is working slow today."  It is difficult to define objective measures on the process of a user selecting hits for review and reviewing them. The problems associated with search creation apply to this operation. Using time as a metric does not account for reading and cognitive skills of the user along with the user performing other activities during the review process. Data are usually gathered on the search creation and Hit file review process by subjective techniques, such as questionnaires to evaluate system effectiveness.  In addition to efficiency of the search process, the quality of the search results are also measured by precision and recall. Precision is a measure of the accuracy of the search process. It directly evaluates the correlation of the query to the database and indirectly is a measure of the completeness of the indexing algorithm. If the indexing algorithm tends to generalize by having a high threshold on the index term selection process or by using concept indexing, then precision is lower, no matter how accurate the similarity algorithm between query and index. Recall is a measure of the ability of the search to find all of the relevant items that are in the database. The following are the formulas for precision and recall:  Number _ Re trieved _ Re levant  Precision = ó7:----;------ó----;ór-----------:ó  Number _ Total_ Re trieved  Number   Re trieved  Re levant  Recall =  Number   Possible   Re levant  where Number JPossihle_Relevant is the number of relevant items in the database, Number_Retrieved_Relevant is the number of relevant items in the Hit file, and Number_TotalJ(etrieved is the total number of items in the Hit File. In controlled environments it is possible to get values for both of these measures and relate them to each other. Two of the values in the formulas, Number_Retrieved_Relevant and NumberJTotal^Retrieved, are always available. Number JPossible-Relevant poses a problem in uncontrolled environments because it suggests that all relevant items in the database are known. This was possible with very small databases in some of the early experiments in information systems. To gain the insights associated with testing a search against a large database makes collection of this data almost Information System Evaluation                                                                  263  impossible. Two approaches have been suggested. The first is to use a sampling technique across the database, performing relevance judgments on the returned items. This would form the basis for an estimate of the total relevant items in the database (Gilbert-79). The other technique is to apply different search strategies to the same database for the same query. An assumption is then made that all relevant items in the database will be found in the aggregate from all of the searches (Sparck Jones-75). This later technique is what is applied in the TREC-experiments. In this controlled environment it is possible to create Precision/Recall graphs by reviewing the Hit file in ranked order and recording the changes in precision and recall as each item is judged.  In an operational system it is unrealistic to calculate recall because there is no reasonable approach to determine Number_Possible_Relevant. It is possible, however, to calculate precision values associated with queries, assuming the user provides relevance judgments. There is a pragmatic modification that is required to the denominator factor of Number_TotalJRetrieved The user can not be forced to review all of the items in the Hit file. Thus, there is a likely possibility that there will be items found by the query that are not retrieved for review. The adjustment to account for this operational scenario is to redefine the denominator to Number_Total__Reviewed versus Nnumber_Total_Retrieved. Under this condition the Precision factor becomes the precision associated with satisfying the user's information need versus the precision of the query. If reviewing three relevant items satisfies the user's objective in the search, additional relevant items in a Hit file do not contribute to the objective of the information system. The other factor that needs to be accounted for is the user not reviewing items in the Hit file because the summary information in the status display is sufficient to judge the item is not likely to be relevant. Under this definition, precision is a more accurate measure of the use of the user's time.  Although precision and recall formed the initial basis for measuring the effectiveness of information systems, they encounter mathematical ambiguities and a lack of parallelism between their properties (Salton-83). In particular, what is the value of recall if there are no relevant items in the database or recall if no items are retrieved (Fairthorne-64, Robertson-69)? In both cases the mathematical formula becomes 0/0. The lack of parallelism comes from the intuitiveness that finding more relevant items should increase retrieval effectiveness measures and decrease with retrieval of non-relevant items. Recall is unaffected when non-relevant items are retrieved. Another measure that is directly related to retrieving non-relevant items can be used in defining how effective an information system is operating. This measure is called Fallout and defined as (Salton-83):  Number _ Retrieved _ Nonrelevant  Fallout = ó~-ó-------óó¶óó------------------- Number _ Total _ Nonre levant  where Number JTotal_Nonreievant is the total number of non-relevant items in the database. Fallout can be viewed as the inverse of recall and will never encounter the  situation of 0/0 unless all the items in the database are relevant to the search. It can 264                                                                                              Chapter 11  be viewed as the probability that a retrieved item is non-relevant. Recall can be viewed as the probability that a retrieved item is relevant. From a system perspective, the ideal system demonstrates maximum recall and minimum fallout. This combination implicitly has maximum precision. Of the three measures (precision, recall and fallout), fallout is least sensitive to the accuracy of the search process. The large value for the denominator requires significant changes in the number of retrieved items to affect the current value. Examples of precision, fallout and recall values for systems tested in TREC-4 are given in Section 11.3.  There are other measures of search capabilities that have been proposed. A new measure that provides additional insight in comparing systems or algorithms is the "Unique Relevance Recall" (URR) metric. URR is used to compare more two or more algorithms or systems. It measures the number of relevant items that are retrieved by one algorithm that are not retrieved by the others:  Number_ unique  relevant  Unique_Relevance_Recall =-------------------------=------------ Number _ relevant  Numberjunique_relevant is the number of relevant items retrieved that were not retrieved by other algorithms. When many algorithms are being compared, the definition of uniquely found items for a particular system can be modified, allowing a small number of other systems to also find the same item and still be considered unique. This is accomplished by defining a percentage (Pu) of the total number of systems that can find an item and still consider it unique. Number_relevant can take on two different values based upon the objective of the evaluation:  VALUE                                        INTERPRETATION  Total Number Retrieved               the total number of relevant items found by all  Relevant   (TNRR)                       algorithms  Total Unique Relevant                  the total number of unique items found by all  Retrieved (TURR)                       the algorithms  ABCDEFGHI          JKLM  3       4       2       22      1        100     200    22      100      10     500    6       15 Figure 11.2a Number Relevant Items information System Evaluation  265  Algorithm I  Algorithm II  Algorithm III  Figure 11.2b Four Algorithms With Overlap of Relevant Retrieved  Using TNRR as the denominator provides a measure for an algorithm of the percent of the total items that were found that are unique and found by that algorithm. It is a measure of the contribution of uniqueness to the total relevant items that the algorithm provides. Using the second measure, TURR, as the denominator, provides a measure of the percent of total unique items that could be found that are actually found by the algorithm. Figure 11.2a and 11.2b provide an example of the overlap of relevant items assuming there are four different algorithms. Figure 11.2a gives the number of items in each area of the overlap diagram in Figure 11.2b. If a relevant item is found by only one or two techniques as a "unique item," then from the diagram the following values URR values can be produced:  Algorithm I Algorithm II Algorithm III Algorithm IV  -  6 unique items (areas A, C, E)  -  16 unique items (areas B, C, J)  -  29 unique items (areas E, H, L)  -  31 unique items (areas J, L, M)  TURR =  Algorithm  Algorithm I Algorithm II  URRjnrr  URRturr  6/985 = .0061     6/61 =    .098 16/985 = .0162    16/61=   .262 266                                                                                              Chapter  Algorithm III                    29/985 = .0294    29/61 =   .475  AlgorithmlV                    31/985 = .0315    31/61 =.508  The URR value is used in conjunction with Precision, Recall and Fallout to determine the total effectiveness of an algorithm compared to other algorithms. The URRtnrr value indicates what portion of all unique items retrieved by all of the algorithms was retrieved by a specific algorithm. The URRturr value indicates the portion of possible unique items that a particular algorithm found. In the example, Algorithm IV found 50 per cent of all unique items found across all the algorithms. The results indicate that if I wanted to increase my recall by running two algorithms, I would choose algorithm III or IV in addition to the algorithm with the highest recall value. Like Precision, URR can be calculated since it is based upon the results of retrieval versus results based upon the complete database. It assists in determining the utility of using multiple search algorithm to improve overall system performance (see Chapter 7).  Other measures have been proposed for judging the results of searches (Keen-71,Salton-83):  Novelty Ratio: ratio of relevant and not known to the user to total relevant retrieved  Coverage Ratio: ratio of relevant items retrieved to total relevant by the user before the search  Sought Recall: ratio of the total relevant reviewed by the user after the  search to the total relevant the user would have liked to examine  In some systems, programs filter text streams, software categorizes data or intelligent agents alert users if important items are found. In these systems, the Information Retrieval System makes decisions without any human input and their decisions are binary in nature (an item is acted upon or ignored). These systems are called binary classification systems for which effectiveness measurements are created to determine how algorithms are working (Lewis-95). One measure is the utility measure that can be defined as (Cooper-73):  U = a*(Relevant_Retrieved) + p*(Non-Relevant_Not Retrieved) 8*(Non-Re!evant_Retrieved) - y*(Relevant_Not Retrieved)  where a and P are positive weighting factors the user places on retrieving relevant items and not retrieving non-relevant items while 5 and y are factors associated with the negative weight of not retrieving relevant items or retrieving non-relevant items.    This formula can be simplified to account only for retrieved items with p  and y equal to zero (Lewss-96). Another family of effectiveness measures called the E-measure that combines recall and precision into a single score was proposed by Van Rijsbergen (Rijsbergen-79).
issr-0129	Information System Evaluation                                                                  267  11.3 Measurement Example-TREC-ResuSts  Until the creation of the Text Retrieval Conferences (TREC) by the Defense Advance Research Projects Agency (DARPA) and the National Institute of Standards and Technology (NIST), experimentation in the area of information retrieval was constrained by the researcher's ability to manually create a test database. One of the first test databases was associated with the Cranfield I and II tests (Cleverdon-62, Cleverdon-66). It contained 1400 documents and 225 queries. It became one of the standard test sets and has been used by a large number of researchers. Other test collections have been created by Fox and Sparck Jones (Fox83, Sparck Jones-79). Although there has been some standard usage of the same test data, in those cases the evaluation techniques varied sufficiently so that it has been almost impossible to compare results and derive generalizations. This lack of a common base for experimentation constrained the ability of researchers to explain relationships between different experiments and thus did not provide a basis to determine system improvements (Sparck Jones-81). Even if there had been a better attempt at uniformity in use of the standard collections, all of the standard test sets suffered from a lack of size that prevented realistic measurements for operational environments.  The goal of the Text Retrieval Conference was to overcome these problems by making a very large, diverse test data set available to anyone interested in using it as a basis for their testing and to provide a yearly conference to share the results. There have been five TREC-conferences since 1992, usually held in the Fall. Two types of retrieval are examined at TREC: "adhoc" query, and "routing" (dissemination), In TREC-the normal two word Ltad hoc" is concatenated into a single word. As experience has been gained from TREC-1 to TREC-5, the details and focus of the experiments have evolved. TREC-provides a set of training documents and a set of test documents, each over 1 Gigabyte in size. It also provides a set of training search topics (along with relevance judgments from the database) and a set of test topics. The researchers send to the TREC-sponsor the list of the top 200 items in ranked order that satisfy the search statements. These lists are used in determining the items to be manually reviewed for relevance and for calculating the results from each system. The search topics are "user need" statements rather than specific queries. This allows maximum flexibility for each researcher to translate the search statement to a query appropriate for their system and assists in the determination of whether an item is relevant.  Figure 11.3 describes the sources and the number and size of items in the test database (Harrnan-95). Figure 11.3 also includes statistics on the number of terms in an item and number of unique terms in the test databases. The database was initially composed of disks I and 2. In later TRECs, disk 3 of data was added to focus on the routing tests. Figure 11.3b includes in the final column the statistics for the Cranfield test collection. Comparing the Cranfield collection to the contents of disk 1 shows that the TREC-test database is approximately 200 times larger and the 268                                                                                               Chapter 11  average length of the items is doubled. Also the dictionary size of unique words is 20 times larger. All of the documents are formatted in Standard Generalized Markup Language (SGML) with a Document Type Definition (DTD) included for each collection allowing easy parsing. SGML is a superset of HTML and is one of the major standards used by the publishing industry.  It was impossible to perform relevance judgments on all of the items in the test databases (over 700,000 items) to be used in recall and fallout formulas. The option of performing a random sample that would find the estimated 200 or more relevant items for each test search would require a very large sample size to be manually analyzed. Instead, the pooling method proposed by Sparck Jones was used. The top 200 documents based upon the relevance rank from each of the researchers was pooled, redundant items were eliminated and the resultant set was manually reviewed for relevance. In general one-third of the possible items retrieved were unique (e.g., out of 3300 items 1278 were unique in TREC-1) (Harman-93). This ratio also been shown to be true in other experiments (Katzer82). In TREC, each test topic was judged by one person across all of the possible documents to ensure consistency of relevance judgment.  The search Topics in the initial TREC-consisted of a Number, Domain (e.g., Science and Technology), Title, Description of what constituted a relevant item, Narrative natural language text for the search, and Concepts which were specific search terms.  The following describes the source contents of each of the disks shown in Figure 11.3 available for TREC analysis:  Diskl  WSJ - Wall street journal (1987, 1988, 1989)  AP    - AP Newswire (1989)  ZIFF - Articles from Computer Select disks (ZIFF-Davis Publishing)  FR    - Federal Register (1989)  DOE - Short Abstracts from DOE Publications  Disk 2  WSJ    - Wall Street Journal (1990, 1991, 1992)  AP      - AP Newswire (1988)  ZIFF   - Articles from Computer Select disks (ZIFF-Davis Publishing)  FR      - Federal register (1988)  Disk 3  SJM1M - San Jose Mercury News (1993) AP      - AP Newswire (1990)  ZIFF   - Articles from Computer Select disks (ZIFF-Davis Publishing) PAT   -U.S. Patents (1993) Information System Evaluation  269  Subset of collection WSJ (disks 12) AP ZIFF FR        (disks DOE Cranfield   SJMN (disk 3)   I2)  test      PAT (disk 3)  database  Size     of    Collection        (Mbytes)        (diskl) 270 259 245 ^62 186 1.5  (disk 2) 247 241 178 211    (disk 3) 290 242 349 245    Number of Records        (disk 1) 98,732 84,678 75,180 25,960 226,087 1400  (disk 2) 74,520 79,919 56,920 19,860    (disk 3) 90,257 78,321 161,021 6,711    Median Number        Terms per record        (diskl) 182 353 181 313 82 79  (disk 2) 218 346 167 315    (disk 3) 279 358 119 2896    Average Number        of Terms per record        (diskl) 329 375 412 1017 89 88  (disk 2) 377 370 394 1073    (disk 3) 337 379 263 3543    Total Number        of Unique Terms        (disk 1) 156,298 197,608 173,501 126,258  8226  Figure 11.3b TREC-Training and Adhoc Test Collection  Collection Source Size in MBytes Mean     Terms      pei record Median      Terms per record Total Records  ZIFF (disk 3) 249 263 119 161,021  FR (1994) 283 456 390 55,554  IR Digest 7 2,383 2,225 455  News Groups 237 340 235 102,598  Virtual Worlds 28 416 225 10,152  Figure 11.3a Routing Test Database (from TREC-5 Conference Proceedings to be published, Harmon-96)  Precision and recall were calculated in the initial TREC. To experiment with a measure called Relative Operating Characteristic (ROC) curves, calculation of Probability of Detection (same as Recall formula) and calculation of Probability of False Alarm (same as Fallout) was also tried. This use of a set of common evaluation formulas between systems allows for consistent comparison between different executions of the same algorithm and between different algorithms. The results are represented on Recall-Precision and Recall-Fallout graphs (ROC curves). Figure 11.4 shows how the two graphs appear. The x-axis plots the recall from zero to 1.0 based upon the assumption that the relevant items judged in the pooling technique account for all relevant items. The precision or fallout value at each of the discrete recall values is calculated based upon reviewing the items, in relevance 270  Chapter  rank score order, that it requires to reach that recall value. For example, assume there are 200 relevant items. A particular system, to achieve a recall of 40 per cent (.4) requiring retrieval of 80 of the relevant items, requires retrieving the top 160 items with the highest relevance scores. Associated with the Precision/Recall graph, for the x-axis value of .4, the y-axis value would be 80/160 or .5. There are sufficient sources of potential errors in generating the graphs, that they should only be used as relative comparisons between algorithms rather than absolute performance indicators. It has been proven they do provide useul comparative information.  In addition to the search measurements, other standard information on system performance such as system timing, storage, and specific descriptions on the tests are collected on each system. This data is useful because the TREC-objective is to support the migration of techniques developed in a research environment into operational systems.  TREC-5 was held in November 1996. The results from each conference have varied based upon understanding from previous conferences and new objectives. A general trend has been followed to make the tests in each TRECcloserto realistic operational uses of information systems (Harman-96).  0 .2 .4   .6   .8 1.0  Recall  0 .2 .4   .6   .8 1.0  Recall  Figure 11.4 Examples of TREC-Result Charts  TREC-1 (1992) was constrained by researchers trying to get their systems to work with the very large test databases. TREC-2 in August 1993 was the first real test of the algorithms which provided insights for the researchers into areas in which their systems needed work. The search statements (user need statements) were very large and complex. They reflect long-standing information needs versus adhoc requests. By TREC-3, the participants were experimenting with techniques for query expansion and the importance of constraining searches to passages within items versus the total item. There were trade offs available between manual and automatic query expansion and the benefits from combining results from multiple retrieval techniques. Some of the experiments were driven by the introduction of shorter and less complex search statements.  The "concept" field, which contained Information System Evaluation  271  terms related to the query that a user might be expected to be aware of, was eliminated from the search statements. This change was a major source for the interest into query expansion techniques. TREC-4 introduced significantly shorter queries (average reduction from 119 terms in TREC-3 to 16 terms in TREC-4) and introduced five new areas of testing called "tracks" (Harman-96). The queries were shortened by dropping the title and a narrative field, which provided additional description of a relevant item.  The multilingual track expanded TREC-4 to test a search in a Spanish test set of 200 Mbytes of articles from the "El Norte" newspaper. The interactive track modified the previous adhoc search testing from a batch to an interactive environment. Since there are no standardized tools for evaluating this environment, the TREC-5 goals included development of evaluation methodologies as well as investigating the search aspects. The database merging task investigated methods for merging results from multiple subcollections into a single Hit file. The confusion track dealt with corrupted data. Data of this type are found in Optical Character Reader (OCR) conversion of hardcopy to characters or speech input. The database for TREC-had random errors created in the text. Usually in real world situations, the errors in these systems tend not to be totally random but bursty or oriented towards particular characters. Finally, additional tests were performed on the routing (dissemination) function that focused on three different objectives: high precision, high recall and balanced precision and recall. Rather than ranking all items, a binary text classification system approach was pursued where each item is either accepted or rejected (Lewis-96, Lewis-95).  Adhoc Manual vs Automatic  0.00      0.10     0.20      0.30     0.40     0.50  Figure 11.5 TREC-1 Adhoc manual versus Automatic Query (from TREC-1 Conference Proceedings, page 15, Harmon-93) 272  Chapter 1  Routing Manual vs Automatic  0.00     0.10      0.20      0.30  0.40     0.50      0.60     0.70     0.80 Recall  0.90      1.00  Figure 11.6 TREC-1 Routing Manual versus Automatic Results (from TREC-1 Conference Proceedings, page 18, Harmon-93)  Insights into the advancements in information retrieval can be gained by looking at changes in results between TRECs mitigated by the changes in the test search statements. Adhoc query results from TREC-1 were calculated for automatic and manual query construction. Automatic query construction is based upon automatic generation of the query from the Topic fields. Manual construction is also generated from the Topic field manually with some machine assistance if desired. Figures 11.5 shows the Precision/Recall results top two systems for each hmethod. The precision values were very low compared to later TRECs. It also shows that there was very little difference between manual construction of a query and automatic construction.  Routing (dissemination) also allowed for both an automatic and a manual query construction process. The generation of the query followed the same guidelines as the generation of the queries for the adhoc process. Figure 11.6 shows the results from the top two manual and automatic routing systems. In this case, unlike the adhoc query process, the automatic query building process is better as shown by the results from the "fund" system.  By TREC-3 and TREC-4 the systems were focusing on how to accommodate the shorter queries. It is clear that if the shorter queries had been executed for TREC-1, the results would have been worse than those described. Figures 11.7 and 11.8 show the precision recall results for Automatic and Manual adhoc searches for TREC-3 and TREC-4 (Harman-96). The significant reduction in query size caused even the best algorithms shown in the figures to perform worse in TREC-4 than in TREC-3. The systems that historically perform best at TRECs (e.g., City University, London cityal, 1NQUERY - INQ201, Cornell University Information System Evaluation  273  a o   TREC-3 vs TREC-4 Automatic  0.8 ^    0.6    0.4    0.2    0.0    ó´ó cityal (TREC-3) -¶^ó INQ101 (TREC-3) ~-~=ió CmlEA (TREC-3) ó-#ó pircsl     (TREC-3)  óaó CmlAE (TREC^) ó´ó pircsl (TREC-4) ómó cityal (TREC-4) ó*ó INQ201 (TREC-4)  0.0  0.2  0.4                0.6  Recall  0.8  1.0  Figure 11.7 Automatic AdHoc Query results from TREC-3 and TREC-4 (from TREC-5 Conference Proceedings to be published, Harmon-96)  TREC-3 vs TREC-4 Manual  *¶-¶ INQ102     (TREC-3)  *-ï Brkly7      (TREC-3)  *ïó ASSCTV1 (TREC-3)  ï?.....pircs2        (TEIEC-3)  *ó CnQst2      (TREC-4)  ïó INQ202     (TREC-4)  ?ó BrklylO     (TREC-4)  ?ó pircs2        (TREC-4)  Recall  Figure 11.8 Manual AdHoc Query results fromTREC3 and TREC4  (from TREC-5 Conference Proceedings to be published, Harmon-96) 274                                                                                              Chapter 11  CrnlEA) all experienced 14 per cent to 36 per cent drops in retrieval performance. The manual experiments also suffered from a similar significant decrease in performance. The following is the legend to the Figures:  Crnl EA - Cornell University - SMART system - vector based weighted system pircsl     - Queens College - PIRCS system - spreading activation on 550  word subdocuments from documents  cityal     - City University, London - Okapi system - probabilistic term weighting INQ201 - University of Massachusetts - INQUERY system - probabilistic  weighting using inference netscitri2            -   RMIT, Australia  standard cosine with OKAPI measure  CnQst2   - Excalibur Corporation - Retrievalware - two pass weights brkly-10 - University of California, Berkley - logistic regression model based on  6 measures of document relevance ASSCTV1 - Mead Data Central, Inc. - query expansion via thesaurus  Even though all systems experienced significant problems when the size of the queries was reduced, a comparison between Figure 11.5 and Figures 11.7 and 11.8 shows a significant improvement in the Precision/Recall capabilities of the systems. A significant portion of this improvement occurred between TREC-1 and TREC-2.  By participating on a yearly basis, systems can determine the effects of changes they make and compare them with how other approaches are doing. Many of the systems change their weighting and similarity measures between TRECs. INQUERY determined they needed better weighting formulas for long documents so they used the City University algorithms for longer items and their own version of a probabilistic weighting scheme for shorter items. Another example of the learning from previous TRECs is the Cornell "SMART" system that made major modifications to their cosine weighting formula introducing a non-cosine length normalization technique that performs well for all lengths of documents. They also changed their expansion of a query by using the top 20 highest ranked items from a first pass to generate additional query terms for a second pass. They used 50 terms in TREC-4 versus the 300 terms used in TREC-3. These changes produced significant improvements and made their technique the best in the Automatic Adhoc for TREC-4 versus being lower in TREC-3.  In the manual query method, most systems used the same search algorithms. The difference was in how they manually generated the query. The major techniques are the automatic generation of a query that is edited, total manual generation of the query using reference information (e.g., online dictionary or thesaurus) and a more complex interaction using both automatic generation and manual expansion.  When TREC-introduced the more realistic short search statements, the value of previously discovered techniques had to be reevaluated. Passage retrieval (limiting the similarity measurement to a logical subsets of the item) had a major impact in TREC-3 but minimal utility in TREC-4. Also more systems began making use of    multiple algorithms and selecting the best combination  based upon Information System Evaluation  275  characteristics of the items being searched. A lot more effort was spent on testing better ways of expanding queries (due to their short length) while limiting the expanded terms to reduce impacts on precision. The automatic techniques showed a consistent degradation from TREC-3 to TREC-4. For the Manual Adhoc results, starting at about a level of .6, there was minimal difference between the TRECs. The Routing systems are very similar to the Adhoc systems. The   TREC-3 vs TREC-4 Routing Comparison  0.8     0.6     0.4     0.2 -0.0  i   ïï¶#-   INQI03 (TREC-3)  óª.....cityrl    (TREC-3)  --*.....pircs3    (TREC-3)  -~v~- CmlRR (TREC-3) ó-nó nyuir2    (TREC-3)  ó¶ó INQ203 (TREC-4) ï cityr2 (TREC-4) ó*ó pircsC (TREC-4) -*ó CmIRE (TREC-4) óió nyuge2 (TREC-4)  0.0  0.2  0.4  0.6  0.8  1.0  Recall  Figure 11.9 Routing results from TREC-3 and TREC-4 (from TREC-5 Conference Proceedings to be published, Harmon-96)  researchers tended to use the same algorithms with minor modifications to adjust for the lack of a permanent database in dissemination systems. Not surprisingly, the same systems that do well in the Adhoc tests do well in the Routing tests. There was significant diversity on how the search statements were expanded (see TREC-4 proceedings). Unlike the Adhoc results, the comparison of TREC-3 and TREC-4 Routing shown in Figure 11.9 has minimal changes with a slight increase in precision. The following is the legend for the Routing comparison for systems not defined in the adhoc legend:  nyuge2  - GE Corporate Research and New York University - use of natural language processing to identify syntactic phrases  nyuir2    - New York University - use of natural language processing to identify syntactic phrases 276  Chapter 11  cityr      - City University, London  As with the adhoc results, comparing Figure 11.6 with Figure 11.8 shows the significant improvement in Routing capability between TREC-1 and the later  Spanish TREC-4  0.2  0.0  ó  UCFSP1  ó  SIN010 ..... xerox-sp2  CralSE  ó  gmuauto ......BrklySP3  cilri-sp2 DCUSPO ACQSPA cmlmlO  0.0  Figure 11.10 Results of TREC-4 Spanish Track (from TREC-5 Conference Proceedings to be published, Harmon-96)  TRECs. TREC-5 results were very close to those from TREC-4 but the queries had become more difficult so actual improvements came from not seeing a degradation in the Precision/Recall and Routing graphs.  The multilingual track expanded between TREC-4 and TREC-5 by the introduction of Chinese in addition to the previous Spanish tests. The concept in TREC-5 is that the algorithms being developed should be language independent (with the exception of stemming and stopwords). In TREC-4, the researchers who spent extra time in linguistic work in a foreign language showed better results (e.g., INQUERY enhanced their noun-phrase identifier in their statistical thesaurus generator). The best results in came from the University of Central Florida, which built an extensive synonym list. Figure 11.10 shows the results of the Spanish adhoc search in TREC-4. In TREC-5 significant improvements in precision were made in the systems participating from TREC-4. In Spanish, the Precision-Recall charts are better than those for the Adhoc tests, but the search statements were not as constrained as in the ad hoc. In Chinese, the results varied significantly between the participants with some results worse than the adhoc and some better. This being the Information System Evaluation                                                                   277  first time for Chinese, it is too early to judge the overall types of performance to be expected. But for Spanish, the results indicate the applicability to the developed algorithms to other languages. Experiments with Chinese demonstrates the applicability to a language based upon pictographs that represent words versus an alphabet based language.  The confusion track was preliminary in TREC-4. By TREC-5 the test database had expanded by taking the Federal Register (250Mbytes), creating dvi image files and then running NIST OCR programs against it. This produced approximately 5 per cent corruption typical of OCR operations. A second dataset with closer to 20 per cent corruption was produced by down-sampling the images and redoing the OCR (Voorhees-96). A set of known item topics was created by selecting items that seemed to be unique and creating a description of them. These were used and the evaluation metric was the rank of the item in the Hit file. Most of the search systems used some version of n-gram indexing (see Chapter 4). The results are too preliminary to draw any major conclusions from them.  The results in TREC 8, held in November 1999 did not show any significant improvement over the best TREC 3 or TREC four results in this text for automatic searching. The manual searching did show some improvement because the user interaction techniques are improving with experience. One participant, Readware, did perform significantly better than the other participants. The major change with TREC 8 was the introduction of the Question/Answer track. The goal of the track is to encourage research into systems that return answers versus lists of documents. The user is looking for an answer to an information need and does not want to have to browse through long items to locate the specific information of interest.  The experiment was run based upon 200 fact based short answer questions. The participants returned a ranked list of up to five document-id/string location pairs for each query. The strings were limited to either 50 or 250 characters. The answers were judged based upon the proposed string including units if asked for (e.g., world's population) and for famous objects answers had to pertain to that specific object.  Most researchers processed the request using their normal search algorithms, but included "blind feedback" to increase the precision of the higher ranked hits. Then techniques were used to parse the returned document around the words that caused the hit using natural language techniques to focus on the likely strings to be returned. Most of the participants only tried to return the 250-character string range.  The TREC-series of conferences have achieved their goal of defining a standard test forum for evaluating information retrieval search techniques. It provides a realistic environment with known results. It has been evolving to equate closer to a real world operational environment that allows transition of the test results to inclusion of commercial products with known benefits. By being an open forum, it has encouraged participation by most of the major organizations developing algorithms for information retrieval search.
issr-0130	278                                                                                               Chapter 1:  11.4 Summary  Evaluation of Information Retrieval Systems is essential to understand the source of weaknesses in existing systems and trade offs between using different algorithms. The standard measures of Precision, Recall, and Fallout have been used for the last twenty-five years as the major measures of algorithmic effectiveness. With the insertion of information retrieval technologies into the commercial market and ever growing use on the Internet, other measures will be needed for real time monitoring the operations of systems. One example was given in the modifications to the definition of Precision when a user ends his retrieval activity as soon as sufficient information is found to satisfy the reason for the search.  The measures to date are optimal from a system perspective, and very useful in evaluating the effect of changes to search algorithms. What are missing are the evaluation metrics that consider the total information retrieval system, attempting to estimate the system's support for satisfying a search versus how well an algorithm performs. This would require additional estimates of the effectiveness of techniques to generate queries and techniques to review the results of searches. Being able to take a system perspective may change the evaluation for a particular aspect of the system. For example, assume information visualization techniques are needed to improve the user's effectiveness in locating needed information. Two levels of search algorithms, one optimized for concept clustering the other optimized for precision, may be more effective than a single algorithm optimized against a standard Precision/Recall measure.  In all cases, evaluation of Information Retrieval Systems will suffer from the subjective nature of information. There is no deterministic methodology for understanding what is relevant to a user's search. The problems with information discussed in Chapter 1 directly affect system evaluation techniques in Chapter 11. Users have trouble in translating their mental perception of information being sought into the written language of a search statement. When facts are needed, users are able to provide a specific relevance judgment on an item. But when general information is needed, relevancy goes from a classification process to a continuous function. The current evaluation metrics require a classification of items into relevant or non-relevant. When forced to make this decision, users have a different threshold. These leads to the suggestion that the existing evaluation formulas could benefit from extension to accommodate a spectrum of values for relevancy of an item versus a binary classification. But the innate issue of the subjective nature of relevant judgments will still exist, just at a different level.  Research on information retrieval suffered for many years from a lack of a large, meaningful test corpora. The Text REtrieval Conferences (TRECs), sponsored on a yearly basis, provide a source of a large "ground truth" database of documents, search statements and expected results from searches essential to evaluate algorithms. It also provides a yearly forum where developers of algorithms can share their techniques with their peers.  More recently,  developers are starting Information System Evaluation  279  to combine the best parts of their algorithms with other developers algorithms to produce an improved system.
mir-0002	1.1    Motivation Information retrieval (IR) deals with the representation, storage, organization of, and access to information items. The representation and organization of the information items should provide the user with easy access to the information in which he is interested. Unfortunately, characterization of the user information need is not a simple problem. Consider, for instance, the following hypothetical user information need in the context of the World Wide Web (or just the Web): Find all the pages (documents) containing information on college tennis teams which: (1) are maintained by an university in the USA and (2) participate in the NCAA tennis tournament. To be relevant, the page must include information on the national ranking of the team in the last three years and the email or phone number of the team coach. Clearly, this full description of the user information need cannot be used directly to request information using the current interfaces of Web search engines. Instead, the user must first translate this information need into a query which can be processed by the search engine (or IR system). In its most common form, this translation yields a set of keywords (or index terms) which summarizes the description of the user information need. Given the user query, the key goal of an IR system is to retrieve information which might be useful or relevant to the user. The emphasis is on the retrieval of information as opposed to the retrieval of data.
mir-0003	1.1.1    Information versus Data Retrieval Data retrieval, in the context of an IR system, consists mainly of determining which documents of a collection contain the keywords in the user query which, most frequently, is not enough to satisfy the user information need. In fact, the user of an IR system is concerned more with retrieving ¶information about a 2        INTRODUCTION subject than with retrieving data which satisfies a given query. A data retrieval language aims at retrieving all objects which satisfy clearly defined conditions such as those in a regular expression or in a relational algebra expression. Thus, for a data retrieval system, a single erroneous object among a thousand retrieved objects means total failure. For an information retrieval system, however, the retrieved objects might be inaccurate and small errors are likely to go unnoticed. The main reason for this difference is that information retrieval usually deals with natural language text which is not always well structured and could be semantically ambiguous. On the other hand, a data retrieval system (such as a relational database) deals with data that has a well defined structure and semantics. Data retrieval, while providing a solution to the user of a database system, does not solve the problem of retrieving information about a subject or topic. To be effective in its attempt to satisfy the user information need, the IR system must somehow interpret' the contents of the information items (documents) in a collection and rank them according to a degree of relevance to the user query. This 'interpretation' of a document content involves extracting syntactic and semantic information from the document text and using this information to match the user information need. The difficulty is not only knowing how to extract this information but also knowing how to use it to decide relevance. Thus, the notion of relevance is at the center of information retrieval. In fact, the primary goal of an IR system is to retrieve all the documents which are relevant to a user query while retrieving as few non-relevant documents as possible.
mir-0004	1.1.2    Information Retrieval at the Center of the Stage In the past 20 years, the area of information retrieval has grown well beyond its primary goals of indexing text and searching for useful documents in a collection. Nowadays, research in IR includes modeling, document classification and categorization, systems architecture, user interfaces, data visualization, filtering, languages, etc.   Despite its maturity, until recently, IR was seen as a narrow area of interest mainly to librarians and information experts. Such a tendentious vision prevailed for many years, despite the rapid dissemination, among users of modern personal computers, of IR tools for multimedia and hypertext applications. In the beginning of the 1990s, a single fact changed once and for all these perceptions ó the introduction of the World Wide Web. The Web is becoming a universal repository of human knowledge and culture which has allowed unprecedent sharing of ideas and information in a scale never seen before. Its success is based on the conception of a standard user interface which is always the same no matter what computational environment is used to run the interface. As a result, the user is shielded from details of communication protocols, machine location, and operating systems. Further, any user can create his own Web documents and make them point to any other Web documents without restrictions. This is a key aspect because it turns the Web into a new publishing medium accessible to everybody. As an immediate BASIC CONCEPTS        3 consequence, any Web user can push his personal agenda with little effort and almost at no cost. This universe without frontiers has attracted tremendous attention from millions of people everywhere since the very beginning. Furthermore, it is causing a revolution in the way people use computers and perform their daily tasks. For instance, home shopping and home banking are becoming very popular and have generated several hundred million dollars in revenues. Despite so much success, the Web has introduced new problems of its own. Finding useful information on the Web is frequently a tedious and difficult task. For instance, to satisfy his information need, the user might navigate the space of Web links (i.e., the hyperspace) searching for information of interest. However, since the hyperspace is vast and almost unknown, such a navigation task is usually inefficient. For naive users, the problem becomes harder, which might entirely frustrate all their efforts. The main obstacle is the absence of a well defined underlying data model for the Web, which implies that information definition and structure is frequently of low quality. These difficulties have attracted renewed interest in IR and its techniques as promising solutions. As a result, almost overnight, IR has gained a place with other technologies at the center of the stage.
mir-0005	1.1.3    Focus of the Book Despite the great increase in interest in information retrieval, modern textbooks on IR with a broad (and extensive) coverage of the various topics in the field are still difficult to find. In an attempt to partially fulfill this gap, this book presents an overall view of research in IR from a computer scientist's perspective. This means that the focus of the book is on computer algorithms and techniques used in information retrieval systems. A rather distinct viewpoint is taken by librarians and information science researchers, who adopt a human-centered interpretation of the IR problem. In this interpretation, the focus is on trying to understand how people interpret and use information as opposed to how to structure, store, and retrieve information automatically. While most of this book is dedicated to the computer scientist's viewpoint of the IR problem, the human-centered viewpoint is discussed to some extent in the last two chapters. We put great emphasis on the integration of the different areas which are closed related to the information retrieval problem and thus, should be treated together. For that reason, besides covering text retrieval, library systems, user interfaces, and the Web, this book also discusses visualization, multimedia retrieval, and digital libraries.
mir-0006	1.2    Basic Concepts The effective retrieval of relevant information is directly affected both by the user task and by the logical view of the documents adopted by the retrieval system, as we now discuss. INTRODUCTION Database Figure 1.1    Interaction of the user with the retrieval system through distinct tasks.
mir-0007	1.2.1    The User Task The user of a retrieval system has to translate his information need into a query in the language provided by the system. With an information retrieval system, this normally implies specifying a set of words which convey the semantics of the information need. With a data retrieval system, a query expression (such as, for instance, a regular expression) is used to convey the constraints that must be satisfied by objects in the answer set. In both cases, we say that the user searches for useful information executing a retrieval task. Consider now a user who has an interest which is either poorly denned or which is inherently broad. For instance, the user might be interested in documents about car racing in general. In this situation, the user might use an interactive interface to simply look around in the collection for documents related to car racing. For instance, he might find interesting documents about Formula 1 racing, about car manufacturers, or about the '24 Hours of Le Mans.1 Furthermore, while reading about the k24 Hours of Le Mans\ he might turn his attention to a document which provides directions to Le Alans and, from there, to documents which cover tourism in France. In this situation, we say that the user is browsing the documents in the collection, not searching. It is still a process of retrieving information, but one whose main objectives are not clearly defined in the beginning and whose purpose might change during the interaction with the system. In this book, we1 make a clear distinction between the different tasks the liber of t lie retrieval system might be engaged in. His task might be of two distinct types: information or data retrieval and browsing. Classic information retrieval systems normally allow information or data retrieval. Hypertext systems are usually tuned for providing quick browsing. Modern digital library and Web interfaces might attempt to combine these tasks to provide improved retrieval capabilities.   However, combination of retrieval and browsing is not yet a well BASIC CONCEPTS         5 established approach and is not the dominant paradigm. Figure 1.1 illustrates the interaction of the user through the different tasks we identify. Information and data retrieval are usually provided by most modern information retrieval systems (such as Web interfaces). Further, such systems might also provide some (still limited) form of browsing. While combining information and data retrieval with browsing is not yet a common practice, it might become so in the future. Both retrieval and browsing are, in the language of the World Wide Web, 'pulling' actions. That is, the user requests the information in an interactive manner. An alternative is to do retrieval in an automatic and permanent fashion using software agents which push the information towards the user. For instance, information useful to a user could be extracted periodically from a news service. In this case, we say that the IR system is executing a particular retrieval task which consists of filtering relevant information for later inspection by the user. We briefly discuss filtering in Chapter 2.
mir-0008	1.2.2    Logical View of the Documents Due to historical reasons, documents in a collection are frequently represented through a set of index terms or keywords. Such keywords might be extracted directly from the text of the document or might be specified by a human subject (as frequently done in the information sciences arena). No matter whether these representative keywords are derived automatically or generated by a specialist, they provide a logical view of the document For a precise definition of the concept of a document and its characteristics, see Chapter 6. Modern computers are making it possible to represent a document by its full set of words. In this case, we say that the retrieval system adopts a full text logical view (or representation) of the documents. With very large collections, however, even modern computers might have to reduce the set of representative keywords. This can be accomplished through the elimination of stopwords (such as articles and connectives), the use of stemming (which reduces distinct words to their common grammatical root), and the identification of noun groups (which eliminates adjectives, adverbs, and verbs). Further, compression might be employed. These operations are called text operations (or transformations) and are covered in detail in Chapter 7. Text operations reduce the complexity of the document representation and allow moving the logical view from that of a full text to that of a set of index terms. The full text is clearly the most complete logical view of a document but its usage usually implies higher computational costs. A small set of categories (generated by a human specialist) provides the most concise logical view of a document but its usage might lead to retrieval of poor quality. Several intermediate logical views (of a document) might be adopted by an information retrieval system as illustrated in Figure 1.2. Besides adopting any of the intermediate representations, the retrieval system might also recognize the internal structure normally present in a document (e.g., chapters, sections, subsections, etc.). This INTRODUCTION Figure 1.2    Logical view of a document: from full text to a set of index terms. information on the structure of the document might be quite useful and is required by structured text retrieval models such as those discussed in Chapter 2. As illustrated in Figure 1.2, we view the issue of logically representing a document as a continuum in which the logical view of a document might shift (smoothly) from a full text representation to a higher level representation specified by a human subject.
mir-0010	1.3    Past, Present, and Future 1.3.1    Early Developments For approximately 4000 years, man has organized information for later retrieval and usage. A typical example is the table of contents of a book. Since the volume of information eventually grew beyond a few books, it became necessary to build specialized data structures to ensure faster access to the stored information. An old and popular data structure for faster information retrieval is a collection of selected words or concepts with which are associated pointers to the related information (or documents) ó the index. In one form or another, indexes are at the core of every modern information retrieval system. They provide faster access to the data and allow the query processing task to be speeded up. A detailed coverage of indexes and their usage for searching can be found in Chapter 8. For centuries, indexes were created manually as categorization hierarchies. In fact, most libraries still use some form of categorical hierarchy to classify their volumes (or documents), as discussed in Chapter 14. Such hierarchies have usually been conceived by human subjects from the library sciences field. More recently, the advent of modern computers has made possible the const ruction of large indexes automatically. Automatic indexes provide a view of the retrieval problem which is much more related to the system itself than to the user need. PAST, PRESENT, AND FUTURE        7 In this respect, it is important to distinguish between two different views of the IR problem: a computer-centered one and a human-centered one. In the computer-centered view, the IR problem consists mainly of building up efficient indexes, processing user queries with high performance, and developing ranking algorithms which improve the 'quality' of the answer set. In the human-centered view, the IR problem consists mainly of studying the behavior of the user, of understanding his main needs, and of determining how such understanding affects the organization and operation of the retrieval system. According to this view, keyword based query processing might be seen as a strategy which is unlikely to yield a good solution to the information retrieval problem in the long run. In this book, we focus mainly on the computer-centered view of the IR problem because it continues to be dominant in the market place.
mir-0011	1.3.2    Information Retrieval in the Library Libraries were among the first institutions to adopt IR systems for retrieving information. Usually, systems to be used in libraries were initially developed by academic institutions and later by commercial vendors. In the first generation, such systems consisted basically of an automation of previous technologies (such as card catalogs) and basically allowed searches based on author name and title. In the second generation, increased search functionality was added which allowed searching by subject headings, by keywords, and some more complex query facilities. In the third generation, which is currently being deployed, the focus is on improved graphical interfaces, electronic forms, hypertext features, and open system architectures. Traditional library management system vendors include Endeavor Information Systems Inc., Innovative Interfaces Inc., and EOS International. Among systems developed with a research focus and used in academic libraries, we distinguish Okapi (at City University, London), MELVYL (at University of California), and Cheshire II (at UC Berkeley). Further details on these library systems can be found in Chapter 14.
mir-0012	1.3.3    The Web and Digital Libraries If we consider the search engines on the Web today, we conclude that they continue to use indexes which are very similar to those used by librarians a century ago. What has changed then? Three dramatic and fundamental changes have occurred due to the advances in modern computer technology and the boom of the Web. First, it became a lot cheaper to have access to various sources of information. This allows reaching a wider audience than ever possible before. Second, the advances in all kinds of digital communication provided greater access to networks. This implies that the information source is available even if distantly located and that 8        INTRODUCTION the access can be done quickly (frequently, in a few seconds). Third, the freedom to post whatever information someone judges useful has greatly contributed to the popularity of the Web. For the first time in history, many people have free access to a large publishing medium. Fundamentally, low cost, greater access, and publishing freedom have allowed people to use the Web (and modern digital libraries) as a highly interactive medium. Such interactivity allows people to exchange messages, photos, documents, software, videos, and to 'chat' in a convenient and low cost fashion. Further, people can do it at the time of their preference (for instance, you can buy a book late at night) which further improves the convenience of the service. Thus, high interactivity is the fundamental and current shift in the communication paradigm. Searching the Web is covered in Chapter 13, while digital libraries are covered in Chapter 15. In the future, three main questions need to be addressed. First, despite the high interactivity, people still find it difficult (if not impossible) to retrieve information relevant to their information needs. Thus, in the dynamic world of the Web and of large digital libraries, which techniques will allow retrieval of higher quality? Second, with the ever increasing demand for access, quick response is becoming more and more a pressing factor. Thus, which techniques will yield faster indexes and smaller query response times? Third, the quality of the retrieval task is greatly affected by the user interaction with the system. Thus, how will a better understanding of the user behavior affect the design and deployment of new information retrieval strategies?
mir-0013	1.3.4    Practical Issues Electronic commerce is a major trend on the Web nowadays and one which has benefited millions of people. In an electronic transaction, the buyer usually has to submit to the vendor some form of credit information which can be used for charging for the product or service. In its most common form, such information consists of a credit card number. However, since transmitting credit card numbers over the Internet is not a safe procedure, such data is usually transmitted over a fax line. This implies that, at least in the beginning, the transaction between a new user and a vendor requires executing an off-line procedure of several steps before the actual transaction can take place. This situation can be improved if the data is encrypted for security. In fact, some institutions and companies already provide some form of encryption or automatic authentication for security reasons. However, security is not the only concern. Another issue of major interest is privacy. Frequently, people are willing to exchange information as long as it does not become public. The reasons are many but the most common one is to protect oneself against misuse of private information by third parties. Thus, privacy is another issue which affects the deployment of the Web and which has not been properly addressed yet. Two other very important issues are copyright and patent rights. It is far THE RETRIEVAL PROCESS        9 from clear how the wide spread of data on the Web affects copyright and patent laws in the various countries. This is important because it affects the business of building up and deploying large digital libraries. For instance, is a site which supervises all the information it posts acting as a publisher? And if so, is it responsible for a misuse of the information it posts (even if it is not the source)? Additionally, other practical issues of interest include scanning, optical character recognition (OCR), and cross-language retrieval (in which the query is in one language but the documents retrieved are in another language). In this book, however, we do not cover practical issues in detail because it is not our main focus. The reader interested in details of practical issues is referred to the interesting book by Lesk [501].
mir-0014	1.4    The Retrieval Process At this point, we are ready to detail our view of the retrieval process. Such a process is interpreted in terms of component subprocesses whose study yields many of the chapters in this book. To describe the retrieval process, we use a simple and generic software architecture as shown in Figure 1.3. First of all, before the retrieval process can even be initiated, it is necessary to define the text database. This is usually done by the manager of the database, which specifies the following: (a) the documents to be used, (b) the operations to be performed on the text, and (c) the text model (i.e., the text structure and what elements can be retrieved). The text operations transform the original documents and generate a logical view of them. Once the logical view of the documents is defined, the database manager (using the DB Manager Module) builds an index of the text. An index is a critical data structure because it allows fast searching over large volumes of data. Different index structures might be used, but the most popular one is the inverted file as indicated in Figure 1.3. The resources (time and storage space) spent on defining the text database and building the index are amortized by querying the retrieval system many times. Given that the document database is indexed, the retrieval process can be initiated. The user first specifies a user need which is then parsed and transformed by the same text operations applied to the text. Then, query operations might be applied before the actual query, which provides a system representation for the user need, is generated. The query is then processed to obtain the retrieved documents. Fast query processing is made possible by the index structure previously built. Before been sent to the user, the retrieved documents are ranked according to a likelihood of relevance. The user then examines the set of ranked documents in the search for useful information. At this point, he might pinpoint a subset of the documents seen as definitely of interest and initiate a user feedback cycle. In such a cycle, the system uses the documents selected by the user to change the query formulation. Hopefully, this modified query is a better representation 10        INTRODUCTION user need User Interface Text 4, 10 Text Text  Operations user feedback logical view logical view Query Operations query 6,7 Indexing DB Manager Module inverted file Searching retrieved docs Index ranked docs Ranking Text Database Figure 1.3    The process of retrieving information (the numbers beside each box indicate the chapters that cover the corresponding topic). of the real user need. The small numbers outside the lower right corner of various boxes in Figure 1.3 indicate the chapters in this book which discuss the respective subpro-cesses in detail A brief introduction to each of these chapters can be found in section 1.5. Consider now the user interfaces available with current information retrieval systems (including Web search engines and Web browsers). We first notice that the user almost never declares his information need. Instead, he is required to provide a direct representation for the query that the system will execute. Since most users have no knowledge of text and query operations, the query they provide is frequently inadequate. Therefore, it is not surprising to observe that poorly formulated queries lead to poor retrieval (as happens so often on the Web).
mir-0015	1.5    Organization of the Book For ease of comprehension, this book has a straightforward structure in which four main parts are distinguished: text IR, human-computer interaction (HCI) ORGANIZATION OF THE BOOK 11 for IR, multimedia IR, and applications of IR. Text IR discusses the classic problem of searching a collection of documents for useful information. HCI for IR discusses current trends in IR towards improved user interfaces and better data visualization tools. Multimedia IR discusses how to index document images and other binary data by extracting features from their content and how to search them efficiently. On the other hand, document images that are predominantly text (rather than pictures) are called textual images and are amenable to automatic extraction of keywords through metadescriptors, and can be retrieved using text IR techniques. Applications of IR covers modern applications of IR such as the Web, bibliographic systems, and digital libraries. Each part is divided into topics which we now discuss.
mir-0016	1.5.1    Book Topics The four parts which compose this book are subdivided into eight topics as illustrated in Figure 1.4. These eight topics are as follows. The topic Retrieval Models  Evaluation discusses the traditional models of searching text for useful information and the procedures for evaluating an information retrieval system. The topic Improvements on Retrieval discusses techniques for transforming the query and the text of the documents with the aim of improving retrieval. The topic Efficient Processing discusses indexing and searching approaches for speeding up the retrieval. These three topics compose the first part on Text IR. The topic Interfaces  Visualization covers the interaction of the user with the information retrieval system. The focus is on interfaces which facilitate the process of specifying a query and provide a good visualization of the results. The topic Multimedia Modeling  Searching discusses the utilization of multimedia data with information retrieval systems. The focus is on modeling, indexing, and searching multimedia data such as voice, images, and other binary data. text m. Retrieval Models  Evaluation Improvements on Retrieval Efficient Processing; HUMAN-COMPUTER INTERACTION FOR IR Interfaces  Visualization .......MULTIMEDIA IR Multimedia Modeling  Searching____________ APPLICATIONS OF IR 1 ,r	Bibliographic t	Systems          J *{ The Web   | f I	Digital Libraries I J  Figure 1.4    Topics which compose the book and their relationships. 12 INTRODUCTION The part on applications of IR is composed of three interrelated topics: The Web, Bibliographic Systems, and Digital Libraries. Techniques developed for the first two applications support the deployment of the latter. The eight topics distinguished above generate the 14 chapters, besides this introduction, which compose this book and which we now briefly introduce.
mir-0017	1.5.2    Book Chapters Figure 1.5 illustrates the overall structure of this book.   The reasoning which yielded the chapters from 2 to 15 is as follows. TEXT IR HUMAN-COMPUTER INTERACTION FOR IR MULTIMEDIA. IR APPLICATIONS OF IR f Query Languages J ££)  [Text Languages    J (T) | Query Operations j (IT)  j Text Operations    J (jT) j Indexing  Searching jCl)_________  Parallel and Distributed IR User Interfaces  Visualization    fio Models  Languages     iJiX Indexing  Searching f Searching the Web  ] © [ Information Retrieval in the Library Digital Libraries  ps) Improvements on Retrieval Efficient Processing Figure 1.5    Structure of the book. In the traditional keyword-based approach, the user specifies his information need by providing sets of keywords and the information system retrieves the documents which best approximate the user query. Also, the information system ORGANIZATION OF THE BOOK        13 might attempt to rank the retrieved documents using some measure of relevance. This ranking task is critical in the process of attempting to satisfy the user information need and is the main goal of modeling in IR. Thus, information retrieval models are discussed early in Chapter 2. The discussion introduces many of the fundamental concepts in information retrieval and lays down much of the foundation for the subsequent chapters. Our coverage is detailed and broad. Classic models (Boolean, vector, and probabilistic), modern probabilistic variants (belief network models), alternative paradigms (extended Boolean, generalized vector, latent semantic indexing, neural networks, and fuzzy retrieval), structured text retrieval, and models for browsing (hypertext) are all carefully introduced and explained. Once a new retrieval algorithm (maybe based on a new retrieval model) is conceived, it is necessary to evaluate its performance. Traditional evaluation strategies usually attempt to estimate the costs of the new algorithm in terms of time and space. With an information retrieval system, however, there is the additional issue of evaluating the relevance of the documents retrieved. For this purpose, text reference collections and evaluation procedures based on variables other than time and space are used. Chapter 3 is dedicated to the discussion of retrieval evaluation. In traditional IR, queries are normally expressed as a set of keywords which is quite convenient because the approach is simple and easy to implement. However, the simplicity of the approach prevents the formulation of more elaborate querying tasks. For instance, queries which refer to both the structure and the content of the text cannot be formulated. To overcome this deficiency, more sophisticated query languages are required. Chapter 4 discusses various types of query languages. Since now the user might refer to the structure of a document in his query, this structure has to be defined. This is done by embedding the description of a document content and of its structure in a text language such as the Standard Generalized Markup Language (SGML). As illustrated in Figure 1.5, Chapter 6 is dedicated to the discussion of text languages. Retrieval based on keywords might be of fairly low quality. Two possible reasons are as follows. First, the user query might be composed of too few terms which usually implies that the query context is poorly characterized. This is frequently the case, for instance, in the Web. This problem is dealt with through transformations in the query such as query expansion and user relevance feedback. Such query operations are covered in Chapter 5. Second, the set of keywords generated for a given document might fail to summarize its semantic content properly. This problem is dealt with through transformations in the text such as identification of noun groups to be used as keywords, stemming, and the use of a thesaurus. Additionally, for reasons of efficiency, text compression can be employed. Chapter 7 is dedicated to text operations. Given the user query, the information system has to retrieve the documents which are related to that query. The potentially large size of the document collection (e.g., the Web is composed of millions of documents) implies that specialized indexing techniques must be used if efficient retrieval is to be achieved. Thus, to speed up the task of matching documents to queries, proper indexing and search14       INTRODUCTION ing techniques are used as discussed in Chapter 8. Additionally, query processing can be further accelerated through the adoption of parallel and distributed IR techniques as discussed in Chapter 9. As illustrated in Figure 1.5, all the key issues regarding Text IR, from modeling to fast query processing, are covered in this book. Modern user interfaces implement strategies which assist the user to form a query. The main objective is to allow him to define more precisely the context associated to his information need. The importance of query contextualization is a consequence of the difficulty normally faced by users during the querying process. Consider, for instance, the problem of quickly finding useful information in the Web. Navigation in hyperspace is not a good solution due to the absence of a logical and semantically well defined structure (the Web has no underlying logical model). A popular approach for specifying a user query in the Web consists of providing a set of keywords which are searched for. Unfortunately, the number of terms provided by a common user is small (typically, fewer than four) which usually implies that the query is vague. This means that new user interface paradigms which assist the user with the query formation process are required. Further, since a vague user query usually retrieves hundreds of documents, the conventional approach of displaying these documents as items of a scrolling list is clearly inadequate. To deal with this problem, new data visualization paradigms have been proposed in recent years. The main trend is towards visualization of a large subset of the retrieved documents at once and direct manipulation of the whole subset. User interfaces for assisting the user to form his query and current approaches for visualization of large data sets are covered in Chapter 10. Following this, we discuss the application of IR techniques to multimedia data. The key issue is how to model, index, and search structured documents which contain multimedia objects such as digitized voice, images, and other binary data. Models and query languages for office and medical information retrieval systems are covered in Chapter 11. Efficient indexing and searching of multimedia objects is covered in Chapter 12. Some readers may argue that the models and techniques for multimedia retrieval are rather different from those for classic text retrieval. However, we take into account that images and text are usually together and that with the Web, other media types (such as video and audio) can also be mixed in. Therefore, we believe that in the future, all the above will be treated in a unified and consistent manner. Our book is a first step in that direction. The final three chapters of the book are dedicated to applications of modern information retrieval: the Web, bibliographic systems, and digital libraries. As illustrated in Figure 1.5, Chapter 13 presents the Web and discusses the main problems related to the issue of searching the Web for useful information. Also, our discussion covers briefly the most popular search engines in the Web presenting particularities of their organization. Chapter 14 covers commercial document databases and online public access catalogs. Commercial document databases are still the largest information retrieval systems nowadays. LEXIS-NEXIS, for instance, has a database with 1.3 billion documents and attends to over 120 million query requests annually.  Finally, Chapter 15 discusses modern digital HOW TO USE THIS BOOK        15 libraries. Architectural issues, models, prototypes, and standards are all covered. The discussion also introduces the '5S' model (streams, structures, spaces, scenarios and societies) as a framework for providing theoretical and practical unification of digital libraries.
mir-0018	1.6    How to Use this Book Although several people have contributed chapters for this book, it is really a textbook. The contents and the structure of the book have been carefully designed by the two main authors who also authored or coauthored nine of the 15 chapters in the book. Further, all the contributed chapters have been judiciously edited and integrated into a unifying framework that provides uniformity in structure and style, a common glossary, a common bibliography, and appropriate cross-references. At the end of each chapter, a discussion on research issues, trends, and selected bibliography is included. This discussion should be useful for graduate students as well as for researchers. Furthermore, the book is complemented by a Web page with additional information and resources.
mir-0019	1.6.1    Teaching Suggestions This textbook can be used in many different areas including computer science (CS), information systems, and library science. The following list gives suggested contents for different courses at the undergraduate and graduate level, based on syllabuses of many universities around the world: ï  Information Retrieval (Computer Science, undergraduate): this is the standard course for many CS programs.   The minimum content should include Chapters 1 to 8 and Chapter 10, that is, most of the part on Text IR complemented with the chapter on user interfaces. Some specific topics of those chapters, such as more advanced models for IR and sophisticated algorithms for indexing and searching, can be omitted to fit a one term course. The chapters on Applications of IR can be mentioned briefly at the end. ï  Advanced Information Retrieval (Computer Science, graduate): similar to the previous course but with more detailed coverage of the various chapters particularly modeling and searching (assuming the previous course as a requirement).  In addition, Chapter 9 and Chapters 13 to 15 should be covered completely. Emphasis on research problems and new results is a must. ï  Information  Retrieval  (Information  Systems,   undergraduate):    this course is similar to the CS course, but with a different emphasis. It should include Chapters 1 to 7 and Chapter 10. Some notions from Chapter 8 are 16        INTRODUCTION useful but not crucial. At the end, the system-oriented parts of the chapters on Applications of IR, in particular those on Bibliographic Systems and Digital Libraries, must be covered (this material can be complemented with topics from [501]). ï  Information Retrieval (Library Science, undergraduate): similar to the previous course, but removing the more technical and advanced material of Chapters 2, 5, and 7. Also, greater emphasis should be put on the chapters on Bibliographic Systems and Digital Libraries.   The course should be complemented with a thorough discussion of the user-centered view of the IR problem (for example, using the book by Allen [13]). ï  Multimedia Retrieval (Computer Science, undergraduate or graduate): this course should include Chapters 1 to 3, 6, and 11 to 15. The emphasis could be on multimedia itself or on the integration of classical IR with multimedia. The course can be complemented with one of the many books on this topic, which are usually more broad and technical. ï  Topics in IR (Computer Science, graduate): many chapters of the book can be used for this course. It can emphasize modeling and evaluation or user interfaces and visualization. It can also be focused on algorithms and data structures (in that case, [275] and [825] are good complements).  A multimedia focus is also possible, starting with Chapters 11 and 12 and using more specific books later on. ï  Topics in IR (Information Systems or Library Science, graduate) similar to the above but with emphasis on non-technical parts. For example, the course could cover modeling and evaluation, query languages, user interfaces, and visualization. The chapters on applications can also be considered. ï  Web Retrieval and Information Access (generic, undergraduate or graduate): this course should emphasize hypertext, concepts coming from networks and distributed systems and multimedia.  The kernel should be the basic models of Chapter 2 followed by Chapters 3, 4, and 6.   Also, Chapters 11 and 13 to 15 should be discussed. ï  Digital Libraries (generic, undergraduate or graduate): This course could start with part of Chapters 2 to 4 and 6, followed by Chapters 10, 14, and 15. The kernel of the course could be based on the book by Lesk [501]. IVIore bibliography useful for many of the courses above is discussed in the last section of this chapter.
mir-0020	1.6.2    The Book's Web Page As IR is a very dynamic area nowadays, a book by itself is not enough. For that reason (and many others), the book has a Web home page located and mirrored in the following places (mirrors in USA and Europe are also planned): BIBLIOGRAPHIC DISCUSSION        17 ï  Brazil: http://www.dcc.ufmg.br/irbook ï  Chile: http://sunsite.dcc.ucliile.cl/irbook Comments, suggestions, contributions, or mistakes found are welcome through email to the contact authors given on the Web page. The Web page contains the Table of Contents, Preface, Acknowledgements, Introduction, Glossary, and other appendices to the book. It also includes exercises and teaching materials that will be increasing in volume and changing with time. In addition, a reference collection (containing 1239 documents on Cystic Fibrosis and 100 information requests with extensive relevance evaluation [721]) is available for experimental purposes. Furthermore, the page includes useful pointers to IR syllabuses in different universities, IR research groups, IR publications, and other resources related to IR and this book. Finally, any new important results or additions to the book as well as an errata will be made publicly available there.
mir-0021	1.7    Bibliographic Discussion Many other books have been written on information retrieval, and due to the current widespread interest in the subject, new books have appeared recently. In the following, we briefly compare our book with these previously published works. Classic references in the field of information retrieval are the books by van Rijsbergen [785] and Salton and McGill [698]. Our distinction between data and information retrieval is borrowed from the former. Our definition of the information retrieval process is influenced by the latter. However, almost 20 years later, both books are now outdated and do not cover many of the new developments in information retrieval. Three more recent and also well known references in information retrieval are the book edited by Frakes and Baeza-Yates [275], the book by Witten, Moffat, and Bell [825], and the book by Lesk [501]. All these three books are complementary to this book. The first is focused on data structures and algorithms for information retrieval and is useful whenever quick prototyping of a known algorithm is desired. The second is focused on indexing and compression, and covers images besides text. For instance, our definition of a textual image is borrowed from it. The third is focused on digital libraries and practical issues such as history, distribution, usability, economics, and property rights. On the issue of computer-centered and user-centered retrieval, a generic book on information systems that takes the latter view is due to Allen [13]. There are other complementary books for specific chapters. For example, there are many books on IR and hypertext. The same is true for generic or specific multimedia retrieval, as images, audio or video. Although not an information retrieval title, the book by Rosenfeld and Morville [682] on information architecture of the Web, is a good complement to our chapter on searching the 18        INTRODUCTION Web. The book by Menasce and Almeida [554] demonstrates how to use queue-ing theory for predicting Web server performance. In addition, there are many books that explain how to find information on the Web and how to use search engines. The reference edited by Sparck Jones and Willet [414], which was long awaited, is really a collection of papers rather than an edited book. The coherence and breadth of coverage in our book makes it more appropriate as a textbook in a formal discipline. Nevertheless, this collection is a valuable research tool. A collection of papers on cross-language information retrieval was recently edited by Grefenstette [323]. This book is a good complement to ours for people interested in this particular topic. Additionally, a collection focused on intelligent IR was edited recently by Maybury [550], and another collection on natural language IR edited by Strzalkowski will appear soon [748]. The book by Korfhage [451] covers a lot less material and its coverage is not as detailed as ours. For instance, it includes no detailed discussion of digital libraries, the Web, multimedia, or parallel processing. Similarly, the books by Kowalski [459] and Shapiro et al. [719] do not cover these topics in detail, and have a different orientation. Finally, the recent book by Grossman and Frieder [326] does not discuss the Web, digital libraries, or visual interfaces. For people interested in research results, the main journals on IR are: Journal of the American Society of Information Sciences (JASIS) published by Wiley and Sons, ACM Transactions on Information Systems, Information Processing  Management (IPM, Elsevier), Information Systems (Elsevier), Information Retrieval (Kluwer), and Knowledge and Information Systems (Springer). The main conferences are: ACM SIGIR International Conference on Information Retrieval, ACM International Conference on Digital Libraries (ACM digital library), ACM Conference on Information Knowledge and Management (CIKM), and Text RE-trieval Conference (TREC). Regarding events of regional influence, we would like to acknowledge the SPIRE (South American Symposium on String Processing and Information Retrieval) symposium.
mir-0023	2.1    Introduction Traditional information retrieval systems usually adopt index terms to index and retrieve documents. In a restricted sense, an index term is a keyword (or group of related words) which has some meaning of its own (i.e., which usually has the semantics of a noun). In its more general form, an index term is simply any word which appears in the text of a document in the collection. Retrieval based on index terms is simple but raises key questions regarding the information retrieval task. For instance, retrieval using index terms adopts as a fundamental foundation the idea that the semantics of the documents and of the user information need can be naturally expressed through sets of index terms. Clearly, this is a considerable oversimplification of the problem because a lot of the semantics in a document or user request is lost when we replace its text with a set of words. Furthermore, matching between each document and the user request is attempted in this very imprecise space of index terms. Thus, it is no surprise that the documents retrieved in response to a user request expressed as a set of keywords are frequently irrelevant. If one also considers that most users have no training in properly forming their queries, the problem is worsened with potentially disastrous results. The frequent dissatisfaction of Web users with the answers they normally obtain is just one good example of this fact. Clearly, one central problem regarding information retrieval systems is the issue of predicting which documents are relevant and which are not. Such a decision is usually dependent on a ranking algorithm which attempts to establish a simple ordering of the documents retrieved. Documents appearing at the top of this ordering are considered to be more likely to be relevant. Thus, ranking algorithms are at the core of information retrieval systems. A ranking algorithm operates according to basic premises regarding the notion of document relevance. Distinct sets of premises (regarding document relevance) yield distinct information retrieval models. The IR model adopted determines the predictions of what is relevant and what is not (i.e., the notion of relevance implemented by the system). The purpose of this chapter is to cover the most Important Information retrieval models proposed over the years.  By 19 20        MODELING doing so, the chapter also provides a conceptual basis for most of the remaining chapters in this book. We first propose a taxonomy for categorizing the 15 IR models we cover. Second, we distinguish between two types of user retrieval tasks: ad hoc and filtering. Third, we present a formal characterization of IR models which is useful for distinguishing the various components of a particular model. Last, we discuss each of the IR models included in our taxonomy.
mir-0024	2.2    A Taxonomy of Information Retrieval Models The three classic models in information retrieval are called Boolean, vector, and probabilistic. In the Boolean model, documents and queries are represented as sets of index terms. Thus, as suggested in [327], we say that the model is set theoretic. In the vector model, documents and queries are represented as vectors in a t-dimensional space. Thus, we say that the model is algebraic. In the probabilistic model, the framework for modeling document and query representations is based on probability theory. Thus, as the name indicates, we say that the model is probabilistic. Over the years, alternative modeling paradigms for each type of classic model (i.e., set theoretic, algebraic, and probabilistic) have been proposed. Regarding alternative set theoretic models, we distinguish the fuzzy and the extended Boolean models. Regarding alternative algebraic models, we distinguish the generalized vector, the latent semantic indexing, and the neural network models. Regarding alternative probabilistic models, we distinguish the inference network and the belief network models. Figure 2.1 illustrates a taxonomy of these information retrieval models. Besides references to the text content, the model might also allow references to the structure normally present in written text. In this case, we say that we have a structured model. We distinguish two models for structured text retrieval namely, the non-overlapping lists model and the proximal nodes model. As discussed in Chapter 1, the user task might be one of browsing (instead of retrieval). In Figure 2.1, we distinguish three models for browsing namely, flat, structure guided, and hypertext. The organization of this chapter follows the taxonomy of information retrieval models depicted in the figure.We first discuss the three classic models. Second, we discuss the alternative models for each type of classic model. Third, we cover structured text retrieval models. At the end, we discuss models for browsing. We emphasize that the IR model (Boolean, vector, probabilistic, etc.), the logical view of the documents (full text, set of index terms, etc.), and the user task (retrieval, browsing) are orthogonal aspects of a retrieval system as detailed in Chapter 1. Thus, despite the fact that some models are more appropriate for a certain user task than for another, the same IR model can be used with distinct document logical views to perform different user tasks. Figure 2.2 illustrates the retrieval models most frequently associated with each one of six distinct combinations of a document logical view and a user task. RETRIEVAL: AD HOC AND FILTERING 21 Retrieval: lt; Adhoc Filtering Browsing Classic Models		/ ------boolean      ^ vector        *~ probabilistic ï  Structured Models Non-6v^riappmg tists Proximal Nodes  Browsing Flat Structure Guided Hypertext Set Theoretic Fuzzy Extended Boolean Algebraic Generalized Vector Lat. Semantic Index Neural Networks Probabilistic Inference Network Belief Network Figure 2.1    A taxonomy of information retrieval models. LOGICAL   VIEW   OF  DOCUMENTS U S E R T A S K Index Terms	Full Text	Full Text + Structure Retrieval	Classic Set Theoretic Algebraic Probabilistic	Classic Set Theoretic Algebraic Probabilistic	Structured Browsing	Flat	Flat Hypertext	Structure Guided Hypertext Figure 2.2    Retrieval models most frequently associated with distinct combinations of a document logical view and a user task.
mir-0025	2.3    Retrieval: Ad hoc and Filtering In a conventional information retrieval system, the documents in the collection remain relatively static while new queries are submitted to the system. This operational mode has been termed ad hoc retrieval in recent years and is the 22        MODELING most common form of user task. A similar but distinct task is one in which the queries remain relatively static while new documents come into the system (and leave). For instance, this is the case with the stock market and with news wiring services. This operational mode has been termed filtering. In a filtering task [74], a user profile describing the user's preferences is constructed. Such a profile is then compared to the incoming documents in an attempt to determine those which might be of interest to this particular user. For instance, this approach can be used to select a news article among thousands of articles which are broadcast each day. Other potential scenarios for the application of filtering include the selection of preferred judicial decisions, or the selection of articles from daily newspapers, etc. Typically, the filtering task simply indicates to the user the documents which might be of interest to him. The task of determining which ones are really relevant is fully reserved to the user. Not even a ranking of the filtered documents is provided. A variation of this procedure is to rank the filtered documents and show this ranking to the user. The motivation is that the user can examine a smaller number of documents if he assumes that the ones at the top of this ranking are more likely to be relevant. This variation of filtering is called routing (see Chapter 3) but it is not popular. Even if no ranking is presented to the user, the filtering task can compute an internal ranking to determine potentially relevant documents. For instance, documents with a ranking above a given threshold could be selected; the others would be discarded. Any IR model can be adopted to rank the documents, but the vector model is usually preferred due to its simplicity. At this point, we observe that filtering is really a type of user task (or operational mode) and not a model of information retrieval. Thus, the task of filtering and the IR model adopted are orthogonal aspects of an IR system. In a filtering task, the crucial step is not the ranking itself but the construction of a user profile which truly reflects the user's preferences. Many approaches for constructing user profiles have been proposed and here we briefly discuss a couple of them. A simplistic approach for constructing a user profile is to describe the profile through a set of keywords and to require the user to provide the necessary keywords. The approach is simplistic because it requires the user to do too much. In fact, if the user is not familiar with the service which generates the upcoming documents, he might find it fairly difficult to provide the keywords which appropriately describe his preferences in that context. Furthermore, an attempt by the user to familiarize himself with the vocabulary of the upcoming documents might turn into a tedious and time consuming exercise. Thus, despite its feasibility, requiring the user to precisely describe his profile might be impractical. A more elaborate alternative is to collect information from the user about his preferences and to use this information to build the user profile dynamically. This can be accomplished as follows. In the very beginning, the user provides a set of keywords which describe an initial (and primitive) profile of his preferences. As new documents arrive, the system uses this profile to select documents which are potentially of interest and A FORMAL CHARACTERIZATION OF IR MODELS        23 shows them to the user. The user then goes through a relevance feedback cycle (see Chapter 5) in which he indicates not only the documents which are really relevant but also the documents which are non-relevant. The system uses this information to adjust the user profile description such that it reflects the new preferences just declared. Of course, with this procedure the profile is continually changing. Hopefully, however, it stabilizes after a while and no longer changes drastically (unless, of course, the user's interests shift suddenly). Chapter 5 illustrates mechanisms which can be used to dynamically update a keyword-based profile. Prom the above, it should be clear that the filtering task can be viewed as a conventional information retrieval task in which the documents are the ones which keep arriving at the system. Ranking can be computed as before. The difficulty with filtering resides in describing appropriately the user's preferences in a user profile. The most common approaches for deriving a user profile are based on collecting relevant information from the user, deriving preferences from this information, and modifying the user profile accordingly. Since the number of potential applications of filtering keeps increasing, we should see in the future a renewed interest in the study and usage of the technique.
mir-0026	2.4    A Formal Characterization of IR Models We have argued that the fundamental premises which form the basis for a ranking algorithm determine the IR model. Throughout this chapter, we will discuss different sets of such premises. However, before doing so, we should state clearly what exactly an IR model is. Our characterization is as follows. Definition An information retrieval model is a quadruple [D,Q, T, R{qi,dj)J where (1)  D is a set composed of logical views (or representations) for the documents in the collection. (2)  Q is a set composed of logical views (or representations) for the user information needs. Such representations are called queries. (3)  Tis a framework for modeling document representations, queries, and their relationships. (4)  R{qi,dj) is a ranking function which associates a real number with a query qt Ä Q and a document representation dj G D.  Such ranking defines an ordering among the documents with regard to the query qi. To build a model, we think first of representations for the documents and for the user information need. Given these representations, we then conceive the framework in which they can be modeled. This framework should also provide the intuition for constructing a ranking function. For instance, for the classic Boolean model, the framework is composed of sets of documents and the standard operations on sets. For the classic vector model, the framework is composed of a 24        MODELING t-dimensional vectorial space and standard linear algebra operations on vectors. For the classic probabilistic model, the framework is composed of sets, standard probability operations, and the Bayes' theorem. In the remainder of this chapter, we discuss the various IR models shown in Figure 2.1. Throughout the discussion, we do not explicitly instantiate the components D, Q, T, and R(qu dj) of each model. Such components should be quite clear from the discussion and can be easily inferred.
mir-0027	2.5    Classic Information Retrieval In this section we briefly present the three classic models in information retrieval namely, the Boolean, the vector, and the probabilistic models.
mir-0028	2.5.1    Basic Concepts The classic models in information retrieval consider that each document is described by a set of representative keywords called index terms. An index term is simply a (document) word whose semantics helps in remembering the document's main themes. Thus, index terms are used to index and summarize the document contents. In general, index terms are mainly nouns because nouns have meaning by themselves and thus, their semantics is easier to identify and to grasp. Adjectives, adverbs, and connectives are less useful as index terms because they work mainly as complements. However, it might be interesting to consider all the distinct words in a document collection as index terms. For instance, this approach is adopted by some Web search engines as discussed in Chapter 13 (in which case, the document logical view is full text). We postpone a discussion on the problem of how to generate index terms until Chapter 7, where the issue is covered in detail. Given a set of index terms for a document, we notice that not all terms are equally useful for describing the document contents. In fact, there are index terms which are simply vaguer than others. Deciding on the importance of a term for summarizing the contents of a document is not a trivial issue. Despite this difficulty, there are properties of an index term which are easily measured and which are useful for evaluating the potential of a term as such. For instance, consider a collection with a hundred thousand documents. A word which appears in each of the one hundred thousand documents is completely useless as an index term because it does not tell us anything about which documents the user might be interested in. On the other hand, a word which appears in just five documents is quite useful because it narrows down considerably the space of documents which might be of interest to the user. Thus, it should be clear that distinct index terms have varying relevance when used to describe document contents. This effect is captured through the assignment of numerical weights to each index term of a document. CLASSIC INFORMATION RETRIEVAL        25 Let ki be an index term, dj be a document, and WitJ gt; 0 be a weight associated with the pair (ki.dj). This weight quantifies the importance of the index term for describing the document semantic contents. Definition Let t be the number of index terms in the system and ki be a generic index term. K = {/ci,..., kt} is the set of all index terms. A weight wtj gt; 0 is associated with each index term ki of a document dj. For an index term which does not appear in the document text, Wij = 0. With the document dj is associated an index term vector dj represented by dj = (wij,W2j, . ï ï ,wt,j)-Further, let gi be a function that returns the weight associated with the index term ki in any t-dimensional vector (i.e., gi{dj) = Wij). As we later discuss, the index term weights are usually assumed to be mutually independent. This means that knowing the weight w%^ associated with the pair (ki,dj) tells us nothing about the weight Wi+ii3 associated with the pair (ki+i,dj). This is clearly a simplification because occurrences of index terms in a document are not uncorrelated. Consider, for instance, that the terms computer and network are used to index a given document which covers the area of computer networks. Frequently, in this document, the appearance of one of these two words attracts the appearance of the other. Thus, these two words are correlated and their weights could reflect this correlation. While mutual independence seems to be a strong simplification, it does simplify the task of computing index term weights and allows for fast ranking computation. Furthermore, taking advantage of index term correlations for improving the final document ranking is not a simple task. In fact, none of the many approaches proposed in the past has clearly demonstrated that index term correlations are advantageous (for ranking purposes) with general collections. Therefore, unless clearly stated otherwise, we assume mutual independence among index terms. In Chapter 5 we discuss modern retrieval techniques which are based on term correlations and which have been tested successfully with particular collections. These successes seem to be slowly shifting the current understanding towards a more favorable view of the usefulness of term correlations for information retrieval systems. The above definitions provide support for discussing the three classic information retrieval models, namely, the Boolean, the vector, and the probabilistic models, as we now do.
mir-0029	2.5.2    Boolean Model The Boolean model is a simple retrieval model based on set theory and Boolean algebra. Since the concept of a set is quite intuitive, the Boolean model provides a framework which is easy to grasp by a common user of an IR system. Furthermore, the queries are specified as Boolean expressions which have precise semantics. Given its inherent simplicity and neat formalism, the Boolean model received great attention in past years and was adopted by many of the early commercial bibliographic systems. 26        MODELING k Figure 2.3    The three conjunctive components for the query [q = ka A (fc V -gt;kc)]. Unfortunately, the Boolean model suffers from major drawbacks. First, its retrieval strategy is based on a binary decision criterion (i.e., a document is predicted to be either relevant or non-relevant) without any notion of a grading scale, which prevents good retrieval performance. Thus, the Boolean model is in reality much more a data (instead of information) retrieval model. Second, while Boolean expressions have precise semantics, frequently it is not simple to translate an information need into a Boolean expression. In fact, most users find it difficult and awkward to express their query requests in terms of Boolean expressions. The Boolean expressions actually formulated by users often are quite simple (see Chapter 10 for a more thorough discussion on this issue). Despite these drawbacks, the Boolean model is still the dominant model with commercial document database systems and provides a good starting point for those new to the field. The Boolean model considers that index terms are present or absent in a document. As a result, the index term weights are assumed to be all binary, i.e., Wij G {0,1}. A query q is composed of index terms linked by three connectives: not, and, or.Thus, a query is essentially a conventional Boolean expression which can be represented as a disjunction of conjunctive vectors (i.e., in disjunctive normal form ó DNF). For instance, the query [q = ka A (fc V ^kc)] can be written in disjunctive normal form as [qdnf ó (1? 1gt; 1) V (1? 1? 0) V (1,0,0)], where each of the components is a binary weighted vector associated with the tuple (ka, fc, kc). These binary weighted vectors are called the conjunctive components of qdnf-Figure 2.3 illustrates the three conjunctive components for the query q. Definition     For the Boolean model, the index term weight variables are all binary i.e., u^j £ {0,1}.  A query q is a conventional Boolean expression.  Let qdnf be the disjunctive normal form for the query q. Further, let qcc be any of the conjunctive components of qdnf- The similarity of a document dj to the query q is defined as simid   a) = { J         \ 0    otherwise CLASSIC INFORMATION RETRIEVAL         27 If sim(dj1q) = 1 then the Boolean model predicts that the document dj is relevant to the query q (it might not be). Otherwise, the prediction is that the document is not relevant. The Boolean model predicts that each document is either relevant or non-relevant There is no notion of a partial match to the query conditions. For instance, let dj be a document for which dj = (0,1,0). Document dj includes the index term fc but is considered non-relevant to the query [q = feo A(fc V-ifec)]. The main advantages of the Boolean model are the clean formalism behind the model and its simplicity. The main disadvantages are that exact matching may lead to retrieval of too few or too many documents (see Chapter 10). Today, it is well known that index term weighting can lead to a substantial improvement in retrieval performance. Index term weighting brings us to the vector model.
mir-0030	2.5.3    Vector Model The vector model [697, 695] recognizes that the use of binary weights is too limiting and proposes a framework in which partial matching is possible. This is accomplished by assigning non-binary weights to index terms in queries and in documents. These term weights are ultimately used to compute the degree of similarity between each document stored in the system and the user query. By sorting the retrieved documents in decreasing order of this degree of similarity, the vector model takes into consideration documents which match the query terms only partially. The main resultant effect is that the ranked document answer set is a lot more precise (in the sense that it better matches the user information need) than the document answer set retrieved by the Boolean model. Definition For the vector model, the weight W{j associated with a pair ($, d3) is positive and non-binary. Further, the index terms in the query are also weighted. Let Wi^q be the weight associated with the pair [ki,q], where w^q gt; 0. Then, the query vector q is defined as q = (wiyq,W2,q,..gt; lt;,wtiq) where t is the total number of index terms in the system. As before, the vector for a document Therefore, a document dj and a user query q are represented as t-dimensional vectors as shown in Figure 2.4. The vector model proposes to evaluate the degree of similarity of the document dj with regard to the query q as the correlation between the vectors dj and q. This correlation can be quantified, for instance, by the cosine of the angle between these two vectors. That is, -    ti     \          dj9q sirn(dj,q)    =    --fóó \dj\ x \q\ 28        MODELING Figure 2.4    The cosine of 0 is adopted as sim(d3,q). where \dj\ and \q\ are the norms of the document and query vectors. The factor \q\ does not affect the ranking (i.e., the ordering of the documents) because it is the same for all documents. The factor \d3\ provides a normalization in the space of the documents. Since wzj gt; 0 and w^q gt; 0, sim(q, dj) varies from 0 to +1. Thus, instead of attempting to predict whether a document is relevant or not, the vector model ranks the documents according to their degree of similarity to the query. A document might be retrieved even if it matches the query only partially. For instance, one can establish a threshold on sim(d3,q) and retrieve the documents with a degree of similarity above that threshold. But to compute rankings we need first to specify how index term weights are obtained. Index term weights can be calculated in many different ways. The work by Salton and McGill [698] reviews various term-weighting techniques. Here, we do not discuss them in detail. Instead, we concentrate on elucidating the main idea behind the most effective term-weighting techniques. This idea is related to the basic principles which support clustering techniques, as follows. Given a collection C of objects and a vague description of a set A, the goal of a simple clustering algorithm might be to separate the collection C of objects into two sets: a first one which is composed of objects related to the set A and a second one which is composed of objects not related to the set A. Vague description here means that we do not have complete information for deciding precisely which objects are and wThich are not in the set A. For instance, one might be looking for a set .4 of cars which have a price comparable to that of a Lexus 400. Since it is not clear what the term comparable means exactly, there is not a precise (and unique) description of the set A. More sophisticated clustering algorithms might attempt to separate the objects of a collection into various clusters (or classes) according to their properties. For instance, patients of a doctor specializing in cancer could be classified into five classes: terminal, advanced, metastasis, diagnosed, and healthy. Again, the possible class descriptions might be imprecise (and not unique) and the problem is one of deciding to which of these classes a new patient should be assigned. In what follows, however, wre only discuss the simpler version of the clustering problem (i.e., the one which considers only two classes) because all that is required is a decision on which documents are predicted to be relevant and which ones are predicted to be not relevant (with regard to a given user query). CLASSIC INFORMATION RETRIEVAL        29 To view the IR problem as one of clustering, we refer to the early work of Salton. We think of the documents as a collection C of objects and think of the user query as a (vague) specification of a set A of objects. In this scenario, the IR problem can be reduced to the problem of determining which documents are in the set A and which ones are not (i.e., the IR problem can be viewed as a clustering problem). In a clustering problem, two main issues have to be resolved. First, one needs to determine what are the features which better describe the objects in the set A. Second, one needs to determine what are the features which better distinguish the objects in the set A from the remaining objects in the collection C. The first set of features provides for quantification of intra-cluster similarity, while the second set of features provides for quantification of inter-cluster dissimilarity. The most successful clustering algorithms try to balance these two effects. In the vector model, intra-clustering similarity is quantified by measuring the raw frequency of a term ki inside a document d3. Such term frequency is usually referred to as the tf factor and provides one measure of how well that term describes the document contents (i.e., intra-document characterization). Furthermore, inter-cluster dissimilarity is quantified by measuring the inverse of the frequency of a term ki among the documents in the collection. This factor is usually referred to as the inverse document frequency or the idf factor. The motivation for usage of an idf factor is that terms which appear in many documents are not very useful for distinguishing a relevant document from a non-relevant one. As with good clustering algorithms, the most effective term-weighting schemes for IR try to balance these two effects. Definition Let N be the total number of documents in the system and n2 be the number of documents in which the index term ki appears. Let freqij be the raw frequency of term ki in the document dj (i.e., the number of times the term k{ is mentioned in the text of the document dj). Then, the normalized frequency fi,j of term ki in document dj is given by maxi   freqij where the maximum is computed over all terms which are mentioned in the text of the document d3. If the term kt does not appear in the document d3 then fzj = 0. Further, let idfi, inverse document frequency for kt, be given by idf, = log -                                                                                  (2.2) The best known term-weighting schemes use weights which are given by u-1J = /IJxlogó                                                                         (2.3) "I 30        MODELING or by a variation of this formula. Such term-weighting strategies are called tf-idf schemes. Several variations of the above expression for the weight Wij are described in an interesting paper by Salton and Buckley which appeared in 1988 [696]. However, in general, the above expression should provide a good weighting scheme for many collections. For the query term weights, Salton and Buckley suggest :logó                                           (2.4) maxi  freqi^ where freq%A is the raw frequency of the term ki in the text of the information request q. The main advantages of the vector model are: (1) its term-weighting scheme improves retrieval performance; (2) its partial matching strategy allows retrieval of documents that approximate the query conditions; and (3) its cosine ranking formula sorts the documents according to their degree of similarity to the query. Theoretically, the vector model has the disadvantage that index terms are assumed to be mutually independent (equation 2.3 does not account for index term dependencies). However, in practice, consideration of term dependencies might be a disadvantage. Due to the locality of many term dependencies, their indiscriminate application to all the documents in the collection might in fact hurt the overall performance. Despite its simplicity, the vector model is a resilient ranking strategy with general collections. It yields ranked answer sets which are difficult to improve upon without query expansion or relevance feedback (see Chapter 5) within the framework of the vector model. A large variety of alternative ranking methods have been compared to the vector model but the consensus seems to be that, in general, the vector model is either superior or almost as good as the known alternatives. Furthermore, it is simple and fast. For these reasons, the vector model is a popular retrieval model nowadays.
mir-0031	2.5.4    Probabilistic Model In this section, we describe the classic probabilistic model introduced in 1976 by Roberston and Sparck Jones [677] which later became known as the binary independence retrieval (BIR) model.   Our discussion is intentionally brief and focuses mainly on highlighting the key features of the model. With this purpose in mind, we do not detain ourselves in subtleties regarding the binary independence assumption for the model. The section on bibliographic discussion points to references which cover these details. The probabilistic model attempts to capture the IR problem within a probabilistic framework. The fundamental idea is as follows. Given a user query, there is a set of documents which contains exactly the relevant documents and CLASSIC INFORMATION RETRIEVAL        31 no other. Let us refer to this set of documents as the ideal answer set. Given the description of this ideal answer set, we would have no problems in retrieving its documents. Thus, we can think of the querying process as a process of specifying the properties of an ideal answer set (which is analogous to interpreting the IR problem as a problem of clustering). The problem is that we do not know exactly what these properties are. All we know is that there are index terms whose semantics should be used to characterize these properties. Since these properties are not known at query time, an effort has to be made at initially guessing what they could be. This initial guess allows us to generate a preliminary probabilistic description of the ideal answer set which is used to retrieve a first set of documents. An interaction with the user is then initiated with the purpose of improving the probabilistic description of the ideal answer set. Such interaction could proceed as follows. The user takes a look at the retrieved documents and decides which ones are relevant and which ones are not (in truth, only the first top documents need to be examined). The system then uses this information to refine the description of the ideal answer set. By repeating this process many times, it is expected that such a description will evolve and become closer to the real description of the ideal answer set. Thus, one should always have in mind the need to guess at the beginning the description of the ideal answer set. Furthermore, a conscious effort is made to model this description in probabilistic terms. The probabilistic model is based on the following fundamental assumption. Assumption (Probabilistic Principle) Given a user query q and a document d3 in the collection, the probabilistic model tries to estimate the probability that the user will find the document d3 interesting (i.e., relevant). The model assumes that this probability of relevance depends on the query and the document representations only. Further, the model assumes that there is a subset of all documents which the user prefers as the answer set for the query q. Such an ideal answer set is labeled R and should maximize the overall probability of relevance to the user. Documents in the set R are predicted to be relevant to the query. Documents not in this set are predicted to be non-relevant This assumption is quite troublesome because it does not state explicitly how to compute the probabilities of relevance. In fact, not even the sample space which is to be used for defining such probabilities is given. Given a query qy the probabilistic model assigns to each document dj, as a measure of its similarity to the query, the ratio P{d3 relevant-to q)/P(d3 non-relevant-to q) which computes the odds of the document d3 being relevant to the query q. Taking the odds of relevance as the rank minimizes the probability of an erroneous judgement [282, 785]. Definition For the probabilistic model, the index term weight variables are all binary i.e., w^j £ {0,1}, Wi,q Ä {0,1}. A query q is a subset of index tenn^ Let R be the set of documents known (or initially guessed) to be relevant Let R be the complement of R (i.e., the set of non-relevant documents). Let P(R\dj) 32        MODELING be the probability that the document dj is relevant to the query q and P(R\dj) be the probability that dj is non-relevant to q. The similarity sim(d3,q) of the document dj to the query q is defined as the ratio Using Bayes' rule, .   ,_     .      P(dj\R)xP{R) stm(d1)q) =       lx__!------ióiP(dj\R)xP(R) P(dj\R) stands for the probability of randomly selecting the document d3 from the set R of relevant documents. Further, P(R) stands for the probability that a document randomly selectedjrom the entire collection is relevant. The meanings attached to P(dj\R) and P(R) are analogous and complementary. Since P(R) and P(R) are the same for all the documents in the collection, we write, sim(dj,q)    -¶ Assuming independence of index terms, sim(dj,q)    ~         *   J P(kl\R) stands for the probability that the index term ki is present in a document randomly selected from the set R. P(ki\R) stands for the probability that the index term ki is not present in a document randomly selected from the set R. The probabilities associated with the set R have meanings which are analogous to the ones just described. Taking logarithms, recalling that P(ki\R) + P(kt\R) = 1, and ignoring factors which are constant for all documents in the context of the same query, we can finally write   which is a key expression for ranking computation in the probabilistic model. Since we do not know the set R at the beginning, it is necessaryjto devise a method for initially computing the probabilities P(k,\R) and P(kt\R). There are many alternatives for such computation. We discuss a couple of them below. CLASSIC INFORMATION RETRIEVAL        33 In the very beginning (i.e., immediately after the query specification), there are no retrieved documents. Thus, one has to make simplifying assumptions such as: (a) assume that P(ki\R) is constant for all index terms ki (typically, equal to 0.5) and (b) assume that the distribution of index terms among the non-relevant documents can be approximated by the distribution of index terms among all the documents in the collection. These two assumptions yield P(ki\R)    =    0.5 where, as already denned, m is the number of documents which contain the index term ki and N is the total number of documents in the collection. Given this initial guess, we can then retrieve documents which contain query terms and provide an initial probabilistic ranking for them. After that, this initial ranking is improved as follows. Let V be a subset of the documents initially retrieved and ranked by the probabilistic model Such a subset can be defined, for instance, as the top r ranked documents where r is a previously defined threshold. Further, let V% be the subset of V composed of the documents in V which contain the index term k%. For simplicity, we also use V and Vi to refer to the number of elements in these sets (it should always be clear when the used variable refers to the set or to the number of elements in it). For improving the probabilistic ranking, we need to improve our guesses for P(ki\R) and P(ki\R). This can be accomplished with the following assumptions: (a) we can approximate P(ki\R) by the distribution of the index term ki among the documents retrieved so far, and (b) we can approximate P(ki\R) by considering that all the non-retrieved documents are not relevant. With these assumptions, we can write, This process can then be repeated recursively. By doing so, we are able to improve on our guesses for the probabilities P(ki\R) and P(k{\R) without any assistance from a human subject (contrary to the original idea). However, we can also use assistance from the user for definition of the subset V as originally conceived. The last formulas for P(ki\R) and P(ki\R) pose problems for small values of V and Vz which arise in practice (such as V = 1 and Vt =0). To circumvent these problems, an adjustment factor is often added in which yields R)    =    Vi+∞-5 P(ki\R)   = V + l n% - Vj + 0.5 AT - V + 1 34        MODELING An adjustment factor which is constant and equal to 0.5 is not always satisfactory. An alternative is to take the fraction rii/N as the adjustment factor which yields Rgt;         N-V + l This completes our discussion of the probabilistic model. The main advantage of the probabilistic model, in theory, is that documents are ranked in decreasing order of their probability of being relevant. The disadvantages include: (1) the need to guess the initial separation of documents into relevant and non-relevant sets; (2) the fact that the method does not take into account the frequency with which an index term occurs inside a document (i.e., all weights are binary); and (3) the adoption of the independence assumption for index terms. However, as discussed for the vector model, it is not clear that independence of index terms is a bad assumption in practical situations.
mir-0032	2.5.5    Brief Comparison of Classic Models In general, the Boolean model is considered to be the weakest classic method. Its main problem is the inability to recognize partial matches which frequently leads to poor performance. There is some controversy as to whether the probabilistic model outperforms the vector model. Croft performed some experiments and suggested that the probabilistic model provides a better retrieval performance. However, experiments done afterwards by Salton and Buckley refute that claim. Through several different measures, Salton and Buckley showed that the vector model is expected to outperform the probabilistic model with general collections. This also seems to be the dominant thought among researchers, practitioners, and the Web community, where the popularity of the vector model runs high.
mir-0034	2.6.1    Fuzzy Set Model Representing documents and queries through sets of keywords yields descriptions which are only partially related to the real semantic contents of the respective documents and queries. As a result, the matching of a document to the query terms is approximate (or vague). This can be modeled by considering that each ALTERNATIVE SET THEORETIC MODELS        35 query term defines a fuzzy set and that each document has a degree of membership (usually smaller than 1) in this set. This interpretation of the retrieval process (in terms of concepts from fuzzy theory) is the basic foundation of the various fuzzy set models for information retrieval which have been proposed over the years. Instead of reviewing several of these models here, we focus on a particular one whose description fits well with the models already covered in this chapter. Thus, our discussion is based on the fuzzy set model for information retrieval proposed by Ogawa, Morita, and Kobayashi [616]. Before proceeding, we briefly introduce some fundamental concepts. Fuzzy Set Theory Fuzzy set theory [846] deals with the representation of classes whose boundaries are not well defined. The key idea is to associate a membership function with the elements of the class. This function takes values in the interval [0,1] with 0 corresponding to no membership in the class and 1 corresponding to full membership. Membership values between 0 and 1 indicate marginal elements of the class. Thus, membership in a fuzzy set is a notion intrinsically gradual instead of abrupt (as in conventional Boolean logic). Definition A fuzzy subset A of a universe of discourse U is characterized by a membership function \±a ' U ó* [0,1] which associates with each element u of U a number ^a{^) in the interval [0,1]. The three most commonly used operations on fuzzy sets are: the complement of a fuzzy set, the union of two or more fuzzy sets, and the intersection of two or more fuzzy sets. They are defined as follows. Definition Let U be the universe of discourse, A and B be two fuzzy subsets of U, and A be the complement of A relative to U. Also, let u be an element of U. Then, Fuzzy sets are useful for representing vagueness and imprecision and have been applied to various domains. In what follows, we discuss their application to information retrieval. Fuzzy Information Retrieval As discussed in Chapters 5 and 7, one additional approach to modeling the information retrieval process is to adopt a thesaurus (which defines terni re36        MODELING lationships). The basic idea is to expand the set of index terms in the query with related terms (obtained from the thesaurus) such that additional relevant documents (i.e., besides the ones which would be normally retrieved) can be retrieved by the user query. A thesaurus can also be used to model the information retrieval problem in terms of fuzzy sets as follows. A thesaurus can be constructed by defining a term-term correlation matrix c (called keyword connection matrix in [616]) whose rows and columns are associated to the index terms in the document collection. In this matrix lt;?, a normalized correlation factor qj between two terms ki and ki can be defined by where rii is the number of documents which contain the term fe$, n\ is the number of documents which contain the term ki, and n^i is the number of documents which contain both terms. Such a correlation metric is quite common and has been used extensively with clustering algorithms as detailed in Chapter 5. We can use the term correlation matrix c to define a fuzzy set associated to each index term fc$. In this fuzzy set, a document dj has a degree of membership HUj computed as ki g d3 which computes an algebraic sum (here implemented as the complement of a negated algebraic product) over all terms in the document dj. A document dj belongs to the fuzzy set associated to the term ki if its own terms are related to ki. Whenever there is at least one index term ki of dj which is strongly related to the index ki (i.e., c^i ~ 1), then /i^j ~ 1 and the index kz is a good fuzzy index for the document dj. In the case when all index terms of dj are only loosely related to ku the index ki is not a good fuzzy index for dj (i.e., \i^j ~ 0). The adoption of an algebraic sum over all terms in the document dj (instead of the classic max function) allows a smooth transition for the values of the \ix j factor. The user states his information need by providing a Boolean-like query expression. As also happens with the classic Boolean model (see the beginning of this chapter), this query is converted to its disjunctive normal form. For instance, the query [q = ka A ( V -ªfcc)] can be written in disjunctive normal form as [qdnf = (1,1,1) V (1,1,0) V (1,0,0)], where each of the components is a binary weighted vector associated to the tuple (ka, h,kc). These binary weighted vectors are the conjunctive components of Qdnf- Let cc% be a reference to the 2-th conjunctive component. Then, Qdnf = CCi    V   CÄ2   V    ...    V   CCp where p is the number of conjunctive components of qdnf-   The procedure to compute the documents relevant to a query is analogous to the procedure adopted ALTERNATIVE SET THEORETIC MODELS        37 A, CC2   + CC3 Figure 2.5    Fuzzy document sets for the query [q = ka  A   (fegt;   V Ä {1,2,3}, is a conjunctive component. Dq is the query fuzzy set. Each I, I by the classic Boolean model. The difference is that here we deal with fuzzy (instead of crispy or Boolean) sets. We proceed with an example. Consider again the query [q = ka A (fe V -ifcc)]. Let Da be the fuzzy set of documents associated to the index ka. This set is composed, for instance, by the documents dj which have a degree of membership /xaj greater than a predefined threshold K. Further, let Da be the complement of the set Da. The fuzzy set Da is associated to kai the negation of the index term ka. Analogously, we can define fuzzy sets D^ and Dc associated to the index terms fc and kC) respectively. Figure 2.5 illustrates this example. Since the sets are all fuzzy, a document dd might belong to the set Da, for instance, even if the text of the document d3 does not mention the index ka. The query fuzzy set Dq is a union of the fuzzy sets associated with the three conjunctive components of qdnf (which are referred to as cci, CC2, and CC3). The membership /iqj of a document dj in the fuzzy answer set Dq is computed as follows. 2=1 =      1 - (I- {laJ^bjVcj) X X (1 - flaJ(l - M6,j where fiij, i Ä {a, h, c}, is the membership of dj in the fuzzy set associated with k%. As already observed, the degree of membership in a disjunctive fuzzy set is computed here using an algebraic sum, instead of the more common max function. Further, the degree of membership in a conjunctive fuzzy set is computed here using an algebraic product, instead of the more common min function. This adoption of algebraic sums and products yields degrees of membership which 38        MODELING vary more smoothly than those computed using the min and max functions and thus seem more appropriate to an information retrieval system. This example illustrates how this fuzzy model ranks documents relative to the user query. The model uses a term-term correlation matrix to compute correlations between a document dj and its fuzzy index terms. Further, the model adopts algebraic sums and products (instead of max and min) to compute the overall degree of membership of a document dj in the fuzzy set defined by the user query. Ogawa, Morita, and Kobayashi [616] also discuss how to incorporate user relevance feedback into the model but such discussion is beyond the scope of this chapter. Fuzzy set models for information retrieval have been discussed mainly in the literature dedicated to fuzzy theory and are not popular among the information retrieval community. Further, the vast majority of the experiments with fuzzy set models has considered only small collections which make comparisons difficult to make at this time.
mir-0035	2.6.2    Extended Boolean Model Boolean retrieval is simple and elegant. However, since there is no provision for term weighting, no ranking of the answer set is generated. As a result, the size of the output might be too large or too small (see Chapter 10 for details on this issue). Because of these problems, modern information retrieval systems are no longer based on the Boolean model. In fact, most of the new systems adopt at their core some form of vector retrieval. The reasons are that the vector space model is simple, fast, and yields better retrieval performance. One alternative approach though is to extend the Boolean model with the functionality of partial matching and term weighting. This strategy allows one to combine Boolean query formulations with characteristics of the vector model. In what follows, we discuss one of the various models which are based on the idea of extending the Boolean model with features of the vector model. The extended Boolean model, introduced in 1983 by Salton, Fox, and Wu [703], is based on a critique of a basic assumption in Boolean logic as follows. Consider a conjunctive Boolean query given by q ó kx A ky. According to the Boolean model, a document which contains either the term kx or the term ky is as irrelevant as another document which contains neither of them. However, this binary decision criteria frequently is not in accordance with common sense. An analogous reasoning applies when one considers purely disjunctive queries. When only two terms are considered, we can plot queries and documents in a two-dimensional map as shown in Figure 2.6. A document dj is positioned in this space through the adoption of weights wxj and wyj associated with the pairs [kx,dj] and [ky,dj], respectively. We assume that these weights are normalized and thus lie between 0 and 1. For instance, these weights can be computed as normalized tf-idf factors as follows. = /          idf* max,   idf * (0,1) (0,0) ALTERNATIVE SET THEORETIC MODELS        39 (1,1) ky  (1.0)   (0,0)! (1,0) * x                                                                * x Figure 2.6    Extended Boolean logic considering the space composed of two terms kx and ky only. where, as defined by equation 2.3, fXJ is the normalized frequency of term kx in document dj and idj% is the inverse document frequency for a generic term k{. For simplicity, in the remainder of this section, we refer to the weight wx^ as x, to the weight wyj as y, and to the document vector d3 = (wx,3,wy,3) as the point dj = (x,y). Observing Figure 2.6 we notice two particularities. First, for a disjunctive query qor ó kxV fcy, the point (0, 0) is the spot to be avoided. This suggests taking the distance from (0,0) as a measure of similarity with regard to the query qor. Second, for a conjunctive query qand = kx A ky, the point (1,1) is the most desirable spot. This suggests taking the complement of the distance from the point (1,1) as a measure of similarity with regard to the query qand-Furthermore, such distances can be normalized which yields, sim(qorid)    = sim(qandid)    =    1If the weights are all Boolean (i.e., wxj £ {0,1}), a document is always positioned in one of the four corners (i.e., (0,0), (0,1), (1,0), or (1,1)) and the values for sim(qor,d) are restricted to 0, l/\/2, and 1. Analogously, the values for sim(qand,d) are restricted to 0, 1 ó l/\/2, and 1. Given that the number of index terms in a document collection is t, the Boolean model discussed above can be naturally extended to consider Euclidean distances in a t-dimensional space. However, a more comprehensive generalization is to adopt the theory of vector norms as follows. The p-norrn model generalizes the notion of distance to include not only Euclidean distances but also p-distances, where 1 lt; p lt; oc is a newly introduced parameter whose value must be specified at query time. A generalized disjunctive 40        MODELING query is now represented by qor ó a-i   v     A/2   v     ... v   /im Analogously, a generalized conjunctive query is now represented by qand = h  Ap k2 Ap   ... Ap km The respective query-document similarities are now given by t{ + x\ + ... + x m sim{qand)dj)    =    1 - I 771 where each X{ stands for the weight w^d associated to the pair [ki,d3]. The p norm as defined above enjoys a couple of interesting properties as follows. First, when p = 1 it can be verified that .     /           , N           .      /              -, \        Xi   {-          \  X sim(qor,dj) = sim{qand,dj) = Second, when p = oo it can be verified that sim(qor, dj) = sim(qand,d3) = min[xi) Thus, for p = 1, conjunctive and disjunctive queries are evaluated by a sum of term-document weights as done by vector-based similarity formulas (which compute the inner product). Further, for p = oo, queries are evaluated according to the formalism of fuzzy logic (which we view as a generalization of Boolean logic). By varying the parameter p between 1 and infinity, we can vary the p-norm ranking behavior from that of a vector-like ranking to that of a Boolean-like ranking. This is quite powerful and is a good argument in favor of the extended Boolean model. The processing of more general queries is done by grouping the operators in a predefined order. For instance, consider the query q = (fci Ap 2) Vp k%. The similarity sim(q, d3) between a document dj and this query is then computed as /        /                      \ {!-[--------2--------)    )   +x3 [gt;¶-{-------2-------;   ; t4 sim{q,d) = V                                           / This procedure can be applied recursively no matter the number of AND/OR operators. ALTERNATIVE ALGEBRAIC MODELS        41 One additional interesting aspect of this extended Boolean model is the possibility of using combinations of different values of the parameter p in a same query request. For instance, the query (fci V2 k2) A∞∞ fc3 could be used to indicate that k\ and k2 should be used as in a vector system but that the presence of ks is required (i.e., the conjunction is interpreted as a Boolean operation). Despite the fact that it is not clear whether this additional functionality has any practical impact, the model does allow for it and does so in a natural way (without the need for clumsy extensions to handle special cases). We should also observe that the extended Boolean model relaxes Boolean algebra interpreting Boolean operations in terms of algebraic distances. In this sense, it is really a hybrid model which includes properties of both the set theoretic models and the algebraic models. For simplicity, we opted for classifying the model as a set theoretic one. The extended Boolean model was introduced in 1983 but has not been used extensively. However, the model does provide a neat framework and might reveal itself useful in the future.
mir-0036	2.7    Alternative Algebraic Models In this section, we discuss three alternative algebraic models namely, the generalized vector space model, the latent semantic indexing model, and the neural network model.
mir-0037	2.7.1    Generalized Vector Space Model As already discussed, the three classic models assume independence of index terms. For the vector model, this assumption is normally interpreted as follows. Definition     Let ki be a vector associated with the index term kz. Independence of index terms in the vector model implies thai the set of vectors {k\, k2, kt) is linearly independent and forms a basis for the subspace of interest.   The dimension of this space is the number t of index terms in the collection. Frequently, independence among index terms is interpreted in a more restrictive sense to mean pairwise orthogonality among the index term vectors i.e., to mean that for each pair of index term vectors k% and k3 we have ki ïkJ =0. In 1985, however, Wong, Ziarko, and Wong [832] proposed an interpretation in which the index term vectors are assumed linearly independent but are not pairwise orthogonal. Such interpretation leads to the generalized vector space model which we now discuss. 42        MODELING In the generalized vector space model, two index term vectors might be non-orthogonal. This means that index term vectors are not seen as the orthogonal vectors which compose the basis of the space. Instead, they are themselves composed of smaller components which are derived from the particular collection at hand as follows. Definition Given the set {k\, k2, ..., kt} of index terms in a collection, as before, let Wij be the weight associated with the term-document pair [fc$, dj]. If the Wii3 weights are all binary then all possible patterns of term co-occurrence (inside documents) can be represented by a set of2t minterms given by mi = (0,0,..., 0), rn2 = (1,0,. ..,0)7 ..., m2t = (1,1,. ..,1). Let gi(rrij) return the weight {0,1} of the index term kz in the minterm nrij. Thus, the minterm mi (for which gi(mi) = 0, for all i) points to the documents containing none of the index terms. The minterm m2 (for which gi(m2) = 1, for i = l, and #2(7712) = 0, for i gt; 1) points to the documents containing solely the index term k±. Further, the minterm m2t points to the documents containing all the index terms. The central idea in the generalized vector space model is to introduce a set of pairwise orthogonal vectors rhi associated with the set of minterms and to adopt this set of vectors as the basis for the subspace of interest. Definition     Let us define the following set of rhi vectors fax    =    (1,0,...,0,0) m2    =    (0,1,...,0,0) rh2t    =    (0,0,...,0,1) where each vector rhi is associated with the respective minterm m*. Notice that rhi ï fh3 =0 for all i ¶=£ j and thus the set of rhi vectors is, by definition, pairwise orthogonal. This set of rhi vectors is then taken as the orthonormal basis for the generalized vector space model. Pairwise orthogonality among the rhi vectors does not imply independence among the index terms. On the contrary, index terms are now correlated by the rhi vectors. For instance, the vector rh^ is associated with the minterm m4 = (l,l,...,0) which points to the documents in the collection containing the index terms fcx, k2, and no others. If such documents do exist in the collection under consideration then we say that the minterm m4 is active and that a dependence between the index terms A"i and ^2 is induced. If we consider this point more carefully, we notice that the generalized vector model adopts as a basic foundation the idea that co-occurrence of index terms inside documents in the collection induces dependencies among these index terms. Since this is an idea which was introduced many years before the generalized vector space ALTERNATIVE ALGEBRAIC MODELS        43 model itself, novelty is not granted. Instead, the main contribution of the model is the establishment of a formal framework in which dependencies among index terms (induced by co-occurrence patterns inside documents) can be nicely represented. The usage of index term dependencies to improve retrieval performance continues to be a controversial issue. In fact, despite the introduction in the 1980s of more effective algorithms for incorporating term dependencies (see Chapter 5), there is no consensus that incorporation of term dependencies in the model yields effective improvement with general collections. Thus, it is not clear that the framework of the generalized vector model provides a clear advantage in practical situations. Further, the generalized vector model is more complex and computationally more expensive than the classic vector model. To determine the index term vector ki associated with the index term ki, we simply sum up the vectors for all minterms mr in which the term kz is in state 1 and normalize. Thus, ft,    = Y Z-^Vr, d3  | 9i(dj)=9i(mr) for all I These equations provide a general definition for the index term vector ki in terms of the mr vectors. The term vector ki collects all the mr vectors in which the index term ki is in state 1. For each rhT vector, a correlation factor a,r is defined. Such a correlation factor sums up the weights Wij associated with the index term ki and each document dj whose term occurrence pattern coincides exactly with that of the minterm mr. Thus, a minterm is of interest (in which case it is said to be active) only if there is at least one document in the collection which matches its term occurrence pattern. This implies that no more than N minterms can be active, where N is the number of documents in the collection. Therefore, the ranking computation does not depend on an exponential number of minterms as equation 2.5 seems to suggest. Notice that the internal product ki ï kj can now be used to quantify a degree of correlation between the index terms ki and kj. For instance, Ki ï Kj ó-                      y                       £i,r ^ ^j,r Vr i 0t(mr) = l A g3{mr)-l which, as later discussed in Chapter 5, is a good technique for quantifying index term correlations. In the classic vector model, a document dj and a user query q are expressed by d3 = J2viwiJ ^ anc* % = Ylviwi,g ^i, respectively. In the generalized vector space model, these representations can be directly translated to the space of minterm vectors mr by applying equation 2.5.  The resultant dj and q vectors 44        MODELING are then used for computing the ranking through a standard cosine similarity function. The ranking that results from the generalized vector space model combines the standard Wij term-document weights with the correlation factors air-However, since the usage of term-term correlations does not necessarily yield improved retrieval performance, it is not clear in which situations the generalized model outperforms the classic vector model. Furthermore, the cost of computing the ranking in the generalized model can be fairly high with large collections because, in this case, the number of active minterms (i.e., those which have to be considered for computing the ki vectors) might be proportional to the number of documents in the collection. Despite these drawbacks, the generalized vector model does introduce new ideas which are of importance from a theoretical point of view.
mir-0038	2.7.2    Latent Semantic Indexing Model As discussed earlier, summarizing the contents of documents and queries through a set of index terms can lead to poor retrieval performance due to two effects. First, many unrelated documents might be included in the answer set. Second, relevant documents which are not indexed by any of the query keywords are not retrieved. The main reason for these two effects is the inherent vagueness associated with a retrieval process which is based on keyword sets. The ideas in a text are more related to the concepts described in it than to the index terms used in its description. Thus, the process of matching documents to a given query could be based on concept matching instead of index term matching. This would allow the retrieval of documents even when they are not indexed by query index terms. For instance, a document could be retrieved because it shares concepts with another document which is relevant to the given query. Latent semantic indexing is an approach introduced in 1988 which addresses these issues (for clustering-based approaches which also address these issues, see Chapter 5). The main idea in the latent semantic indexing model [287] is to map each document and query vector into a lower dimensional space which is associated with concepts. This is accomplished by mapping the index term vectors into this lower dimensional space. The claim is that retrieval in the reduced space may be superior to retrieval in the space of index terms. Before proceeding, let us define basic terminology. Definition    As before, let t he the number of index terms in the collection and N be the total number of documents. Define M=(Mij) as a term-document association matrix with t rows and N columns.   To each element M^  of this matrix is assigned a weight W{^ associated with the term-document pair {kt.dj\. This u\,j weight could be generated using the tf-idf weighting technique common in the classic vector space model ALTERNATIVE ALGEBRAIC MODELS        45 Latent semantic indexing proposes to decompose the M association matrix in three components using singular value decomposition as follows. M = KSD1 The matrix K is the matrix of eigenvectors derived from the term-to-term correlation matrix given by MM1 (see Chapter 5). The matrix Dl is the matrix of eigenvectors derived from the transpose of the document-to-document matrix given by MlM. The matrix S is an r x r diagonal matrix of singular values where r ó min(t, N) is the rank of M. Consider now that only the s largest singular values of S are kept along with their corresponding columns in K and Dt (i.e., the remaining singular values of 5 are deleted). The resultant Ms matrix is the matrix of rank s which is closest to the original matrix M in the least square sense. This matrix is given by Ms = KsSsDi s where 5, 5 lt; r, is the dimensionality of a reduced concept space. The selection of a value for s attempts to balance two opposing effects. First, s should be large enough to allow fitting all the structure in the real data. Second, s should be small enough to allow filtering out all the non-relevant representational details (which are present in the conventional index-term based representation). The relationship between any two documents in the reduced space of dimensionality 5 can be obtained from the MlMs matrix given by =    (KlS.Dta)tKtSaDta =   DsSsSsDl =    (DsSs)(DsSsy In the above matrix, the (i,j) element quantifies the relationship between documents dx and dj. To rank documents with regard to a given user query, we simply model the query as a pseudo-document in the original M term-document matrix. Assume the query is modeled as the document with number 0. Then, the first row in the matrix MlMs provides the ranks of all documents with respect to this query. Since the matrices used in the latent semantic indexing model are of rank s, s ´t, and s ´ N, they form an efficient indexing scheme for the documents in the collection. Further, they provide for elimination of noise (present in index term-based representations) and removal of redundancy. The latent semantic indexing model introduces an interesting conceptualization of the information retrieval problem based on the theory of singular value decomposition. Thus, it has its value as a new theoretical framework. Whether it is superior in practical situations with general collections remains to be verified. 46 MODELING Query Terms Document Terms Documents Figure 2.7    A neural network model for information retrieval.
mir-0039	2.7.3    Neural Network Model In an information retrieval system, document vectors are compared with query vectors for the computation of a ranking. Thus, index terms in documents and queries have to be matched and weighted for computing this ranking. Since neural networks are known to be good pattern matchers, it is natural to consider their usage as an alternative model for information retrieval. It is now well established that our brain is composed of billions of neurons. Each neuron can be viewed as a basic processing unit which, when stimulated by input signals, might emit output signals as a reactive action. The signals emitted by a neuron are fed into other neurons (through synaptic connections) which can themselves emit new output signals. This process might repeat itself through several layers of neurons and is usually referred to as a spread activation process. As a result, input information is processed (i.e., analyzed and interpreted) which might lead the brain to command physical reactions (e.g., motor actions) in response. A neural network is an oversimplified graph representation of the mesh of interconnected neurons in a human brain. The nodes in this graph are the processing units while the edges play the role of the synaptic connections. To simulate the fact that the strength of a synaptic connection in the human brain changes over time, a weight is assigned to each edge of our neural network. At each instant, the state of a node is defined by its activation level (which is a function of its initial state and of the signals it receives as input). Depending on its activation level, a node A might send a signal to a neighbor node B. The strength of this signal at the node B depends on the weight associated with the edge between the nodes .A and B. A neural network for information retrieval can be defined as illustrated in Figure 2.7. The model depicted here is based on the work in [815]. We first ALTERNATIVE ALGEBRAIC MODELS        47 observe that the neural network in Figure 2.7 is composed of three layers: one for the query terms, one for the document terms, and a third one for the documents themselves. Observe the similarity between the topology of this neural network and the topology of the inference and belief networks depicted in Figures 2.9 and 2.10. Here, however, the query term nodes are the ones which initiate the inference process by sending signals to the document term nodes. Following that, the document term nodes might themselves generate signals to the document nodes. This completes a first phase in which a signal travels from the query term nodes to the document nodes (i.e., from the left to the right in Figure 2.7). The neural network, however, does not stop after the first phase of signal propagation. In fact, the document nodes in their turn might generate new signals which are directed back to the document term nodes (this is the reason for the bidirectional edges between document term nodes and document nodes). Upon receiving this stimulus, the document term nodes might again fire new signals directed to the document nodes, repeating the process. The signals become weaker at each iteration and the spread activation process eventually halts. This process might activate a document di even when such a document does not contain any query terms. Thus, the whole process can be interpreted as the activation of a built-in thesaurus. To the query term nodes is assigned an initial (and fixed) activation level equal to 1 (the maximum). The query term nodes then send signals to the document term nodes which are attenuated by normalized query term weights whq. For a vector-based ranking, these normalized weights can be derived from the weights Wi:q defined for the vector model by equation 2.4. For instance, where the normalization is done using the norm of the query vector. Once the signals reach the document term nodes, these might send new signals out directed towards the document nodes. These signals are attenuated by normalized document term weights Wij derived from the weights Wij defined for the vector model by equation 2.3. For instance, where the normalization is done using the norm of the document vector. The signals which reach a document node are summed up. Thus, after the first round of signal propagation, the activation level of the document node associated to the document dj is given by x /=rt v^i=i 48         MODELING which is exactly the ranking provided by the classic vector model. To improve the retrieval performance, the network continues with the spreading activation process after the first round of propagation. This modifies the initial vector ranking in a process analogous to a user relevance feedback cycle (see Chapter 5). To make the process more effective, a minimum activation threshold might be defined such that document nodes below this threshold send no signals out. Details can be found in [815]. There is no conclusive evidence that a neural network provides superior retrieval performance with general collections. In fact, the model has not been tested extensively with large document collections. However, a neural network does present an alternative modeling paradigm. Further, it naturally allows retrieving documents which are not initially related to the query terms ó an appealing functionality.
mir-0040	2.8    Alternative Probabilistic Models One alternative which has always been considered naturally appealing for quantifying document relevance is the usage of probability theory and its main streams. One such stream which is gaming increased attention concerns the Bayesian belief networks which we now discuss. Bayesian (belief) networks are useful because they provide a clean formalism for combining distinct sources of evidence (past queries, past feedback cycles, and distinct query formulations) in support of the rank for a given document. This combination of distinct evidential sources can be used to improve retrieval performance (i.e., to improve the 'quality' of the ranked list of retrieved documents) as has been demonstrated in the work of Turtle and Croft [771]. In this chapter we discuss two models for information retrieval based on Bayesian networks. The first model is called inference network and provides the theoretical basis for the retrieval engine in the Inquery system [122]. Its success has attracted attention to the use of Bayesian networks with information retrieval systems. The second model is called belief network and generalizes the first model. At the end, we briefly compare the two models. Our discussion below uses a style which is quite distinct from that employed by Turtle and Croft in their original writings. Particularly, we pay more attention to probabilistic argumentation during the development of the model. We make a conscious effort of consistently going back to the Bayesian formalism for motivating the major design decisions. It is our view that such an explanation strategy allows for a more precise argumentation which facilitates the task of grasping the subtleties involved. Before proceeding, we briefly introduce Bayesian networks.
mir-0041	2.8.1    Bayesian Networks Bayesian networks [630] are directed acyclic graphs (DAGs) in which the nodes represent random variables, the arcs portray causal relationships between these ALTERNATIVE PROBABILISTIC MODELS 49 variables, and the strengths of these causal influences are expressed by conditional probabilities. The parents of a node (which is then considered as a child node) are those judged to be direct causes for it. This causal relationship is represented in the DAG by a link directed from each parent node to the child node. The roots of the network are the nodes without parents. Let Xi be a node in a Bayesian network G and Tx% be the set of parent nodes of #$. The influence of Yx% on Xi can be specified by any set of functions Fi(xi,FXl) that satisfy lt;, rSl)   = o  lt;  Fi(xurXt)   lt;   i where X{ also refers to the states of the random variable associated to the node This specification is complete and consistent because the product Y[ii Fi(xi,Tx constitutes a joint probability distribution for the nodes in G. Figure 2.8    An example of a Bayesian network. Figure 2.8 illustrates a Bayesian network for a joint probability distribution P(xi,X2,X3,X4,x5). In this case, the dependencies declared in the network allow the natural expression of the joint probability distribution in terms of local conditional probabilities (a key advantage of Bayesian networks) as follows. The probability P(#i) is called the prior probability for the network and can be used to model previous knowledge about the semantics of the application.
mir-0042	2.8.2    Inference Network Model The two most traditional schools of thought in probability are based on the frequentist view and the epistemological view. The frequentist view takes probability as a statistical notion related to the laws of chance. The epistemological 50 MODELING Figure 2.9    Basic inference network model. view interprets probability as a degree of belief whose specification might be devoid of statistical experimentation. This second viewpoint is important because we frequently refer to probabilities in our daily lives without a clear definition of the statistical experiment which yielded those probabilities. The inference network model [772, 771] takes an epistemological view of the information retrieval problem. It associates random variables with the index terms, the documents, and the user queries. A random variable associated with a document dj represents the event of observing that document (i.e., the model assumes that documents are being observed in the search for relevant documents). The observation of the document dj asserts a belief upon the random variables associated with its index terms. Thus, observation of a document is the cause for an increased belief in the variables associated with its index terms. Index term and document variables are represented as nodes in the network. Edges are directed from a document node to its term nodes to indicate that observation of the document yields improved belief on its term nodes. The random variable associated with the user query models the event that the information request specified by the query has been met. This random variable is also represented by a node in the network. The belief in this (query) node is a function of the beliefs in the nodes associated with the query terms. Thus, edges are directed from the index term nodes to the query node. Figure 2.9 illustrates an inference network for information retrieval. The document d3 has A?2, kii and kt as its index terms. This is modeled by directing the edges from the node d3 to the nodes 2, k^ and kt. The query q is composed of the index terms k\, 2, and fcj. This is modeled by directing the edges from the nodes k\% 2* and k{ to the node q. Notice that Figure 2.9 also includes three extra nodes: Q2, #1, and /. The nodes $2 and qi are used to model an (alternative) Boolean formulation qj for the query q (in this case, q\ = (k\ A klt;i) V kt).  When such ALTERNATIVE PROBABILISTIC MODELS        51 (additional) information is available, the user information need / is supported by both q and q\. In what follows, we concentrate our attention on the support provided to the query node q by the observation of a document d3. Later on, we discuss the impact of considering multiple query representations for an information need /. This is important because, as Turtle and Croft have demonstrated, a keyword-based query formulation (such as q) can be combined with a Boolean-like query formulation (such as q{) to yield improved retrieval performance for the same information need. The complete inference network model also includes text nodes and query concept nodes but the model discussed above summarizes the essence of the approach. A simplifying assumption is made which states that all random variables in the network are binary. This seems arbitrary but it does simplify the modeling task and is general enough to capture all the important relationships in the information retrieval problem. Definition Let k be a t-dimensional vector defined by k = (i, 2, ï ï ï ,kt) where k\, k2, ..., kt are binary random variables i.e., k% Ä {0,1}. These variables define the 2l possible states for k. Further, let d3 be a binary random variable associated with a document dj and let q be a binary random variable associated with the user query. Notice that q is used to refer to the query, to the random variable associated with it, and to the respective node in the network. This is also the case for dj and for each index term ki. We allow this overloading in syntax because it should always be clear whether we are referring to either the query or to its associated random variable. The ranking of a document dj with respect to a query q is a measure of how much evidential support the observation of dj provides to the query q. In an inference network, the ranking of a document dj is computed as P(q A dj) where q and d3 are short representations for q = 1 and dj = 1, respectively. In general, such a ranking is given by Vfc  VA: P(qAdj)    =    l-P(qAdj) 52        MODELING which is obtained by basic conditioning and the application of Bayes' rule. Notice that P{q\dj x k) = P(q\k) because the ki nodes separate the query node q from the document node d3.  Also, the notation q A dj is a short representation for The instantiation of a document node d3 (i.e., the observation of the document) separates its children index term nodes making them mutually independent (see Bayesian theory for details). Thus, the degree of belief asserted to each index term node ki by instantiating the document node dj can be computed separately. This implies that P(k\d3) can be computed in product form which yields (from equation 2.6), P(q/\dj)    =    TP(q\k)x P{k%\dj) ] x P(d3)      (2.7) P{q/\d3)    =    l-P(q/\dj where P{ki\d3) = 1 -P{kl\d3). Through proper specification of the probabilities P(q\k)i P(kt\dj), and P(dj), we can make the inference network cover a wide range of useful information retrieval ranking strategies. Later on, we discuss how to use an inference network to subsume the Boolean model and tf-idf ranking schemes. Let us first cover the specification of the P(d3) probabilities. Prior Probabilities for Inference Networks Since the document nodes are the root nodes in an inference network, they receive a prior probability distribution which is of our choosing. This prior probability reflects the probability associated to the event of observing a given document dj (to simplify matters, a single document node is observed at a time). Since we have no prior preferences for any document in particular, we usually adopt a prior probability distribution which is uniform. For instance, in the original work on inference networks [772, 771], the probability of observing a document d3 is set to l/N where N is the total number of documents in the system. Thus, The choice of the value 1/Ar for the prior probability P{dj) is a simple and natural specification given that our collection is composed of N documents. However, other specifications for P(d3) might also be interesting. For instance, in the cosine formula of the vector model, the contribution of an index term to ALTERNATIVE PROBABILISTIC MODELS        53 the rank of the document d3 is inversely proportional to the norm of the vector dj. The larger the norm of the document vector, the smaller is the relative contribution of its index terms to the document final rank. This effect can be taken into account through proper specification of the prior probabilities P(dj) as follows. =   \-P{d3) where \d3 | stands for the norm of the vector d3. Therefore, in this case, the larger the norm of a document vector, the smaller its associated prior probability. Such specification reflects a prior knowledge that we have about the behavior of vector-based ranking strategies (which normalize the ranking in the document space). As commanded by Bayesian postulates, previous knowledge of the application domain should be asserted in the specification of the priors in the network, as we have just done. Inference Network for the Boolean Model Here we demonstrate how an inference network can be tuned to subsume the Boolean model. First, for the Boolean model, the prior probabilities P{d3) are all set to I/AT because the Boolean model makes no prior distinction on documents. Thus, p(d3) = i Regarding the conditional probabilities P(ki\dj) and P(q\k), the specification is as follows. 0    otherwise P(kt\dj)    =    l-Pihldj) which basically states that, when the document dj is being observed, only the nodes associated with the index terms of the document dj are active (i.e., have an induced probability greater than 0). For instance, observation of a document node dj whose term vector is composed of exactly the index terms A:2, kt, and kt (see Figure 2.9) activates the index term nodes {k2,kt,kt} and no others. Once the beliefs in the index term nodes have been computed, we can use them to compute the evidential support they provide to the user query q as 54        MODELING follows. P(q\k)    =    /  1    if 3$ô   I   (Qcc Ä qdnf) A(Vfct, 9i{k) = gi(qcc)) 1  ;          \ 0    otherwise P{q\k)    =    1-P(lt;?|fc) where qcc and gdn/ are as defined for the classic Boolean model. The above equation basically states that one of the conjunctive components of the user query (expressed in disjunctive normal form) must be matched by the set of active terms in k (in this case, those activated by the document observed) exactly. Substituting the above definitions for P(q\h), P(ki\dj), and P(dj) into equation 2.7, it can be easily shown that the set of documents retrieved is exactly the set of documents returned by the Boolean model as defined in section 2.5.2. Thus, an inference network can be used to subsume the Boolean model without difficulties. Inference Network for tf-idf Ranking Strategies For tf-idf ranking strategies (i.e., those related to the vector model), we adopt prior probabilities which reflect our prior knowledge of the importance of document normalization. Thus, we set the prior P(dj) to l/|dj| as follows. P(dj)    =    -1-                                                                     (2.8) P(dj)    =    1 Further, we have to decide where to introduce the tf (term-frequency) and the idf (inverse-document-frequency) factors in the network. For that purpose, we consider that the tf and idf factors are normalized (as in equation 2.1) and that these normalized factors are strictly smaller than 1. We first focus on capturing the impact of the tf factors in the network. Normalized tf factors are taken into account through the beliefs asserted upon the index term nodes as follows. P(ki\dj)    =   fij                                                                   (2.9) These equations simply state that, according to the observed document dj, the relevance of a term hi is determined by its normalized term-frequency factor. We are now in a position to consider the influence of idf factors. They are taken into account through the specification of the impact of index term nodes ALTERNATIVE PROBABILISTIC MODELS        55 on the query node. Define a vector ki given by, ki=k  \   (9i(k) = 1 A V^ 9j{%) = 0) The vector k% is a reference to the state of the vector k in which the node ki is active and all others are inactive. The motivation is that tf-idf ranking strategies sum up the individual contributions of index terms and ki allows us to consider the influence of the term ki in isolation. We are now ready to define the influence of the index term nodes in the query node q as _    I idfi    if  k = hA gi(q) = 1                                     ( "   \ o     if j^£vft($) = o                                 (2*10) P(q\k)    =    1-P(q\k) where idfz here is a normalized version of the idf factor defined in equation 2.2. By applying equations 2.8, 2.9, and 2.10 to equation 2.7, we can then write JJ x ói P{ds) I =    f IJPCJfeil^) ) x P{dj) x Y^p(ki\di) x __         I   '       y    _____    V                          \                             T ¶         V   1 /7 T      V    óóóóóóóó        Ky t   A       15       lt;*ª                          7                             11   1   A   6Ci/ 2   ^ Mjl       Vt      rf- ~^A    (g=1                        1~^J =    l-PfgAd,-) which provides a tf-idf-like ranking. Unfortunately, Cj depends on a product of the various probabilities P(ki\dj) which vary from document to document and thus the ranking is distinct from the one provided by the vector model. Despite this peculiarity in the tf-idf ranking generated, it has been shown that an inference network is able to provide good retrieval performance with general collections. The reason is that the network allows us to consistently combine evidence from distinct evidential sources to improve the final ranking, as we now discuss. Combining Evidential Sources In Figure 2.9, the first query node q is the standard keyword-based query formulation for the user information need /. The second query q\ is a Boolean-iike query formulation for the same information need (i.e., an additional evidential source collected from a specialist). The joint support these two query formulations provide to the information need node / can be modeled through, for 56        MODELING instance, an OR operator (i.e., / = q V qi). In this case, the ranking provided by the inference network is computed as, P(I A dj)    =   ^2 PCW x p(k\do) x p(ds) k =    X^1 ~ P(5l*) P(^)) x P(^ldi) x P(^) k which might yield a retrieval performance which surpasses the retrieval performance obtained with each of the query nodes in isolation as demonstrated in [771].
mir-0043	2.8.3    Belief Network Model The belief network model, introduced in 1996 by Ribeiro-Neto and Muntz [674], is also based on an epistemological interpretation of probabilities. However, it departs from the inference network model by adopting a clearly defined sample space. As a result, it yields a slightly different network topology which provides a separation between the document and the query portions of the network. This is the main difference between the two models and one which has theoretical implications. The Probability Space The probability space adopted was first introduced by Wong and Yao [830] and works as follows. All documents in the collection are indexed by index terms and the universe of discourse U is the set K of all index terms. Definition The set K = {i,..., kt} is the universe of discourse and defines the sample space for the belief network model Let u C K be a subset of K. To each subset u is associated a vector k such that gi(k) = 1 4=$- ki Ä u. The introduction of the vector k is useful to keep the notation compatible with the one which has been used throughout this chapter. Each index term is viewed as an elementary concept and AT as a concept space.   A concept u is a subset of K and might represent a document in the collection or a user query.   In a belief network, set relationships are specified using random variables as follows. Definition    7b each index term k% is associated a binary random variable which is also referred to as ki. The random variable kt is set to 1 to indicate that the index k% is a member of a concept/set represented by k. ALTERNATIVE PROBABILISTIC MODELS        57 This association of concepts with subsets is useful because it allows us to express the logical notions of conjunction, disjunction, negation, and implication as the more familiar set-theoretic notions of intersection, union, complementation, and inclusion. Documents and user queries can be defined as concepts in the sample space K as follows. Definition A document d3 in the collection is represented as a concept (i.e., a set) composed of the terms which are used to index dj. Analogously, a user query q is represented as a concept composed of the terms which are used to index q. A probability distribution P is defined over K as follows. Let c be a generic concept in the space K representing a document or user query. Then, P(c)    =    ^P(c|w)xP(U)                                                         (2.11) P(u)    =    ^- 1                                                                            (2.12) Equation 2.11 defines P(c) as the degree of coverage of the space K by c. Such a coverage is computed by contrasting each of the concepts in K with c (through P(c\u)) and by summing up the individual contributions. This sum is weighted by the probability P{u) with which u occurs in K. Since at the beginning the system has no knowledge of the probability with which a concept u occurs in the space K, we can assume that each u is equally likely which results in equation 2.12. Belief Network Model In the belief network model, the user query q is modeled as a network node to which is associated a binary random variable (as in the inference network model) which is also referred to as q. This variable is set to 1 whenever q completely covers the concept space K. Thus, when we assess P(q) we compute the degree of coverage of the space K by q. This is equivalent to assessing the degree of belief associated with the following proposition: Is it true that q completely covers all possible concepts in K? A document d3 is modeled as a network node with which is associated a binary random variable which is also referred to as d3. This variable is 1 to indicate that dj completely covers the concept space A". When we assess P(dj)% we compute the degree of coverage of the space K by dj. This is equivalent to assessing the degree of belief associated with the following proposition: Is it true that dj completely covers all possible concepts in K? According to the above formalism, the user query and the documents in the collection are modeled as subsets of index terms. Each of these subsets is interpreted as a concept embedded in the concept space K which works as a common sample space.   Furthermore, user queries and documents are modeled 58 MODELING Figure 2.10   Basic belief network model. identically. This is an important observation because it defines the topology of the belief network. Figure 2.10 illustrates our belief network model. As in the inference network model, a query q is modeled as a binary random variable which is pointed to by the index term nodes which compose the query concept. Documents are treated analogously to user queries (i.e., both are concepts in the space K). Thus, contrary to the inference network model, a document node is pointed to by the index term nodes which compose the document. This is the topological difference between the two models and one which has more implications than it seems at first glance. The ranking of a document dj relative to a given query q is interpreted as a concept matching relationship and reflects the degree of coverage provided to the concept dj by the concept q. Assumption In the belief network model, P(dj\q) is adopted as the rank of the document dj with respect to the query q. By the application of Bayes' theorem, we can write P{d3\q) = P(d3 A q)/P(q). Since P(q) is a constant for all documents in the collection, we can write P(dj\q) ~ P(dj A q) i.e., the rank assigned to a document dj is directly proportional to P(dj A q). This last probability is computed through the application of equation 2.11 which yields P(dj\q) In the belief network of Figure 2.10, instantiation of the index term variables logically separates the nodes q and d making them mutually independent (i.e., the document and query portions of the network are logically separated by inALTERNATIVE PROBABILISTIC MODELS stantiation of the index term nodes). Therefore, 59 which can be rewritten as P(k) (2.13) Vfc To complete the belief network we need to specify the conditional probabilities P(q\k) and P(dj\k). Distinct specifications of these probabilities allow the modeling of different ranking strategies (corresponding to different IR models). We now discuss how to specify these probabilities to subsume the vector model. For the vector model, the probabilities P{q\k) and P(d3\k) are specified as follows. Let, hi = k  |   (#(*?) = 1 A as before. Also, P(q\k)    = 0 9j(k)=0) if k = h A gt(q) = 1 otherwise P(q\k)    =    1-P(q\k) Further, define 0 P(dj\k)    =    1-P(dj\k) if k = ki A 9i(dj) = 1 otherwise Then, the ordering of the retrieved documents (i.e., the ranking) defined by P(dj\q) coincides with the ordering generated by the vector model as specified in section 2.5.3. Thus, the belief network model can be tuned to subsume the vector model which cannot be accomplished with the inference network model.
mir-0044	2.8.4    Comparison of Bayesian Network Models There is a close resemblance between the belief network model and the inference network model. However, this resemblance hides important differences between the two models. First, the belief network model is based on a set-theoretic view 60        MODELING of the IR ranking problem and adopts a clearly defined sample space. The inference network model takes a purely epistemological view of the IR problem which is more difficult to grasp (because, for instance, the sample space is not clearly defined). Second, the belief network model provides a separation between the document and the query portions of the network which facilitates the modeling of additional evidential sources such as past queries and past relevance information. Third, as a result of this document-query space separation, the belief network model is able to reproduce any ranking strategy generated by the inference network model while the converse is not true. To see that the belief network ranking subsumes any ranking generated by an inference network, compare equations 2.6 and 2.13. The key distinction is between the terms P(d3\k) and P(k\d3). For the latter, instantiation of the document node dj separates the index term nodes making them mutually independent. Thus, the joint probability P(k\d3) can always be computed as the product of the individual probabilities P(kl\dj). However, the computation of P(dj\k) might be non-decomposable in a product of term-based probabilities. As a result, P(d3\k) can express any probability function denned with P{k\d3) while the converse is not true. One should not infer from the above comparison that the inference network model is not a good model. On the contrary, it has been shown in the literature that the inference network model allows top retrieval performance to be accomplished with general collections. Further, it is the retrieval model used by the Inquery system. The point of the comparison is that, from a theoretical point of view, the belief network model is more general. Also, it provides a separation between the document space and the query space which simplifies the modeling task.
mir-0045	2.8.5    Computational Costs of Bayesian Networks In the inference network model, according to equation 2.6, only the states which have a single document active node are considered. Thus, the cost of computing the ranking is linear on the number of documents in the collection. As with conventional collections, index structures such as inverted files (see Chapter 8) are used to restrict the ranking computation to those documents which have terms in common with the query. Thus, the cost of computing an inference network ranking has the same complexity as the cost of computing a vectorial ranking. In the belief network model, according to equation 2.13, the only states (of the roots nodes) considered (for computing the rank of a document d3) are the ones in which the active nodes are exactly those associated with the query terms. Thus, again, the cost of computing the ranking is linear on the number of documents in the collection. If index structures are used, the cost of computing a belief network ranking has the same complexity as the cost of computing a vectorial ranking. STRUCTURED TEXT RETRIEVAL MODELS        61 Therefore, the Bayesian network models discussed here do not impose significant additional costs for ranking computation. This is so because the networks presented do not include cycles, which implies that belief propagation can be done in a time proportional to the number of nodes in the network.
mir-0046	2.8.6    The Impact of Bayesian Network Models The classic Boolean model is based on a neat formalism but is not very effective for information retrieval. The classic vector model provides improved answer sets but lacks a more formal framework. Many attempts have been made in the past to combine the best features of each model. The extended Boolean model and the generalized vector space model are two well known examples. These past attempts are grounded in the belief that the combination of selected properties from distinct models is a promising approach towards improved retrieval. Bayesian network models constitute modern variants of probabilistic reasoning whose major strength (for information retrieval) is a framework which allows the neat combination of distinct evidential sources to support a relevance judgement (i.e., a numerical rank) on a given document. In this regard, belief networks seem more appropriate than previous approaches and more promising. Further, besides allowing the combination of Boolean and vector features, a belief network can be naturally extended to incorporate evidential information derived from past user sessions [674] and feedback cycles [332]. The inference network model has been successfully implemented in the Inquery retrieval system [122] and compares favorably with other retrieval systems. However, despite these promises, whether Bayesian networks will become popular and widely used for information retrieval remains to be seen.
mir-0047	2.9    Structured Text Retrieval Models Consider a user with a superior visual memory. Such a user might then recall that the specific document he is interested in contains a page in which the string 'atomic holocaust appears in italic in the text surrounding a Figure whose label contains the word 'earth.' With a classic information retrieval model, this query could be expressed as ['atomic holocaust' and 'earth'] which retrieves all the documents containing both strings. Clearly, however, this answer contains many more documents than desired by this user. In this particular case, the user would like to express his query through a richer expression such as same-page (near ("atomic holocausts Figure (label ("earth')))) which conveys the details in his visual recollection. Further, the user might be interested in an advanced interface which simplifies the task of specifying this (now complex) query. This example illustrates the appeal of a query language which allows us to combine the specification of strings (or patterns) with the 62        MODELING specification of structural components of the document. Retrieval models which combine information on text content with information on the document structure are called structured text retrieval models. For a query such as the one illustrated above, a structured text retrieval system searches for all the documents which satisfy the query. Thus, there is no notion of relevance attached to the retrieval task. In this sense, the current structured text retrieval models are data (instead of information) retrieval models. However, the retrieval system could search for documents which match the query conditions only partially. In this situation, the matching would be approximate and some ranking would have to be used for ordering the approximate answers. Thus, a structured text retrieval algorithm can be seen as an information retrieval algorithm for which the issue of appropriate ranking is not well established. In fact, this is an actual, interesting, and open research problem. At the end of the 1980s and throughout the 1990s, various structured text retrieval models have appeared in the literature. Usually, the more expressive the model, the less efficient is its query evaluation strategy. Thus, selection of a structured model for a given application must be exercised with care. A good policy is to select the most efficient model which supports the functionality required by the application in view. Here, we do not survey all the structured text retrieval models. Instead, we briefly discuss the main features of two of them, namely, a model based on non-overlapping lists and a model based on proximal nodes. These two models should provide a good overview of the main issues and tradeoffs in structured text retrieval. We use the term match point to refer to the position in the text of a sequence of words which matches (or satisfies) the user query. Thus, if the user specifies the simple query ['atomic holocaust in Hiroshima'] and this string appears in three positions in the text of a document dj, we say that the document dj contains three match points. Further, we use the term region to refer to a contiguous portion of the text and the term node to refer to a structural component of the document such as a chapter, a section, a subsection, etc. Thus, a node is a region with predefined topological properties which are known both to the author of the document and to the user who searches the document system.
mir-0048	2.9.1    Mode! Based on Non-Overlapping Lists Burkowski [132, 133] proposes to divide the whole text of each document in non-overlapping text regions which are collected in a list Since there are multiple ways to divide a text in non-overlapping regions, multiple lists are generated. For instance, we might have a list of all chapters in the document, a second list of all sections in the document, and a third list of all subsections in the document. These lists are kept as separate and distinct data structures. While the text regions in the same (flat) list have no overlapping, text regions from distinct lists might overlap. Figure 2.11 illustrates four separate lists for the same document. STRUCTURED TEXT RETRIEVAL MODELS        63  Chapter Ñ Sections Subsections Subsubsections Figure 2.11    Representation of the structure in the text of a document through four separate (flat) indexing lists. To allow searching for index terms and for text regions, a single inverted file (see Chapter 8 for a definition of inverted files) is built in which each structural component stands as an entry in the index. Associated with each entry, there is a list of text regions as a list of occurrences. Moreover, such a list could be easily merged with the traditional inverted file for the words in the text. Since the text regions are non-overlapping, the types of queries which can be asked are simple: (a) select a region which contains a given word (and does not contain other regions); (b) select a region A which does not contain any other region B (where B belongs to a list distinct from the list for A); (c) select a region not contained within any other region, etc.
mir-0049	2.9.2    Model Based on Proximal Nodes Navarro and Baeza-Yates [41, 589, 590] propose a model which allows the definition of independent hierarchical (non-flat) indexing structures over the same document text. Each of these indexing structures is a strict hierarchy composed of chapters, sections, paragraphs, pages, and lines which are called nodes (see Figure 2.12). To each of these nodes is associated a text region. Further, two distinct hierarchies might refer to overlapping text regions. Given a user query which refers to distinct hierarchies, the compiled answer is formed by nodes which all come from only one of them. Thus, an answer cannot be composed of nodes which come from two distinct hierarchies (which allows for faster query processing at the expense of less expressiveness). Notice, however, that due to the hierarchical structure, nested text regions (coming from the same hierarchy) are allowed in the answer set. Figure 2.12 illustrates a hierarchical indexing structure composed of four 64        MODELING -ï    Chapter Sections Subsections Subsubsections Figure 2.12    Hierarchical index for structural components and flat index for words. levels (corresponding to a chapter, sections, subsections, and subsubsections of the same document) and an inverted list for the word 'holocaust.' The entries in this inverted list indicate all the positions in the text of the document in which the word 'holocaust' occurs. In the hierarchy, each node indicates the position in the text of its associated structural component (chapter, section, subsection, or subsubsection). The query language allows the specification of regular expressions (to search for strings), the reference to structural components by name (to search for chapters, for instance), and a combination of these. In this sense, the model can be viewed as a compromise between expressiveness and efficiency. The somewhat limited expressiveness of the query language allows efficient query processing by first searching for the components which match the strings specified in the query and, subsequently, evaluating which of these components satisfy the structural part of the query. Consider, for instance, the query [(*section) with ('holocaust')] which searches for sections, subsections, or subsubsections which contain the word 'holocaust.7 A simple query processing strategy is to traverse the inverted list for the term 'holocaust1 and, for each entry in the list (which indicates an occurrence of the term 'holocaust' in the text), search the hierarchical index looking for sections, subsections, and subsubsections containing that occurrence of the term. A more sophisticated query processing strategy is as follows. For the first entry in the list for "holocaust,1 search the hierarchical index as before. This implies traversing down the hierarchy until no more successful matches occur (or the bottom of the hierarchy is reached). Let the last matching structural component be referred to as the innermost matching component. Once this first search is concluded, do not start ail over again for the following entry in the MODELS FOR BROWSING        65 inverted list. Instead, verify whether the innermost matching component also matches the second entry in the list. If it does, we immediately conclude that the larger structural components above it (in the hierarchy) also do. Proceed then to the third entry in the list, and so on. Notice that the query processing is accelerated because only the nearby (or proximal) nodes in the list need to be searched at each time. This is the reason for the label proximal nodes. The model based on proximal nodes allows us to formulate queries which are more complex than those which can be formulated in the model based on non-overlapping lists. To speed up query processing, however, only nearby (proximal) nodes are looked at which imposes restrictions on the answer set retrieved (all nodes must come from the same hierarchy). More complex models for structured retrieval have been proposed in the literature as discussed in [41, 590].
mir-0050	2.10    Models for Browsing As already observed, the user might not be interested in posing a specific query to the system. Instead, he might be willing to invest some time in exploring the document space looking for interesting references. In this situation, we say that the user is browsing the space instead of searching. Both with browsing and searching, the user has goals which he is pursuing. However, in general, the goal of a searching task is clearer in the mind of the user than the goal of a browsing task. As is obvious, this is not a distinction which is valid in all scenarios. But, since it is simple and provides a clear separation between the tasks of searching and browsing, it is adopted here. We distinguish three types of browsing namely, flat, structure guided, and hypertext.
mir-0051	The idea here is that the user explores a document space which has a flat organization. For instance, the documents might be represented as dots in a (two-dimensional) plan or as elements in a (single dimension) list. The user then glances here and there looking for information within the documents visited. For instance, he might look for correlations among neighbor documents or for keywords which are of interest to Mm. Such keywords could then be added to the original query in an attempt to provide better contextualization. This is a process called relevance feedback which is discussed in detail in Chapter 5. Also, the user could explore a single document in a flat manner. For example, he could use a browser to look into a Web page, using the arrows and the scroll bar. One disadvantage is that in a given page or screen there may not be any indication about the context where the user is. For example, if he opens a novel at a random page, he might not know in which chapter that page is. Web search engines such as "Yahoo!1 provide, besides the standard search interface, a hierarchical directory which can be used for browsing (and frequently, for searching). However, the organization is not flat as discussed below. 66        MODELING
mir-0052	2.10.2    Structure Guided Browsing To facilitate the task of browsing, the documents might be organized in a structure such as a directory. Directories are hierarchies of classes which group documents covering related topics. Such hierarchies of classes have been used to classify document collections for many centuries now. Thus, it seems natural to adapt them for use with modern browsing interfaces. In this case, we say that the user performs a structure guided type of browsing. The same idea can be applied to a single document. For example, if we are browsing an electronic book, a first level of content could be the chapters, the second level, all sections, and so on. The last level would be the text itself (flat). A good user interface could go down or up those levels in a focused manner, assisting the user with the task of keeping track of the context. Besides the structure which directs the browsing task, the interface can also include facilities such as a history map which identifies classes recently visited. This might be quite useful for dealing with very large structures - an issue discussed in Chapters 10 and 13. When searching, the occurrences can also be displayed showing just the structure (for example, using the table of contents). This allows us to see the occurrences in a global context instead of in a page of text that may have no indication of wThere we are.
mir-0053	2.10.3    The Hypertext Model One fundamental concept related to the task of writing down text is the notion of sequencing. Written text is usually conceived to be read sequentially. The reader should not expect to fully understand the message conveyed by the writer by randomly reading pieces of text here and there. One might rely on the text structure to skip portions of the text but this might result in miscommunication between reader and writer. Thus, a sequenced organizational structure lies underneath most written text. When the reader fails to perceive such a structure and abide by it, he frequently is unable to capture the essence of the writer's message. Sometimes, however, we are looking for information which is subsumed by the whole text but which cannot be easily captured through sequential reading. For instance, while glancing at a book about the history of the wars fought by man, we might be momentarily interested solely in the regional wars in Europe. We know that this information is in the book, but we might have a hard time finding it because the writer did not organize his writings with this purpose (he might have organized the wars chronologically). In such a situation, a different organization of the text is desired. However, there is no point in rewriting the whole text. Thus, the solution is to define a new organizational structure besides the one already in existence. One way to accomplish such a goal is through the design of a hypertext. MODELS FOR BROWSING        67 Hypertext Definition and the Navigational Task A hypertext is a high level interactive navigational structure which allows us to browse text non-sequentially on a computer screen. It consists basically of nodes which are correlated by directed links in a graph structure. To each node is associated a text region which might be a chapter in a book, a section in an article, or a Web page. Two nodes A and B might be connected by a directed link Iab which correlates the texts associated with these two nodes. In this case, the reader might move to the node B while reading the text associated with the node A. In its most conventional form, a hypertext link Iab is attached to a specific string inside the text for node A. Such a string is marked specially (for instance, its characters might appear in a different color or underlined) to indicate the presence of the underlying link. While reading the text, the user might come across a marked string. If the user clicks on that string, the underlying directed link is followed, and a new text region (associated with the node at the destination) is displayed on the screen. The process of navigating the hypertext can be understood as a traversal of a directed graph. The linked nodes of the graph represent text nodes which are semantically related. While traversing this graph the reader visualizes a flow of information which was conceived by the designer of the hypertext. Consider our previous example regarding a book on the wars fought by man. One might design a hypertext composed of two distinct webs (here, a web is simply a connected component formed by a subset of all links in the hypertext). While the first web might be designed to provide access to the local wars fought in Europe in chronological order, the second web might be designed to provide access to the local wars fought by each European country. In this way, the user of this hypertext can access the information according to his particular need. When the hypertext is large, the user might lose track of the organizational structure of the hypertext. The effect is that the user might start to take bad navigational decisions which might sidetrack him from his main goal (which usually consists of finding a piece of information in the hypertext). When this happens, the user is said to be lost in hyperspace [604]. To avoid this problem, it is desirable that the hypertext include a hypertext map which shows where the user is at all times. In its simplest form, this map is a directed graph which displays the current node being visited. Additionally, such a map could include information on the paths the user has traveled so far. This can be used to remind the user of the uselessness of following paths which have been explored already. While navigating a hypertext, the user is restricted to the intended flow of information previously conceived by the hypertext designer. Thus, the task of designing a hypertext should take into account the needs of its potential users. This implies the execution of a requirement analysis phase before starting the actual implementation of the hypertext. Such a requirement analysis is critically important but is frequently overlooked. Furthermore, during the hypertext navigation, the user might find it difficult to orient himself. This difficulty arises even in the presence of a guiding 68        MODELING tool such as the hypertext map discussed above. One possible reason is an excessively complex hypertext organization with too many links which allow the user to travel back and forth. To avoid this problem, the hypertext can have a simpler structure which can be quickly remembered by the user at all times. For instance, the hypertext can be organized hierarchically to facilitate the navigational task. Definition of the structure of the hypertext should be accomplished in a domain modeling phase (done after a requirement analysis phase). Further, after the modeling of the domain, a user interface design should be concluded prior to implementation. Only then, can we say that we have a proper hypertext structure for the application at hand. In the Web, however, pages are usually implemented with no attention paid to requirement analysis, domain modeling, and user interface design. As a result, Web pages are frequently poorly conceived and often fail to provide the user with a proper hypertext structure for assistance with the information seeking task. With large hypertexts, it might be difficult for the user to position himself in the part of the whole graph which is of most interest to him. To facilitate this initial positioning step, a search based on index terms might be used. In [540], Manber discusses the advantages of this approach. Hypertexts provided the basis for the conception and design of the hypertext markup language (HTML) and the hypertext transfer protocol (HTTP) which originated the World Wide Web (which we simply refer to as the Web). In Chapter 13, we discuss the Web in detail. We briefly discuss some of its features below. About the Web When one talks about the Web, the first concept which comes to mind is that of a hypertext. In fact, we frequently think of the WTeb as a huge distributed hypertext domain. However, the Web is not exactly a proper hypertext because it lacks an underlying data model, it lacks a navigational plan, and it lacks a consistently designed user interface. Each one of the millions of Web page designers devises his own interface with its own peculiar characteristics. Many times we visit a Web site simply looking for a phone number and cannot find it because it is buried in the least expected place of the local hypertext structure. Thus, the Web user has no underlying metaphor to assist him in the search for information of interest. Instead of saying that the Web is a hypertext, we prefer to say that it is a pool of (partially) interconnected webs. Some of these webs might be characterized as a local hypertext (in the sense that they have an underlying structure which enjoys some consistency) but others might be simply a collection of pages designed separately (for instance, the web of a university department whose professors design their own pages). Despite not being exactly a hypertext, the Web has provided us with a new dimension in communication functionality because it is easily accessible world wide at very low cost. And maybe most important, the Web has no control body setting up regulations and censorship rules. As a BIBLIOGRAPHIC DISCUSSION        69 result, for the first time in the history of mankind, any one person can publish his writings through a large medium without being subjected to the filtering of an editorial board. For a more thorough discussion of these and many other issues related to the Web, the user is referred to Chapter 13.
mir-0054	2.11    Trends and Research issues There are three main types of products and systems which can benefit directly from research in models for information retrieval: library systems, specialized retrieval systems, and the Web. Regarding library systems, there is currently much interest in cognitive and behavioral issues oriented particularly at a better understanding of which criteria the users adopt to judge relevance. Prom the point of view of the computer scientist, a main question is how this knowledge about the user affects the ranking strategies and the user interface implemented by the system. A related issue is the investigation of how models other than the Boolean model (which is still largely adopted by most large commercial library systems) affect the user of a library. A specialized retrieval system is one which is developed with a particular application in mind. For instance, the LEXIS-NEXIS retrieval system (see Chapter 14), which provides access to a very large collection of legal and business documents, is a good example of a specialized retrieval system. In such a system, a key problem is how to retrieve (almost) all documents which might be relevant to the user information need without also retrieving a large number of unrelated documents. In this context, sophisticated ranking algorithms are highly desirable. Since ranking based on single evidential sources is unlikely to provide the appropriate answers, research on approaches for combining several evidential sources seems highly relevant (as demonstrated at the various TREC conferences, see Chapter 3 for details). In the Web, the scenario is quite distinct and unique. In fact, the user of the Web frequently does not know what he wants or has great difficulty in properly formulating his request. Thus, research in advanced user interfaces is highly desirable. From the point of view of the ranking engine, an interesting problem is to study how the paradigm adopted for the user interface affects the ranking. Furthermore, it is now well established that the indexes maintained by the various Web search engines are almost disjoint (e.g., the ten most popular search engines have indexes whose intersection corresponds to less than 2% of the total number of pages indexed). In this scenario, research on meta-search engines (i.e., engines which work by fusing the rankings generated by other search engines) seems highly promising.
mir-0055	2.12    Bibliographic Discussion Early in 1960, Maron and Kuhns [547] had already discussed the issues of relevance and probabilistic indexing in information retrieval.  Twenty-three years 70        MODELING later, Salton and McGill wrote a book [698] which became a classic in the field. The book provides a thorough coverage of the three classic models in information retrieval namely, the Boolean, the vector, and the probabilistic models. Another landmark reference is the book by van Rijsbergen [785] which, besides also covering the three classic models, presents a thorough and enjoyable discussion on the probabilistic model. The book edited by Prakes and Baeza-Yates [275] presents several data structures and algorithms for IR and is more recent. Further, it includes a discussion of ranking algorithms by Harman [340] which provides interesting insights into the history of information retrieval from 1960 to 1990. Boolean operations and their implementation are covered in [803]. The inadequacy of Boolean queries for information retrieval was characterized early on by Verhoeff, Goffman, and Belzer [786]. The issue of adapting the Boolean formalism to operate with other frameworks received great attention. Book-stein discusses the problems related with merging Boolean and weighted retrieval systems [101] and the implications of Boolean structure for probabilistic retrieval [103]. Losee and Bookstein [522] cover the usage of Boolean queries with probabilistic retrieval. Anick et al. [21] propose an interface based on natural language for Boolean retrieval. A thesaurus-based Boolean retrieval system is proposed in [493]. The vector model is maybe the most popular model among the research community in information retrieval. Much of this popularity is due to the long-term research of Salton and his associates [697, 704]. Most of this research revolved around the SMART retrieval system developed at Cornell University [695, 842, 696]. Term weighting for the vector model has also been investigated thoroughly. Simple term weighting was used early on by Salton and Lesk [697]. Sparck Jones introduced the idf factor [409, 410] and Salton and Yang verified its effectiveness for improving retrieval [704]. Yu and Salton [842] further studied the effects of term weighting in the final ranking. Salton and Buckley [696] summarize 20 years of experiments in term weighting with the SMART system. Raghavan and Wong [665] provide a critical analysis of the vector model. The probabilistic model was introduced by Robertson and Sparck Jones [677] and is thoroughly discussed in [785]. Experimental studies with the model were conducted by Sparck Jones [411, 412] which used feedback from the user to estimate the initial probabilities. Croft and Harper [199] proposed a method to estimate these probabilities without feedback from the user. Croft [198] later on added within-document frequency weights into the model. Fuhr discusses probabilistic indexing through polynomial retrieval functions [281, 284]. Cooper, Gey, and Dabney [186] and later on Gey [295] propose the use of logistic regression with probabilistic retrieval. Lee and Kantor [494] study the effect of inconsistent expert judgements on probabilistic retrieval. Puhr [282] reviews various variants of the classic probabilistic model. Cooper [187], in a seminal paper, raises troubling questions on the utilization of the probabilistic ranking principle in information retrieval. The inference network model was introduced by Turtle and Croft [772, 771] in 1990. Haines and Croft [332] discuss the utilization of inference networks for user relevance feedback (see Chapter 5).   Callan, Lu, and Croft [139] use an BIBLIOGRAPHIC DISCUSSION        71 inference network to search distributed document collections. Callan [138], in his turn, discusses the application of inference networks to information filtering. The belief network model, due to Ribeiro-Neto and Muntz [674], generalizes the inference network model. The extended Boolean model was introduced by Salton, Fox, and Wu [703]. Lee, Kim, Kim, and Lee [496] discuss the evaluation of Boolean operators with the extended Boolean model, while properties of the model are discussed in [495]. The generalized vector space model was introduced in 1985 by Wong, Ziarko, and Wong [832, 831]. Latent semantic indexing was introduced in 1988 by Furnas, Deerwester, Dumais, Landauer, Harshman, Streeter, and Lochbaum [287]. In a subsequent paper, Bartell, Cottrell, and Belew [62] show that latent semantic indexing can be interpreted as a special case of multidimensional scaling. Regarding neural network models for information retrieval, our discussion in this book is based mainly on the work by Wilkinson and Hingston [815]. But we also benefited from the writings of Kwok on the subject and related topics [466, 467, 469, 468]. The fuzzy set model (for information retrieval) covered in this book is due to Ogawa, Morita, and Kobayashi [616]. The utilization of fuzzy theory in information retrieval goes back to the 1970s with the work of Radecki [658, 659, 660, 661], of Sachs [691], and of Tahani [755]. Bookstein [102] proposes the utilization of fuzzy operators to deal with weighted Boolean searches. Kraft and Buel [461] utilize fuzzy sets to generalize a Boolean system. Miyamoto, Miyake, and Nakayama [567] discuss the generation of a pseudothesaurus using co-occurrences and fuzzy operators. Subsequently, Miyamoto and Nakayama [568] discuss the utilization of this thesaurus with information retrieval systems. Our discussion on structured text is based on the survey by [41]. Another survey of interest (an older one though) is the work by MacLeod [533]. Burkowski [132, 133] proposed a model based on non-overlapping regions. Clarke, Cormack, and Burkowski [173] extended this model with overlapping capabilities. The model based on proximal nodes was proposed by Navarro and Baeza-Yates [589, 590]. In [534], MacLeod introduced a model based on a single hierarchy which also associates attributes with nodes in the hierarchy (for database-like querying) and hypertext links with pairs of nodes. Kilpelainen and Mannila [439] discuss the retrieval from hierarchical texts through the specification of partial patterns. In [183], Consens and Milo discuss algebras for querying text regions. A classic reference on hypertexts is the book by Nielsen [604], Another popular reference is the book by Shneiderman and Kearsley [727]. Conklin [181] presents an introductory survey of the area. The Communications of the ACM dedicated an special edition [177] to hypermedia which discusses in detail the Dexter model ó a reference standard on the terminology and semantics of basic hypermedia concepts. A subsequent edition [178] was dedicated to the presentation of various models for supporting the design of hypermedia applications.
mir-0057	3.1    Introduction Before the final implementation of an information retrieval system, an evaluation of the system is usually carried out. The type of evaluation to be considered depends on the objectives of the retrieval system. Clearly, any software system has to provide the functionality it was conceived for. Thus, the first type of evaluation which should be considered is a functional analysis in which the specified system functionalities are tested one by one. Such an analysis should also include an error analysis phase in which, instead of looking for functionalities, one behaves erratically trying to make the system fail. It is a simple procedure which can be quite useful for catching programming errors. Given that the system has passed the functional analysis phase, one should proceed to evaluate the performance of the system. The most common measures of system performance are time and space. The shorter the response time, the smaller the space used, the better the system is considered to be. There is an inherent tradeoff between space complexity and time complexity which frequently allows trading one for the other. In Chapter 8 we discuss this issue in detail. In a system designed for providing data retrieval, the response time and the space required are usually the metrics of most interest and the ones normally adopted for evaluating the system. In this case, we look for the performance of the indexing structures (which are in place to accelerate the search), the interaction with the operating system, the delays in communication channels, and the overheads introduced by the many software layers which are usually present. We refer to such a form of evaluation simply as performance evaluation. In a system designed for providing information retrieval, other metrics, besides time and space, are also of interest. In fact, since the user query request is inherently vague, the retrieved documents are not exact answers and have to be ranked according to their relevance to the query. Such relevance ranking introduces a component which is not present in data retrieval systems and which plays a central role in information retrieval. Thus, information retrieval systems require the evaluation of how precise is the answer set. This type of evaluation is referred to as retrieval performance evaluation. 74        RETRIEVAL EVALUATION In this chapter, we discuss retrieval performance evaluation for information retrieval systems. Such an evaluation is usually based on a test reference collection and on an evaluation measure. The test reference collection consists of a collection of documents, a set of example information requests, and a set of relevant documents (provided by specialists) for each example information request. Given a retrieval strategy 5, the evaluation measure quantifies (for each example information request) the similarity between the set of documents retrieved by S and the set of relevant documents provided by the specialists. This provides an estimation of the goodness of the retrieval strategy S. In our discussion, we first cover the two most used retrieval evaluation measures: recall and precision. We also cover alternative evaluation measures such as the E measure, the harmonic mean, satisfaction, frustration, etc. Following that, we cover four test reference collections namely, TIPSTER/TREC, CACM, CISI, and Cystic Fibrosis.
mir-0058	3.2    Retrieval Performance Evaluation When considering retrieval performance evaluation, we should first consider the retrieval task that is to be evaluated. For instance, the retrieval task could consist simply of a query processed in batch mode (i.e., the user submits a query and receives an answer back) or of a whole interactive session (i.e., the user specifies his information need through a series of interactive steps with the system). Further, the retrieval task could also comprise a combination of these two strategies. Batch and interactive query tasks are quite distinct processes and thus their evaluations are also distinct. In fact, in an interactive session, user effort, characteristics of the interface design, guidance provided by the system, and duration of the session are critical aspects which should be observed and measured. In a batch session, none of these aspects is nearly as important as the quality of the answer set generated. Besides the nature of the query request, one has also to consider the setting where the evaluation will take place and the type of interface used. Regarding the setting, evaluation of experiments performed in a laboratory might be quite distinct from evaluation of experiments carried out in a real life situation. Regarding the type of interface, while early bibliographic systems (which still dominate the commercial market as discussed in Chapter 14) present the user with interfaces which normally operate in batch mode, newer systems (which are been popularized by the high quality graphic displays available nowadays) usually present the user with complex interfaces which often operate interactively. Retrieval performance evaluation in the early days of computer-based information retrieval systems focused primarily on laboratory experiments designed for batch interfaces. In the 1990s, a lot more attention has been paid to the evaluation of real life experiments. Despite this tendency, laboratory experimentation is still dominant. Two main reasons are the repeatability and the scalability provided by the closed setting of a laboratory. RETRIEVAL PERFORMANCE EVALUATION        75 In this book, we focus mainly on experiments performed in laboratories. In this chapter in particular we discuss solely the evaluation of systems which operate in batch mode. Evaluation of systems which operate interactively is briefly discussed in Chapter 10.
mir-0059	3.2.1    Recall and Precision Consider an example information request / (of a test reference collection) and its set R of relevant documents. Let \R\ be the number of documents in this set. Assume that a given retrieval strategy (which is being evaluated) processes the information request / and generates a document answer set A. Let \A\ be the number of documents in this set. Further, let \Ra\ be the number of documents in the intersection of the sets R and A. Figure 3.1 illustrates these sets. The recall and precision measures are defined as follows. ï Recall is the fraction of the relevant documents (the set R) which has been retrieved i.e., Recall = \R\ ï Precision is the fraction of the retrieved documents (the set A) which is relevant i.e., Precision = \A\ Recall and precision, as defined above, assume that all the documents in the answer set A have been examined (or seen). However, the user is not usually presented with all the documents in the answer set A at once.   Instead, the Relevant Docs in Answer Set                 ^-------_^ Collection Relevant Docs         Answer Set' |*|                       \A\ Figure 3.1    Precision and recall for a given example information request. 76        RETRIEVAL EVALUATION documents in A are first sorted according to a degree of relevance (i.e., a ranking is generated). The user then examines this ranked list starting from the top document. In this situation, the recall and precision measures vary as the user proceeds with his examination of the answer set A. Thus, proper evaluation requires plotting a precision versus recall curve as follows. As before, consider a reference collection and its set of example information requests. Let us focus on a given example information request for which a query q is formulated. Assume that a set Rq containing the relevant documents for q has been defined. Without loss of generality, assume further that the set Rq is composed of the following documents Rq = {^3,^5,^9, ^25, ^39^44, ^56^71)^89? d\2z]                                   (3.1) Thus, according to a group of specialists, there are ten documents which are relevant to the query q. Consider now a new retrieval algorithm which has just been designed. Assume that this algorithm returns, for the query g, a ranking of the documents in the answer set as follows. Ranking for query q: 1.	d\2Z ï	6.  CI9 ï	11.	"38 2.		7. d5n	12.	d48 3.	d56 ï	8.  di29	13. 4.	de	9-  ^187	14.	dn 5.	d8	10.  ^25 ï	15. The documents that are relevant to the query q are marked with a bullet after the document number. If we examine this ranking, starting from the top document, we observe the following points. First, the document di23 which is ranked as number 1 is relevant. Further, this document corresponds to 10% of all the relevant documents in the set Rq. Thus, we say that we have a precision of 100% at 10% recall. Second, the document d$ß which is ranked as number 3 is the next relevant document. At this point, we say that we have a precision of roughly 66% (two documents out of three are relevant) at 20% recall (two of the ten relevant documents have been seen). Third, if we proceed with our examination of the ranking generated we can plot a curve of precision versus recall as illustrated in Figure 3.2. The precision at levels of recall higher than 50% drops to 0 because not all relevant documents have been retrieved. This precision versus recall curve is usually based on 11 (instead often) standard recall levels which are 0%, 10%, 20%, ..., 100%. For the recall level 0%, the precision is obtained through an interpolation procedure as detailed below. In the above example, the precision and recall figures are for a single query. Usually, however, retrieval algorithms are evaluated by running them for several distinct queries. In this case, for each query a distinct precision versus recall curve is generated. To evaluate the retrieval performance of an algorithm over RETRIEVAL PERFORMANCE EVALUATION 77 Figure 3.2    Precision at 11 standard recall levels. all test queries, we average the precision figures at each recall level as follows. P(r) = Pi(r) (3.2) t=l where P(r) is the average precision at the recall level r, Nq is the number of queries used, and Pi(r) is the precision at recall level r for the i-th query. Since the recall levels for each query might be distinct from the 11 standard recall levels, utilization of an interpolation procedure is often necessary. For instance, consider again the set of 15 ranked documents presented above. Assume that the set of relevant documents for the query q has changed and is now given by (3.3) In this case, the first relevant document in the ranking for query q is d5e which provides a recall level of 33.3% (with precision also equal to 33.3%) because, at this point, one-third of all relevant documents have already been seen. The second relevant document is di29 which provides a recall level of 66.6% (with precision equal to 25%). The third relevant document is d$ which provides a recall level of 100% (with precision equal to 20%). The precision figures at the 11 standard recall levels are interpolated as follows. Let fj, j E {0,1,2,..., 10}, be a reference to the j-th standard recall level (i.e., rs is a reference to the recall level 50%). Then, P(rj)-= max r3lt;rlt;r^t   P(r) (3.4) 78 RETRIEVAL EVALUATION 120 100 -gt; 80 -60 20 40 60 Recall 80 100 120 Figure  3.3    Interpolated precision at  11 standard recall levels relative to Rq  = which states that the interpolated precision at the j-th standard recall level is the maximum known precision at any recall level between the j-th recall level and the (j -f l)-th recall level. In our last example, this interpolation rule yields the precision and recall figures illustrated in Figure 3.3. At recall levels 0%, 10%, 20%, and 30%, the interpolated precision is equal to 33.3% (which is the known precision at the recall level 33.3%). At recall levels 40%, 50%, and 60%, the interpolated precision is 25% (which is the precision at the recall level 66.6%). At recall levels 70%, 80%, 90%, and 100%, the interpolated precision is 20% (which is the precision at recall level 100%). The curve of precision versus recall which results from averaging the results for various queries is usually referred to as precision versus recall figures. Such average figures are normally used to compare the retrieval performance of distinct retrieval algorithms. For instance, one could compare the retrieval performance of a newly proposed retrieval algorithm with the retrieval performance of the classic vector space model. Figure 3.4 illustrates average precision versus recall figures for two distinct retrieval algorithms. In this case, one algorithm has higher precision at lower recall levels while the second algorithm is superior at higher recall levels. One additional approach is to compute average precision at given document cutoff values. For instance, we can compute the average precision when 5, 10, 15, 20, 30, 50, or 100 relevant documents have been seen. The procedure is analogous to the computation of average precision at 11 standard recall levels but provides additional information on the retrieval performance of the ranking algorithm. Average precision versus recall figures are now a standard evaluation strategy for information retrieval systems and are used extensively in the information retrieval literature.    They are useful because thev allow us to evaluate RETRIEVAL PERFORMANCE EVALUATION        79 100 120 Figure 3.4    Average recall versus precision figures for two distinct retrieval algorithms. quantitatively both the quality of the overall answer set and the breadth of the retrieval algorithm. Further, they are simple, intuitive, and can be combined in a single curve. However, precision versus recall figures also have their disadvantages and their widespread usage has been criticized in the literature. We return to this point later on. Before that, let us discuss techniques for summarizing precision versus recall figures by a single numerical value. Single Value Summaries Average precision versus recall figures are useful for comparing the retrieval performance of distinct retrieval algorithms over a set of example queries. However, there are situations in which we would like to compare the retrieval performance of our retrieval algorithms for the individual queries. The reasons are twofold. First, averaging precision over many queries might disguise important anomalies in the retrieval algorithms under study. Second, when comparing two algorithms, we might be interested in investigating whether one of them outperforms the other for each query in a given set of example queries (notice that this fact can be easily hidden by an average precision computation). In these situations, a single precision value (for each query) can be used. This single value should be interpreted as a summary of the corresponding precision versus recall curve. Usually, this single value summary is taken as the precision at a specified recall level. For instance, we could evaluate the precision when we observe the first relevant document and take this precision as the single value summary. Of course, as seems obvious, this is not a good approach. More interesting strategies can be adopted as we now discuss. 80        RETRIEVAL EVALUATION Average Precision at Seen Relevant Documents The idea here is to generate a single value summary of the ranking by averaging the precision figures obtained after each new relevant document is observed (in the ranking). For instance, consider the example in Figure 3.2. The precision figures after each new relevant document is observed are 1, 0.66, 0.5, 0.4, and 0.3. Thus, the average precision at seen relevant documents is given by (l+0.66+0.5-H).4-H).3)/5 or 0.57. This measure favors systems which retrieve relevant documents quickly (i.e., early in the ranking). Of course, an algorithm might present a good average precision at seen relevant documents but have a poor performance in terms of overall recall. R-Precision The idea here is to generate a single value summary of the ranking by computing the precision at the R-th position in the ranking, where R is the total number of relevant documents for the current query (i.e., number of documents in the set Rq). For instance, consider the examples in Figures 3.2 and 3.3. The value of R-precision is 0.4 for the first example (because R = 10 and there are four relevant documents among the first ten documents in the ranking) and 0.33 for the second example (because R = 3 and there is one relevant document among the first three documents in the ranking). The R-precision measure is a useful parameter for observing the behavior of an algorithm for each individual query in an experiment. Additionally, one can also compute an average R-precision figure over all queries. However, using a single number to summarize the full behavior of a retrieval algorithm over several queries might be quite imprecise. Precision Histograms The R-precision measures for several queries can be used to compare the retrieval history of two algorithms as follows. Let RPa{i) and RPb(i) be the R-precision values of the retrieval algorithms A and B for the i-th query. Define, for instance, the difference RPA/B(i) = RPA(i) - RPB(i)                                                             (3.5) A value of RPa/b^) equal to 0 indicates that both algorithms have equivalent performance (in terms of R-precision) for the i-th query.   A positive value of RPa/b(^) indicates a better retrieval performance by algorithm A (for the i-th query) while a negative value indicates a better retrieval performance by algorithm B. Figure 3.5 Illustrates the RPaib^) values (labeled R-Precision A/B) for two hypothetical retrieval algorithms over ten example queries.  The algorithm .4 is superior for eight queries while the algorithm B performs better for the two other queries (numbered 4 and 5). This type of bar graph is called a precision histogram and allows us to quickly compare the retrieval performance history of two algorithms through visual inspection. Summary Table Statistics Single mine measures can also be stored in a table to provide a statistical summary regarding the set of all the queries in a retrieval task. For instance, these RETRIEVAL PERFORMANCE EVALUATION        81 1,5  , 1,0 0,5 Q. n U 1             2            3 -1,0 ' igt;          6           7          8           9          10 -1,5 Query Number Figure 3.5    A precision histogram for ten hypothetical queries. summary table statistics could include: the number of queries used in the task, the total number of documents retrieved by all queries, the total number of relevant documents which were effectively retrieved when all queries are considered, the total number of relevant documents which could have been retrieved by all queries, etc. Precision and Recall Appropriateness Precision and recall have been used extensively to evaluate the retrieval performance of retrieval algorithms. However, a more careful reflection reveals problems with these two measures [451, 664, 754]. First, the proper estimation of maximum recall for a query requires detailed knowledge of all the documents in the collection. With large collections, such knowledge is unavailable which implies that recall cannot be estimated precisely. Second, recall and precision are related measures which capture different aspects of the set of retrieved documents. In many situations, the use of a single measure which combines recall and precision could be more appropriate. Third, recall and precision measure the effectiveness over a set of queries processed in batch mode. However, with modern systems, interactivity (and not batch processing) is the key aspect of the retrieval process. Thus, measures which quantify the informativeness of the retrieval process might now be more appropriate. Fourth, recall and precision are easy to define when a linear ordering of the retrieved documents is enforced. For systems which require a weak ordering though, recall and precision might be inadequate. 82        RETRIEVAL EVALUATION
mir-0060	3.2.2    Alternative Measures Since recall and precision, despite their popularity, are not always the most appropriate measures for evaluating retrieval performance, alternative measures have been proposed over the years. A brief review of some of them is as follows. The Harmonic Mean As discussed above, a single measure which combines recall and precision might be of interest. One such measure is the harmonic mean F of recall and precision [422] which is computed as where r(j) is the recall for the j-th document in the ranking, P(j) is the precision for the j-th document in the ranking, and F(j) is the harmonic mean of r(j) and P(j) (thus, relative to the j-th document in the ranking). The function F assumes values in the interval [0,1]. It is 0 when no relevant documents have been retrieved and is 1 when all ranked documents are relevant. Further, the harmonic mean F assumes a high value only when both recall and precision are high. Therefore, determination of the maximum value for F can be interpreted as an attempt to find the best possible compromise between recall and precision. The E Measure Another measure which combines recall and precision was proposed by van Ri-jsbergen [785] and is called the E evaluation measure. The idea is to allow the user to specify whether he is more interested in recall or in precision.   The E measure is defined as follows. where r(j) is the recall for the j-th document in the ranking, P(j) is the precision for the j-th document in the ranking, E(j) is the E evaluation measure relative to r(j) and P{j), and b is a user specified parameter which reflects the relative importance of recall and precision. For 6 = 1, the E(j) measure works as the complement of the harmonic mean F(j). Values of b greater than 1 indicate that the user is more interested in precision than in recall while values of b smaller than I indicate that the user is more interested in recall than in precision. RETRIEVAL PERFORMANCE EVALUATION 83 User-Oriented Measures Recall and precision are based on the assumption that the set of relevant documents for a query is the same, independent of the user. However, different users might have a different interpretation of which document is relevant and which one is not. To cope with this problem, user-oriented measures have been proposed such as coverage ratio, novelty ratio, relative recall, and recall effort [451]. As before, consider a reference collection, an example information request /, and a retrieval strategy to be evaluated. Let R be the set of relevant documents for / and A be the answer set retrieved. Also, let U be the subset of R which is known to the user. The number of documents in U is \U\. The intersection of the sets A and U yields the documents known to the user to be relevant which were retrieved. Let \Rk\ be the number of documents in this set. Further, let \Ru\ be the number of relevant documents previously unknown to the user which were retrieved. Figure 3.6 illustrates the situation. The coverage ratio is defined as the fraction of the documents known (to the user) to be relevant which has actually been retrieved i.e., coverage = \Rk\ \U\ The novelty ratio is defined as the fraction of the relevant documents retrieved which was unknown to the user i.e., novelty = \Ru\ \Ru\ + \Rk\ A high coverage ratio indicates that the system is finding most of the relevant documents the user expected to see. A high novelty ratio indicates that the system is revealing (to the user) many new relevant documents which were previously unknown. Relevant Docs Relevant Docs known to the User M Answer Set Relevant Docs known to the User which were retrieved \Rk\ Relevant Docs previously unknown to the User which were retrieved Figure 3.6    Coverage and novelty ratios for a given example information request. 84       RETRIEVAL EVALUATION Additionally, two other measures can be defined as follows. The relative recall is given by the ratio between the number of relevant documents found (by the system) and the number of relevant documents the user expected to find. In the case when the user finds as many relevant documents as he expected, he stops searching and the relative recall is equal to 1. The recall effort is given by the ratio between the number of relevant documents the user expected to find and the number of documents examined in an attempt to find the expected relevant documents. Other Measures Other measures which might be of interest include the expected search length, which is good for dealing with sets of documents weakly ordered, the satisfaction, which takes into account only the relevant documents, and the frustration, which takes into account only the non-relevant documents [451].
mir-0061	3.3    Reference Collections In this section we discuss various reference collections which have been used throughout the years for the evaluation of information retrieval systems. We first discuss the TIPSTER/TREC collection which, due to its large size and thorough experimentation, is usually considered to be the reference test collection in information retrieval nowadays. Following that, we cover the CACM and ISI collections due to their historical importance in the area of information retrieval. We conclude this section with a brief discussion of the Cystic Fibrosis collection. It is a small collection whose example information requests were extensively studied by four groups of specialists before generation of the relevant document sets.
mir-0062	3.3.1    The TREC Collection Research in information retrieval has frequently been criticized on two fronts. First, that it lacks a solid formal framework as a basic foundation. Second, that it lacks robust and consistent testbeds and benchmarks. The first of these criticisms is difficult to dismiss entirely due to the Inherent degree of psychological suhjectiveness associated with the task of deciding on the relevance of a given document (which characterizes information, as opposed to data, retrieval). Thus, at least for now, research in information retrieval will have to proceed without a solid formal underpinning. The second of these criticisms, however, can be acted upon. For three decades, experimentation in information retrieval was based on relatively small test collections which did not reflect the main issues present in a large bibliographical environment. Further, comparisons between various retrieval systems were difficult to make because distinct groups conducted experiments focused on distinct aspects of retrieval (even when the same test collection was used) and there were no widely accepted benchmarks. REFERENCE COLLECTIONS        85 In the early 1990s, a reaction to this state of disarray was initiated under the leadership of Donna Harman at the National Institute of Standards and Technology (NIST), in Maryland. Such an effort consisted of promoting a yearly conference, named TREC for Text REtrieval Conference, dedicated to experimentation with a large test collection comprising over a million documents. For each TREC conference, a set of reference experiments is designed. The research groups which participate in the conference use these reference experiments for comparing their retrieval systems. A clear statement of the purpose of the TREC conferences can be found in the NIST TREC Web site [768] and reads as follows. The TREC conference series is co-sponsored by the National Institute of Standards and Technology (NIST) and the Information Technology Office of the Defense Advanced Research Projects Agency (DARPA) as part of the TIPSTER Text Program. The goal of the conference series is to encourage research in information retrieval from large text applications by providing a large test collection, uniform scoring procedures, and a forum for organizations interested in comparing their results. Attendance at TREC conferences is restricted to those researchers and developers who have performed the TREC retrieval tasks and to selected government personnel from sponsoring agencies. Participants in a TREC conference employ a wide variety of retrieval techniques, including methods using automatic thesauri, sophisticated term weighting, natural language techniques, relevance feedback, and advanced pattern matching. Each system works with the same test collection that consists of about 2 gigabytes of text (over 1 million documents) and a given set of information needs called "topics.' Results are run through a common evaluation package so that groups can compare the effectiveness of different techniques and can determine how differences between systems affect performance. Since the collection was built under the TIPSTER program, it is frequently referred to as the TIPSTER or the TIPSTER/TREC test collection.    Here, however, for simplicity we refer to it as the TREC collection. The first TREC conference was held at NIST in November 1992, while the second TREC conference occurred in August 1993. In November 1997, the sixth TREC conference was held (also at NIST) and counted the following participating organizations (extracted from [794]): Apple Computer                                      City Univ., London ATT Labs Research                               CLARITECH Corporation Australian National Univ.                          Cornell LTniv./SaBIR Research, Inc. Carnegie Mellon Univ.                               CSIRO (Australia) CEA (France)                                          Daimler Benz Res. Center, Ulm Center for Inf. Res., Russia                        Dublin LTniv. Center 86 RETRIEVAL EVALUATION Duke Univ./Univ. of Colorado/Bellcore ETH (Switzerland) FS Consulting, Inc. GE Corp./Rutgers Univ. George Mason Univ./NCR Corp. Harris Corp. IBM T.J. Watson Res. (2 groups) ISS (Singapore) ITI (Singapore) APL, Johns Hopkins Univ. LEXIS-NEXIS MDS at RMIT, Australia MIT/IBM Almaden Res. Center MSI/IRIT/Univ. Toulouse NEC Corporation New Mexico State Univ. (2 groups) NSA (Speech Research Group) Open Text Corporation Oregon Health Sciences Univ. Queens College, CUNY Rutgers Univ. (2 groups) Siemens AG SRI International TwentyOne Univ. California, Berkeley Univ. California, San Diego Univ. Glasgow Univ. Maryland, College Park Univ. Massachusetts, Amherst Univ. Montreal Univ. North Carolina (2 groups) Univ. Sheffield/Univ. Cambridge Univ. Waterloo Verity, Inc. Xerox Res. Centre Europe The seventh TREC conference was held again at NIST in November of 1998. In the following, we briefly discuss the TREC document collection and the (benchmark) tasks at the TREC conferences. As with most test collections, the TREC collection is composed of three parts: the documents, the example information requests (called topics in the TREC nomenclature), and a set of relevant documents for each example information request. Further, the TREC conferences also include a set of tasks to be used as a benchmark. The Document Collection The TREC collection has been growing steadily over the years. At TREC-3, the collection size was roughly 2 gigabytes while at TREC-6 it had gone up to roughly 5.8 gigabytes. In the beginning, copyright restrictions prevented free distribution of the collection and, as a result, the distribution CD-ROM disks had to be bought. In 1998, however, an arrangement was made which allows free access to the documents used in the most recent TREC conferences. As a result, TREC disk 4 and TREC disk 5 are now available from NIST at a small fee (US$200 in 1998) to cover distribution costs. Information on how to obtain the collection (which comes with the disks) and the topics with their relevant document sets (which have to be retrieved through the network) can be obtained directly from the NIST TREC Web site [768]. The TREC collection is distributed in six CD-ROM disks of roughly 1 gigabyte of compressed text each- The documents come from the following sources: WSJ        -? Wail Street Journal ^p           __, Associated Press (news wire) ZIFF        -* Computer Selects (articles), Ziff-Davis FR           ^ Federal Register REFERENCE COLLECTIONS 87 DOE        -+ US DOE Publications (abstracts) SJMN      ó¶gt; San Jose Mercury News PAT         -+ US Patents FT           ó* Financial Times CR          óª Congressional Record FBIS        ógt; Foreign Broadcast Information Service LAT         -gt; L.A Times Table 3.1 illustrates the contents of each disk and some simple statistics regarding the collection  (extracted from [794]).   Documents from all subcollections are Disk	Contents	Size	Number	Words/Doc.	Words/Doc. Mb	Docs	(median)	(mean) 1	WSJ, 1987-1989	267	98,732	245	434.0 AP, 1989	254	84,678	446	473.9 ZIFF	242	75,180	200	473.0 FR, 1989	260	25,960	391	1315.9 DOE	184	226,087	111	120.4 2	WSJ, 1990-1992	242	74,520	301	508.4 AP, 1988	237	79,919	438	468.7 ZIFF	175	56,920	182	451.9 FR, 1988	209	19,860	396	1378.1 3	SJMN, 1991	287	90,257	379	453.0 AP, 1990	237	78,321	451	478.4 ZIFF	345	161,021	122	295.4 PAT, 1993	243	6,711	4,445	5391.0 4	FT, 1991-1994	564	210,158	316	412.7 FR, 1994	395	55,630	588	644.7 CR, 1993	235	27,922	288	1373.5 5	FBIS	470	130,471	322	543.6 LAT	475	131,896	351	526.5 6	FBIS	490	120,653	348	581.3 Table 3.1    Document collection used at TREC-6. Stopwords are not removed and no stemming is performed (see Chapter 7 for details on stemming). tagged with SGML (see Chapter 6) to allow easy parsing (which implies simple coding for the groups participating at TREC conferences). Major structures such as a field for the document number (identified by lt;DOCNOgt;) and a field for the document text (identified by lt;TEXTgt;) are common to all documents. Minor structures might be different across subcollections to preserve parts of the structure in the original document. This has been the philosophy for formatting decisions at NIST: preserve as much of the original structure as possible while providing a common framework which allows simple decoding of the data. An example of a TREC document is the document numbered 880406-0090 RETRIEVAL EVALUATION lt;docgt; lt;docnogt; WSJ880406-0090 lt;/docnogt; lt;hlgt; ATT Unveils Services to Upgrade Phone Networks Under Global Plan lt;/hlgt; lt;authorgt; Janet Guyon (WSJ Staff) lt;/authorgt; lt; dateline gt; New York lt;/dateline gt; lt;textgt; American Telephone ; Telegraph Co. introduced the first of a new generation of phone services with broad ... lt;/textgt; lt;/docgt; Figure 3.7   TREC document numbered WSJ880406-0090. in the Wall Street Journal subcollection which is shown in Figure 3.7 (extracted from [342]). Further details on the TREC document collection can be obtained from [794, 768]. The Example Information Requests (Topics) The TREC collection includes a set of example information requests which can be used for testing a new ranking algorithm. Each request is a description of an information need in natural language. In the TREC nomenclature, each test information request is referred to as a topic. An example of an information request in TREC is the topic numbered 168 (prepared for the TREC-3 conference) which is illustrated in Figure 3.8 (extracted from [342]). The task of converting an information request (topic) into a system query (i.e., a set of index terms, a Boolean expression, a fuzzy expression, etc.) must be done by the system itself and is considered to be an integral part of the evaluation procedure. The number of topics prepared for the first six TREC conferences goes up to 350. The topics numbered 1 to 150 were prepared for use with the TREC-1 and TREC-2 conferences. They were written by people who were experienced users of real systems and represented long-standing information needs. The topics numbered 151 to 200 were prepared for use with the TREC-3 conference, are shorter, and have a simpler structure which includes only three subfields (named Title, Description, and Narrative as illustrated in the topic 168 above). The topics numbered 201 to 250 were prepared for use with the TREC-4 conference and are even shorter. At the TREC-5 (which included topics 251-300) and TREC-6 (which included topics 301-350) conferences, the topics were prepared with a composition similar to the topics in TREC-3 (i.e., they were expanded with respect to the topics in TREC-4 which were considered to be too short). REFERENCE COLLECTIONS        89 lt;topgt; lt;numgt; Number: 168 lt;titlegt; Topic: Financing AMTRAK lt;descgt; Description: A document will address the role of the Federal Government in financing the operation of the National Railroad Transportation Corporation (AMTRAK). lt;narrgt; Narrative: A relevant document must provide information on the government's responsibility to make AMTRAK an economically viable entity. It could also discuss the privatization of AMTRAK as an alternative to continuing government subsidies. Documents comparing government subsidies given to air and bus transportation with those provided to AMTRAK would also be relevant. lt;/topgt; Figure 3.8    Topic numbered 168 in the TREC collection. The Relevant Documents for Each Example Information Request At the TREC conferences, the set of relevant documents for each example information request (topic) is obtained from a pool of possible relevant documents. This pool is created by taking the top K documents (usually, K = 100) in the rankings generated by the various participating retrieval systems. The documents in the pool are then shown to human assessors who ultimately decide on the relevance of each document. This technique for assessing relevance is called the pooling method [794] and is based on two assumptions. First, that the vast majority of the relevant documents is collected in the assembled pool. Second, that the documents which are not in the pool can be considered to be not relevant. Both assumptions have been verified to be accurate in tests done at the TREC conferences. A detailed description of these relevance assessments can be found in [342, 794]. The (Benchmark) Tasks at the TREC Conferences The TREC conferences include two main information retrieval tasks [342]. In the first, called ad hoc task, a set of new (conventional) requests are run against a fixed document database. This is the situation which normally occurs in a library where a user is asking new queries against a set of static documents. In the second, called routing task, a set of fixed requests are run against a database whose documents are continually changing. This is like a filtering task in which the same questions are always being asked against a set of dynamic documents (for instance, news clipping services). Unlike a pure filtering task, however, the retrieved documents must be ranked. 90        RETRIEVAL EVALUATION For the ad hoc task, the participant systems receive the test information requests and execute them on a pre-specified document collection. For the routing task, the participant systems receive the test information requests and two distinct document collections. The first collection is used for training and allows the tuning of the retrieval algorithm. The second collection is used for testing the tuned retrieval algorithm. Starting at the TREC-4 conference, new secondary tasks, besides the ad hoc and routing tasks, were introduced with the purpose of allowing more specific comparisons among the various systems. At TREC-6, eight (specific) secondary tasks were added in as follows. ª Chinese Ad hoc task in which both the documents and the topics are in Chinese. ï  Filtering Routing task in which the retrieval algorithm has only to decide whether a new incoming document is relevant (in which case it is taken) or not (in which case it is discarded). No ranking of the documents taken needs to be provided. The test data (incoming documents) is processed in time-stamp order. ï  Interactive Task in which a human searcher interacts with the retrieval system to determine the relevant documents. Documents are ruled relevant or not relevant (i.e., no ranking is provided). ï  NLP Task aimed at verifying whether retrieval algorithms based on natural language processing offer advantages when compared to the more traditional retrieval algorithms based on index terms. ï  Cross languages Ad hoc task in wrhich the documents are in one language but the topics are in a different language. ï  High precision Task in which the user of a retrieval system is asked to retrieve ten documents that answer a given (and previously unknown) information request within five minutes (wall clock time). ï  Spoken document retrieval Task in which the documents are written transcripts of radio broadcast news shows. Intended to stimulate research on retrieval techniques for spoken documents. ï  Very large corpus Ad hoc task in which the retrieval systems have to deal with collections of size 20 gigabytes (7.5 million documents). For TREC-7, the NLP and the Chinese secondary tasks were discontinued. Additionally, the routing task was retired as a main task because there is a consensus that the filtering task is a more realistic type of routing task. TREC-7 also included a new task called Query Task in which several distinct query versions were created for each example information request [794]. The main goal of this task is to allow investigation of query-dependent retrieval strategies, a well known problem with the TREC collection due to the sparsity of the given information requests (which present very little overlap) used in past TREC conferences. REFERENCE COLLECTIONS        91 Besides providing detailed descriptions of the tasks to be executed, the TREC conferences also make a clear distinction between two basic techniques for transforming the information requests (which are in natural language) into query statements (which might be in vector form, in Boolean form, etc.). In the TREC-6 conference, the allowable query construction methods were divided into automatic methods, in which the queries were derived completely automatically from the test information requests, and manual methods, in which the queries were derived using any means other than the fully automatic method [794]. Evaluation Measures at the TREC Conferences At the TREC conferences, four basic types of evaluation measures are used: summary table statistics, recall-precision averages, document level averages, and average precision histograms. Briefly, these measures can be described as follows (see further details on these measures in Section 3.2). ï  Summary table statistics Consists of a table which summarizes statistics relative to a given task. The statistics included are: the number of topics (information requests) used in the task, the number of documents retrieved over all topics, the number of relevant documents which were effectively retrieved for all topics, and the number of relevant documents which could have been retrieved for all topics. ï  Recall-precision averages Consists of a table or graph with average precision (over all topics) at 11 standard recall levels.   Since the recall levels of the individual queries are seldom equal to the standard recall levels, interpolation is used to define the precision at the standard recall levels.   Further, a non-interpolated average precision over seen relevant documents (and over all topics) might be included. ï  Document level averages In this case, average precision (over all topics) is computed at specified document cutoff values (instead of standard recall levels).   For instance, the average precision might be computed when 5, 10, 20, 100 relevant documents have been seen.   Further, the average R-precision value (over all queries) might also be provided. ï  Average precision histogram Consists of a graph which includes a single measure for each separate topic. This measure (for a topic ti) is given, for instance, by the difference between the R-precision (for topic tz) for a target retrieval algorithm and the average R-precision (for topic t%) computed from the results of all participating retrieval systems.
mir-0063	3.3.2    The CACM and ISI Collections The TREC collection is a large collection which requires time consuming preparation before experiments can be carried out effectively at a local site. Further, 92        RETRIEVAL EVALUATION the testing itself is also time consuming and requires much more effort than that required to execute the testing in a small collection. For groups who are not interested in making this investment, an alternative approach is to use a smaller test collection which can be installed and experimented with in a much shorter time. Further, a small collection might include features which are not present in the larger TREC collection. For instance, it is well known that the example information requests at TREC present very little overlap among themselves and thus are not very useful for investigating the impact of techniques which take advantage of information derived from dependencies between the current and past user queries (an issue which received attention at the TREC-7 conference). Further, the TREC collection does not provide good support for experimenting with algorithms which combine distinct evidential sources (such as co-citations, bibliographic coupling, etc.) to generate a ranking. In these situations, alternative (and smaller) test collections might be more appropriate. For the experimental studies in [271], five different (small) test collections were developed: ADI (documents on information science), CACM, INSPEC (abstracts on electronics, computer, and physics), ISI, and Medlars (medical articles). In this section we cover two of them in detail: the CACM and the ISI test collections. Our discussion is based on the work by Fox [272]. The CACM Collection The documents in the CACM test collection consist of all the 3204 articles published in the Communications of the ACM from the first issue in 1958 to the last number of 1979. Those documents cover a considerable range of computer science literature due to the fact that the CACM served for many years as the premier periodical in the field. Besides the text of the documents, the collection also includes information on structured subfields (called concepts by Fox) as follows: ï  author names ï  date information ï  word stems from the title and abstract sections ï  categories derived from a hierarchical classification scheme ï  direct references between articles ï  bibliographic coupling connections ï  number of co-citations for each pair of articles. The subfields 'author names5 and "date information' provide information on authors and date of publication. The subfield 'word stems1 provides, for each document, a list of indexing terms (from the title and abstract sections) which have been stemmed (i.e., reduced to their grammatical roots as explained in Chapter 7). The subfield "categories' assigns a list of classification categories (from the Computing Reviews category scheme) to each document.  Since the REFERENCE COLLECTIONS        93 categories are fairly broad, the number of categories for any given document is usually smaller than five. The subfield 'direct references' provides a list of pairs of documents [da,d] in which each pair identifies a document da which includes a direct reference to a document d^. The subfield 'bibliographic coupling' provides a list of triples [di,d2,ttCited] m which the documents d\ and c?2 both include a direct reference to a same third document dj and the factor ncite(i counts the number of documents dj cited by both d\ and cfe- The subfield 'co-citations' provides a list of triples [di,d2 ,nCiting] in which the documents d\ and cfe are both cited by a same third document dj and the factor nCiting counts the number of documents dj citing both d\ and d^. Thus, the CACM collection provides a unique environment for testing retrieval algorithms which are based on information derived from cross-citing patterns ó a topic which has attracted much attention in the past. The CACM collection also includes a set of 52 test information requests. For instance, the information request numbered 1 reads as follows. What articles exist which deal with TSS (Time Sharing System), an operating system for IBM computers? For each information request, the collection also includes two Boolean query formulations and a set of relevant documents. Since the information requests are fairly specific, the average number of relevant documents for each information request is small and around 15. As a result, precision and recall figures tend to be low. The ISI Collection The 1460 documents in the ISI (often referred to as CISI) test collection were selected from a previous collection assembled by Small [731] at the Institute of Scientific Information (ISI). The documents selected (which are about information sciences) were those most cited in a cross-citation study done by Small. The main purpose of the ISI collection is to support investigation of similarities based on terms and on cross-citation patterns. The documents in the ISI collection include three types of subfieids as follows. ï  author names ï  word stems from the title and abstract sections ï  number of co-citations for each pair of articles. The meaning of each of these subfieids is as in the CACM collection. The ISI collection includes a total of 35 test information requests (in natural language) for which there are Boolean query formulations. It also includes 41 additional test information requests for which there is no Boolean query formulation (only the version in natural language).  The information requests are 94        RETRIEVAL EVALUATION fairly general which resulted in a larger number of relevant documents to each request (around 50). However, many of these relevant documents have no terms in common with the information requests which implies that precision and recall figures tend to be low. Statistics for the CACM and ISI Collections Tables 3.2 and 3.3 provide comparative summary statistics for the CACM and the ISI test collections. Collection    Num. Docs    Num. Terms    Terms/Docs. CACM              3204               HM46                401 ISI___________1460_________7392_________104.9 Table 3.2   Document statistics for the CACM and ISI collections. Collection    Number       Terms        Relevants     Relevants Queries    per Query    per Query    in Top 10 CACM             52             1L4              153               L9 ISI________35 k 76         8.1________4^8________1.7 Table 3.3   Query statistics for the CACM and ISI collections. We notice that, compared to the size of the collection, the ISI collection has a much higher percentage of relevant documents per query (3.4%) than the CACM collection (0.5%). However, as already discussed, many of the relevant documents in the ISI collection have no terms in common with the respective information requests which usually yields low precision. Related Test Collections At the Virginia Polytechnic Institute and State University, Fox has assembled together nine small test collections in a CD-ROM. These test collections have sizes comparable to those of the CACM and ISI collections, but include their own particularities. Since they have been used throughout the years for evaluation of information retrieval systems, they provide a good setting for the preliminary testing of information retrieval algorithms. A list of these nine test collections is provided in Table 3.4.
mir-0064	3.3.3    The Cystic Fibrosis Collection The cystic fibrosis (CF) collection [721] is composed of 1239 documents indexed with the term * cystic fibrosis' in the National Library of Medicine's MEdigital libraryINE database. Each document contains the following fields: REFERENCE COLLECTIONS        95 Collection	Subject	Num. Docs	Num. Queries ADI	Information Science	82	35 CACM	Computer Science	3200	64 ISI	Library Science	1460	76 CRAN	Aeronautics	1400	225 LISA	Library Science	6004	35 MED	Medicine	1033	30 NLM	Medicine	3078	155 NPL	Elec. Engineering	11,429	100 TIME	General Articles	423	83 Table 3.4    Test collections related to the CACM and ISI collections. ï  MEdigital libraryINE accession number ï  author ï  title ï  source ï  major subjects ï  minor subjects ï  abstract (or extract) ï  references ï  citations. The collection also includes 100 information requests (generated by an expert with two decades of clinical and research experience with cystic fibrosis) and the documents relevant to each query. Further, 4 separate relevance scores are provided for each relevant document. These relevance scores can be 0 (which indicates non-relevance), 1 (which indicates marginal relevance), and 2 (which indicates high relevance). Thus, the overall relevance score for a document (relative to a given query) varies from 0 to 8. Three of the relevance scores were provided by subject experts while the fourth relevance score was provided by a medical bibliographer. Table 3.5 provides some statistics regarding the information requests in the CF collection. We notice that the number of queries with at least one relevant document is close to the total number of queries in the collection. Further, for various relevance thresholds (the minimum value of relevance score used to characterize relevance), the average number of relevant documents per query is between 10 and 30. The CF collection, despite its small size, has two important characteristics. First, its set of relevance scores was generated directly by human experts through a careful evaluation strategy. Second, it includes a good number of information requests (relative to the collection size) and, as a result, the respective query vectors present overlap among themselves.   This allows experimentation 96        RETRIEVAL EVALUATION Relevance	Queries	Min.	Num.	Max.	Num.	Avg.	Num. Threshold	At Least 1	Rel	Docs	Rel	Docs	Rel	Docs Rel Doc 1                  100                  2                   189                31.9 2                  100                  1                   130                 18.1 3                   99                   1                   119                 14.9 4                   99                   1                   114                 14.1 5                   99                   1                    93                  10.7 6_________94__________1___________53__________6.4 Table 3.5    Summary statistics for the information requests in the CF collection. with retrieval strategies which take advantage of past query sessions to improve retrieval performance.
mir-0065	3.4    Trends and Research Issues A major trend today is research in interactive user interfaces. The motivation is a general belief that effective retrieval is highly dependent on obtaining proper feedback from the user. Thus, evaluation studies of interactive interfaces will tend to become more common in the near future. The main issues revolve around deciding which evaluation measures are most appropriate in this scenario. A typical example is the informativeness measure [754] introduced in 1992. Furthermore, the proposal, the study, and the characterization of alternative measures to recall and precision, such as the harmonic mean and the E measures, continue to be of interest.
mir-0066	3.5    Bibliographic Discussion A nice chapter on retrieval performance evaluation appeared in the book by Salton and McGill [698]. Even if outdated, it is still interesting reading. The book by Khorfage [451] also includes a full chapter on retrieval evaluation.  A recent paper by Mizzaro [569] presents a very complete survey of relevance studies throughout the years. About 160 papers are discussed in this paper. Two recent papers by Shaw, Burgin, and Howel [422, 423] discuss standards and evaluations in test collections for cluster-based and vector-based retrieval models. These papers also discuss the advantages of the harmonic mean (of recall and precision) as a single alternative measure for recall and precision. Problems with recall and precision related to systems which require a weak document ordering are discussed by Raghavan, Bollmann, and Jung [664, 663]. Tague-Sutcliffe proposes a measure of iiiformativeiiess for evaluating interactive user sessions [754]. BIBLIOGRAPHIC DISCUSSION        97 Our discussion of the TREC collection is based on the papers by Har-man [342] and by Vorhees and Harman [794]. The TREC collection is the most important reference collection nowadays for evaluation of complex information requests which execute on a large collection. Our coverage of the CACM and ISI collections is based on the work by Fox [272]. These collections are small, require short setup time, and provide a good environment for testing retrieval algorithms which are based on information derived from cross-citing patterns ó a topic which has attracted much attention in the past [94, 435, 694, 730, 732, 809] and which might nourish again in the context of the Web. The discussion on the Cystic Fibrosis (CF) collection is based on the work by Shaw, Wood, Wood, and Tibbo [721]. The CF collection is also small but includes a set of relevance scores carefully generated by human experts. Furthermore, its example information requests present overlap among themselves which allows the testing of retrieval algorithms that take advantage of past user sessions to improve retrieval performance.
mir-0068	4.1    Introduction We cover in this chapter the different kinds of queries normally posed to text retrieval systems. This is in part dependent on the retrieval model the system adopts, i.e., a full-text system will not answer the same kinds of queries as those answered by a system based on keyword ranking (as Web search engines) or on a hypertext model. In Chapter 8 we explain how the user queries are solved, while in this chapter we show which queries can be formulated. The type of query the user might formulate is largely dependent on the underlying information retrieval model. The different models for text retrieval systems are covered in Chapter 2. As in previous chapters, we want to distinguish between information retrieval and data retrieval, as we use this dichotomy to classify different query languages. We have chosen to distinguish first languages that allow the answer to be ranked, that is, languages for information retrieval. As covered in Chapter 2, for the basic information retrieval models, keyword-based retrieval is the main type of querying task. For query languages not aimed at information retrieval, the concept of ranking cannot be easily defined, so we consider them as languages for data retrieval. Furthermore, some query languages are not intended for final users and can be viewed as languages that a higher level software package should use to query an on-line database or a CD-ROM archive. In that case, we talk about protocols rather than query languages. Depending on the user experience, a different query language will be used. For example, if the user knows exactly what he wants, the retrieval task is easier and ranking may not even be needed. An important issue is that most query languages try to use the content (i.e., the semantics) and the structure of the text (i.e., the text syntax) to find relevant documents. In that sense, the system may fail to find the relevant answers (see Chapter 3). For this reason, a number of techniques meant to enhance the usefulness of the queries exist. Examples include the expansion of a word to the set of its synonyms or the use of a thesaurus and stemming to 99 100        QUERY LANGUAGES put together all the derivatives of the same word. Moreover, some words which are very frequent and do not carry meaning (such as 'the'), called stopwords, may be removed. This subject is covered in Chapter 7. Here we assume that all the query preprocessing has already been done. Although these operations are usually done for information retrieval, many of them can also be useful in a data retrieval context. When we want to emphasize the difference between words that can be retrieved by a query and those which cannot, we call the former 'keywords.' Orthogonal to the kind of queries that can be asked is the subject of the retrieval unit the information system adopts. The retrieval unit is the basic element which can be retrieved as an answer to a query (normally a set of such basic elements is retrieved, sometimes ranked by relevance or other criterion). The retrieval unit can be a file, a document, a Web page, a paragraph, or some other structural unit which contains an answer to the search query. Prom this point on, we will simply call those retrieval units 'documents,' although as explained this can have different meanings (see also Chapter 2). This chapter is organized as follows. We first show the queries that can be formulated with keyword-based query languages. They are aimed at information retrieval, including simple words and phrases as well as Boolean operators which manipulate sets of documents. In the second section we cover pattern matching, which includes more complex queries and is generally aimed at complementing keyword searching with more powerful data retrieval capabilities. Third, we cover querying on the structure of the text, which is more dependent on the particular text model. Finally, we finish with some standard protocols used on the Internet and by CD-ROM publishers.
mir-0069	4.2    Keyword-Based Querying A query is the formulation of a user information need. In its simplest form, a query is composed of keywords and the documents containing such keywords are searched for. Keyword-based queries are popular because they are intuitive, easy to express, and allow for fast ranking. Thus, a query can be (and in many cases is) simply a word, although it can in general be a more complex combination of operations involving several words. In the rest of this chapter we will refer to single-word and multiple-word queries as basic queries.   Patterns, which are covered in section 4.3, are also considered as basic queries.
mir-0070	4.2.1    Single-Word Queries The most elementary query that can be formulated in a text retrieval system is a word. Text documents are assumed to be essentially long sequences of words. Although some models present a more general view, virtually all models allow us KEYWORD-BASED QUERYING         101 to see the text in this perspective and to search words. Some models are also able to see the internal division of words into letters. These latter models permit the searching of other types of patterns, which are covered in section 4.3. The set of words retrieved by these extended queries can then be fed into the word-treating machinery, say to perform thesaurus expansion or for ranking purposes. A word is normally defined in a rather simple way. The alphabet is split into 'letters' and 'separators,' and a word is a sequence of letters surrounded by separators. More complex models allow us to specify that some characters are not letters but do not split a word, e.g. the hyphen in 'on-line.' It is good practice to leave the choice of what is a letter and what is a separator to the manager of the text database. The division of the text into words is not arbitrary, since words carry a lot of meaning in natural language. Because of that, many models (such as the vector model) are completely structured on the concept of words, and words are the only type of queries allowed (moreover, some systems only allow a small set of words to be extracted from the documents). The result of word queries is the set of documents containing at least one of the words of the query. Further, the resulting documents are ranked according to a degree of similarity to the query. To support ranking, two common statistics on word occurrences inside texts are commonly used: 'term frequency' which counts the number of times a word appears inside a document and 'inverse document frequency' which counts the number of documents in which a word appears. See Chapter 2 for more details. Additionally, the exact positions where a word appears in the text may be required for instance, by an interface which highlights each occurrence of that word.
mir-0071	4.2.2    Context Queries Many systems complement single-word queries with the ability to search words in a given context, that is, near other words. Words which appear near each other may signal a higher likelihood of relevance than if they appear apart. For instance, we may want to form phrases of words or find words which are proximal in the text. Therefore, we distinguish two types of queries: ï  Phrase  is a sequence of single-word queries. An occurrence of the phrase is a sequence of words. For instance, it is possible to search for the word kenhance,' and then for the word 'retrieval/ In phrase queries it is normally understood that the separators in the text need not be the same as those in the query (e.g., two spaces versus one space), and uninteresting words are not considered at all. For instance, the previous example could match a text such as '...enhance the retrieval...1. Although the notion of a phrase is a very useful feature in most cases, not all systems implement it. ?  Proximity   A more relaxed version of the phrase query is the proximity query. In this case, a sequence of single words or phrases is given, together 102        QUERY LANGUAGES AND translation syntax                      syntactic Figure 4.1 An example of a query syntax tree. It will retrieve all the documents which contain the word 'translation' as well as either the word 'syntax' or the word 'syntactic'. with a maximum allowed distance between them. For instance, the above example could state that the two words should occur within four words, and therefore a match could be c. . .enhance the power of retrieval. . ..' This distance can be measured in characters or words depending on the system. The words and phrases may or may not be required to appear in the same order as in the query. Phrases can be ranked in a fashion somewhat analogous to single words (see Chapters 2 and 5 for details). Proximity queries can be ranked in the same way if the parameters used by the ranking technique do not depend on physical proximity. Although it is not clear how to do better ranking, physical proximity has semantic value. This is because in most cases the proximity means that the words are in the same paragraph, and hence related in some way.
mir-0072	4.2.3    Boolean Queries The oldest (and still heavily used) form of combining keyword queries is to use Boolean operators.   A Boolean query   has a syntax composed of atoms (i.e., basic queries) that retrieve documents, and of Boolean operators which work on their operands (which are sets of documents) and deliver sets of documents. Since this scheme is in general compositional (i.e., operators can be composed over the results of other operators), a query syntax tree is naturally defined, where the leaves correspond to the basic queries and the internal nodes to the operators. The query syntax tree operates on an algebra over sets of documents (and the final answer of the query is also a set of documents). This is much as, for instance, the syntax trees of arithmetic expressions where the numbers and variables are the leaves and the operations form the internal nodes. Figure 4.1 shows an example. The operators most commonly used, given two basic queries or Boolean KEYWORD-BASED QUERYING         103 subexpressions t\ and e2, are: © OR The query (ei OR 62) selects all documents which satisfy ei or 62-Duplicates are eliminated. Æ AND The query (ei AND e2) selects all documents which satisfy both e\ and e2Æ BUT The query (ei BUT e2) selects all documents which satisfy e\ but not e2. Notice that classical Boolean logic uses a NOT operation, where (NOT e2) is valid whenever 62 is not. In this case all documents not satisfying e2 should be delivered, which may retrieve a huge amount of text and is probably not what the user wants. The BUT operator, instead, restricts the universe of retrievable elements to the result of e\.\ Besides selecting the appropriate documents, the IR system may also sort the documents by some criterion, highlight the occurrences within the documents of the words mentioned in the query, and allow feedback by taking the answer set as a basis to reformulate the query. With classic Boolean systems, no ranking of the retrieved documents is normally provided. A document either satisfies the Boolean query (in which case it is retrieved) or it does not (in which case it is not retrieved). This is quite a limitation because it does not allow for partial matching between a document and a user query. To overcome this limitation, the condition for retrieval must be relaxed. For instance, a document which partially satisfies an AND condition might be retrieved. In fact, it is widely accepted that users not trained in mathematics find the meaning of Boolean operators difficult to grasp. With this problem in mind, a Lfuzzy Boolean1 set of operators has been proposed. The idea is that the meaning of AND and OR can be relaxed, such that instead of forcing an element to appear in all the operands (AND) or at least in one of the operands (OR), they retrieve elements appearing in some operands (the AND may require it to appear in more operands than the OR). Moreover, the documents are ranked higher when they have a larger number of elements in common with the query (see Chapter 2).
mir-0073	4.2.4    Natural Language Pushing the fuzzy Boolean model even further, the distinction between AND and OR can be completely blurred, so that a query becomes simply an enumeration of words and context queries. All the documents matching a portion of the user query are retrieved. Higher ranking is assigned to those documents matching more parts of the query. The negation can be handled by letting the user express t  Notice that the same problem arises in the relational calculus, which is shown similar to the relational algebra only when "unsafe' expressions are avoided.   Unsafe expressions are those that make direct or indirect reference to a universe of elements, as NOT does. 104        QUERY LANGUAGES that some words are not desired, so that the documents containing them are penalized in the ranking computation. A threshold may be selected so that the documents with very low weights are not retrieved. Under this scheme we have completely eliminated any reference to Boolean operations and entered into the field of natural language queries. In fact, one can consider that Boolean queries are a simplified abstraction of natural language queries. A number of new issues arise once this model is used, especially those related to the proper way to rank an element with respect to a query. The search criterion can be re-expressed using a different model, where documents and queries are considered just as a vector of 'term weights' (with one coordinate per interesting keyword or even per existing text word) and queries are considered in exactly the same way (context queries are not considered in this case). Therefore, the query is now internally converted into a vector of term weights and the aim is to retrieve all the vectors (documents) which are close to the query (where closeness has to be defined in the model). This allows many interesting possibilities, for instance a complete document can be used as a query (since it is also a vector), which naturally leads to the use of relevance feedback techniques (i.e., the user can select a document from the result and submit it as a new query to retrieve documents similar to the selected one). The algorithms for this model are totally different from those based on searching patterns (it is even possible that not every text word needs to be searched but only a small set of hopefully representative keywords extracted from each document). Natural language querying is also covered in Chapter 14.
mir-0074	4.3    Pattern Matching In this section we discuss more specific query formulations (based on the concept of a pattern) which allow the retrieval of pieces of text that have some property. These data retrieval queries are useful for linguistics, text statistics, and data extraction.  Their result can be fed into the composition mechanism described above to form phrases and proximity queries, comprising what we have called basic queries. Basic queries can be combined using Boolean expressions. In this sense we can view these data retrieval capabilities as enhanced tools for information retrieval. However, it is more difficult to rank the result of a pattern matching expression. A pattern is a set of syntactic features that must occur in a text segment. Those segments satisfying the pattern specifications are said to 'match" the pattern. We are interested in documents containing segments which match a given search pattern. Each system allows the specification of some types of patterns, which range from very simple (for example, words) to rather complex (such as regular expressions). In general, as more powerful is the set of patterns allowed, more involved are the queries that the user can formulate and more complex is the implementation of the search. The most used types of patterns are: PATTERN MATCHING         105 ´ Words A string (sequence of characters) which must be a word in the text (see section 4.2). This is the most basic pattern. Æ Prefixes A string which must form the beginning of a text word. For instance, given the prefix 'comput' all the documents containing words such as 'computer,' 'computation,' 'computing,' etc. are retrieved. Æ Suffixes A string which must form the termination of a text word. For instance, given the suffix 'ters' all the documents containing words such as 'computers,' 'testers,' 'painters,' etc. are retrieved. ´ Substrings A string which can appear within a text word. For instance, given the substring 'tal' all the documents containing words such as 'coastal,' 'talk,' 'metallic,' etc. are retrieved. This query can be restricted to find the substrings inside words, or it can go further and search the substring anywhere in the text (in this case the query is not restricted to be a sequence of letters but can contain word separators). For instance, a search for 'any flow' will match in the phrase '. . .many flowers. . ..' ï  Ranges A pair of strings which matches any word lying between them in lexicographical order. Alphabets are normally sorted, and this induces an order into the strings which is called lexicographical order (this is indeed the order in which words in a dictionary are listed).  For instance, the range between words 'held' and 'hold' will retrieve strings such as 'hoax' and 'hissing.' ï  Allowing errors   A word together with an error threshold.  This search pattern retrieves all text words which are "similar' to the given word. The concept of similarity can be defined in many ways. The general concept is that the pattern or the text may have errors (coming from typing, spelling, or from optical character recognition software, among others), and the query should try to retrieve the given word and what are likely to be its erroneous variants. Although there are many models for similarity among words, the most generally accepted in text retrieval is the Levenshtein distance, or simply edit distance.   The edit distance between two strings is the minimum number of character insertions, deletions, and replacements needed to make them equal (see Chapter 6). Therefore, the query specifies the maximum number of allowed errors for a word to match the pattern (i.e., the maximum allowed edit distance). This model can also be extended to search substrings (not only words), retrieving any text segment which is at the allowed edit distance from the search pattern. Under this extended model, if a typing error splits 'flower' into "f lo wer' it could still be found with one error, while in the restricted case of words it could not (since neither kflo' nor 'wer' are at edit distance 1 fro in k flower").  Variations on this distance model are of use in computational biology for searching on DNA or protein sequences as well as in signal processing. ï  Regular expressions   Some text retrieval systems allow searching for regular expressions. A regular expression is a rather general pattern built 106        QUERY LANGUAGES up by simple strings (which are meant to be matched as substrings) and the following operators: -  union: if ei and 62 are regular expressions, then (ei|e2) matches what t\ or 62 matches. -  concatenation: if e\ and e2 are regular expressions, the occurrences of (ei 62) are formed by the occurrences of e\ immediately followed by those of 62 (therefore simple strings can be thought of as a concatenation of their individual letters). -  repetition: if e is a regular expression, then (e*) matches a sequence of zero or more contiguous occurrences of e. For instance, consider a query like 'pro (blem | tein) (s | e) (0 | 1 2)*" (where e denotes the empty string). It will match words such as cproblem021 and 'proteins.' As in previous cases, the matches can be restricted to comprise a whole word, to occur inside a word, or to match an arbitrary text segment. This can also be combined with the previous type of patterns to search a regular expression allowing errors. ï Extended patterns It is normal to use a more user-friendly query language to represent some common cases of regular expressions. Extended patterns are subsets of the regular expressions which are expressed with a simpler syntax. The retrieval system can internally convert extended patterns into regular expressions, or search them with specific algorithms. Each system supports its own set of extended patterns, and therefore no formal definition exists. Some examples found in many new systems are: -  classes of characters, i.e.   one or more positions within the pattern are matched by any character from a pre-defined set.  This involves features such as case-insensitive matching, use of ranges of characters (e.g., specifying that some character must be a digit), complements (e.g., some character must not be a letter), enumeration (e.g., a character must be a vowel), wild cards (i.e., a position within the pattern matches with anything), among others. -  conditional expressions, i.e., a part of the pattern may or may not appear. -  wild characters which match any sequence in the text, e.g. any word which starts as kflo1 and ends with 4ers,' which matches 'flowers' as well as 'flounders/ -  combinations that allow some parts of the pattern to match exactly and other parts with errors.
mir-0075	4.4    Structural Qyeries Up to now we have considered the text collection as a set of documents which can be queried with regard to their text content. This model is unable to take advantage of novel text features which are becoming commonplace, such as the (a) STRUCTURAL QUERIES (c) 107 Figure 4.2    The three main structures:   (a) form-like fixed structure, (6) hypertext structure, and (c) hierarchical structure. text structure. The text collections tend to have some structure built into them, and allowing the user to query those texts based on their structure (and not only their content) is becoming attractive. The standardization of languages to represent structured texts such as HTML has pushed forward in this direction (see Chapter 6). As discussed in Chapter 2, mixing contents and structure in queries allows us to pose very powerful queries, which are much more expressive than each query mechanism by itself. By using a query language that integrates both types of queries, the retrieval quality of textual databases can be improved. This mechanism is built on top of the basic queries, so that they select a set of documents that satisfy certain constraints on their content (expressed using words, phrases, or patterns that the documents must contain). On top of this, some structural constraints can be expressed using containment, proximity, or other restrictions on the structural elements (e.g., chapters, sections, etc.) present in the documents. The Boolean queries can be built on top of the structural queries, so that they combine the sets of documents delivered by those queries. In the Boolean syntax tree (recall the example of Figure 4.1) the structural queries form the leaves of the tree. On the other hand, structural queries can themselves have a complex syntax. We divide this section according to the type of structures found in text databases. Figure 4.2 illustrates them. Although structured query languages should be amenable for ranking, this is still an open problem. In what follows it is important to distinguish the difference between the structure that a text may have and what can be queried about that structure. In general, natural language texts may have any desired structure. However, different models allow the querying of only some aspects of the real structure. When we say that the structure allowed is restricted in some way, we mean that only the aspects which follow this restriction can be queried, albeit the text may have more structural information. For instance, it is possible that an article has a nested structure of sections and subsections, but the query model does not accept recursive structures. In this case we will not be able to query for sections included in others, although this may be the case in the texts documents under consideration. 108        QUERY LANGUAGES
mir-0076	4.4.1    Fixed Structure The structure allowed in texts was traditionally quite restrictive. The documents had a fixed set of fields, much like a filled form. Each field had some text inside. Some fields were not present in all documents. Only rarely could the fields appear in any order or repeat across a document. A document could not have text not classified under any field. Fields were not allowed to nest or overlap. The retrieval activity allowed on them was restricted to specifying that a given basic pattern was to be found only in a given field. Most current commercial systems use this model. This model is reasonable when the text collection has a fixed structure. For instance, a mail archive could be regarded as a set of mails, where each mail has a sender, a receiver, a date, a subject, and a body field. The user can thus search for the mails sent to a given person with 'football' in the subject field. However, the model is inadequate to represent the hierarchical structure present in an HTML document, for instance. If the division of the text into fields is rigid enough, the content of some fields can even be interpreted not as text but as numbers, dates, etc. thereby allowing different queries to be posed on them (e.g., month ranges in dates). It is not hard to see that this idea leads naturally to the relational model, each field corresponding to a column in the database table. Looking at the database as a text allows us to query the textual fields with much more power than is common in relational database systems. On the other hand, relational databases may make better use of their knowledge on the data types involved to build specialized and more efficient indices. A number of approaches towards combining these trends have been proposed in recent years, their main problem being that they do not achieve optimal performance because the text is usually stored together with other types of data. Nevertheless, there are several proposals that extend SQL (Structured Query Language) to allow full-text retrieval. Among them we can mention proposals by leading relational database vendors such as Oracle and Sybase, as well as SFQL, which is covered in section 4.5.
mir-0077	4.4.2    Hypertext Hypertexts probably represent the maximum freedom with respect to structuring power. A hypertext is a directed graph where the nodes hold some text and the links represent connections between nodes or between positions inside the nodes (see Chapter 2). Hypertexts have received a lot of attention since the explosion of the Web, which is indeed a gigantic hypertext-like database spread across the world. However, retrieval from a hypertext began as a merely navigational activity. That is. the user had to manually traverse the hypertext nodes following links to search what he wanted. It was not possible to query the hypertext based on its structure. Even in the Web one can search by the text contents of the nodes, but not by their structural connectivity. STRUCTURAL QUERIES        109 An interesting proposal to combine browsing and searching on the Web is WebGlimpse. It allows classical navigation plus the ability to search by content in the neighborhood of the current node. Currently, some query tools have appeared that achieve the goal of querying hypertexts based on their content and their structure. This problem is covered in detail in Chapter 13.
mir-0078	4.4.3    Hierarchical Structure An intermediate structuring model which lies between fixed structure and hypertext is the hierarchical structure. This model represents a recursive decomposition of the text and is a natural model for many text collections (e.g., books, articles, legal documents, structured programs, etc.). Figure 4.3 shows an example of such a hierarchical structure. The simplification from hypertext to a hierarchy allows the adoption of faster algorithms to solve queries. As a general rule, the more powerful the model, the less efficiently it can be implemented. Our aim in this section is to analyze and discuss the different approaches presented by the hierarchical models. We first present a selection of the most representative models and then discuss the main subjects of this area. Chapter 4					(T chapter^) 4.1 Introduction We cover in this chapter the different kinds of ..				(^ section^)	C section  ") 4.4 Structural Queries ; C^tltle^)             QigureJ) j,  Introduction     We cover...	Structural.....                     ^  figure section title "structural" Figure 4.3   An example of a hierarchical structure: the page of a book, its schematic view, and a parsed query to retrieve the figure. 110        QUERY LANGUAGES A Sample of Hierarchical Models PAT Expressions These are built on the same index as the text index, i.e. there is no special separate index on the structure. The structure is assumed to be marked in the text by tags (as in HTML), and therefore is defined in terms of initial and final tags. This allows a dynamic scheme where the structure of interest is not fixed but can be determined at query time. For instance, since tags need not to be especially designed as normal tags, one can define that the end-of-lines are the marks in order to define a structure on lines. This also allows for a very efficient implementation and no additional space overhead for the structure. Each pair of initial and final tags defines a region, which is a set of contiguous text areas. Externally computed regions are also supported. However, the areas of a region cannot nest or overlap, which is quite restrictive. There is no restriction on areas of different regions. Apart from text searching operations, it is possible to select areas containing (or not) other areas, contained (or not) in other areas, or followed (or not) by other areas. A disadvantage is that the algebra mixes regions and sets of text positions which are incompatible and force complex conversion semantics. For instance, if the result of a query is going to generate overlapping areas (a fact that cannot be determined beforehand) then the result is converted to positions. Also, the dynamic definition of regions is flexible but requires the structure to be express-able using tags (also called 'markup', see Chapter 6), which for instance does not occur in some structured programming languages. Overlapped Lists These can be seen as an evolution of PAT Expressions. The model allows for the areas of a region to overlap, but not to nest. This elegantly solves the problems of mixing regions and sets of positions. The model considers the use of an inverted list (see Chapter 8) where not only the words but also the regions are indexed. Apart from the operations of PAT Expressions, the model allows us to perform set union, and to combine regions. Combination means selecting the minimal text areas which include any two areas taken from two regions. A 'followed by' operator imposes the additional restriction that the first area must be before the second one. An kn words' operator generates the region of all (overlapping) sequences of n words of the text (this is further used to retrieve elements close to each other). If an operation produces a region with nested areas, only the minimal areas are selected. An example is shown in Figure 2.11. The implementation of this model can also be very efficient. It is not clear, however, whether overlapping is good or not for capturing the structural properties that information has in practice. A new proposal allows the structure to be nested and overlapped, showing that more interesting operators can still be implemented. STRUCTURAL QUERIES         111 Lists of References These are an attempt to make the definition and querying of structured text uniform, using a common language. The language goes beyond querying structured text, so we restrict our attention to the subset in which we are interested. The structure of documents is fixed and hierarchical, which makes it impossible to have overlapping results. All possible regions are defined at indexing time. The answers delivered are more restrictive, since nesting is not allowed (only the top-level elements qualify) and all elements must be of the same type, e.g. only sections, or only paragraphs. In fact, there are also hypertext links but these cannot be queried (the model also has navigational features). A static hierarchical structure makes it possible to speak in terms of direct ancestry of nodes, a concept difficult to express when the structure is dynamic. The language allows for querying on 'path expressions,' which describe paths in the structure tree. Answers to queries are seen as lists of 'references.' A reference is a pointer to a region of the database. This integrates in an elegant way answers to queries and hypertext links, since all are lists of references. Proximal Nodes This model tries to find a good compromise between expressiveness and efficiency. It does not define a specific language, but a model in which it is shown that a number of useful operators can be included achieving good efficiency. The structure is fixed and hierarchical. However, many independent structures can be defined on the same text, each one being a strict hierarchy but allowing overlaps between areas of different hierarchies. An example is shown in Figure 2.12. A query can relate different hierarchies, but returns a subset of the nodes of one hierarchy only (i.e., nested elements are allowed in the answers, but no overlaps). Text matching queries are modeled as returning nodes from a special 'text hierarchy.' The model specifies a fully compositional language where the leaves of the query syntax tree are formed by basic queries on contents or names of structural elements (e.g., all chapters). The internal nodes combine results. For efficiency, the operations defined at the internal nodes must be implementable looking at the identity and text areas of the operands, and must relate nodes which are close in the text. It has been shown that many useful operators satisfy this restriction: selecting elements that (directly or transitively) include or are included in others; that are included at a given position (e.g., the third paragraph of each chapter); that are shortly before or after others; set manipulation; and many powerful variations. Operations on content elements deliver a set of regions with no nesting, and those results can be fully integrated into any query. This ability to integrate the text into the model is very useful. On the other hand, some queries requiring non-proximal operations are not allowed, for instance semijoins. An example of a semijoin is 'give me the titles of all the chapters referenced in this chapter/ 112        QUERY LANGUAGES Tree Matching This model relies on a single primitive: tree inclusion, whose main idea is as follows. Interpreting the structure both of the text database and of the query (which is defined as a pattern on the structure) as trees, determine an embedding of the query into the database which respects the hierarchical relationships between nodes of the query. Two variants are studied. Ordered inclusion forces the embedding to respect the left-to-right relations among siblings in the query, while unordered inclusion does not. The leaves of the query can be not only structural elements but also text patterns, meaning that the ancestor of the leaf must contain that pattern. Simple queries return the roots of the matches. The language is enriched by Prolog-like variables, which can be used to express requirements on equality between parts of the matched substructure and to retrieve another part of the match, not only the root. Logical variables are also used for union and intersection of queries, as well as to emulate tuples and join capabilities. Although the language is set oriented, the algorithms work by sequentially obtaining each match. The use of logical variables and unordered inclusion makes the search problem intractable (NP-hard in many cases). Even the good cases have an inefficient solution in practice. Discussion A survey of the main hierarchical models raises a number of interesting issues, most of them largely unresolved up to now. Some of them are listed below. Static or dynamic structure As seen, in a static structure there are one or more explicit hierarchies (which can be queried, e.g., by ancestry), while in a dynamic structure there is not really a hierarchy, but the required elements are built on the fly. A dynamic structure is implemented over a normal text index, while a static one may or may not be. A static structure is independent of the text markup, while a dynamic one is more flexible for building arbitrary structures. Restrictions on the structure The text or the answers may have restrictions about nesting and/or overlapping. In some cases these restrictions exist for efficiency reasons. In other cases, the query language is restricted to avoid restricting the structure.   This choice is largely dependent on the needs of each application. Integration with text In many structured models, the text content is merely seen as a secondary source of information which is used only to restrict the matches of structural elements. In classic IR models, on the other side, information on the structure is the secondaxv element which is used onlv to restrict text matches. For an effective QUERY PROTOCOLS         113 integration of queries on text content with queries on text structure, the query language must provide for full expressiveness of both types of queries and for effective means of combining them. Query language Typical queries on structure allow the selection of areas that contain (or not) other areas, that are contained (or not) in other areas, that follow (or are followed by) other areas, that are close to other areas, and set manipulation. Many of them are implemented in most models, although each model has unique features. Some kind of standardization, expressiveness taxonomy, or formal categorization would be highly desirable but does not exist yet.
mir-0079	4.5    Query Protocols In this section we briefly cover some query languages that are used automatically by software applications to query text databases. Some of them are proposed as standards for querying CD-ROMs or as intermediate languages to query library systems. Because they are not intended for human use, we refer to them as protocols rather than languages. More information on protocols can be found in Chapters 14 and 15. The most important query protocols are: ï  Z39.50 is a protocol approved as a standard in 1995 by ANSI and NISO. This protocol is intended to query bibliographical information using a standard interface between the client and the host database manager which is independent of the client user interface and of the query database language at the host. The database is assumed to be a text collection with some fixed fields (although it is more flexible than usual). The Z39.50 protocol is used broadly and is part, for instance, of WAIS (see below). The protocol does not only specify the query language and its semantics, but also the way in which client and server establish a session, communicate and exchange information, etc.   Although originally conceived only to operate on bibliographical information (using the Machine Readable Cataloging Record (MARC) format), it has been extended to query other types of information as well. ï  WAIS   (Wide Area Information Service) is a suite of protocols that was popular at the beginning of the 1990s before the boom of the Web.  The goal of WAIS was to be a network publishing protocol and to be able to query databases through the Internet. In the CD-ROM publishing arena, there are several proposals for query protocols. The main goal of these protocols is to provide 'disk interchangeability." This means more flexibility in data communication between primary information providers and end users. It also enables significant cost savings since it allows access to diverse information without the need to buy, install, and train users for different data retrieval applications. We briefly cover three of these proposals: 114        QUERY LANGUAGES Æ CCL (Common Command Language) is a NISO proposal (Z39.58 or ISO 8777) based on Z39.50. It defines 19 commands that can be used interactively. It is more popular in Europe, although very few products use it. It is based on the classical Boolean model. ï  CD-RDx   (Compact Disk Read only Data exchange) uses a client-server architecture and has been implemented in most platforms.   The client is generic while the server is designed and provided by the CD-ROM publisher who includes it with the database in the CD-ROM. It allows fixed-length fields, images, and audio, and is supported by such US national agencies as the CIA, NASA, and GSA. ï  SFQL   (Structured Full-text Query Language) is based on SQL and also has a client-server architecture. SFQL has been adopted as a standard by the aerospace community (the Air Transport Association/Aircraft Industry Association). Documents are rows in a relational table and can be tagged using SGML. The language defines the format of the answer, which has a header and a variable length message area. The language does not define any specific formatting or markup. For example, a query in SFQL is: Select abstract from journal.papers where title contains  "text search" The language supports Boolean and logical operators, thesaurus, proximity operations, and some special characters such as wild cards and repetition. For example: where paper contains  "retrieval"  or like  "info ï/," and date gt; 1/1/98 Compared with CCL or CD-RDx, SFQL is more general and flexible, although it is based on a relational model, which is not always the best choice for a document database.
mir-0080	4.6    Trends and Research Issues We reviewed in this chapter the main aspects of the query languages that retrieve information from textual databases. Our discussion covered from the most classic tools to the most novel capabilities that are emerging, from searching words to extended patterns, from the Boolean model to querying structures. Table 4.1 shows the different basic queries allowed in the different models. Although the probabilistic and the Bayesian belief network (BBN) models are based on word queries, they can incorporate set operations. We present in Figure 4.4 the types of operations we covered and how they can be structured (not all of them exist in all models and not ail of them have to be used to form a query). The figure shows, for instance, that we can form a query using Boolean operations over phrases (skipping structural queries), which TRENDS AND RESEARCH ISSUES 115 Model	Queries allowed Boolean	word, set operations Vector	words Probabilistic	words BBN	words Table 4.1    Relationship between types of queries and models. can be formed by words and by regular expressions (skipping the ability to allow errors). natural language Boolean queries fuzzy Boolean structural queries proximity basic queries pattern matching words keywords and context substrings prefixes suffixes regular expressions extended patterns Figure 4.4    The types of queries covered and how they are structured. The area of query languages for text databases is definitely moving towards higher flexibility. While text models are moving towards the goal of achieving a better understanding of the user needs (by providing relevance feedback, for instance), the query languages are allowing more and more power in the specification of the query. While extended patterns and searching allowing errors permit us to find patterns without complete knowledge of what is wanted, querying on the structure of the text (and not only on its content) provides greater expressiveness and increased functionality. Another important research topic is visual query languages. Visual metaphors can help non-experienced users to pose complex Boolean queries. Also, a visual query language can include the structure of the document. This topic is related to user interfaces arid visualization and is covered in Chapter 10. 116        QUERY LANGUAGES
mir-0081	4.7    Bibliographic Discussion The material on classical query languages (most simple patterns, Boolean model, and fixed structure) is based on current commercial systems, such as Fulcrum, Verity, and others, as well as on non-commercial systems such as Glimpse [540] and Igrep [26]. The fuzzy Boolean model is described in [703]. The Levenshtein distance is described in [504] and [25]. Soundex is explained in [445]. A comparison of the effectiveness of different similarity models is given in [595]. A good source on regular expressions is [375]. A rich language on extended patterns is described in [837]. A classical reference on hypertext is [181]. The WebGlimpse system is presented in [539]. The discussion of hierarchical text is partially based on [41]. The original proposals are: PAT Expressions [693], Overlapped Lists [173] and the new improved proposal [206], Lists of References [534], Proximal Nodes [590], and Tree Matching [439]. PAT Expressions are the basic model of the PAT Text Searching System [309]. A simple structured text model is presented in [36] and a visual query language that includes structure is discussed in [44]. More information on Z39.50 can be obtained from [23]. More information on WAIS is given in [425]. For details on SFQL see [392].
mir-0083	5.1    Introduction Without detailed knowledge of the collection make-up and of the retrieval environment, most users find it difficult to formulate queries which are well designed for retrieval purposes. In fact, as observed with Web search engines, the users might need to spend large amounts of time reformulating their queries to accomplish effective retrieval. This difficulty suggests that the first query formulation should be treated as an initial (naive) attempt to retrieve relevant information. Following that, the documents initially retrieved could be examined for relevance and new improved query formulations could then be constructed in the hope of retrieving additional useful documents. Such query reformulation involves two basic steps: expanding the original query with new terms and reweighting the terms in the expanded query. In this chapter, we examine a variety of approaches for improving the initial query formulation through query expansion and term reweighting. These approaches are grouped in three categories: (a) approaches based on feedback information from the user; (b) approaches based on information derived from the set of documents initially retrieved (called the local set of documents); and (c) approaches based on global information derived from the document collection. In the first category, user relevance feedback methods for the vector and probabilistic models are discussed. In the second category, two approaches for local analysis (i.e., analysis based on the set of documents initially retrieved) are presented. In the third category, two approaches for global analysis are covered. Our discussion is not aimed at completely covering the area, neither does it intend to present an exhaustive survey of query operations. Instead, our discussion is based on a selected bibliography which, we believe, is broad enough to allow an overview of the main issues and tradeoffs involved in query operations. Local and global analysis are highly dependent on clustering algorithms. Thus, clustering is covered throughout our discussion. However, there is no intention of providing a complete survey of clustering algorithms for information retrieval. 117 118        QUERY OPERATIONS
mir-0084	5-2    User Relevance Feedback Relevance feedback is the most popular query reformulation strategy. In a relevance feedback cycle, the user is presented with a list of the retrieved documents and, after examining them, marks those which are relevant. In practice, only the top 10 (or 20) ranked documents need to be examined. The main idea consists of selecting important terms, or expressions, attached to the documents that have been identified as relevant by the user, and of enhancing the importance of these terms in a new query formulation. The expected effect is that the new query will be moved towards the relevant documents and away from the non-relevant ones. Early experiments using the Smart system [695] and later experiments using the probabilistic weighting model [677] have shown good improvements in precision for small test collections when relevance feedback is used. Such improvements come from the use of two basic techniques: query expansion (addition of new terms from relevant documents) and term reweighting (modification of term weights based on the user relevance judgement). Relevance feedback presents the following main advantages over other query reformulation strategies: (a) it shields the user from the details of the query reformulation process because all the user has to provide is a relevance judgement on documents; (b) it breaks down the whole searching task into a sequence of small steps which are easier to grasp; and (c) it provides a controlled process designed to emphasize some terms (relevant ones) and de-emphasize others (non-relevant ones). In the following three subsections, we discuss the usage of user relevance feedback to (a) expand queries with the vector model, (b) reweight query terms with the probabilistic model, and (c) reweight query terms with a variant of the probabilistic model.
mir-0085	5.2.1    Query Expansion and Term Reweighting for the Vector Model The application of relevance feedback to the vector model considers that the term-weight vectors of the documents identified as relevant (to a given query) have similarities among themselves (i.e., relevant documents resemble each other). Further, it is assumed that non-relevant documents have term-weight vectors which are dissimilar from the ones for the relevant documents. The basic idea is to reformulate the query such that it gets closer to the term-weight vector space of the relevant documents. Let us define some additional terminology regarding the processing of a given query q as follows, Dr: set of relevant documents, as identified by the user, among the retrieved documents; Dn: set of non-relevant documents among the retrieved documents; Cr: set of relevant documents among all documents in the collection; USER RELEVANCE FEEDBACK         119 |Z)r|, |Dn|, \Cr\:   number of documents in the sets Dr, Dni and Cr, respectively; a,/?, 7: tuning constants. Consider first the unrealistic situation in which the complete set Cr of relevant documents to a given query q is known in advance. In such a situation, it can be demonstrated that the best query vector for distinguishing the relevant documents from the non-relevant documents is given by, The problem with this formulation is that the relevant documents which compose the set Cr are not known a priori. In fact, we are looking for them. The natural way to avoid this problem is to formulate an initial query and to incrementally change the initial query vector. This incremental change is accomplished by restricting the computation to the documents known to be relevant (according to the user judgement) at that point. There are three classic and similar ways to calculate the modified query qm as follows, Standard JRochio :    qm    = a q + ]P   dj VdjEDr Ide-Regular :    qm    = a q + (3   ^    dj   ó 7 \fd3eDr Ide^Dec^Hi :    qm    = a q +  j3   ^    d3   -  7   maxnon_Teievani{d]) where maxnon-reievant(dj) is a reference to the highest ranked non-relevant document. Notice that now Dr and Dn stand for the sets of relevant and non-relevant documents (among the retrieved ones) according to the user judgement, respectively. In the original formulations, Rochio [678] fixed a = 1 and Ide [391] fixed a = ft = 7 = 1. The expressions above are modern variants. The current understanding is that the three techniques yield similar results (in the past, Ide Dec-Hi was considered slightly better). The Rochio formulation is basically a direct adaptation of equation 5.1 in which the terms of the original query are added in. The motivation is that in practice the original query q may contain important information. Usually, the information contained in the relevant documents is more important than the information provided by the non-relevant documents [698]. This suggests making the constant 7 smaller than the constant /3. An alternative approach is to set ~ to 0 which yields a positive feedback strategy. The main advantages of the above relevance feedback techniques are simplicity and good results. The simplicity is due to the fact that the modified term weights are computed directly from the set of retrieved documents.  The good 120       QUERY OPERATIONS results are observed experimentally and are due to the fact that the modified query vector does reflect a portion of the intended query semantics. The main disadvantage is that no optimality criterion is adopted.
mir-0086	5.2.2    Term Reweighting for the Probabilistic Model The probabilistic model dynamically ranks documents similar to a query q according to the probabilistic ranking principle. Prom Chapter 2, we already know that the similarity of a document dj to a query q can be expressed as 2=1 szm(d^q) a V wUq wid     log       v   J   '     + log     p^ -              (5.2) l-P(kt\R)        *    P(ki\R) where P(fci|i?) stands for the probability of observing the term k{ in the set R of relevant documents and P(ki\R) stands for the probability of observing the term k{ in the set R of non-relevant documents. Initially, equation 5.2 cannot be used because the probabilities P(kl\R) and P(kt\R) are unknown. A number of different methods for estimating these probabilities automatically (i.e., without feedback from the user) were discussed in Chapter 2. With user feedback information, these probabilities are estimated in a slightly different way as follows. For the initial search (when there are no retrieved documents yet), assumptions often made include: (a) P(ki\R) is constant for all terms ki (typically 0.5) and (b) the term probability distribution P(ki\R) can be approximated by the distribution in the whole collection. These two assumptions yield: P{k{\R)   =   0.5 where, as before, nt stands for the number of documents in the collection which contain the term k(. Substituting into equation 5.2, we obtain t                                            AT f ,     N     v^              ,     iv-rii sirriin^iaiidj.q) ~ x     ......     1-i For the feedback searches, the accumulated statistics related to the relevance or non-relevance of previously retrieved documents are used to evaluate the probabilities P(ki\R) and P(ki\~R). As before, let Dr be the set of relevant retrieved documents (according to the user judgement) and Drj be the subset of DT composed of the documents which contain the term k{. Then, the probabilities P(ki\R) and P(ki\R) can be approximated by (5.3) USER RELEVANCE FEEDBACK        121 Using these approximations, equation 5.2 can rewritten as r      \Dr,i\                  n,-\Dr,i\ Notice that here, contrary to the procedure in the vector space model, no query expansion occurs. The same query terms are being reweighted using feedback information provided by the user. Formula 5.3 poses problems for certain small values of \Dr\ and \Dr^\ that frequently arise in practice (\Dr\ = 1, \Dr^\ = 0). For this reason, a 0.5 adjustment factor is often added to the estimation of P(kt\R) and P(ki\R) yielding This 0.5 adjustment factor may provide unsatisfactory estimates in some cases, and alternative adjustments have been proposed such as rii/N or (n^ ó |Dr,z|) /{N - \Dr\) [843]. Taking rii/N as the adjustment factor (instead of 0.5), equation 5.4 becomes )-    N-\Dr\+l The main advantages of this relevance feedback procedure are that the feedback process is directly related to the derivation of new weights for query terms and that the term reweighting is optimal under the assumptions of term independence and binary document indexing (wi,q Ä {0,1} and u^j Ä {0,1}). The disadvantages include: (1) document term weights are not taken into account during the feedback loop; (2) weights of terms in the previous query formulations are also disregarded; and (3) no query expansion is used (the same set of index terms in the original query is reweighted over and over again). As a result of these disadvantages, the probabilistic relevance feedback methods do not in general operate as effectively as the conventional vector modification methods. To extend the probabilistic model with query expansion capabilities, different approaches have been proposed in the literature ranging from term weighting for query expansion to term clustering techniques based on spanning trees. All of these approaches treat probabilistic query expansion separately from probabilistic term reweighting. While we do not discuss them here, a brief history of research on this issue and bibliographical references can be found in section 5.6.
mir-0087	5.2.3    A Variant of Probabilistic Term Reweighting The discussion above on term reweighting is based on the classic probabilistic niodel introduced by Robertson and Sparck Jones in 1976. In 1983, Croft extended this weighting scheme by suggesting distinct initial search methods 122        QUERY OPERATIONS and by adapting the probabilistic formula to include within-document frequency weights. This variant of probabilistic term reweighting is more flexible (and also more powerful) and is briefly reviewed in this section. The formula 5.2 for probabilistic ranking can be rewritten as sim(dj,q) a where Fij,q is interpreted as a factor which depends on the triple [K,d3,q]. In the classic formulation, FMilt;7 is computed as a function of P(ki\R) and P(kt\R) (see equation 5.2). In his variant, Croft proposed that the initial search and the feedback searches use distinct formulations. For the initial search, he suggested Juj    = max(fij) where fx j is a normalized within-document frequency. The parameters C and K should be adjusted according to the collection. For automatically indexed collections, C should be initially set to 0. For the feedback searches, Croft suggested the following formulation for F.        fc I log   P(W     iloc1^*'^   7 IJlt;9"VC+   gl-P(h\R) +1∞g    P(kt\R)   )   ^ where P(kt\R) and P{kt\R) are computed as in equation 5.4. This variant of probabilistic term reweighting has the following advantages: (1) it takes into account the within-document frequencies; (2) it adopts a normalized version of these frequencies; and (3) it introduces the constants C and K which provide for greater flexibility. However, it constitutes a more complex formulation and, as before, it operates solely on the terms originally in the query (without query expansion).
mir-0088	5.2.4    Evaluation of Relevance Feedback Strategies Consider the modified query vector qm generated by the Rochio formula and assume that we want to evaluate its retrieval performance. A simplistic approach is to retrieve a set of documents using qm, to rank them using the vector formula, and to measure recall-precision figures relative to the set of relevant documents (provided by the experts) for the original query vector q. In general, the results siiow spectacular improvements. Unfortunately, a significant part of this improvement results from the higher ranks assigned to the set R of documents AUTOMATIC LOCAL ANALYSIS        123 already identified as relevant during the feedback process [275]. Since the user has seen these documents already (and pointed them as relevants), such evaluation is unrealistic. Further, it masks any real gains in retrieval performance due to documents not seen by the user yet. A more realistic approach is to evaluate the retrieval performance of the modified query vector qm considering only the residual collection i.e., the set of all documents minus the set of feedback documents provided by the user. Because highly ranked documents are removed from the collection, the recall-precision figures for qm tend to be lower than the figures for the original query vector q. This is not a limitation because our main purpose is to compare the performance of distinct relevance feedback strategies (and not to compare the performance before and after feedback). Thus, as a basic rule of thumb, any experimentation involving relevance feedback strategies should always evaluate recall-precision figures relative to the residual collection.
mir-0089	5.3    Automatic Local Analysis In a user relevance feedback cycle, the user examines the top ranked documents and separates them into two classes: the relevant ones and the non-relevant ones. This information is then used to select new terms for query expansion. The reasoning is that the expanded query will retrieve more relevant documents. Thus, there is an underlying notion of clustering supporting the feedback strategy. According to this notion, known relevant documents contain terms which can be used to describe a larger cluster of relevant documents. In this case, the description of this larger cluster of relevant documents is built interactively with assistance from the user. A distinct approach is to attempt to obtain a description for a larger cluster of relevant documents automatically. This usually involves identifying terms which are related to the query terms. Such terms might be synonyms, stemming variations, or terms which are close to the query terms in the text (i.e., terms with a distance of at most k words from a query term). Two basic types of strategies can be attempted: global ones and local ones. In a global strategy, all documents in the collection are used to determine a global thesaurus-like structure which defines term relationships. This structure is then shown to the user who selects terms for query expansion. Global strategies are discussed in section 5.4. In a local strategy, the documents retrieved for a given query q are examined at query time to determine terms for query expansion. This is similar to a relevance feedback cycle but might be done without assistance from the user (i.e., the approach might be fully automatic). Two local strategies are discussed below: local clustering and local context analysis. The first is based on the work done by Attar and Fraenkel in 1977 and is used here to establish many of the fundamental ideas and concepts regarding the usage of clustering for query expansion. The second is a recent work done by Xu and Croft in 1996 and illustrates the advantages of combining techniques from both local and global analysis. 124        QUERY OPERATIONS
mir-0090	5.3.1    Query Expansion Through Local Clustering Adoption of clustering techniques for query expansion is a basic approach which has been attempted since the early years of information retrieval. The standard approach is to build global structures such as association matrices which quantify term correlations (for instance, number of documents in which two given terms co-occur) and to use correlated terms for query expansion. The main problem with this strategy is that there is not consistent evidence that global structures can be used effectively to improve retrieval performance with general collections. One main reason seems to be that global structures do not adapt well to the local context defined by the current query. One approach to deal with this effect is to devise strategies which aim at optimizing the current search. Such strategies are based on local clustering and are now discussed. Our discussion is based on the original work by Attar and Praenkel which appeared in 1977. We first define basic terminology as follows. Definition Let V(s) be a non-empty subset of words which are grammatical variants of each other. A canonical form s ofV(s) is called a stem. For instance, if V(s)={polish,polishing,polished} then $=polish. For a detailed discussion on stemming algorithms see Chapter 7. While stems are adopted in our discussion, the ideas below are also valid for non-stemmed keywords. We proceed with a characterization of the local nature of the strategies covered here. Definition For a given query q, the set D\ of documents retrieved is called the local document set Further, the set V/ of all distinct words in the local document set is called the local vocabulary. The set of all distinct stems derived from the set Vi is referred to as Si. We operate solely on the documents retrieved for the current query. Since it is frequently necessary to access the text of such documents, the application of local strategies to the Web is unlikely at this time. In fact, at a client machine, retrieving the text of 100 Web documents for local analysis would take too long, reducing drastically the interactive nature of Web interfaces and the satisfaction of the users. Further, at the search engine site, analyzing the text of 100 Web documents would represent an extra spending of CPU time which is not cost effective at this time (because search engines depend on processing a high number of queries per unit of time for economic survival). However, local strategies might be quite useful in the environment of intranets such as, for instance, the collection of documents issued by a large business company. Further, local strategies might also be of great assistance for searching information in specialized document collections (for instance, medical document collections). Local feedback strategies are based on expanding the query with terms correlated to the query terms. Such correlated terms are those present in local dusters built from the local document set.   Thus, before we discuss local AUTOMATIC LOCAL ANALYSIS        125 query expansion, we discuss strategies for building local clusters. Three types of clusters are covered: association clusters, metric clusters, and scalar clusters. Association Clusters An association cluster is based on the co-occurrence of stems (or terms) inside documents. The idea is that stems which co-occur frequently inside documents have a synonymity association. Association clusters are generated as follows. Definition The frequency of a stem Si in a document dj, dj G Di, is referred to as fs,L,j- Let m=(mij) be an association matrix with \Si\ rows and \Di\ columns, where mlJ=fs%j. Let fhl be the transpose of m. The matrix s=mfht is a local stem-stem association matrix. Each element su^v in s expresses a correlation cu,v between the stems $u and sv namely, The correlation factor cUiV quantifies the absolute frequencies of co-occurrence and is said to be unnormalized. Thus, if we adopt su,v = cu,v                                                                                                      (5.6) then the association matrix s is said to be unnormalized.  An alternative is to normalize the correlation factor. For instance, if we adopt then the association matrix s is said to be normalized. The adoption of normalization yields quite distinct associations as discussed below. Given a local association matrix s, we can use it to build local association clusters as follows. Definition Consider the u-th row in the association matrix s (i.e., the row with all the associations for the stem su). Let Su(n) be a function which takes the u-th row and returns the set of n largest values sUiV1 where v varies over the set of local stems and v ^ u.  Then Su(n) defines a local association cluster around the stem su. If su,v is given by equation 5.6, the association cluster is said to be unnormalized. If su^v is given by equation 5.7f the association cluster is said to be normalized. Given a query q, we are normally interested in finding clusters only for the \q\ query terms. Further, it is desirable to keep the size of such clusters small This means that such clusters can be computed efficiently at query time. 126        QUERY OPERATIONS Despite the fact that the above clustering procedure adopts stems, it can equally be applied to non-stemmed keywords. The procedure remains unchanged except for the usage of keywords instead of stems. Keyword-based local clustering is equally worthwhile trying because there is controversy over the advantages of using a stemmed vocabulary, as discussed in Chapter 7. Metric Clusters Association clusters are based on the frequency of co-occurrence of pairs of terms in documents and do not take into account where the terms occur in a document. Since two terms which occur in the same sentence seem more correlated than two terms which occur far apart in a document, it might be worthwhile to factor in the distance between two terms in the computation of their correlation factor. Metric clusters are based on this idea. Definition Let the distance r(kt,kj) between two keywords kt and k3 be given by the number of words between them in a same document. If k2 and k3 are in distinct documents we take r(ki,kj) = oo. A local stem-stem metric correlation matrix s is defined as follows. Each element su,v of s expresses a metric correlation cu,v between the stems su and sv namely; In this expression, as already defined, V(su) and V(sv) indicate the sets of keywords which have su and sv as their respective stems. Variations of the above expression for cu,L. have been reported in the literature (such as l/r2(ki, kj)) but the differences in experimental results are not remarkable. The correlation factor cu^v quantifies absolute inverse distances and is said to be unnormalized. Thus, if we adopt then the association matrix s is said to be unnormalized.  An alternative is to normalize the correlation factor. For instance, if we adopt (5.9) u'v     |V'(M| x \V(sv)\ then the association matrix sis said to be normalized. Given a local metric matrix s*. we can use it to build local metric clusters as follows. Definition     Consider the u-th row in the metric correlation matrix s (i.e., the row with all the associations for tht stern su).   Let Su(n) be a function which AUTOMATIC LOCAL ANALYSIS         127 takes the u-th row and returns the set of n largest values su^v, where v varies over the set of local stems and v ^ u. Then Su(n) defines a local metric cluster around the stem su. If su,v is given by equation 5.8, the metric cluster is said to be unnormalized. If su^v is given by equation 5.9, the metric cluster is said to be normalized. ScaSar Clusters One additional form of deriving a synonymity relationship between two local stems (or terms) su and sv is by comparing the sets Su(n) and Sv(n). The idea is that two stems with similar neighborhoods have some synonymity relationship. In this case we say that the relationship is indirect or induced by the neighborhood. One way of quantifying such neighborhood relationships is to arrange all correlation values su^ in a vector su, to arrange all correlation values sVil in another vector svi and to compare these vectors through a scalar measure. For instance, the cosine of the angle between the two vectors is a popular scalar similarity measure. Definition Let su = (su,i, su$-gt; ï ï ï 1 su,n) and sv = (sw,i, sL,,2, ï - ï, Sy,n) be two vectors of correlation values for the stems su and sv. Further, let s = {su%v) be a scalar association matrix.  Then, each su^v can be defined as \SU\  X  Kl The correlation matrix sis said to be induced by the neighborhood. Using it. a scalar cluster is then defined as follows. Definition Let Su(n) be a function which returns the set of n largest values su,v, v y£ u7 defined according to equation 5.10. Then, Su{n) defines a scalar cluster around the stem su. Interactive Search Formulation Stems (or terms) that belong to clusters associated to the query stems (or terms) can be used to expand the original query. Such stems are called neighbors (of the query stems) and are characterized as follows. A stem su which belongs to a cluster (of size n) associated to another stem sv (i.e., su G Sv(n)) is said to be a neighbor of sv. Sometimes, su is also called a searchonym of sv but here we opt for using the terminology neighbor. While neighbor sterns are said to have a synonymity relationship, they are not necessarily synonyms in the grammatical sense. Often, neighbor steins represent distinct keywords which are though correlated by the current query context. The local aspect of this correlation is reflected in the fact that the documents and stems considered in the correlation matrix are all local (i.e., dj E Alt; su Ä V}). 128        QUERY OPERATIONS Figure 5.1    Stem su as a neighbor of the stem sv. Figure 5.1 illustrates a stem (or term) su which is located within a neighborhood Sv(n) associated with the stem (or term) sv. In its broad meaning, neighbor stems are an important product of the local clustering process since they can be used for extending a search formulation in a promising unexpected direction, rather than merely complementing it with missing synonyms. Consider the problem of expanding a given user query q with neighbor stems (or terms). One possibility is to expand the query as follows. For each stem sv Ä q, select m neighbor stems from the cluster Sv(n) (which might be of type association, metric, or scalar) and add them to the query. Hopefully, the additional neighbor stems will retrieve new relevant documents. To cover a broader neighborhood, the set Sv(n) might be composed of stems obtained using correlation factors (i.e., cUilgt;) normalized and unnormalized. The qualitative interpretation is that an unnormalized cluster tends to group stems whose ties are due to their large frequencies, while a normalized cluster tends to group stems which are more rare. Thus, the union of the two clusters provides a better representation of the possible correlations. Besides the merging of normalized and unnormalized clusters, one can also use information about correlated stems to improve the search. For instance, as before, let two stems $u and sv be correlated with a correlation factor cu,v. If cu,v is larger than a predefined threshold then a neighbor stem of su can also be interpreted as a neighbor stem of sv and vice versa. This provides greater flexibility, particularly with Boolean queries. To illustrate, consider the expression (su -f sv) where the + symbol stands for disjunction- Let su* be a neighbor stem of su. Then, one can try both (sugt; + sv) and (su-t-su') as synonym search expressions, because of the correlation given by cu^v. AUTOMATIC LOCAL ANALYSIS         129 Experimental results reported in the literature usually support the hypothesis of the usefulness of local clustering methods. Furthermore, metric clusters seem to perform better than purely association clusters. This strengthens the hypothesis that there is a correlation between the association of two terms and the distance between them. We emphasize that all the qualitative arguments in this section are explicitly based on the fact that all the clusters are local (i.e., derived solely from the documents retrieved for the current query). In a global context, clusters are derived from all the documents in the collection which implies that our qualitative argumentation might not stand. The main reason is that correlations valid in the whole corpora might not be valid for the current query.
mir-0091	5.3.2    Query Expansion Through Local Context Analysis The local clustering techniques discussed above are based on the set of documents retrieved for the original query and use the top ranked documents for clustering neighbor terms (or stems). Such a clustering is based on term (stems were considered above) co-occurrence inside documents. Terms which are the best neighbors of each query term are then used to expand the original query q. A distinct approach is to search for term correlations in the whole collection ó an approach called global analysis. Global techniques usually involve the building of a thesaurus which identifies term relationships in the whole collection. The terms are treated as concepts and the thesaurus is viewed as a concept relationship structure. Thesauri are expensive to build but, besides providing support for query expansion, are useful as a browsing tool as demonstrated by some search engines in the Web. The building of a thesaurus usually considers the use of small contexts and phrase structures instead of simply adopting the context provided by a whole document. Furthermore, with modern variants of global analysis, terms which are closest to the whole query (and not to individual query terms) are selected for query expansion. The application of ideas from global analysis (such as small contexts and phrase structures) to the local set of documents retrieved is a recent idea which we now discuss. Local context analysis [838] combines global and local analysis and works as follows. First, the approach is based on the use of noun groups (i.e., a single noun, two adjacent nouns, or three adjacent nouns in the text), instead of simple keywords, as document concepts. For query expansion, concepts are selected froiri the top ranked documents (as in local analysis) based on their co-occurrence with query terms (no stemming). However, instead of documents, passages (i.e., a text window of fixed size) are used for determining co-occurrence (as in global analysis). More specifically, the local context analysis procedure operates in three steps. ï First, retrieve the top n ranked passages using the original query.   This is accomplished by breaking up the documents initially retrieved by the 130        QUERY OPERATIONS query in fixed length passages (for instance, of size 300 words) and ranking these passages as if they were documents. ï  Second, for each concept c in the top ranked passages, the similarity sirn(q)c) between the whole query q (not individual query terms) and the concept c is computed using a variant of tf-idf ranking. ï  Third, the top m ranked concepts (according to sim(q,c)) are added to the original query q. To each added concept is assigned a weight given by 1 ó 0.9 x i/m where i is the position of the concept in the final concept ranking. The terms in the original query q might be stressed by assigning a weight equal to 2 to each of them. Of these three steps, the second one is the most complex and the one which we now discuss. The similarity sim(q, c) between each related concept c and the original query q is computed as follows.  where n is the number of top ranked passages considered. The function /(c, ki) quantifies the correlation between the concept c and the query term fcj and is given by where pfij is the frequency of term k2 in the j-th passage and pfCiJ is the frequency of the concept c in the j-th passage. Notice that this is the standard correlation measure defined for association clusters (by Equation 5.5) but adapted for passages. The inverse document frequency factors are computed as idjx    =    mar(l,ó  where Ar is the number of passages in the collection, npt is the number of passages containing the term kv and npc is the number of passages containing the concept c. The factor 5 is a constant parameter which avoids a value equal to zero for sirn(q,c) (which is useful, for instance, if the approach is to be used with probabilistic frameworks such as that provided by belief networks). Usually, 8 is a small factor with values close to 0.1 (10% of the maximum of 1). Finally, the idft factor in the exponent is introduced to emphasize infrequent query terms. The procedure above for computing sirniq.c) is a non-trivial variant of tf-idf ranking.  Furthermore, it lias been adjusted for operation with TREC data AUTOMATIC GLOBAL ANALYSIS        131 and did not work so well with a different collection. Thus, it is important to have in mind that tuning might be required for operation with a different collection. We also notice that the correlation measure adopted with local context analysis is of type association. However, we already know that a correlation of type metric is expected to be more effective. Thus, it remains to be tested whether the adoption of a metric correlation factor (for the function /(c, fe^)) makes any difference with local context analysis.
mir-0092	5.4    Automatic Global Analysis The methods of local analysis discussed above extract information from the local set of documents retrieved to expand the query. It is well accepted that such a procedure yields improved retrieval performance with various collections. An alternative approach is to expand the query using information from the whole set of documents in the collection. Strategies based on this idea are called global analysis procedures. Until the beginning of the 1990s, global analysis was considered to be a technique which failed to yield consistent improvements in retrieval performance with general collections. This perception has changed with the appearance of modern procedures for global analysis. In the following, we discuss two of these modern variants. Both of them are based on a thesaurus-like structure built using all the documents in the collection. However, the approach taken for building the thesaurus and the procedure for selecting terms for query expansion are quite distinct in the two cases.
mir-0093	5.4.1    Query Expansion based on a Similarity Thesaurus In this section we discuss a query expansion model based on a global similarity thesaurus which is constructed automatically [655]. The similarity thesaurus is based on term to term relationships rather than on a matrix of co-occurrence (as discussed in section 5.3). The distinction is made clear in the discussion below. Furthermore, special attention is paid to the selection of terms for expansion and to the reweighting of these terms. In contrast to previous global analysis approaches, terms for expansion are selected based on their similarity to the whole query rather than on their similarities to individual query terms. A similarity thesaurus is built considering term to term relationships. However, such relationships are not derived directly from co-occurrence of terms inside documents. Rather, they are obtained by considering that the terms are concepts in a concept space. In this concept space, each term is indexed by the documents in which it appears. Thus, terms assume the original role of documents while documents are interpreted as indexing elements. The following definitions establish the proper framework. Definition As before (see Chapter 2), let t he the number of terms in the collection, N be the number of documents in the collection, and fx j be the frequency 132        QUERY OPERATIONS of occurrence of the term k{ in the document dj. Further, let tj be the number of distinct index terms in the document dj and itfj be the inverse term frequency for document dj. Then, t itfj = log ó analogously to the definition of inverse document frequency. Within this framework, to each term k{ is associated a vector fc$ given by where, as in Chapter 2, Wi0 is a weight associated to the index-document pair [ki,dj]. Here, however, these weights are computed in a rather distinct form as follows. where maxj(fij) computes the maximum of all factors /^ for the i-th term (i.e., over all documents in the collection). We notice that the expression above is a variant of tf-idf weights hut one which considers inverse term frequencies instead. The relationship between two terms ku and kv is computed as a correlation factor cu^v given by wuj x wvj                                               (5.12) We notice that this is a variation of the correlation measure used for computing scalar association matrices (defined by Equation 5.5). The main difference is that the weights are based on interpreting documents as indexing elements instead of repositories for term co-occurrence. The global similarity thesaurus is built through the computation of the correlation factor cu,v for each pair of indexing terms [ku,kv] in the collection (analogously to the procedure in section 5.3). Of course, this is computationally expensive. However, this global similarity thesaurus has to be computed only once and can be updated incrementally. Given the global similarity thesaurus, query expansion is done in three steps as follows. ï First, represent the query in the concept space used for representation of the index terms. AUTOMATIC GLOBAL ANALYSIS        133 ï  Second, based on the global similarity thesaurus, compute a similarity sirn(q,kv) between each term kv correlated to the query terms and the whole query q. ï  Third,  expand the query with the top r  ranked terms  according to sim(q, kv). For the first step, the query is represented in the concept space of index term vectors as follows. Definition     To the query q is associated a vector q in the term-concept space given by q= y}Twliqki where Wi,q is a weight associated to the index-query pair [kt,q].   This weight is computed analogously to the index-document weight formula in equation 5.11. For the second step, a similarity sim(q, kv) between each term kv (correlated to the query terms) and the user query q is computed as sim(q,kv) = q- kv = kueQ where cu,v is the correlation factor given in equation 5.12. As illustrated in Figure 5.2, a term might be quite close to the whole query while its distances to individual query terms are larger. This implies that the terms selected here for query expansion might be distinct from those selected by previous global analysis methods (which adopted a similarity to individual query terms for deciding terms for query expansion). For the third step, the top r ranked terms according to sim(q, kv) are added to the original query q to form the expanded query q . To each expansion term kv in the query q is assigned a weight wvq' given by ^ _   sirnjq.ky) Wv,q    ~~ Np The expanded query q is then used to retrieve new documents to the user. This completes the technique for query expansion based on a similarity thesaurus. Contrary to previous global analysis approaches, this technique has yielded improved retrieval performance (in the range of 20%) with three different collections. It is worthwhile making one final observation. Consider a document dj which is represented in the term-concept space by dj = *%2ktÄd u?ij^´- Further, assume that the original query q is expanded to include all tne t index terms 134        QUERY OPERATIONS Q=lKa,Kb) Figure 5.2    The distance of a given term kv to the query centroid Qc might be quite distinct from the distances of kv to the individual query terms. (properly weighted) in the collection.  Then, the similarity sim(q^dj) between the document dj and the query q can be computed in the term-concept space by E vj X wu,q X cu,v Such an expression is analogous to the formula for query-document similarity in the generalized vector space model (see Chapter 2). Thus, the generalized vector space model can be interpreted as a query expansion technique. The main differences with the term-concept idea are the weight computation and the fact that only the top r ranked terms are used for query expansion with the term-concept technique.
mir-0094	5.4.2    Query Expansion based on a Statistical Thesaurus In this section, we discuss a query expansion technique based on a global statistical thesaurus [200], Despite also being a global analysis technique, the approach is quite distinct from the one described above which is based on a similarity thesaurus. The global thesaurus is composed of classes which group correlated terms in the context of the whole collection. Such correlated terms can then be used to expand the original user query. To be effective, the terms selected for expansion must have high term discrimination values [699] which implies that they must be low frequency terms. However, it is difficult to cluster low frequency terms effectively due to the small amount of information, about them (they occur in few documents). To circumvent this problem, we cluster documents into AUTOMATIC GLOBAL ANALYSIS        135 classes instead and use the low frequency terms in these documents to define our thesaurus classes. In this situation, the document clustering algorithm must produce small and tight clusters. A document clustering algorithm which produces small and tight clusters is the complete link algorithm which works as follows (naive formulation). (1)  Initially, place each document in a distinct cluster. (2)  Compute the similarity between all pairs of clusters. (3)  Determine the pair of clusters [CU,CV] with the highest inter-cluster similarity. (4)  Merge the clusters Cu and Cv. (5)  Verify a stop criterion. If this criterion is not met then go back to step 2. (6)  Return a hierarchy of clusters. The similarity between two clusters is defined as the minimum of the similarities between all pairs of inter-cluster documents (i.e., two documents not in the same cluster). To compute the similarity between documents in a pair, the cosine formula of the vector model is used. As a result of this minimality criterion, the resultant clusters tend to be small and tight. Consider that the whole document collection has been clustered using the complete link algorithm. Figure 5.3 illustrates a small portion of the whole cluster hierarchy in which sirn(Cu,Cv) = 0.15 and sim(Cu+v,Cz) =0.11 where CujrV is a reference to the cluster which results from merging Cu and Cv. Notice that the similarities decrease as we move up in the hierarchy because high level clusters include more documents and thus represent a looser grouping. Thus, the tightest clusters lie at the bottom of the clustering hierarchy. Given the document cluster hierarchy for the whole collection, the terms that compose each class of the global thesaurus are selected as follows. ï Obtain from the user three parameters: threshold class (TC), number of Figure  5.3    Hierarchy of three clusters (inter-cluster similarities indicated in the ovals) generated by the complete link algorithm. 136        QUERY OPERATIONS documents in a class (NDC), and minimum inverse document frequency (MIDF). ï  Use the parameter TC as a threshold value for determining the document clusters that will be used to generate thesaurus classes. This threshold has to be surpassed by sim(CU} Cv) if the documents in the clusters Cu and Cv are to be selected as sources of terms for a thesaurus class. For instance, in Figure 5.3, a value of 0.14 for TC returns the thesaurus class Cu+V while a value of 0.10 for TC returns the classes Cu+V and Cu+V+Z. ï  Use the parameter NDC as a limit on the size of clusters (number of documents) to be considered.   For instance, if both Cu+V and Cu+u+2 are preselected (through the parameter TC) then the parameter NDC might be used to decide between the two. A low value of NDC might restrict the selection to the smaller cluster Cu+V. ï  Consider the set of documents in each document cluster preselected above (through the parameters TC and NDC). Only the lower frequency documents are used as sources of terms for the thesaurus classes. The parameter MIDF defines the minimum value of inverse document frequency for any term which is selected to participate in a thesaurus class.   By doing so, it is possible to ensure that only low frequency terms participate in the thesaurus classes generated (terms too generic are not good synonyms). Given that the thesaurus classes have been built, they can be used for query expansion. For this, an average term weight wtc for each thesaurus class C is computed as follows. where \C\ is the number of terms in the thesaurus class C and wt,c is a Pre~ computed weight associated with the term-class pair [kt, C]. This average term weight can then be used to compute a thesaurus class weight we as The above weight formulations have been verified through experimentation and have yielded good results. Experiments with four test collections (ADI, Medlars, CACM, and LSI; see Chapter 8 for details on these collections) indicate that global analysis using a thesaurus built by the complete link algorithm might yield consistent improvements in retrieval performance. The main problem with this approach is the initialization of the parameters TC, NDC, and MIDF. The threshold value TC depends on the collection and can be difficult to set properly. Inspection of the cluster hierarchy is almost always necessary for assisting with the setting of TC. Care must be exercised because a TRENDS AND RESEARCH ISSUES        137 high value of TC might yield classes with too few terms while a low value of TC might yield too few classes. The selection of the parameter NDC can be decided more easily once TC has been set. However, the setting of the parameter MIDF might be difficult and also requires careful consideration.
mir-0095	5.5    Trends and Research Issues The relevance feedback strategies discussed here can be directly applied to the graphical interfaces of modern information systems. However, since interactivity is now of greater importance, new techniques for capturing feedback information from the user are desirable. For instance, there is great interest in graphical interfaces which display the documents in the answer set as points in a 2D or 3D space. The motivation is to allow the user to quickly identify (by visual inspection) relationships among the documents in the answer. In this scenario, a rather distinct strategy for quantifying feedback information might be required. Thus, relevance strategies for dealing with visual displays are an important research problem. In the past, global analysis was viewed as an approach which did not yield good improvements in retrieval performance. However, new results obtained at the beginning of the 1990s changed this perception. Further, the Web has provided evidence that techniques based on global analysis might be of interest to the users. For instance, this is the case with the highly popular 'Yahoo!' software which uses a manually built hierarchy of concepts to assist the user with forming the query. This suggests that investigating the utilization of global analysis techniques in the Web is a promising research problem. Local analysis techniques are interesting because they take advantage of the local context provided with the query. In this regard, they seem more appropriate than global analysis techniques. Furthermore, many positive results have been reported in the literature. The application of local analysis techniques to the Web, however, has not been explored and is a promising research direction. The main challenge is the computational burden imposed on the search engine site due to the need to process document texts at query time. Thus, a related research problem of relevance is the development of techniques for speeding up query processing at the search engine site. In truth, this problem is of interest even if one considers only the normal processing of the queries because the search engines depend on processing as many queries as possible for economic survival. The combination of local analysis, global analysis, visual displays, and interactive interfaces is also a current and important research problem. Allowing the user to visually explore the document space and providing him with clues which assist with the query formulation process are highly relevant issues. Positive results in this area might become a turning point regarding the design of user interfaces and are likely to attract wide attention. 138        QUERY OPERATIONS
mir-0096	5.6    Bibliographic Discussion Query expansion methods have been studied for a long time. While the success of expansion methods throughout the years has been debatable, more recently researchers have reached the consensus that query expansion is a useful and little explored (commercially) technique. Useful because its modern variants can be used to consistently improve the retrieval performance with general collections. Little explored because few commercial systems (and Web search engines) take advantage of it. Early work suggesting the expansion of a user query with closely related terms was done by Maron and Kuhns in 1960 [547]. The classic technique for combining query expansion with term reweighting in the vector model was studied by Rocchio in 1965 (using the Smart system [695] as a testbed) and published later on [678], Ide continued the studies of Rocchio and proposed variations to the term reweighting formula [391]. The probabilistic model was introduced by Robertson and Sparck Jones [677] in 1976. A thorough and entertaining discussion of this model can be found in the book by van Rijsbergen [785]. Croft and Harper [199] suggested that the initial search should use a distinct computation. In 1983, Croft [198] proposed to extend the probabilistic formula to include within-document frequencies and introduced the C and K parameters. Since the probabilistic model does not provide means of expanding the query, query expansion has to be done separately. In 1978, Harper and van Rijsbergen [345] used a term-term clustering technique based on a maximum spanning tree to select terms for probabilistic query expansion. Two years later, they also introduced a new relevance weighting scheme, called EMIM [344], to be used with their query expansion technique. In 1981, Wu and Salton [835] used relevance feedback to reweight terms (using a probabilistic formula) extracted from relevant documents and used these terms to expand the query. Empirical results showed improvements in retrieval performance. Our discussion on user relevance feedback for the vector and probabilistic models in section 5.2 is based on four sources: a nice paper by Salton and Buckley [696], the book by van Rijsbergen [785], the book by Salton and McGill [698], and two book chapters by Harman [340, 339]. Regarding automatic query expansion, Lesk [500] tried variations of term-term clustering in the Smart system without positive results. Following that, Sparck Jones and Barber [413] and Minker, Wilson and Zimmerman [562] also observed no improvements with query expansion based on term-term global clustering. These early research results left the impression that query expansion based on global analysis is not an effective technique. However, more recent results show that this is not the case. In fact, the research results obtained by Vorhees [793], by Crouch and Yang [200], and by Qiu and Prei [655] indicate that query expansion based on global analysis can consistently yield improved retrieval performance. Our discussion on query expansion through local clustering is based on early work by Attar and Fraenkel [35] from 1977.   The idea of local context BIBLIOGRAPHIC DISCUSSION        139 analysis is much more recent and was introduced by Xu and Croft [838] in 1996. The discussion on query expansion using a global similarity thesaurus is based on the work by Qiu and Prei [655]. Finally, the discussion on query expansion using a global statistical thesaurus is based on the work of Crouch and Yang [200] which is influenced by the term discrimination value theory introduced by Salton, Yang, and Yu [699] early in 1975. Since query expansion frequently is based on some form of clustering, our discussion covered a few clustering algorithms. However, our aim was not to provide a thorough review of clustering algorithms for information retrieval. Such a review can be found in the work of Rasmussen [668].
mir-0098	6.1    Introduction Text is the main form of communicating knowledge. Starting with hieroglyphs, the first written surfaces (stone, wood, animal skin, papyrus, and rice paper), and paper, text has been created everywhere, in many forms and languages. We use the term document to denote a single unit of information, typically text in a digital form, but it can also include other media. In practice, a document is loosely denned. It can be a complete logical unit, like a research article, a book or a manual. It can also be part of a larger text, such as a paragraph or a sequence of paragraphs (also called a passage of text), an entry in a dictionary, a judge's opinion on a case, the description of an automobile part, etc. Furthermore, with respect to its physical representation, a document can be any physical unit, for example a file, an email, or a World Wide Web (or just Web) page. A document has a given syntax and structure which is usually dictated by the application or by the person who created it. It also has a semantics, specified by the author of the document (who is not necessarily the same as the creator). Additionally, a document may have a presentation style associated with it, which specifies how it should be displayed or printed. Such a style is usually given by the document syntax and structure and is related to a specific application (for example, a Web browser). Figure 6.1 depicts all these relations. A document can also have information about itself, called metadata. The next section explains different types of metadata and their relevance. The syntax of a document can express structure, presentation style, semantics, or even external actions. In many cases one or more of these elements are implicit or are given together. For example, a structural element (e.g., a section) can have a fixed formatting style. The semantics of a document is also associated with its use. For example, Postscript directives are designed for drawing. The syntax of a document can be implicit in its content, or expressed in a simple declarative language or even in a programming language. For example, many editor formats are declarative while a TeX document uses a powerful typesetting language. Although a powerful language could be easier to parse than the data itself, it might be difficult to convert documents in that language to other formats.  Many syntax languages are proprietary and specific, but open 141 142        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES Document Syntax Text + Structure + Other Media Presentation Style Semantics Figure 6.1    Characteristics of a document. and generic languages are better because documents can be interchanged between applications and are more flexible. Text can also be written in natural language. However, at present the semantics of natural language is still not easy for a computer to understand. The current trend is to use languages which provide information on the document structure, format, and semantics while being readable by humans as well as computers. The Standard Generalized Markup Language (SGML), which is covered later on in this chapter, tries to balance all the issues above. Metadata, markup, and semantic encoding represent different levels of formalization of the document contents. Most documents have a particular formatting style. However, new applications are pushing for external formatting such that information can be represented independently of style, and vice versa. The presentation style can be embedded in the document, as in TeX or Rich Text Format (RTF). Style can be complemented by macros (for example, LaTeX in the case of TeX). In most cases, style is defined by the document author. However, the reader may decide part of the style (for example, by setting options in a Web browser). The style of a document defines how the document is visualized in a computer window or a printed page, but can also include treatment of other media such as audio or video. In this chapter we first cover metadata. Following that we discuss text characteristics such as formats and natural language statistics. Next we cover languages to describe text structure, presentation style, or semantics. The last part is devoted to multimedia formats and languages.
mir-0099	6.2    Metadata Most documents and text collections have associated with them what is known as metadata. Metadata is information on the organization of the data, the various data domains, and the relationship between them. In short, metadata is 'data about the data."  For instance, in a database management system, the schema METADATA        143 specifies some of the metadata, namely, the name of the relations, the fields or attributes of each relation, the domain of each attribute, etc. Common forms of metadata associated with text include the author, the date of publication, the source of the publication, the document length (in pages, words, bytes, etc.), and the document genre (book, article, memo, etc.). For example, the Dublin Core Metadata Element Set [807] proposes 15 fields to describe a document. Following Marchionini [542], we refer to this kind of information as Descriptive Metadata, metadata that is external to the meaning of the document, and pertains more to how it was created. Another type of metadata characterizes the subject matter that can be found within the document's contents. We will refer to this as Semantic Metadata. Semantic Metadata is associated with a wide number of documents and its availability is increasing. All books published within the USA are assigned Library of Congress subject codes, and many journals require author-assigned key terms that are selected from a closed vocabulary of relevant terms. For example, biomedical articles that appear within the MEdigital libraryINE (see Chapter 3) system are assigned topical metadata pertaining to disease, anatomy, Pharmaceuticals, and so on. To standardize semantic terms, many areas use specific ontologies, which are hierarchical taxonomies of terms describing certain knowledge topics. An important metadata format is the Machine Readable Cataloging Record (MARC) which is the most used format for library records. MARC has several fields for the different attributes of a bibliographic entry such as title, author, etc. Specific uses of MARC are given in Chapter 14. In the USA, a particular version of MARC is used: USMARC, which is an implementation of ANSI/NISO Z39.2, the American National Standard for Bibliographic Information Interchange. The USMARC format documents contain the definitions and content for the fields that have to be used in records structured according to Z39.2. This standard is maintained by the Library of Congress of the USA. With the increase of data in the Web, there are many initiatives to add metadata information to Web documents. In the Web, metadata can be used for many purposes. Some of them are cataloging (BibTeX is a popular format for this case), content rating (for example, to protect children from reading some type of documents), intellectual property rights, digital signatures (for authentication), privacy levels (who should and who should not have access to a document), applications to electronic commerce, etc. The new standard for Web metadata is the Resource Description Framework (RDF), which provides interoperability between applications. This framework allows the description of Wreb resources to facilitate automated processing of the information. It does not assume any particular application or semantic domain. It consists of a description of nodes and attached attribute/value pairs. Nodes can be any WTeb resource, that is, any Uniform Resource Identifier (URI), which includes the Uniform Resource Locator (URL). Attributes are properties of nodes, and their values are text strings or other nodes (Web resources or metadata instances). To describe the semantics, values from, for example, the Dublin Core library metadata URL can be used. Other predefined vocabularies for authoring metadata are expected, in particular for content rating and for digital signatures.   In addition, currently. 144        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES there are many Web projects on ontologies for different application domains (see also Chapters 13 and 15). Metadata is also useful for metadescriptions of nontextual objects. For example, a set of keywords that describe an image. These keywords can later be used to search for the image using classic text information retrieval techniques (on the metadescriptions).
mir-0100	6.3    Text With the advent of the computer, it was necessary to code text in binary digits. The first coding schemes were EBCDIC and ASCII, which used seven bits to code each possible symbol. Later, ASCII was standardized to eight bits (ISO-Latin), to accommodate several languages, including accents and other diacritical marks. Nevertheless, ASCII is not suitable for oriental languages such as Chinese or Japanese Kanji, where each symbol might represent a concept and therefore thousands of them exist. For this case, a 16-bit code exists called Unicode (ISO 10616) [783]. In this section we cover different characteristics of text. First, the possible formats of text, ASCII being the simplest format. Second, how the information content of text can be measured, followed by different models for it. Finally, we mention briefly how we can measure similarity between strings or pieces of text.
mir-0101	6.3.1    Formats There is no single format for a text document, and an IR system should be able to retrieve information from many of them. In the past, IR systems would convert a document to an internal format. However, that has many disadvantages, because the original application related to the document is not useful any more. On top of that, we cannot change the contents of a document. Current IR systems have filters that can handle most popular documents, in particular those of word processors with some binary syntax such as Word, WordPerfect or FrameMaker. Even then, good filters might not be possible if the format is proprietary and its details are not public. This is not the case for full ASCII syntax, as in TeX documents. Although documents can be in a binary format (for example, parts of a Word document), documents that are represented in human-readable ASCII form imply more portability and are easier to modify (for example, they can be edited with different applications). Other text formats were developed for document interchange. Among these we should mention the Rich Text Format (RTF), which is used by word processors and has ASCII syntax. Other important formats were developed for displaying or printing documents. The most popular ones are the Portable Document Format (PDF) and Postscript (which is a powerful programming language for drawing). Other interchange formats are used to encode electronic mail, for example MIME (Multipurpose internet Mail Exchange). MIME supports multiple character sets, multiple languages, and multiple media. On top of these formats, nowadays many files are compressed. Text compression is treated in detail in Chapter 7, but here we comment on the most TEXT        145 popular compression software and associated formats. These include Compress (Unix), ARJ (PCs), and ZIP (for example gzip in Unix and Winzip in Windows). Other tools allow us to convert binary files, in particular compressed text, to ASCII text such that it can be transmitted through a communication line using only seven bits. Examples of these tools are uuencode/uudecode and binhex.
mir-0102	6.3.2    Information Theory Written text has a certain semantics and is a way to communicate information. Although it is difficult to formally capture how much information is there in a given text, the distribution of symbols is related to it. For example, a text where one symbol appears almost all the time does not convey much information. Information theory defines a special concept, entropy, to capture information content (or equivalently, information uncertainty). If the alphabet has a symbols, each one appearing with probability pz (probability here is defined as the symbol frequency over the total number of symbols) in a text, the entropy of this text is defined as E= -' 2=1 In this formula the a symbols of the alphabet are coded in binary, so the entropy is measured in bits. As an example, for a = 2, the entropy is 1 if both symbols appear the same number of times or 0 if only one symbol appears. We say that the amount of information in a text can be quantified by its entropy. The definition of entropy depends on the probabilities (frequencies) of each symbol. To obtain those probabilities we need a text model. So we say that the amount of information in a text is measured with regard to the text model. This concept is also important, for example, in text compression, where the entropy is a limit on how much the text can be compressed, depending on the text model. In our case we are interested in natural language, as we now discuss.
mir-0103	6.3.3    Modeling Natural Language Text is composed of symbols from a finite alphabet. We can divide the symbols in two disjoint subsets: symbols that separate words and symbols that belong to words. It is well known that symbols are not uniformly distributed. If we consider just letters (a to z), we observe that vowels are usually more frequent than most consonants. For example, in English, the letter V has the highest frequency. A simple model to generate text is the binomial model In it, each symbol is generated with a certain probability. However, natural language has a dependency on previous symbols. For example, in English, a letter ki' cannot appear after a letter V and vowels or certain consonants have a higher probability 146        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES of occurring. Therefore, the probability of a symbol depends on previous symbols. We can use a finite-context or Markovian model to reflect this dependency. The model can consider one, two, or more letters to generate the next symbol. If we use k letters, we say that it is a fc-order model (so the binomial model is considered a 0-order model). We can use these models taking words as symbols. For example, text generated by a 5-order model using the distribution of words in the Bible might make sense (that is, it can be grammatically correct), but will be different from the original. More complex models include finite-state models (which define regular languages), and grammar models (which define context free and other languages). However, finding the right grammar for natural language is still a difficult open problem. The next issue is how the different words are distributed inside each document. An approximate model is Zipf's Law [847, 310], which attempts to capture the distribution of the frequencies (that is, number of occurrences) of the words in the text. The rule states that the frequency of the i-ih most frequent word is l/ie times that of the most frequent word. This implies that in a text of n words with a vocabulary of V words, the z-th most frequent word appears n/{ie Hy{9)) times, where Hy{9) is the harmonic number of order 9 of V, defined as so that the sum of all frequencies is n. The left side of Figure 6.2 illustrates the distribution of frequencies considering that the words are arranged in decreasing order of their frequencies. The value of 6 depends on the text. In the most simple formulation, 0 = 1, and therefore Hy{9) = O(logn). However, this simplified version is very inexact, and the case 9 gt; 1 (more precisely, between 1.5 and 2.0) fits better the real data [26]. This case is very different, since the distribution is much more skewed, and Hy{6) = 0(1). Experimental data suggests that a better model is fc/(c-h£)* where c is an additional parameter and k is such that all frequencies add to n. This is called a Mandelbrot distribution [561]. Since the distribution of words is very skewed (that is, there are a few hundred words which take up 50% of the text), words that are too frequent, such as stopwords, can be disregarded. A stopword is a word which does not carry meaning in natural language and therefore can be ignored (that is, made not searchable), such as 4a,' 'the,' 'by,' etc. Fortunately the most frequent words are stopwords and therefore, half of the words appearing in a text do not need to be considered. This allows us, for instance, to significantly reduce the space overhead of indices for natural language texts. For example, the most frequent words in the TREC-2 collection (see Chapter 3 for details on this reference collection and others) are "the,1 fcof,' 'and,1 "a,1 "to1 and in1 (see also Chapter 7). Another issue is the distribution of words in the documents of a collection. A simple model is to consider that each word appears the same number of times in every document.   However, this is not true in practice.   A better model is TEXT 147 Words Text size Figure 6.2    Distribution of sorted word frequencies (left) and size of the vocabulary (right). to consider a negative binomial distribution, which says that the fraction of documents containing a word k times is a + k-l k -a-k where p and a are parameters that depend on the word and the document collection. For example, for the Brown Corpus [276] and the word *said\ we have p = 9.24 and a = 0.42 [171]. The latter reference gives other models derived from a Poisson distribution. The next issue is the number of distinct words in a document. This set of words is referred to as the document vocabulary. To predict the growth of the vocabulary size in natural language text, we use the so-called Heaps' Law [352]. This is a very precise law which states that the vocabulary of a text of size n words is of size V = KnÆ = 0{nP), where K and /3 depend on the particular text. The right side of Figure 6.2 illustrates how the vocabulary size varies with the text size. K is normally between 10 and 100, and f3 is a positive value less than one. Some experiments [26, 42] on the TREC-2 collection show that the most common values for /3 are between 0.4 and 0.6. Hence, the vocabulary of a text grows sublinearly with the text size, in a proportion close to its square root. Notice that the set of different words of a language is fixed by a constant (for example, the number of different English words is finite). However, the limit is so high that it is much more accurate to assume that the size of the vocabulary is O(n) instead of O(l), although the number should stabilize for huge enough texts. On the other hand, many authors argue that the number keeps growing anyway because of typing or spelling errors. Heaps' law also applies to collections of documents because, as the total text size grows, the predictions of the model become more accurate. Furthermore, this model is also valid for the World Wide Web (see Chapter 13). The last issue is the average length of words. This relates the text size in 148        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES words with the text size in bytes (without accounting for punctuation and other extra symbols). For example, in the different subcollections of the TREC-2 collection, the average word length is very close to 5 letters, and the range of variation of this average in each subcollection is small (from 4.8 to 5.3 letters). If we remove the stopwords, the average length of a word increases to a number between 6 and 7 (letters). If we take only the words of the vocabulary, the average length is higher (about 8 or 9). This defines the total space needed for the vocabulary. Heaps' law implies that the length of the words in the vocabulary increases logarithmically with the text size and thus, that longer and longer words should appear as the text grows. However, in practice, the average length of the words in the overall text is constant because shorter words are common enough (e.g. stop-words). This balance between short and long words, such that the average word length remains constant, has been noticed many times in different contexts, and can also be explained by a finite-state model in which: (a) the space character has probability close to 0.2; (b) the space character cannot appear twice subsequently; and (c) there are 26 letters [561]. This simple model is consistent with Zipf's and Heaps' laws. The models presented in this section are used in Chapters 8 and 13, in particular Zipf's and Heaps' laws.
mir-0104	6.3.4    Similarity Models In this section we define notions of syntactic similarity between strings or documents. Similarity is measured by a distance function. For example, if we have strings of the same length, we can define the distance between them as the number of positions that have different characters. Then, the distance is 0 if they are equal. This is called the Hamming distance. A distance function should also be symmetric (that is, the order of the arguments does not matter) and should satisfy the triangle inequality (that is, distance(a, c) lt; distance(a, b) 4- distance(b, c)). An important distance over strings is the edit or Levenshtein distance mentioned earlier. The edit distance is defined as the minimum number of characters, insertions, deletions, and substitutions that we need to perform in any of the strings to make them equal. For instance, the edit distance between 'color1 and 'colour1 is one, while the edit distance between 'survey1 arid 'surgery' is two. The edit distance is considered to be superior for modeling syntactic errors than other more complex methods such as the Soundex system, which is based on phonetics [595]. Extensions to the concept of edit distance include different weights for each operation, adding transpositions, etc. There are other measures. For example, assume that we are comparing two given strings and the only operation allowed is deletion of characters. Then, after all non-common characters have been deleted, the remaining sequence of characters (not necessarily contiguous in the original string, but in the same order) is the longest common subsequence (LCS) of both strings. For example, the LCS of 'survey' and "surgery' is %surey.' MARKUP LANGUAGES        149 Similarity can be extended to documents. For example, we can consider lines as single symbols and compute the longest common sequence of lines between two files. This is the measure used by the dif f command in Unix-like operating systems. The main problem with this approach is that it is very time consuming and does not consider lines that are similar. The latter drawback can be fixed by taking a weighted edit distance between lines or by computing the LCS over all the characters. Other solutions include extracting fingerprints (any piece of text that in some sense characterizes it) for the documents and comparing them, or finding large repeated pieces. There are also visual tools to see document similarity. For example, Dotplot draws a rectangular map where both coordinates are file lines and the entry for each coordinate is a gray pixel that depends on the edit distance between the associated lines.
mir-0105	6.4    Markup Languages Markup is defined as extra textual syntax that can be used to describe formatting actions, structure information, text semantics, attributes, etc. For example, the formatting commands of TeX (a popular text formatting software) could be considered markup. However, formal markup languages are much more structured. The marks are called tags, and usually, to avoid ambiguity, there is an initial and ending tag surrounding the marked text. The standard metalanguage for markup is SGML, as already mentioned. An important subset of SGML is XML (extensible Markup Language), the new metalanguage for the Web. The most popular markup language used for the Web, HTML (HyperText Markup Language), is an instance of SGML. All these languages and examples of them are described below.
mir-0106	6.4.1    SGML SGML stands for Standard Generalized Markup Language (ISO 8879) and is a metalanguage for tagging text developed by a group led by Goldfarb [303] based on earlier work done at IBM. That is, SGML provides the rules for defining a markup language based on tags. Each instance of SGML includes a description of the document structure called a document type definition. Hence, an SGML document is defined by: (1) a description of the structure of the document and (2) the text itself marked with tags which describe the structure. We will explain later the syntax associated with the tags. The document type definition is used to describe and name the pieces that a document is composed of and define how those pieces relate to each other. Part of the definition can be specified by an SGML document type declaration (DTD). 150        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES Other parts, such as the semantics of elements and attributes, or application conventions, cannot be expressed formally in SGML. Comments can be used, however, to express them informally. This means that all of the rules for applying SGML markup to documents are part of the definition, and those that can be expressed in SGML syntax are represented in the DTD. The DTD does not define the semantics (that is, the meaning, presentation, and behavior), or intended use, of the tags. However, some semantic information can be included in comments embedded in the DTD, while more complete information is usually present in separate documentation. This additional documentation typically describes the elements, or logical pieces of data, the attributes, and information about those pieces of data. For example, two tags can have the same name but different semantics in two different applications. Tags are denoted by angle brackets (lt;tagnamegt;). Tags are used to identify the beginning and ending of pieces of the document, for example a quote in a literary text. Ending tags are specified by adding a slash before the tag name (e.g., lt;/tagnamegt;). For example, the tag lt;/authoxgt; could be used to identify the element 'name of author,' which appears in italics and generates a link to a biographic sketch. Tag attributes are specified at the beginning of the element, inside the angle brackets and after the nametag using the syntax attname=value. Figure 6.3 gives an example of a simple DTD and a document using it. While we do not intend to discuss SGML syntax here, we give a brief description of the example such that the reader can grasp the main ideas. Each ELEMENT represents a tag denoted by its name. The two following characters indicate if the starting and ending tags are compulsory (-) or optional (0). For example, the ending tag for prolog is necessary while for sender it is not. Following that, the inside portion of the content tag is specified using a regular expression style syntax where V stands for concatenation, 4|' stands for logical or, '?' stands for zero or one occurrence, '*' stands for zero or more occurrences, and k+' stands for one or more occurrences of the preceding element. The content tag can be composed of the combination of other tag contents, ASCII characters (PCDATA), and binary data (NDATA), or EMPTY. The possible attributes of a tag are given in an attribute list (ATTLIST) identified by the tag name, followed by the name of each attribute, its type, and if it is required or not (otherwise, the default value is given). An SGML document instance is associated with the DTD so that the various tools working with the data know which are the correct tags and how they are organized. The document description generally does not specify how a document should look, for example when it is printed on paper or displayed on a screen. Because SGML separates content from format, we can create very good models of data that have no mechanism for describing the format, hence, no standard way to output the data in a formatted fashion. Therefore, output specifications, which are directions on how to format a document, are often added to SGML documents. For this purpose, output specification standards such as DSSSL (Document Style Semantic Specification Language) and FOSI (Formatted Output Specification Instance) were devised. Both of these standards define mechanisms for associating style information with SGML document instances. MARKUP LANGUAGES   151 lt;!óSGML DTD for electronic messages --gt; lt;.'ELEMENT e-mail        ---(prolog, contents) gt; lt;!ELEMENT prolog        ---(sender, address*, subject?, Cc+) gt; lt;!ELEMENT (sender I address I subject I Cc) - 0 (#PCDATA) gt; lt;!ELEMENT contents                  - - (par I image I audio) + gt; lt;! ELEMENT par                             - 0 (ref I #PCDATA)+ gt; lt;! ELEMENT ref                             - 0 EMPTY gt; lt; [ELEMENT (image I audio) - -  (#NDATA) gt; lt;!ATTLIST e-mail id                         ID                #REQUIRED date.sent   DATE                                      #REQUIRED status                 (secret I public )  public gt; lt;!ATTLIST ref id                         IDREF              #REQUIRED gt; lt;!ATTLIST (image   | audio ) id                         ID                #REQUIRED gt; lt;!óExample of use of previous DTDógt; lt;!D0CTYPE e-mail SYSTEM "e-mail.dtd"gt; lt;e-mail id=94108rby date_sent=02101998gt; lt;prologgt; lt;sendergt; Pablo Neruda lt;/sendergt; lt;addressgt; Federico Garcia Lorca lt;/addressgt; lt;addressgt; Ernest Hemingway lt;/addressgt; lt;subjectgt; Pictures of my house in Isla Negra lt;Ccgt; Gabriel Garcia Marquez lt;/Ccgt; lt;/prologgt; lt;contentsgt; lt;pargt; As promised in my previous letter, I am sending two digital pictures to show you my house and the splendid view of the Pacific Ocean from my bedroom (photo lt;ref idref=F2gt;). lt;/pargt; lt;image id=Flgt; "photol.gif" lt;/imagegt; lt;image id=F2gt; "photo2.jpg" lt;/imagegt; lt;pargt; Regards from the South,  Pablo. lt;/contentsgt; Figure 6.3    DTD for structuring electronic mails and an example of its use. They are the components of an SGML system used for defining, for instance, that the data identified by a tag should be typeset in italics. One important use of SGML is in the Text Encoding Initiative (TEI). The TEI is a cooperative project that started in 1987 and includes several US associations related to the humanities and linguistics. The main goal is to generate guidelines for the preparation and interchange of electronic texts for scholarly 152        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES research, as well as for industry. In addition to the guidelines, TEI provides several document formats through SGML DTDs. One of the most used formats is TEI Lite. The TEI Lite DTD can be used stand-alone or together with the full set of TEI DTD files.
mir-0107	6.4.2    HTML HTML stands for HyperText Markup Language and is an instance of SGML. HTML was created in 1992 and has evolved during the past years, 4.0 being the latest version, released as a recommendation at the end of 1997. Currently it is being extended in many ways to solve its many limitations, for example, to be able to write mathematical formulas. Most documents on the Web are stored and transmitted in HTML. HTML is a simple language well suited for hypertext, multimedia, and the display of small and simple documents. HTML is based on SGML, and although there is an HTML DTD (Document Type Definition), most HTML instances do not explicitly make reference to the DTD. The HTML tags follow all the SGML conventions and also include formatting directives. HTML documents can have other media embedded within them, such as images or audio in different formats. HTML also has fields for metadata, which can be used for different applications and purposes. If we also add programs (for example, using Javascript) inside a page, some people call it dynamic HTML (or DHTML). This should not be confused with a Microsoft proposal (also called dynamic HTML) of an Application Programming Interface (API) for accessing and manipulating HTML documents. Figure 6.4 gives an example of an HTML document together with its output in a Web browser. Because HTML does not fix the presentation style of a document, in 1997, Cascade Style Sheets (CSS) were introduced. CSS offer a powerful and manageable way for authors, artists, and typographers to create visual effects that improve the aesthetics of HTML pages in the Web. Style sheets can be used one after another (called cascading) to define the presentation style for different elements of an HTML page. Style sheets separate information about presentation from document content, which in turn simplifies Web site maintenance, promotes Web page accessibility, and makes the Web faster. However, CSS support In current browsers is still modest. Another disadvantage is that two style sheets do not have to be consistent nor complete, so the stylistic result might not be good, in particular regarding color. CSS are supposed to balance the expectations of the author and of the reader regarding presentation Issues. Nevertheless, it is not clear who or in which cases the author or the reader should define the presentation. The evolution of HTML implies support for backward compatibility and also for forward compatibility, because people should also be able to see new documents with old browsers. HTML 4.0 has been specified in three flavors: strict, transitional, and frameset. Strict HTML only worries about non-presentational MARKUP LANGUAGES   153 lt;htmlgt; lt;headgt; lt;titlegt;HTML Examplelt;/titlegt; lt;meta naine=rby content="Just an example"gt; lt;/headgt; lt;bodygt; lt;hlgt;HTML Examplelt;/hlgt; lt;pgt; lt;hrgt; lt;pgt; HTML has many lt;igt;tagslt;/igt;, among them: lt;ligt; links to other lt;a href=http://www.w3c.org/gt;pageslt;/agt; (a from anchor), lt;ligt; paragraphs (p) , headings (hi, h2, etc), font types (b, i), lt;ligt; horizontal rules (hr), indented lists and items (ul, li), lt;ligt; images (img), tables, forms, etc. lt;hrgt; lt;Pgt; lt;img align=left src="at_work.gif"gt; This page is lt;bgt;alwayslt;/bgt; under construction. lt;/bodygt; lt;/htmlgt; HTML Example HTML has many tags, among them: ¶ links to other gag^. (a from anchor), ï  paragraphs (p), headings (hi,fc2,ete), font types fa i), m horizontal rules (hr), indented lists and items (ul, li), ï  images (img), tables, forms, etc. This page is always under construction. Figure 6.4    Example of an HTML document and how it is seen in a browser. 154        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES markup, leaving all the displaying information to CSS. Transitional HTML uses all the presentational features for pages that should be read for old browsers that do not understand CSS. Frameset HTML is used when you want to partition the browser window in two or more frames. HTML 4.0 includes support for style sheets, internationalization, frames, richer tables and forms, and accessibility options for people with disabilities. Typical HTML applications use a fixed small set of tags in conformance with a single SGML specification. Fixing a small set of tags allows users to leave the language specification out of the document and makes it much easier to build applications, but this advantage comes at the cost of severely limiting HTML in several important aspects. In particular, HTML does not: ï  allow users to specify their own tags or attributes in order to parameterize or otherwise semantically qualify their data; ï  support the specification of nested structures needed to represent database schemas or object-oriented hierarchies; ï  support the kind of language specification that allows consuming applications to check data for structural validity on importation. In contrast to HTML stands generic SGML. A generic SGML application is one that supports SGML language specifications of arbitrary complexity and makes possible the qualities of extensibility, structure, and validation missing in HTML. SGML makes it possible to define your own formats for your own documents, to handle large and complex documents, and to manage large information repositories. However, full SGML contains many optional features that are not needed for Web applications and have proven to have a cost/benefit ratio unattractive to current vendors of Web browsers. All these reasons led to the development of XML, a simpler metalanguage that is described in the next section.
mir-0108	6.4.3    XML XML stands for extensible Markup Language and is a simplified subset of SGML. That is, XML is not a markup language, as HTML is, but a metalanguage that is capable of containing markup languages in the same way as SGML. XML allows a human-readable semantic markup, which is also machine-readable. As a result, XML makes it easier to develop and deploy new specific markup, enabling automatic authoring, parsing, and processing of networked data. In some ways, XML alkws one to do many things that today are done by Java scripts or other program interfaces. XML does not have many of the restrictions imposed by HTML but on the other hand imposes a more rigid syntax on the markup, which becomes important at processing time. In XML, ending tags cannot be omitted. Also, tags for elements that do not have any content, like BR and IMG, are specially marked by a slash before the closing angle bracket. XML also distinguishes upper MARKUP LANGUAGES         155 lt;?XML VERSION="i.O"  RMD="NOKrE"   ?gt; lt;e-mail id="94108rbyn  date^sent="02101998"gt; lt;prologgt; lt;sendergt; Pablo Neruda lt;/sendergt; lt;addressgt; Federico Garcia Lorca lt;/addressgt; lt;addressgt; Ernest Hemingway lt;/addressgt; lt;subjectgt; Pictures of my house in Isla Negra lt;Ccgt; Gabriel Garcia Marquez lt;/Ccgt; lt;/prologgt; lt;contentsgt; lt;pargt; As promised in my previous letter, I am sending two digital pictures to show you my house and the splendid view of the Pacific Ocean from my bedroom (photo lt;ref idref="F2"/gt;) . lt;/pargt; lt;image id="Fl" ref="photol.gif" /gt; lt;image id="F2"gt; ref="photo2.jpg" /gt; lt;pargt; Regards from the South, Pablo. lt;/pargt; lt;/contentsgt; Figure 6.5    An XML document without a DTD analogous to the previous SGML example. and lower case, so img and IMG are different tags (this is not true in HTML). In addition, all attribute values must be between quotes. This implies that parsing XML without knowledge of the tags is easier. In particular, using a DTD is optional. If there is no DTD, the tags are obtained while the parsing is done. With respect to SGML, there are a few syntactic differences, and many more restrictions. Listing all these differences is beyond the scope of this book, but Figure 6.5 shows an example of a DTDless XML document based on the previous electronic mail DTD given for SGML (see Figure 6.3). The RMD attribute stands for Required Markup Declaration, which indicates whether a DTD must be used or not (no DTD in this case). Other possible values are INTERNAL which means that the DTD is inside the document or ALL (default value) which allows the use of external sources for part or the whole DTD as in SGML. XML allows any user to define new tags, define more complex structures (for example, unbounded nesting with the same rules of SGML) and has data validation capabilities. As XML is very new, there is still some discussion of how it will change or impact Internet applications. XML is a profile of SGML that eliminates many of the difficulties of implementing things, so for the most part it behaves just like SGML, as shown before. As mentioned, XML removes the requirement for the existence of a DTD, which can be parsed directly from the data. Removing the DTD places even more importance on the application documentation. This can also have a large impact on the functions that the software 156        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES provides. For example, it means that if an XML editor does not use a DTD, how will it help the user to tag the documents consistently? These problems should be resolved in the near future. In the case of semantic ambiguity between tag names, one goal is to have a namespace such that there is a convention for its use. The Extensible Style sheet Language (XSL) is the XML counterpart of Cascading Style Sheets. XSL is designed to transform and style highly-structured, data-rich documents written in XML. For example, with XSL it would be possible to automatically extract a table of contents from a document. The syntax of XSL has been defined using XML. In addition to adding style to a document, XSL can be used to transform XML documents to HTML and CSS. This is analogous to macros in a word processor. Another extension to XML, defined using XML, is the Extensible Linking Language (XLL). XLL defines different types of links, including external and internal links. In particular, any element type can be the origin of a link and outgoing links can be defined on documents that cannot be modified. The behavior of the links is also more generic. The object linked can be embedded in, or replace the document. It is also possible to generate a new context without changing the current application (for example, the object is displayed in a new window). Recent uses of XML include: ï  Mathematical Markup Language (MathML): two sets of tags, one for presentation of formulas and another for the meaning of mathematical expressions. ï  Synchronized Multimedia Integration Language (SMIL): a declarative language for scheduling multimedia presentations in the Web, where the position and activation time of different objects can be specified. ï  Resource Description Format (already covered in section 6.2): metadata information for XML should be given using RDF. The XML movement is one indication that a parseable, hierarchical object model will play an increasingly major role in the evolution of HTML. The next generation of HTML should be based on a suite of XML tag sets to be used together with mathematics, synchronized multimedia, and vector graphics (possibly using the XML-based languages already mentioned). That is, the emphasis will be on structuring and modeling data rather than on presentation and layout issues.
mir-0109	6.5    Multimedia Multimedia usually stands for applications that handle different types of digital data originating from distinct types of media. The most common types of media in multimedia applications are text, sound, images, and video (which is an animated sequence of images). The digital data originating from each of these four MULTIMEDIA        157 types of media is quite distinct in volume, format, and processing requirements (for instance, video and audio impose real time constraints on their processing). As an immediate consequence, different types of formats are necessary for storing each type of media. In this section we cover formats and standard languages for multimedia applications. In contrast with text formats, most formats for multimedia are partially binary and hence can only be processed by a computer. Also, the presentation style is almost completely defined, perhaps with the exception of some spatial or temporal attributes.
mir-0110	6.5.1    Formats Multimedia includes images, audio and video, as well as other binary data. We now briefly survey the main formats used for all these data types. They are used mainly in the Web and in digital libraries (see Chapters 13 and 15). There are several formats for images. The simplest formats are direct representations of a bit-mapped (or pixel-based) display such as XBM, BMP, or PCX. However, those formats consume too much space. For example, a typical computer screen which uses 256 colors for each pixel might require more than 1 Mb (one megabyte) in storage just for describing the content of a single screen frame. In practice, images have a lot of redundancy and can be compressed efficiently. So, most popular image formats incorporate compression such as CompuServe's Graphic Interchange Format (GIF). GIF is good for black and white pictures, as well as pictures that have a small number of colors or gray levels (say 256). To improve compression ratios for higher resolutions, lossy compression was developed. That is, uncompressing a compressed image does not give the original. This is done by the Joint Photographic Experts Group (JPEG) format, which tries to eliminate parts of the image that have less impact on the human eye. This format is parametric, in the sense that the loss can be tuned. Another common image format is the Tagged Image File Format (TIFF). This format is used to exchange documents between different applications and different computer platforms. TIFF has fields for metadata and also supports compression as well as different numbers of colors. Yet another format is Truevision Targa image file (TGA), which is associated with video game boards. There are many more image formats, many of them associated to particular applications ranging from fax (bi-level image formats such as JBIG) to fingerprints (highly accurate and compressed formats such as WSQ) and satellite images (large resolution and full-color images). In 1996 a new bit-mapped image format was proposed for the Internet: Portable Network Graphics (PNG). This format could be important in the future. Audio must be digitalized first in order to be stored properly. The most common formats for small pieces of digital audio are AU, MIDI, and WAVE. MIDI is an standard format to interchange music between electronic instruments and computers. For audio libraries other formats are used such as RealAudio or CD formats. 158        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES There are several formats for animations or moving images (similar to video or TV), but here we mention only the most popular ones. The main one is MPEG (Moving Pictures Expert Group) which is related to JPEG. MPEG works by coding the changes with respect to a base image which is given at fixed intervals. In this way, MPEG profits from the temporal image redundancy that any video has. Higher quality is achieved by using more frames and higher resolution. MPEG specifies different compression levels, but usually not all the applications support all of them. This format also includes the audio signal associated with the video. Other video formats are AVI, FLI, and QuickTime. AVI may include compression (CinePac), as well as QuickTime, which was developed by Apple. As for MPEG, audio is also included.
mir-0111	6.5.2    Textual Images A particular class of images that is very important in office systems, multimedia retrieval and digital libraries are images of documents that contain mainly typed or typeset text. These are called textual images and are obtained by scanning the documents, usually for archiving purposes ó a procedure that also makes the images (and their associated text) available to anyone through a computer network. The fact that a large portion of a textual image is text can be used for retrieval purposes and efficient compression. Although we do not cover image compression in this chapter, we have seen that the most popular image formats include some form of compression embedded in them. In the case of textual images, further compression can be achieved by extracting the different text symbols or marks from the image, building a library of symbols for them, and representing each one (within the image) by a position in the library. As many symbols are repeated, the compression ratio is quite good. Although this technique is lossy (because the reconstructed image is not equal to the original), the reconstructed image can be read without problems. Additional information can be stored to reproduce the original image, but for most applications this is not needed. If the image contains non-textual information such as logos or signatures, which might be necessary to reproduce, they may be extracted through a segmentation process, stored, and compressed separately. When needed, the textual and non-textual parts of the image can be combined and displayed together. Regarding the retrieval of textual images, several alternatives are possible as follows: ï  At creation time or when added to the database, a set of ke}rwords that describe the image is associated with it (for example, metadata can be used). Later, conventional text retrieval techniques can be applied to those keywords. This alternative is valid for any multimedia object. ï  Use OCR to extract the text of the image. The resultant ASCII text can be used to extract keywords, as before, or as a full-text description of the MULTIMEDIA        159 image. Depending on the document type, the OCR output could be reasonably good or actually quite bad (consider the first page of a newspaper, with several columns, different font types and sizes). In any case, many typos are introduced and a usual keyword-based query might miss many documents (in this case, an approximate search is better, but also slower). Use the symbols extracted from the images as basic units to combine image retrieval techniques (see Chapter 12) with sequence retrieval techniques (see Chapter 8). In this case, the query is transformed into a symbol sequence that has to match approximately another symbol sequence in the compressed image. This idea seems promising but has not been pursued yet.
mir-0112	6.5.3    Graphics and Virtual Reality There are many formats proposed for three-dimensional graphics. Although this topic is not fully relevant to information retrieval, we include some information here for the sake of completeness. Our emphasis here is on the Web. The Computer Graphics Metafile (CGM) standard (ISO 8632) is defined for the open interchange of structured graphical objects and their associated attributes. CGM specifies a two-dimensional data interchange standard which allows graphical data to be stored and exchanged between graphics devices, applications, and computer systems in a device-independent manner. It is a structured format that can represent vector graphics (for example, polylines or ellipses), raster graphics, and text. Although initially CGM was a vector graphics format, it has been extended to include raster capabilities and provides a very useful format for combined raster and vector images. A metafile is a collection of elements. These elements may be the geometric components of the picture, such as polyline or polygon; the appearance of these components; or how to interpret a particular metafile or a particular picture. The CGM standard specifies which elements are allowed to occur in which positions in a metafile. The Virtual Reality Modeling Language (VRML, ISO/IEC 14772-1) is a file format for describing interactive 3D objects and wrorlds and is a subset of the Silicon Graphics Openlnventor file format. VRML is also intended to be a universal interchange format for integrated 3D graphics and multimedia. VRML may be used in a variety of application areas such as engineering and scientific visualization, multimedia presentations, entertainment and educational titles, Web pages, and shared virtual worlds. VRML has become the de facto standard modeling language for the Web.
mir-0113	6.5.4    HyTime The Hypermedia/Time-based Structuring Language {HyTime) is a standard (ISO/IEC 10744) defined for multimedia documents markup.    HyTime is an SGML architecture that specifies the generic hypermedia structure of documents. 160        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES Following the guiding principle of SGML, HyTime-defined structure is independent of any presentation of the encoded document. As an architecture, HyTime allows DTDs to be written for individual document models that use HyTime constructs, specifying how these document sets tailor the composition of these constructs for their particular representational needs. The standard also provides several metaDTDs, facilitating the design of new multimedia markup languages. The hypermedia concepts directly represented by HyTime include ï  complex locating of document objects, ï  relationships (hyperlinks) between document objects, and ï  numeric, measured associations between document objects. The HyTime architecture has three parts: the base linking and addressing architecture, the scheduling architecture (derived from the base architecture), and the rendition architecture (which is an application of the scheduling architecture). The base architecture addresses the syntax and semantics of hyperlinks. For most simple hypermedia presentations, this should be enough. The scheduling module of HyTime defines the abstract representation of arbitrarily complex hypermedia structures, including music and interactive presentations. Its basic mechanism is a simple one: the sequencing of object containers along axes measured in temporal or spatial units. The rendition module is essentially an application of the scheduling architecture that defines a general mechanism for defining the creation of new schedules from existing schedules by applying special "rendition rules' of different types. HyTime does not directly specify graphical interfaces, user navigation, user interaction, or the placement of media on time lines and screen displays. These aspects of document processing are rendered from the HyTime constructs in a manner specified by mechanisms such as style sheets, as is done with SGML documents. One application of HyTime, is the Standard Music Description Language (SMdigital library). SDML is an architecture for the representation of music information, either alone, or in conjunction with other media, also supporting multimedia time sequencing information. Another application is the Metafile for Interactive Documents (MID). MID is a common interchange structure, based on SGML and HyTime. that takes data from various authoring systems and structures it for display on dissimilar presentation systems, with minimal human intervention.
mir-0114	6.6    Trends and Research issues Many changes and proposals are happening, and very rapidly, in particular due to the advent of the Web. At this point, the reader must be lost in a salad of acronyms (we were ton!), in spite of the fact that we have only mentioned the most important languages and formats. The most important of these are included in the Glossary at the end of this book. Soine people believe that new Languages TEI Lite TRENDS AND RESEARCH ISSUES        161 SGML                                            :         DSSSL 1 XSL A CSS Style sheets Next           RDF      MathML    SMIL Generation HTML Figure 6.6    Taxonomy of Web languages. format specifications such as CSS or XML take away the simplicity of HTML, which is the basis of its success. Only the future will tell. Figure 6.6 illustrates a taxonomy of the main languages considered. Solid lines indicate instances of a metalanguage (for example, HTML is an instance of SGML), while dashed lines indicate derived languages. The main trend is the convergence and integration of the different efforts, the Web being the main application. A European alternative to SGML is the Open Document Architecture (ODA) which is also a standard (ISO 8613 [398]). ODA was designed to share documents electronically without losing control over the content, structure, and layout of those documents. ODA defines a logical structure (like SGML), a layout and the content (including vector and raster graphics). An ODA file can also be formatted, processable, or formatted processable. Formatted files cannot be edited and have information about content and layout. The other two types can be edited. Processable files also have logical information in addition to content, while formatted processable files have everything. ODA is not used very much nowadays (see also Chapter 11). Recent developments include: ï  An object model is being defined:   the document object model (DOM). DOM will provide an interoperable set of classes and methods to manipulate HTML and XML objects from programming languages such as Java. ï  Integration between VRML and Dynamic HTML, providing a set of evolving features aod architecture extensions to HTML and Web browsers that includes cascading style sheets and document object models. ï  Integration between the Standard Exchange for Product Data format (STEP,  ISO   10303)   and  SGML.   STEP  covers  product  data from  a broad range of industries, and provides extensive support for modeling. 162        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES automated storage schema generation, life-cycle maintenance, and other management facilities. ï  Efforts to convert MARC to SGML by denning a DTD, as well as converting MARC to XML. This has potential possibilities for enhanced access and navigation and presentation of MARC record data and the associated information. ï  CGM has become of interest to Web researchers and commercial vendors for its use on the Internet, by developing a new encoding which can be parsed by XML. ï  Several new proposals have appeared. Among them we can mention SDML (Signed Document Markup Language), VML (Vector Markup Language), and PGML (Precision Graphics Markup Language).   The latter is based on the 2D imaging model of Postscript and PDF.
mir-0115	6.7    Bibliographic Discussion The document model used in the introduction is based on [437]. Specific information on Web metadata is given in [487, 753]. Most of the information about markup languages and related issues is from the World Wide Web Consortium (see www.w3. org), in particular information on new developments such as DOM or SMIL. More information on SGML and XML is given by Goldfarb [303, 304]. Additional references in SGML are [369, 756] (in particular, the SGML example has been adapted from [24]). There are hundreds of books on HTML. Two sources for HTML 4.0 are [207, 796], A book on CSS is [517]. For information on XML, XSL, and XLL see [795, 799, 798]. For a discussion about the advantages and disadvantages of XML and related languages see [182, 106, 455, 436]. More information on multimedia formats can be found in [501]. Formats for images and compression of textual images are covered in detail in [825].
mir-0117	7.1     Introduction As discussed in Chapter 2, not all words are equally significant for representing the semantics of a document. In written language, some words carry more meaning than others. Usually, noun words (or groups of noun words) are the ones which are most representative of a document content. Therefore, it is usually considered worthwhile to preprocess the text of the documents in the collection to determine the terms to be used as index terms. During this preprocessing phase other useful text operations can be performed such as elimination of stop-words, stemming (reduction of a word to its grammatical root), the building of a thesaurus, and compression. Such text operations are discussed in this chapter. We already know that representing documents by sets of index terms leads to a rather imprecise representation of the semantics of the documents in the collection. For instance, a term like 'the" has no meaning whatsoever by itself and might lead to the retrieval of various documents which are unrelated to the present user query. We say that using the set of all words in a collection to index its documents generates too much noise for the retrieval task. One way to reduce this noise is to reduce the set of words which can be used to refer to (i.e., to index) documents. Thus, the preprocessing of the documents in the collection might be viewed simply as a process of controlling the size of the vocabulary (i.e., the number of distinct words used as an index terms). It is expected that the use of a controlled vocabulary leads to an improvement in retrieval performance. While controlling the size of the vocabulary is a common technique with commercial systems, it does introduce an additional step in the indexing process which is frequently not easily perceived by the users. As a result, a common user might be surprised with some of the documents retrieved and with the absence of other documents which he expected to see. For instance, he might remember that a certain document contains the string 'the house of the lord and notice that such a document is not present among the top 20 documents retrieved in 163 164        TEXT OPERATIONS response to his query request (because the controlled vocabulary contains neither 'the' nor 'of'). Thus, it should be clear that, despite a potential improvement in retrieval performance, text transformations done at preprocessing time might make it more difficult for the user to interpret the retrieval task. In recognition of this problem, some search engines in the Web are giving up text operations entirely and simply indexing all the words in the text. The idea is that, despite a more noisy index, the retrieval task is simpler (it can be interpreted as a full text search) and more intuitive to a common user. Besides document preprocessing, other types of operations on documents can also be attempted with the aim of improving retrieval performance. Among these we distinguish the construction of a thesaurus representing conceptual term relationships and the clustering of related documents. Thesauri are also covered in this chapter. The discussion on document clustering is covered in Chapter 5 because it is an operation which might depend on the current user query. Text normalization and the building of a thesaurus are strategies aimed at improving the precision of the documents retrieved. However, in the current world of very large digital libraries, improving the efficiency (in terms of time) of the retrieval process has also become quite critical. In fact, Web search engines are currently more concerned with reducing query response time than with improving precision and recall figures. The reason is that they depend on processing a high number of queries per unit of time for economic survival. To reduce query response time, one might consider the utilization of text compression as a promising alternative. A good compression algorithm is able to reduce the text to 30-35% of its original size. Thus, compressed text requires less storage space and takes less time to be transmitted over a communication link. The main disadvantage is the time spent compressing and decompressing the text. Until recently, it was generally understood that compression does not provide substantial gains in processing time because the extra time spent compressing/decompressing text would offset any gains in operating with compressed data. Further, the use of compression makes the overall design and implementation of the information system more complex. However, modern compression techniques are slowly changing this understanding towards a more favorable view of the adoption of compression techniques. By modern compression techniques we mean good compression and decompression speeds, fast random access without the need to decode the compressed text from the beginning, and direct searching on the compressed text without decompressing it, among others. Besides compression, another operation on text which is becoming more and more important is encryption. In fact, due to the fast popularization of services in the Web (including all types of electronic commerce), key (and old) questions regarding security and privacy have surfaced again. More than ever before, impersonation and unauthorized access might result in great prejudice and financial damage to people and organizations. The solution to these problems is not simple but can benefit from the operation of encrypting text. Discussing encrypted text is beyond the scope of this book but an objective and brief introduction to the topic can be found in [501]. DOCUMENT PREPROCESSING         165 In this chapter, we first discuss five preprocessing text operations including thesauri. Following that, we very briefly summarize the problem of document clustering (which is discussed in detail in Chapter 5). Finally, a thorough discussion on the issue of text compression, its modern variations, and its main implications is provided.
mir-0118	7.2    Document Preprocessing Document preprocessing is a procedure which can be divided mainly into five text operations (or transformations): (1)  Lexical analysis of the text with the objective of treating digits, hyphens, punctuation marks, and the case of letters. (2)  Elimination of stopwords with the objective of filtering out words with very low discrimination values for retrieval purposes. (3)  Stemming of the remaining words with the objective of removing affixes (i.e., prefixes and suffixes) and allowing the retrieval of documents containing syntactic variations of query terms (e.g., connect, connecting, connected, etc). (4)  Selection of index terms to determine which words/stems (or groups of words) will be used as an indexing elements.    Usually, the decision on whether a particular word will be used as an index term is related to the syntactic nature of the word.   In fact, noun words frequently carry more semantics than adjectives, adverbs, and verbs. (5)  Construction of term categorization structures such as a thesaurus, or extraction of structure directly represented in the text, for allowing the expansion of the original query with related terms (a usually useful procedure). In the following, each of these phases is discussed in detail. But, before proceeding, let us take a look at the logical view of the documents which results after each of the above phases is completed. Figure 1.2 is repeated here for convenience as Figure 7.1. As already discussed, by aggregating the preprocessing phases, we are able to move the logical view of the documents (adopted by the system) from that of a full text to that of a set of high level indexing terms.
mir-0119	7.2.1    Lexical Analysis of the Text Lexical analysis is the process of converting a stream of characters (the text of the documents) into a stream of words (the candidate words to be adopted as index terms). Thus, one of the major objectives of the lexical analysis phase is the identification of the words in the text. At first glance, all that seems to be involved is the recognition of spaces as word separators (in which case, multiple 166 TEXT OPERATIONS Figure 7.1    Logical view of a document throughout the various phases of text preprocessing. spaces are reduced to one space). However, there is more to it than this. For instance, the following four particular cases have to be considered with care [263]: digits, hyphens, punctuation marks, and the case of the letters (lower and upper case). Numbers are usually not good index terms because, without a surrounding context, they are inherently vague. For instance, consider that a user is interested in documents about the number of deaths due to car accidents between the years 1910 and 1989. Such a request could be specified as the set of index terms {deaths, car, accidents, years, 1910, 1989}. However, the presence of the numbers 1910 and 1989 in the query could lead to the retrieval, for instance, of a variety of documents which refer to either of these two years. The problem is that numbers by themselves are just too vague. Thus, in general it is wise to disregard numbers as index terms. However, we have also to consider that digits might appear mixed within a word. For instance, '510B.C/ is a clearly important index term. In this case, it is not clear what rule should be applied. Furthermore, a sequence of 16 digits identifying a credit card number might be highly relevant in a given context and, in this case, should be considered as an index term. A preliminary approach for treating digits in the text might be to remove all words containing sequences of digits unless specified otherwise (through regular expressions). Further, an advanced lexical analysis procedure might perform some date and number normalization to unify formats. Hyphens pose another difficult decision to the lexical analyzer. Breaking up hyphenated words might be useful due to inconsistency of usage. For instance, this allows treating "state-of-the-art' and 'state of the art' identically. However, there are words which include hyphens as an integral part. For instance, gilt-edge, B-49, etc. Again, the most suitable procedure seems to adopt a general role and specify the exceptions on a case by case basis. Normally, punctuation marks are removed entirely in the process of lexical analysis.  While some punctuation marks are an integral part of the word (for DOCUMENT PREPROCESSING         167 instance, '510B.C.7), removing them does not seem to have an impact in retrieval performance because the risk of misinterpretation in this case is minimal. In fact, if the user specifies '510B.C in his query, removal of the dot both in the query term and in the documents will not affect retrieval. However, very particular scenarios might again require the preparation of a list of exceptions. For instance, if a portion of a program code appears in the text, it might be wise to distinguish between the variables 'x.id' and *xid.' In this case, the dot mark should not be removed. The case of letters is usually not important for the identification of index terms. As a result, the lexical analyzer normally converts all the text to either lower or upper case. However, once more, very particular scenarios might require the distinction to be made. For instance, when looking for documents which describe details about the command language of a Unix-like operating system, the user might explicitly desire the non-conversion of upper cases because this is the convention in the operating system. Further, part of the semantics might be lost due to case conversion. For instance, the words Bank and bank have different meanings ó a fact common to many other pairs of words. As pointed out by Fox [263], all these text operations can be implemented without difficulty. However, careful thought should be given to each one of them because they might have a profound impact at document retrieval time. This is particularly worrisome in those situations in which the user finds it difficult to understand what the indexing strategy is doing. Unfortunately, there is no clear solution to this problem. As already mentioned, some Web search engines are opting for avoiding text operations altogether because this simplifies the interpretation the user has of the retrieval task. Whether this strategy will be the one of choice in the long term remains to be seen.
mir-0120	7.2.2    Elimination of Stopwords As discussed in Chapter 2, words which are too frequent among the documents in the collection are not good discriminators. In fact, a word which occurs in 80% of the documents in the collection is useless for purposes of retrieval. Such words are frequently referred to as stopwords and are normally filtered out as potential index terms. Articles, prepositions, and conjunctions are natural candidates for a list of stopwords. Elimination of stopwords has an additional important benefit. It reduces the size of the indexing structure considerably. In fact, it is typical to obtain a compression in the size of the indexing structure (for instance, in the size of an inverted list, see Chapter 8) of 40% or more solely with the elimination of stopwords. Since stopword elimination also provides for compression of the indexing structure, the list of stopwords might be extended to include words other than articles, prepositions, and conjunctions. For instance, some verbs, adverbs, and adjectives could be treated as stopwords. In [275]. a list of 425 stopwords is illustrated. Programs in C for lexical analysis are also provided. 168        TEXT OPERATIONS Despite these benefits, elimination of stopwords might reduce recall. For instance, consider a user who is looking for documents containing the phrase Ho be or not to be.: Elimination of stopwords might leave only the term be making it almost impossible to properly recognize the documents which contain the phrase specified. This is one additional reason for the adoption of a full text index (i.e., insert all words in the collection into the inverted file) by some Web search engines.
mir-0121	7.2.3    Stemming Frequently, the user specifies a word in a query but only a variant of this word is present in a relevant document. Plurals, gerund forms, and past tense suffixes are examples of syntactical variations which prevent a perfect match between a query word and a respective document word. This problem can be partially overcome with the substitution of the words by their respective stems. A stem is the portion of a word which is left after the removal of its affixes (i.e., prefixes and suffixes). A typical example of a stem is the word connect which is the stem for the variants connected, connecting, connection, and connections. Stems are thought to be useful for improving retrieval performance because they reduce variants of the same root word to a common concept. Furthermore, stemming has the secondary effect of reducing the size of the indexing structure because the number of distinct index terms is reduced. While the argument supporting stemming seems sensible, there is controversy in the literature about the benefits of stemming for retrieval performance. In fact, different studies lead to rather conflicting conclusions. Frakes [275] compares eight distinct studies on the potential benefits of stemming. While he favors the usage of stemming, the results of the eight experimental studies he investigated do not allow us to reach a satisfactory conclusion. As a result of these doubts, many Web search engines do not adopt any stemming algorithm whatsoever. Frakes distinguishes four types of stemming strategies: affix removal, table lookup, successor variety, and n-grams. Table lookup consists simply of looking for the stem of a word in a table. It is a simple procedure but one which is dependent on data on stems for the whole language. Since such data is not readily available and might require considerable storage space, this type of stemming algorithm might not be practical. Successor variety stemming is based on the determination of morpheme boundaries, uses knowledge from structural linguistics, and is more complex than affix removal stemming algorithms. N-grams stemming is based on the identification of digrams and trigrams and is more a term clustering procedure than a stemming one. Affix removal stemming is intuitive, simple, and can be implemented efficiently. Thus, in the remainder of this section we concentrate our discussion on algorithms for affix removal stemming only. In affix removal the most important part is suffix removal because most variants of a word are generated by the introduction of suffixes (instead of preDOCUMENT PREPROCESSING        169 fixes). While there are three or four well known suffix removal algorithms, the most popular one is that by Porter because of its simplicity and elegance. Despite being simpler, the Porter algorithm yields results comparable to those of the more sophisticated algorithms. The Porter algorithm uses a suffix list for suffix stripping. The idea is to apply a series of rules to the suffixes of the words in the text. For instance, the rule s ógt; 0                                                                                          (7.1) is used to convert plural forms into their respective singular forms by substituting the letter s by nil. Notice that to identify the suffix we must examine the last letters in the word. Furthermore, we look for the longest sequence of letters which matches the left hand side in a set of rules. Thus, application of the two following rules sses    ógt;    ss                                                                                                   (7.2) to the word stresses yields the stem stress instead of the stem stresse. By separating such rules into five distinct phases, the Porter algorithm is able to provide effective stemming while running fast. A detailed description of the Porter algorithm can be found in the appendix.
mir-0122	7.2.4    Index Terms Selection If a full text representation of the text is adopted then all words in the text are used as index terms. The alternative is to adopt a more abstract view in which not all words are used as index terms. This implies that the set of terms used as indices must be selected. In the area of bibliographic sciences, such a selection of index terms is usually done by a specialist. An alternative approach is to select candidates for index terms automatically. Distinct automatic approaches for selecting index terms can be used. A good approach is the identification of noun groups (as done in the Inquery system [122]) which we now discuss. A sentence in natural language text is usually composed of nouns, pronouns, articles, verbs, adjectives, adverbs, and connectives. While the words in each grammatical class are used with a particular purpose, it can be argued that most of the semantics is carried by the noun words. Thus, an intuitively promising strategy for selecting index terms automatically is to use the nouns in the text. This can be done through the systematic elimination of verbs, adjectives, adverbs, connectives, articles, and pronouns. Since it is common to combine two or three nouns in a single component (e.g., computer science), it makes sense to cluster nouns which appear nearby in the text into a single indexing component (or concept). Thus, instead of simply 170        TEXT OPERATIONS using nouns as index terms, we adopt noun groups. A noun group is a set of nouns whose syntactic distance in the text (measured in terms of number of words between two nouns) does not exceed a predefined threshold (for instance, 3). When noun groups are adopted as indexing terms, we obtain a conceptual logical view of the documents in terms of sets of non-elementary index terms.
mir-0123	7.2.5    Thesauri The word thesaurus has Greek and Latin origins and is used as a reference to a treasury of words [261]. In its simplest form, this treasury consists of (1) a precompiled list of important words in a given domain of knowledge and (2) for each word in this list, a set of related words. Related words are, in its most common variation, derived from a synonymity relationship. In general, however, a thesaurus also involves some normalization of the vocabulary and includes a structure much more complex than a simple list of words and their synonyms. For instance, the popular thesaurus published by Peter Roget [679] also includes phrases which means that concepts more complex than single words are taken into account. Roget's thesaurus is of a general nature (i.e., not specific to a certain domain of knowledge) and organizes words and phrases in categories and subcategories. An example of an entry in Roget's thesaurus is as follows: cowardly adjective Ignobly lacking in courage: cowardly turncoats. Syns:   chicken (slang),  chicken-hearted,  craven,  dastardly,  fainthearted, gutless, lily-livered, pusillanimous, unmanly, yellow (slang), yellow-bellied (slang). To the adjective cowardly, Roget's thesaurus associates several synonyms which compose a thesaurus class. While Roget5s thesaurus is of a generic nature, a thesaurus can be specific to a certain domain of knowledge. For instance, the Thesaurus of Engineering and Scientific Terms covers concepts related to engineering and technical terminology. According to Foskett [261], the main purposes of a thesaurus are basically: (a) to provide a standard vocabulary (or system of references) for indexing and searching; (b) to assist users with locating terms for proper query formulation; and (c) to provide classified hierarchies that allow the broadening and narrowing of the current query request according to the needs of the user. In this section, however, we do not discuss how to use a thesaurus for modifying the user query. This issue is covered on Chapter 5 which also discusses algorithms for automatic construction of thesauri. Notice that the motivation for building a thesaurus is based on the fundamental idea of using a con trolled vocabulary for the indexing and searching. A controlled vocabulary presents important advantages such as normalization DOCUMENT PREPROCESSING         171 of indexing concepts, reduction of noise, identification of indexing terms with a clear semantic meaning, and retrieval based on concepts rather than on words. Such advantages are particularly important in specific domains, such as the medical domain for which there is already a large amount of knowledge compiled. For general domains, however, a well known body of knowledge which can be associated with the documents in the collection might not exist. The reasons might be that the document base is new, that it is too large, or that it changes very dynamically. This is exactly the case with the Web. Thus, it is not clear how useful a thesaurus is in the context of the Web. Despite that, the success of the search engine named 'Yahoo!' (see Chapter 13), which presents the user with a term classification hierarchy that can be used to reduce the space to be searched, suggests that thesaurus-based techniques might be quite useful even in the dynamic world of the Web. It is still too early to reach a consensus on the advantages of a thesaurus for the Web. As a result, many search engines simply use all the words in all the documents as index terms (i.e., there is no notion of using the concepts of a controlled vocabulary for indexing and searching purposes). Whether thesaurus-based techniques will flourish in the context of the Web remains to be seen. The main components of a thesaurus are its index terms, the relationships among the terms, and a layout design for these term relationships. Index terms and term relationships are covered below. The layout design for term relationships can be in the form of a list or in the form of a bi-dimensional display. Here, we consider only the more conventional layout structure based on a list and thus, do not further discuss the issue of layout of the terms in a thesaurus. A brief coverage of topics related to this problem can be found in Chapter 10. A more detailed discussion can be found in [261]. Theasurus Index Terms The terms are the indexing components of the thesaurus.   Usually, a term in a thesaurus is used to denote a concept which is the basic semantic unit for conveying ideas. Terms can be individual words, groups of words, or phrases, but most of them are single words. Further, terms are basically nouns because nouns are the most concrete part of speech. Terms can also be verbs in gerund form whenever they are used as nouns (for instance, acting, teaching, etc.). Whenever a concept cannot be expressed by a single word, a group of words is used instead. For instance, many concepts are better expressed by a combination of an adjective with a noun. A typical example is ballistic missiles. In this case, indexing the compound term directly will yield an entry under balistic and no entry under missiles which is clearly inadequate. To avoid this problem, the compound term is usually modified to have the noun as the first word. For instance, we can change the compound term to missiles, ballistic. We notice the use of the plural form missiles instead of the singular form missile. The reasoning is that a thesaurus represents classes of things and thus it is natural to prefer the plural form.   However, the singular form is used for 172        TEXT OPERATIONS compound terms which appear normally in the singular such as body temperature. Deciding between singular and plural is not always a simple matter. Besides the term itself, frequently it is necessary to complement a thesaurus entry with a definition or an explanation. The reason is the need to specify the precise meanings of a term in the context of a particular thesaurus. For instance, the term seal has a meaning in the context of marine animals and a rather distinct meaning in the context of documents. In these cases, the definition might be preceded by a context explanation such as seal (marine animals) and seal (documents) [735]. Thesaurus Term Relationships The set of terms related to a given thesaurus term is mostly composed of synonyms and near-synonyms. In addition to these, relationships can be induced by patterns of co-occurrence within documents. Such relationships are usually of a hierarchical nature and most often indicate broader (represented by BT) or narrower (represented by NT) related terms. However, the relationship might also be of a lateral or non-hierarchical nature. In this case, we simply say that the terms are related (represented by RT). As discussed in Chapter 5, BT and NT relationships define a classification hierarchy where the broader term is associated with a class and its related narrower terms are associated with the instances of this class. Further, it might be that a narrower term is associated with two or more broader terms (which is not the most common case though). While BT and NT relationships can be identified in a fully automatic manner (i.e., without assistance from a human subject), dealing with RT relationships is much harder. One reason seems to be that RT relationships are dependent on the specific context and particular needs of the group of users and thus are difficult to identify without knowledge provided by specialists. On the Use of Thesauri in IR As described by Peter Roget [679, 261], a thesaurus is a classification scheme composed of words and phrases whose organization aims at facilitating the expression of ideas in written text.   Thus, whenever a writer has a difficulty in finding the proper term to express an idea (a common occurrence in serious writing), he can use the thesaurus to obtain a better grasp on the fundamental semantics of terms related to his idea. In the area of information retrieval, researchers have for many years conjectured and studied the usefulness of a thesaurus for helping with the query formation process. Whenever a user wants to retrieve a set of documents, he first builds up a conceptualization of what he is looking for. Such conceptualization is what we call his information need. Given the information need, the user still lias to translate it into a query in the language of the IR system. This usually DOCUMENT CLUSTERING         173 means that a set of index terms has to be selected. However, since the collection might be vast and the user inexperienced, the selection of such initial terms might be erroneous and improper (a very common situation with the largely unknown and highly dynamic collection of documents and pages which compose the Web). In this case, reformulating the original query seems to be a promising course of action. Such a reformulation process usually implies expanding the original query with related terms. Thus, it seems natural to use a thesaurus for assisting the user with the search for related terms. Unfortunately, this approach does not work well in general because the relationships captured in a thesaurus frequently are not valid in the local context of a given user query. One alternative is to determine thesaurus-like relationships at query time. Unfortunately, such an alternative is not attractive for Web search engines which cannot afford to spend a lot of time with the processing of individual queries. This and many other interesting issues related to the use of thesaurus-based techniques in IR are covered in Chapter 5.
mir-0124	7.3    Document Clustering Document clustering is the operation of grouping together similar (or related) documents in classes. In this regard, document clustering is not really an operation on the text but an operation on the collection of documents. The operation of clustering documents is usually of two types: global and local. In a global clustering strategy, the documents are grouped accordingly to their occurrence in the wrhole collection. In a local clustering strategy, the grouping of documents is affected by the context defined by the current query and its local set of retrieved documents. Clustering methods are usually used in IR to transform the original query in an attempt to better represent the user information need. From this perspective, clustering is an operation which is more related to the transformation of the user query than to the transformation of the text of the documents. In this book, document clustering techniques are treated as query operations and thus, are covered in Chapter 5 (instead of here).
mir-0126	7.4.1    Motivation Text compression is about finding ways to represent the text in fewer bits or bytes. The amount of space required to store text on computers can be reduced significantly using compression techniques. Compression methods create a reduced representation by identifying and using structures that exist in the text. From the compressed version, the original text can be reconstructed exactly. Text compression is becoming an important issue in an information retrieval environment.   The widespread use of digital libraries, office automation 174        TEXT OPERATIONS systems, document databases, and the Web has led to an explosion of textual information available online. In this scenario, text compression appears as an attractive option for reducing costs associated with space requirements, input/output (I/O) overhead, and communication delays. The gain obtained from compressing text is that it requires less storage space, it takes less time to be transmitted over a communication link, and it takes less time to search directly the compressed text. The price paid is the time necessary to code and decode the text. A major obstacle for storing text in compressed form is the need for IR systems to access text randomly. To access a given word in a compressed text, it is usually necessary to decode the entire text from the beginning until the desired word is reached. It could be argued that a large text could be divided into blocks that are compressed independently, thus allowing fast random access to each block. However, efficient compression methods need to process some text before making compression effective (usually more than 10 kilobytes). The smaller the blocks, the less effective compression is expected to be. Our discussion here focuses on text compression methods which are suitable for use in an IR environment. For instance, a successful idea aimed at merging the requirements of compression algorithms and the needs of IR systems is to consider that the symbols to be compressed are words and not characters (character-based compression is the more conventional approach). Words are the atoms on which most IR systems are built. Moreover, it is now known that much better compression is achieved by taking words as symbols (instead of characters). Further, new word-based compression methods allow random access to words within the compressed text which is a critical issue for an IR system. Besides the economy of space obtained by a compression method, there are other important characteristics to be considered such as compression and decompression speed. In some situations, decompression speed is more important than compression speed. For instance, this is the case with textual databases in which it is common to compress the text once and to read it many times from disk. Another important characteristic of a compression method is the possibility of performing compressed pattern matching, defined as the task of performing pattern matching in a compressed text without decompressing it. In this case, sequential searching can be speeded up by compressing the search key rather than decoding the compressed text being searched. As a consequence, it is possible to search faster on compressed text because much less text has to be scanned. Chapter 8 presents efficient methods to deal with searching the compressed text directly. When the text collection is large, efficient text retrieval requires specialized index techniques. A simple and popular indexing structure for text collections are the inverted files. Inverted files (see Chapter 8 for details) are especially adequate when the pattern to be searched for is formed by simple words. Since this is a common type of query (for instance, when searching the Web), inverted files are widely used for indexing large text collections. An inverted file is typically composed of (a) a vector containing all the distinct words in the text collection (which is called the vocabulary) and (b) for TEXT COMPRESSION        175 each word in the vocabulary, a list of all documents (identified by document numbers) in which that word occurs. Because each list of document numbers (within the inverted file) is organized in ascending order, specific compression methods have been proposed for them, leading to very efficient index compression schemes. This is important because query processing time is highly related to index access time. Thus, in this section, we also discuss some of the most important index compression techniques. We first introduce basic concepts related to text compression. We then present some of the most important statistical compression methods, followed by a brief review of compression methods based on a dictionary. At the end, we discuss the application of compression to inverted files.
mir-0127	7.4.2    Basic Concepts There are two general approaches to text compression: statistical and dictionary based. Statistical methods rely on generating good probability estimates (of appearance in the text) for each symbol. The more accurate the estimates are, the better the compression obtained. A symbol here is usually a character, a text word, or a fixed number of characters. The set of all possible symbols in the text is called the alphabet The task of estimating the probability on each next symbol is called modeling. A model is essentially a collection of probability distributions, one for each context in which a symbol can be coded. Once these probabilities are available the symbols are converted into binary digits, a process called coding. In practice, both the encoder and decoder use the same model. The decoder interprets the output of the encoder (with reference to the same model) to find out the original symbol. There are two well known statistical coding strategies: Huffman coding and arithmetic coding. The idea of Huffman coding is to assign a fixed-length bit encoding to each different symbol of the text. Compression is achieved by assigning a smaller number of bits to symbols with higher probabilities of appearance. Huffman coding was first proposed in the early 1950s and was the most important compression method until the late 1970s, when arithmetic coding made higher compression rates possible. Arithmetic coding computes the code incrementally, one symbol at a time, as opposed to the Huffman coding scheme in which each different symbol is pre-encoded using a fixed-length number of bits. The incremental nature does not allow decoding a string which starts in the middle of a compressed file. To decode a symbol in the middle of a file compressed with arithmetic coding, it is necessary to decode the whole text from the very beginning until the desired word is reached. This characteristic makes arithmetic coding inadequate for use in an IR environment. Dictionary methods substitute a sequence of symbols by a pointer to a previous occurrence of that sequence. The pointer representations are references to entries in a dictionary composed of a list of symbols (often called phrases) that are expected to occur frequently.   Pointers to the dictionary entries are 176        TEXT OPERATIONS chosen so that they need less space than the phrase they replace, thus obtaining compression. The distinction between modeling and coding does not exist in dictionary methods and there are no explicit probabilities associated to phrases. The most well known dictionary methods are represented by a family of methods, known as the Ziv-Lempel family. Character-based Huffman methods are typically able to compress English texts to approximately five bits per character (usually, each uncompressed character takes 7-8 bits to be represented). More recently, a word-based Huffman method has been proposed as a better alternative for natural language texts. This method is able to reduce English texts to just over two bits per character. As we will see later on, word-based Huffman coding achieves compression rates close to the entropy and allows random access to intermediate points in the compressed text. Ziv-Lempel methods are able to reduce English texts to fewer than four bits per character. Methods based on arithmetic coding can also compress English texts to just over two bits per character. However, the price paid is slower compression and decompression, and the impossibility of randomly accessing intermediate points in the compressed text. Before proceeding, let us present an important definition which will be useful from now on. Definition Compression ratio is the size of the compressed file as a fraction of the uncompressed file.
mir-0128	7.4.3    Statistical Methods In a statistical method, a probability is estimated for each symbol (the modeling task) and, based on this probability, a code is assigned to each symbol at a time (the coding task). Shorter codes are assigned to the most likely symbols. The relationship between probabilities and codes was established by Claude Shannon in his source code theorem [718]. He showed that, in an optimal encoding scheme, a symbol that is expected to occur with probability p should be assigned a code of length iog2 ^ bits. The number of bits in which a symbol is best coded represents the information content of the symbol. The average amount of information per symbol over the whole alphabet is called the entropy of the probability distribution, and is given by: 1 2 Pi E is a lower bound on compression , measured in bits per symbol, which applies to any coding method based on the probability distribution pt. It is important to note that E is calculated from the probabilities and so is a property of the model. See Chapter 6 for more details on this topic. TEXT COMPRESSION        177 Modeling The basic function of a model is to provide a probability assignment for the next symbol to be coded. High compression can be obtained by forming good models of the text that is to be coded. The probability assignment is explained in the following section. Compression models can be adaptive, static, or semi-static. Adaptive models start with no information about the text and progressively learn about its statistical distribution as the compression process goes on. Thus, adaptive models need only one pass over the text and store no additional information apart from the compressed text. For long enough texts, such models converge to the true statistical distribution of the text. One major disadvantage, however, is that decompression of a file has to start from its beginning, since information on the distribution of the data is stored incrementally inside the file. Adaptive modeling is a good option for general purpose compression programs, but an inadequate alternative for full-text retrieval where random access to compressed patterns is a must. Static models assume an average distribution for all input texts. The modeling phase is done only once for all texts to be coded in the future (i.e., somehow a probability distribution is estimated and then used for all texts to be compressed in the future). These models tend to achieve poor compression ratios when the data deviates from initial statistical assumptions. For example, a model adequate for English literary texts will probably perform poorly for financial texts containing a lot of different numbers, as each number is relatively rare and so receives long codes. Semi-static models do not assume any distribution on the data, but learn it in a first pass . In a second pass, they compress the data by using a fixed code derived from the distribution learned from the first pass. At decoding time, information on the data distribution is sent to the decoder before transmitting the encoded symbols. The disadvantages of semi-static models are that they must make two passes over the text and that information on the data distribution must be stored to be used by the decoder to decompress. In situations where interactive data communications are involved it may be impractical to make two passes over the text. However, semi-static models have a crucial advantage in IR contexts: since the same codes are used at every point in the compressed file, direct access is possible. Word-based models take words instead of characters as symbols. Usually, a word is a contiguous string of characters in the set {A..Z, a..z} separated by other characters not in the set {A..Z, a..z}. There are many good reasons to use word-based models in an IR context. First, much better compression rates are achieved by taking words as symbols because words carry a lot of meaning in natural languages and, as a result, their distribution is much more related to the semantic structure of the text than the individual letters. Second, words are the atoms on which most information retrieval systems are built. Words are already stored for indexing purposes and so might be used as part of the model for compression. Third, the word frequencies are also useful in answering queries involving combinations of words because the best strategy is to start with the 178        TEXT OPERATIONS least frequent words first. Since the text is not only composed of words but also of separators, a model must also be chosen for them. There are many different ways to deal with separators. As words and separators always follow one another, two different alphabets are usually used: one for words and one for separators. Consider the following example: each rose, a rose is a rose. In the word-based model, the set of symbols of the alphabet is {a, each, is, rose}, whose frequencies are 2, 1, 1, and 3, respectively, and the set of separators is {',LJ\ U}, whose frequencies are 1 and 5, respectively (where U represents a space). Once it is known that the text starts with a word or a separator, there is confusion about which alphabet to use. In natural language texts, a word is followed by a single space in most cases. In the texts of the TREC-3 collection [342] (see Chapter 3), 70-80% of the separators are single spaces. Another good alternative is to consider the single space that follows a word as part of the same word. That is, if a word is followed by a space, we can encode just the word. If not, we can encode the word and then the following separator. At decoding time, we decode a word and assume that a space follows unless the next symbol corresponds to a separator. Notice that now a single alphabet for words and separators (single space excluded) is used. For instance, in the example above, the single alphabet is {',LJ', a, each, is, rose} and there is no longer an alphabet for separators. As the alphabet excludes the single space then the words are called spaceless words. In some situations word-based models for full-text databases have a potential to generate a great quantity of different codes and care must be exercised to deal with this fact. For instance, as discussed in the section on lexical analysis (at the beginning of this chapter), one has to consider whether a sequence of digits is to be considered as a word. If it is, then a collection which contains one million documents and includes document numbers as identifiers will generate one million words composed solely of digits, each one occurring once in the collection. This can be very inefficient for any kind of compression method available. One possible good solution is to divide long numbers into shorter ones by using a null (or implicit) punctuation marker in between. This diminishes the alphabet size resulting in considerable improvements in the compression ratio and in the decoding time. Another important consideration is the size of the alphabet in word-based schemes. How large is the number of different words in a full-text database? It is empirically known that the vocabulary V of natural language texts with n words grows sublinearly. Heaps [352] shows that V = O(nd), where 0 is a constant dependent on the particular text. For the 2 gigabyte TREC-3 collection [342], p is between 0.4 and 0.6 which means that the alphabet size grows roughly proportional to the square root of n. Even for this growth of the alphabet, the generalized Zipf law shows that the probability distribution is skewed so that the entropy remains constant. This implies that the compression ratio does not degrade as the text (and hence the number of different symbols) grows. Heaps' and Zipfs' laws are explained in Chapter 6. Finally, it is important to mention that word-based Huffman methods need large texts to be effective (i.e., they are not adequate to compress and transmit TEXT COMPRESSION        179 a single Web page over a network). The need to store the vocabulary represents an important space overhead when the text is small (say, less than 10 megabytes). However, this is not a concern in IR in general as the texts are large and the vocabulary is needed anyway for other purposes such as indexing and querying. Coding Coding corresponds to the task of obtaining the representation (code) of a symbol based on a probability distribution given by a model. The main goal of a coder is to assign short codes to likely symbols and long codes to unlikely ones. As we have seen in the previous section, the entropy of a probability distribution is a lower bound on how short the average length of a code can be, and the quality of a coder is measured in terms of how close to the entropy it is able to get. Another important consideration is the speed of both the coder and the decoder. Sometimes it is necessary to sacrifice the compression ratio to reduce the time to encode and decode the text. A semi-static Huffman compression method works in two passes over the text. In a first pass, the modeler determines the probability distribution of the symbols and builds a coding tree according to this distribution. In a second pass, each next symbol is encoded according to the coding tree. Adaptive Huffman compression methods, instead, work in one single pass over the text updating the coding tree incrementally. The encoding of the symbols in the input text is also done during this single pass over the text. The main problem of adaptive Huffman methods is the cost of updating the coding tree as new symbols are read. As with Huffman-based methods, arithmetic coding methods can also be based on static, semi-static or adaptive algorithms. The main strength of arithmetic coding methods is that they can generate codes which are arbitrarily close to the entropy for any kind of probability distribution. Another strength of arithmetic coding methods is that they do not need to store a coding tree explicitly. For adaptive algorithms, this implies that arithmetic coding uses less memory than Huffman-based coding. For static or semi-static algorithms, the use of canonical Huffman codes overcomes this memory problem (canonical Huffman trees are explained later on). In arithmetic coding, the input text is represented by an interval of real numbers between 0 and 1. As the size of the input becomes larger, the interval becomes smaller and the number of bits needed to specify this interval increases. Compression is achieved because input symbols with higher probabilities reduce the interval less than symbols with smaller probabilities and hence add fewer bits to the output code. Arithmetic coding presents many disadvantages over Huffman coding in an IR environment. First, arithmetic coding is much slower than Huffman coding, especially with static and semi-static algorithms. Second, with arithmetic coding, decompression cannot start in the middle of a compressed file. This contrasts with Huffman coding, in which it is possible to index and to decode from 180        TEXT OPERATIONS any position in the compressed text if static or semi-static algorithms are used. Third, word-based Huffman coding methods yield compression ratios as good as arithmetic coding ones. Consequently, Huffman coding is the method of choice in full-text retrieval, where both speed and random access are important. Thus, we will focus the remaining of our discussion on semi-static word-based Huffman coding. Huffman Coding Huffman coding is one of the best known compression methods [386]. The idea is to assign a variable-length encoding in bits to each symbol and encode each symbol in turn. Compression is achieved by assigning shorter codes to more frequent symbols. Decompression uniqueness is guaranteed because no code is a prefix of another. A word-based semi-static model and Huffman coding form a good compression method for text. Figure 7.2 presents an example of compression using Huffman coding on words. In this example the set of symbols of the alphabet is {l, LJ\ a, each, for, is, rose}, whose frequencies are 1, 2, 1, 1, 1, and 3, respectively. In this case the alphabet is unique for words and separators. Notice that the separator *U' is not part of the alphabet because the single space that follows a word is considered as part of the word. These words are called spaceless words (see more about spaceless words in Section 7.4.3). The Huffman tree shown in Figure 7.2 is an example of a binary trie built on binary codes. Tries are explained in Chapter 8. Decompression is accomplished as follows. The stream of bits in the compressed file is traversed from left to right. The sequence of bits read is used to also traverse the Huffman compression tree, starting at the root. Whenever a leaf node is reached, the corresponding word (which constitutes the decompressed symbol) is printed out and the tree traversal is restarted. Thus, according to the tree in Figure 7.2, the presence of the code 0110 in the compressed file leads to the decompressed symbol for. To build a Huffman tree, it is first necessary to obtain the symbols that constitute the alphabet and their probability distribution in the text to be compressed. The algorithm for building the tree then operates bottom up and starts Original text:           for each, rose,  a rose Is a rose Compressed text:     0110 0100 1 0101 00 1 0111 00 1 Figure 7.2    Huffman coding tree for spaceless words. TEXT COMPRESSION 181 by creating for each symbol of the alphabet a node containing the symbol and its probability (or frequency). At this point there is a forest of one-node trees whose probabilities sum up to 1. Next, the two nodes with the smallest probabilities become children of a newly created parent node. With this parent node is associated a probability equal to the sum of the probabilities of the two chosen children. The operation is repeated ignoring nodes that are already children, until there is only one node, which becomes the root of the decoding tree. By delaying the pairing of nodes with high probabilities, the algorithm necessarily places them closer to the root node, making their code smaller. The two branches from every internal node are consistently labeled 0 and 1 (or 1 and 0). Given s symbols and their frequencies in the text, the algorithm builds the Huffman tree in O(slogs) time. The number of Huffman trees which can be built for a given probability distribution is quite large. This happens because interchanging left and right subtrees of any internal node results in a different tree whenever the two subtrees are different in structure, but the weighted average code length is not affected. Instead of using any kind of tree, the preferred choice for most applications is to adopt a canonical tree which imposes a particular order to the coding bits. A Huffman tree is canonical when the height of the left subtree of any node is never smaller than that of the right subtree, and all leaves are in increasing order of probabilities from left to right. Figure 7.3 shows the canonical tree for the example of Figure 7.2. The deepest leaf at the leftmost position of the Huffman canonical tree, corresponding to one element with smallest probability, will contain only zeros, and the following codes will be in increasing order inside each level. At each change of level we shift left one bit in the counting. The table in Figure 7.3 shows the canonical codes for the example of Figure 7.2. A canonical code can be represented by an ordered sequence S of pairs (xl,yl), 1 lt; i lt; t, where xi represents the number of symbols at level ?\ y% represents the numerical value of the first code at level i, and £ is the height of the tree. For our example in Figure 7.3, the ordered sequence is S = ((1,1),( 1,1),((), oo), (4,0)). For instance, the fourth pair (4,0) in 5 corresponds to the fourth level and indicates that there are four nodes at this level and that to the node most to the left is assigned a code, at this level, with value 0. Since this is the fourth level, a value 0 corresponds to the codeword 0000. Symbol	Prob.	Old	Can. code	code each	1/9	0100	0000 ,u	1/9	0101	0001 for	1/9	0110	0010 is	1/9	0111	0011 a	2/9	00	01 rose	3/9	1	1 Figure 7.3    Canonical code. 182        TEXT OPERATIONS (a)  Non-optimal tree L 254 empty nodes 256 elements                          256 elements (b)  Optimal byte tree 254 elements 256 elements   2 elements    254 empty nodes Figure 7.4   Example of byte Huffman tree. One of the properties of canonical codes is that the set of codes having the same length are the binary representations of consecutive integers. Interpreted as integers, the 4-bit codes of the table in Figure 7.3 are 0, 1, 2, and 3, the 2-bit code is 1 and the 1-bit code is also 1. In our example, if the first character read from the input stream is 1, a codeword has been identified and the corresponding symbol can be output. If this value is 0, a second bit is appended and the two bits are again interpreted as an integer and used to index the table and identify the corresponding symbol Once we read C00! we know that the code has four bits and therefore we can read two more bits and use them as an index into the table. This fact can be exploited to enable efficient encoding and decoding with small overhead. Moreover, much less memory is required, which is especially important for large vocabularies. Byte-Oriented Huffman Code The original method proposed by Huffman [386] leads naturally to binary coding trees. In [577], however, it is proposed to build the code assigned to each symbol as a sequence of whole bytes. As a result, the Huffman tree has degree 256 instead of 2. Typically, the code assigned to each symbol contains between 1 and o bytes. For example, a possible code for the word rose could be the 3-byte code k47 131 8/ The construct ion of byte Huffman trees involves some details which must be dealt with. Care must be exercised to ensure that the first levels of the tree have no empty nodes when the code is not binary. Figure 7.4(a) illustrates a ease where a naive extension of the binary Huffman tree construction algorithm might generate a non-optimal byte tree. In this example the alphabet has 512 symbols, all with tiie same probability. The root node has 254 empty spaces that could be occupied by symbols from the second level of the tree, changing their code lengths from 2 bytes to 1 byte. A way to ensure that the empty nodes always go to the lowest level of the tree follows. We calculate beforehand the number of empty nodes that will arise. TEXT COMPRESSION        183 We then compose these empty nodes with symbols of smallest probabilities (for moving the empty nodes to the deepest level of the final tree). To accomplish this, we need only to select a number of symbols equal to 1 + ((v - 256) mod 255), where v is the total number of symbols (i.e., the size of the vocabulary), for composing with the empty nodes. For instance, in the example in Figure 7.4(a), we have that 2 elements must be coupled with 254 empty nodes in the first step (because, 1 -f ((512 - 256) mod 255) = 2). The remaining steps are similar to the binary Huffman tree construction algorithm. All techniques for efficient encoding and decoding mentioned previously can easily be extended to handle word-based byte Huffman coding. Moreover, no significant decrease of the compression ratio is experienced by using bytes instead of bits when the symbols are words. Further, decompression of byte Huffman code is faster than decompression of binary Huffman code. In fact, compression and decompression are very fast and compression ratios achieved are better than those of the Ziv-Lempel family [848, 849]. In practice byte processing is much faster than bit processing because bit shifts and masking operations are not necessary at decoding time or at searching time. One important consequence of using byte Huffman coding is the possibility of performing direct searching on compressed text. The searching algorithm is explained in Chapter 8. The exact search can be done on the compressed text directly, using any known sequential pattern matching algorithm. Moreover, it allows a large number of variations of the exact and approximate compressed pattern matching problem, such as phrases, ranges, complements, wild cards, and arbitrary regular expressions. The algorithm is based on a word-oriented shift-or algorithm and on a fast Boyer-Moore-type filter. For approximate searching on the compressed text it is eight times faster than an equivalent approximate searching on the uncompressed text, thanks to the use of the vocabulary by the algorithm [577, 576]. This technique is not only useful in speeding up sequential search. It can also be used to improve indexed schemes that combine inverted files and sequential search, like Glimpse [540].
mir-0129	7.4.4    Dictionary Methods Dictionary methods achieve compression by replacing groups of consecutive symbols (or phrases) with a pointer to an entry in a dictionary. Thus, the central decision in the design of a dictionary method is the selection of entries in the dictionary. The choice of phrases can be made by static, semi-adaptive, or adaptive algorithms. The simplest dictionary schemes use static dictionaries containing short phrases. Static dictionary encoders are fast as they demand little effort for achieving a small amount of compression. One example that has been proposed several times in different forms is the digram coding, where selected pairs of letters are replaced with codewords. At each step the next two characters are inspected and verified if they correspond to a digram in the dictionary. If so, they are coded together and the coding position is shifted by two characters: otherwise, the single character is represented by its normal code and the coding 184        TEXT OPERATIONS position is shifted by one character. The main problem with static dictionary encoders is that the dictionary might be suitable for one text and unsuitable for another. One way to avoid this problem is to use a semi-static dictionary scheme, constructing a new dictionary for each text to be compressed. However, the problem of deciding which phrases should be put in the dictionary is not an easy task at all. One elegant solution to this problem is to use an adaptive dictionary scheme, such as the one proposed in the 1970s by Ziv and Lempel. The Ziv-Lempel type of adaptive dictionary scheme uses the idea of replacing strings of characters with a reference to a previous occurrence of the string. This approach is effective because most characters can be coded as part of a string that has occurred earlier in the text. If the pointer to an earlier occurrence of a string is stored in fewer bits than the string it replaces then compression is achieved. Adaptive dictionary methods present some disadvantages over the statistical word-based Huffman method. First, they do not allow decoding to start in the middle of a compressed file. As a consequence direct access to a position in the compressed text is not possible, unless the entire text is decoded from the beginning until the desired position is reached. Second, dictionary schemes are still popular for their speed and economy of memory, but the new results in statistical methods make them the method of choice in an IR environment. Moreover, the improvement of computing technology will soon make statistical methods feasible for general use, and the interest in dictionary methods will eventually decrease.
mir-0130	7.4.5    Inverted File Compression As already discussed, an inverted file is typically composed of (a) a vector containing all the distinct words in the text collection (which is called the vocabulary) and (b) for each word in the vocabulary, a list of all documents in which that word occurs. Inverted files are widely used to index large text files. The size of an inverted file can be reduced by compressing the inverted lists. Because the list of document numbers within the inverted list is in ascending order, it can also be considered as a sequence of gaps between document numbers. Since processing is usually done sequentially starting from the beginning of the list, the original document numbers can always be recomputed through sums of the gaps. By observing that these gaps are small for frequent words and large for infrequent words, compression can be obtained by encoding small values with shorter codes. One possible coding scheme for this case is the unary cade, in which an integer x is coded as (x - 1) one bits followed by a zero bit, so the code for the Integer 3 is 110. The second column of Table 7.1 shows unary codes for integers between 1 and 10. Elias [235] presented two other variable-length coding schemes for integers. One is Elias-^ code, which represents the number j by a concatenation of two TEXT COMPRESSION 185 Gap x	Unary	Elias-7	Elias-lt;5	Golomb 6=3 1	0	0	0	00 2	10	100	1000	010 3	110	101	1001	Oil 4	1110	11000	10100	100 5	11110	11001	10101	1010 6	111110	11010	10110	1011 7	1111110	11011	10111	1100 8	11111110	1110000	11000000	11010 9	111111110	1110001	11000001	11011 10	1111111110	1110010	11000010	11100 Table 7.1    Example codes for integers. parts: (1) a unary code for 1+ LlogxJ and (2) a code of [log reJ bits that represents the value of x - 2Llos*J in binary. For x = 5, we have that 1 -h [logxj = 3 and that x - 2LlosxJ = 1. Thus, the Elias-7 code for x = 5 is generated by combining the unary code for 3 (code 110) with the 2-bits binary number for 1 (code 01) which yields the codeword 11001. Other examples of Elias-7 codes are shown in Table 7.1. The other coding scheme introduced by Elias is the Elias-5 code, which represents the prefix indicating the number of binary bits by the Elias-7 code rather than the unary code. For x = 5, the first part is then 101 instead of 110. Thus, the Elias-5 codeword for x = 5 is 10101. In general, the Elias-5 code for an arbitrary integer x requires 1 -f 2[loglog2xJ + [log xj bits. Table 7.1 shows other examples of Elias~lt;$ codes. In general, for small values of x the Elias-7 codes are shorter than the Elias-£ codes. However, in the limit, as x becomes large, the situation is reversed. Golomb [307] presented another run-length coding method for positive integers. The Golomb code is very effective when the probability distribution is geometric. With inverted files, the likelihood of a gap being of size x can be computed as the probability of having x - 1 non-occurrences (within consecutively numbered documents) of that particular word followed by one occurrence. If a word occurs within a document with a probability p, the probability of a gap of size x is then Pr[x\={l-pf~lp which is the geometric distribution. In this case, the model is parameterized and makes use of the actual density of pointers in the inverted file. Let N be the number of documents in the system and V be the size of the vocabulary. Then, the probability p that any randomly selected document contains any randomly 186        TEXT OPERATIONS chosen term can be estimated as number of pointers P==            N x V where the number of pointers represent the 'size' of the index. The Golomb method works as follows. For some parameter b, a gap x gt; 0 is coded as q -h 1 in unary, where q = [(x ó 1)/6J, followed by r = (x ó 1) ó q x b coded in binary, requiring either [logb] or ("log b] bits. That is, if r lt; 2 Llos £gt;j ó i then the number coded in binary requires [logfrj bits, otherwise it requires flog b] bits where the first bit is 1 and the remaining bits assume the value r ó 2'-los^~1 coded in [logb\ binary digits. For example, with b = 3 there are three possible remainders, and those are coded as 0, 10, and 11, for r = 0, r = 1, and r = 2, respectively. Similarly, for b = 5 there are five possible remainders r, 0 through 4, and these are assigned codes 00, 01, 100, 101, and 110. Then, if the value x = 9 is to be coded relative to 6 = 3, calculation yields q = 2 and r == 2, because 9-1 = 2 x 3 + 2. Thus, the encoding is 110 followed by 11. Relative to b = 5, the values calculated are q = 1 and r = 1, resulting in a code of 10 followed by 101. To operate with the Golomb compression method, it is first necessary to establish the parameter b for each term. For gap compression, an appropriate value is b ´ 0.69(Ar//t)^ where N is the total number of documents and ft is the number of documents that contain term t. Witten, Moffat and Bell [825] present a detailed study of different text collections. For all of their practical work on compression of inverted lists, they use Golomb code for the list of gaps. In this case Golomb code gives better compression than either Elias-7 or Elias-£. However, it has the disadvantage of requiring two passes to be generated, since it requires knowledge of ft, the number of documents containing term t. Moffat and Bell [572] show that the index for the 2 gigabytes TREC-3 collection, which contains 162,187,989 pointers and 894,406 distinct terms, when coded with Golomb code, occupies 132 megabytes. Considering the average number of bits per pointer, they obtained 5.73, 6.19, and 6.43 using Golomb, Elias-$, and Elias-7, respectively.
mir-0131	7.5    Comparing Text Compression Techniques Table 7.2 presents a comparison between arithmetic coding, character-based Huffman coding, word-based Huffman coding, and Ziv-Lempel coding, considering the aspects of compression ratio, compression speed, decompression speed, memory space overhead, compressed pattern matching capability, and random access capability. One important objective of any compression method is to be able to obtain good compression ratios. It seems that two bits per character (or 25% compression ratio) is a very good result for natural language texts. Thus, 'very good" in the context of Table 7.2 means a compression ratio under 30%-, 'good' means a compression ratio between 30% and 45%, and 'poor' means a compression ratio over 45%. COMPARING TEXT COMPRESSION TECHNIQUES         187 Character	Word Arithmetic	Huffman	Huffman	Ziv-Lempel Compression ratio	very good	poor	very good	good Compression speed	slow	fast	fast	very fast Decompression speed	slow	fast	very fast	very fast Memory space	low	low	high	moderate Compressed pat. matching	no	yes	yes	yes Random access	no	yes	yes	no Table 7.2    Comparison of the main techniques. Two other important characteristics of a compression method are compression and decompression speeds. Measuring the speed of various compression methods is difficult because it depends on the implementation details of each method, the compiler used, the computer architecture of the machine used to run the program, and so on. Considering compression speed, the LZ78 methods (Unix compress is an example) are among the fastest. Considering decompression speed, the LZ77 methods (gzip is an example) from the Ziv-Lempel are among the fastest. For statistical methods (e.g., arithmetic and semi-static Huffman) the compression time includes the cost of the first pass during which the probability distribution of the symbols are obtained. With two passes over the text to compress, the Huffman-based methods are slower than some Ziv-Lempel methods, but not very far behind. On the other hand, arithmetic methods are slower than Huffman methods because of the complexity of arithmetic coding compared with canonical Huffman coding. Considering decompression speed, word-based Huffman methods are as fast as Ziv-Lempel methods, while character-based Huffman methods are slower than word-based Huffman methods. Again, the complexity of arithmetic coding make them slower than Huffman coding during decompression. All Ziv-Lempel compression methods require a moderate amount of memory during encoding and decoding to store tables containing previously occurring strings. In general, more detailed tables that require more memory for storage yield better compression. Statistical methods store the probability distribution of the symbols of the text during the modeling phase, and the model during both compression and decompression phases. Consequently, the amount of memory depends on the size of the vocabulary of the text in each case, which is high for word-based models and low for character-based models. In an IR environment, two important considerations are whether the compression method allows efficient random access and direct searching on compressed text (or compressed pattern matching). Huffman methods allow random access and decompression can start anywhere in the middle of a compressed file, while arithmetic coding and Ziv-Lempel methods cannot. More recently, practical, efficient, and flexible direct searching methods on compressed texts have been discovered for word-based Huffman compression [575. 576, 577]. 188        TEXT OPERATIONS Direct searching has also been proposed for Ziv-Lempel methods, but only on a theoretical basis, with no implementation of the algorithms [250, 19]. More recently, Navarro and Raffinot [592] presented some preliminary implementations of algorithms to search directly Ziv-Lempel compressed text. Their algorithms are twice as fast as decompressing and searching, but slower than searching the decompressed text. They are also able to extract data from the middle of the compressed text without necessarily decompressing everything, and although some previous text has to be decompressed (i.e., it is not really 'direct access'), the amount of work is proportional to the size of the text to be decompressed (and not to its position in the compressed text).
mir-0132	7.6    Trends and Research issues In this chapter we covered various text transformation techniques which we call simply text operations. We first discussed five distinct text operations for preprocessing a document text and generating a set of index terms for searching and querying purposes. These five text operations were here called lexical analysis, elimination of stopwords, stemming, selection of index terms, and thesauri. The first four are directly related to the generation of a good set of index terms. The fifth, construction of a thesaurus, is more related to the building of categorization hierarchies which are used for capturing term relationships. These relationships can then be used for expanding the user query (manually or automatically) towards a formulation which better suits the user information need. Nowadays, there is controversy regarding the potential improvements to retrieval performance generated by stopwords elimination, stemming, and index terms selection. In fact, there is no conclusive evidence that such text operations yield consistent improvements in retrieval performance. As a result, modern retrieval systems might not use these text operations at all. A good example of this trend is the fact that some Web search engines index all the words in the text regardless of their syntactic nature or their role in the text. Furthermore, it is also not clear that automatic query expansion using thesaurus-based techniques can yield improved retrieval performance. The same cannot be said of the use of a thesaurus to directly assist the user with the query formation process. In fact, the success of the k Yahoo!' Web search engine, which uses a term categorization hierarchy to show term relationships to the user, is an indication that thesaurus-based techniques might be quite useful with the highly interactive interfaces being developed for modern digital library systems. We also briefly discussed the operation of clustering. Since clustering is more an operation of grouping documents than an operation of text transformation, we did not cover it thoroughly here. For a more complete coverage of clustering the reader is referred to Chapter 5. One text operation rather distinct from the previous ones is compression. While the previous text operations aim, in one form or another, at improving the quality of the answer set, the operation of compressing text aims at reducing space. I/O, communication costs, and searching faster in the compressed text (exactly or approximately). In fact, the gain obtained from compressing text is BIBLIOGRAPHIC DISCUSSION        189 that it requires less storage space, takes less time to be transmitted, and permits efficient direct and sequential access to compressed text. For effective operation in an IR environment, a compression method should satisfy the following requirements: good compression ratio, fast coding, fast decoding, fast random access without the need to decode from the beginning, and direct searching without the need to decompress the compressed text. A good compression ratio saves space in secondary storage and reduces communication costs. Fast coding reduces processing overhead due to the introduction of compression into the system. Sometimes, fast decoding is more important than fast coding, as in documentation systems in which a document is compressed once and decompressed many times from disk. Fast random access allows efficient processing of multiple queries submitted by the users of the information system. We compared various compression schemes using these requirements as parameters. We have seen that it is much faster to search sequentially a text compressed by a word-based byte Huffman encoding scheme than to search the uncompressed version of the text. Our discussion suggests that word-based byte Huffman compression (which has been introduced only very recently) shows great promise as an effective compression scheme for modern information retrieval systems. We also discussed the application of compression to index structures such as inverted files. Inverted files are composed of several inverted lists which are themselves formed by document numbers organized in ascending order. By coding the difference between these document numbers, efficient compression can be attained. The main trends in text compression today are the use of semi-static word-based modeling and Huffman coding. The new results in statistical methods, such as byte-Huffman coding, suggest that they are preferable methods for use in an IR environment. Further, with the possibility now of directly searching the compressed text, and the recent work [790] of Vo and Moffat on efficient manipulation of compressed indices, the trend is towards maintaining both the index and the text compressed at all times, unless the user wants to visualize the uncompressed text.
mir-0133	7.7    Bibliographic Discussion Our discussion on lexical analysis and elimination of stopwords is based on the work of Fox [263]. For stemming, we based our discussion on the work of Frakes [274]. The Porter stemming algorithm detailed in the appendix is from [648], while our coverage of thesauri is based on the work of Foskett [261], Here, however, we did not cover automatic generation of thesauri. Such discussion can be found in Chapter 5 and in [739, 735]. Additional discussion on the usefulness of thesauri is presented in [419, 735]. Regarding text compression, several books are available. Most of the topics discussed here are covered in more detail by Witten, Moffat and Bell [825]. They also present implementations of text compression methods, such as Huffman and arithmetic coding, as part of a fully operational retrieval system written in ANSI 190        TEXT OPERATIONS C. Bell, Cleary and Witten [78] cover statistical and dictionary methods, laying particular stress on adaptive methods as well as theoretical aspects of compression, with estimates on the entropy of several natural languages. Storer [747] covers the main compression techniques, with emphasis on dictionary methods. Huffman coding was originally presented in [386]. Adaptive versions of Huffman coding appear in [291, 446, 789]. Word-based compression is considered in [81, 571, 377, 77]. Bounds on the inefficiency of Huffman coding have been presented by [291]. Canonical codes were first presented in [713]. Many properties of the canonical codes are mentioned in [374]. Byte Huffman coding was proposed in [577]. Sequential searching on byte Huffman compressed text is described in [577, 576]. Sequential searching on Ziv-Lempel compressed data is presented in [250, 19]. More recently, implementations of sequential searching on Ziv-Lempel compressed text are presented in [593]. One of the first papers on arithmetic coding is in [675]. Other references are [823, 78]. A variety of compression methods for inverted lists are studied in [573]. The most effective compression methods for inverted lists are based on the sequence of gaps between document numbers, as considered in [77] and in [572]. Their results are based on run-length encodings proposed by Elias [235] and Golomb [307]. A comprehensive study of inverted file compression can be found in [825]. More recently Vo and Moffat [790] have presented algorithms to process the index with no need to fully decode the compressed index.
mir-0135	8.1     Introduction Chapter 4 describes the query operations that can be performed on text databases. In this chapter we cover the main techniques we need to implement those query operations. We first concentrate on searching queries composed of words and on reporting the documents where they are found. The number of occurrences of a query in each document and even its exact positions in the text may also be required. Following that, we concentrate on algorithms dealing with Boolean operations. We then consider sequential search algorithms and pattern matching. Finally, we consider structured text and compression techniques. An obvious option in searching for a basic query is to scan the text sequentially. Sequential or online text searching involves finding the occurrences of a pattern in a text when the text is not preprocessed. Online searching is appropriate when the text is small (i.e., a few megabytes), and it is the only choice if the text collection is very volatile (i.e., undergoes modifications very frequently) or the index space overhead cannot be afforded. A second option is to build data structures over the text (called indices) to speed up the search. It is worthwhile building and maintaining an index when the text collection is large and semi-static. Semi-static collections can be updated at reasonably regular intervals (e.g., daily) but they are not deemed to support thousands of insertions of single words per second, say. This is the case for most real text databases, not only dictionaries or other slow growing literary works. For instance, it is the case for Web search engines or journal archives. Nowadays, the most successful techniques for medium size databases (say up to 200Mb) combine online and indexed searching. We cover three main indexing techniques: inverted files, suffix arrays, and signature files. Keyword-based search is discussed first. We emphasize inverted files, which are currently the best choice for most applications.    Suffix trees 191 192        INDEXING AND SEARCHING and arrays are faster for phrase searches and other less common queries, but are harder to build and maintain. Finally, signature files were popular in the 1980s, but nowadays inverted files outperform them. For all the structures we pay attention not only to their search cost and space overhead, but also to the cost of building and updating them. We assume that the reader is familiar with basic data structures, such as sorted arrays, binary search trees, B-trees, hash tables, and tries. Since tries are heavily used we give a brief and simplified reminder here. Tries, or digital search trees, are multiway trees that store sets of strings and are able to retrieve any string in time proportional to its length (independent of the number of strings stored). A special character is added to the end of the string to ensure that no string is a prefix of another. Every edge of the tree is labeled with a letter. To search a string in a trie, one starts at the root and scans the string character-wise, descending by the appropriate edge of the trie. This continues until a leaf is found (which represents the searched string) or the appropriate edge to follow does not exist at some point (i.e., the string is not in the set). See Figure 8.3 for an example of a text and a trie built on its words. Although an index must be built prior to searching it, we present these tasks in the reverse order. We think that understanding first how a data structure is used makes it clear how it is organized, and therefore eases the understanding of the construction algorithm, which is usually more complex. Throughout this chapter we make the following assumptions. We call n the size of the text database. Whenever a pattern is searched, we assume that it is of length m, which is much smaller than n. We call M the amount of main memory available. We assume that the modifications which a text database undergoes are additions, deletions, and replacements (which are normally made by a deletion plus an addition) of pieces of text of size n1 lt; n. We give experimental measures for many algorithms to give the reader a grasp of the real times involved. To do this we use a reference architecture throughout the chapter, which is representative of the power of today's computers. We use a 32-bit Sun UltraSparc-1 of 167 MHz with 64 Mb of RAM, running Solaris. The code is written in C and compiled with all optimization options. For the text data, we use collections from TREC-2, specifically WSJ, DOE, FR, ZIFF and AP. These are described in more detail in Chapter 3.
mir-0136	8.2    Inverted Files An inverted file (or inverted index) is a word-oriented mechanism for indexing a text collection in order to speed up the searching task. The inverted file structure is composed of two elements: the vocabulary and the occurrences. The vocabulary is the set of all different words in the text. For each such word a list of all the text positions where the word appears is stored. The set of all those lists is called the 'occurrences1 (Figure 8.1 shows an example). These positions can refer to words or characters. Word positions (i.e., position i refers to the i-th word) simplify 1	6	9	11	17	19	24	28	33	40	46	INVERTED 50          55	FILES         J 60 This	is	a	text.	A	text	has	many	words.	Words	are	made    from	letters. 193 Text Vocabulary		Occurrences letters		SOmade		SO... many		28 . text		11, 19...	Inverted Index words		33, 40. . Figure 8.1 A sample text and an inverted index built on it. The words are converted to lower-case and some are not indexed. The occurrences point to character positions in the text. phrase and proximity queries, while character positions (i.e., the position i is the z-th character) facilitate direct access to the matching text positions. Some authors make the distinction between inverted files and inverted lists. In an inverted file, each element of a list points to a document or file name, while inverted lists match our definition. We prefer not to make such a distinction because, as we will see later, this is a matter of the addressing granularity, which can range from text positions to logical blocks. The space required for the vocabulary is rather small. According to Heaps' law (see Chapter 6) the vocabulary grows as O(n^), where (3 is a constant between 0 and 1 dependent on the text, being between 0.4 and 0.6 in practice. For instance, for 1 Gb of the TREC-2 collection the vocabulary has a size of only 5 Mb. This may be further reduced by stemming and other normalization techniques as described in Chapter 7. The occurrences demand much more space. Since each word appearing in the text is referenced once in that structure, the extra space is O(n). Even omitting stopwords (which is the default practice when words are indexed), in practice the space overhead of the occurrences is between 30% and 40% of the text size. To reduce space requirements, a technique called block addressing is used. The text is divided in blocks, and the occurrences point to the blocks where the word appears (instead of the exact positions). The classical indices which point to the exact occurrences are called 'full inverted indices/ By using block addressing not only can the pointers be smaller because there are fewer blocks than positions, but also all the occurrences of a word inside a single block are collapsed to one reference (see Figure 8.2). Indices of only 5% overhead over the text size are obtained with this technique. The price to pay is that, if the exact occurrence positions are required (for instance, for a proximity query), then an online search over the qualifying blocks has to be performed. For instance, block addressing indices with 256 blocks stop working well with texts of 200 Mb. Table-1 8.1 presents the projected space taken by inverted indices for texts of 194        INDEXING AND SEARCHING Block 1                                Block 2                                Block 3 Block 4 This is a text. A text has many words.  Words are made from letters Vocabulary Text Inverted Index Figure 8.2 The sample text split into four blocks, and an inverted index using block addressing built on it. The occurrences denote block numbers. Notice that both occurrences of 'words1 collapsed into one. different sizes, with and without the use of stopwords. The full inversion stands for inverting all the words and storing their exact positions, using four bytes per pointer. The document addressing index assumes that we point to documents which are of size 10 Kb (and the necessary number of bytes per pointer, i.e. one, two, and three bytes, depending on text size). The block addressing index assumes that we use 256 or 64K blocks (one or two bytes per pointer) independently of the text size. The space taken by the pointers can be significantly reduced by using compression. We assume that 45% of all the words are stop-words, and that there is one non-stopword each 11.5 characters. Our estimation for the vocabulary is based on Heaps' lawT with parameters V = 30?20-5. All these decisions were taken according to our experience and experimentally validated. The blocks can be of fixed size (imposing a logical block structure over the text database) or they can be defined using the natural division of the text collection into files, documents. Web pages, or others. The division into blocks of fixed size improves efficiency at retrieval time, i.e. the more variance in the block sizes, the more amount of text sequentially traversed on average. This is because larger blocks match queries more frequently and are more expensive to traverse. Alternatively, the division using natural cuts may eliminate the need for online traversal. For example, if one block per retrieval unit is used and the exact match positions are not required, there is no need to traverse the text for single-word queries, since it is enough to know which retrieval units to report. But if, on the other hand, many retrieval units are packed into a single block, the block has to be traversed to determine which units to retrieve. It is important to notice that in order to use block addressing, the text must be readily available at search time. This is not the case for remote text (as in Web search engines), or if the text is in a CD-ROM that has to be mounted, for instance. Some restricted queries not needing exact positions can still be solved if the blocks are retrieval units. INVERTED FILES        195 Index	Small collection (1 Mb)		Medium (200	collection Mb)	Large collection (2 Gb) Addressing words	45%	73%	36%	64%	35%	63% Addressing documents	19%	26%	18%	32%	26%	47% Addressing 64K blocks	27%	41%	18%	32%	5%	9% Addressing 256 blocks	18%	25%	1.7%	2.4%	0.5%	0.7% Table 8.1 Sizes of an inverted file as approximate percentages of the size the whole text collection. Four granularities and three collections are considered. For each collection, the right column considers that stopwords are not indexed while the left column considers that all words are indexed.
mir-0137	8.2.1    Searching The search algorithm on an inverted index follows three general steps (some may be absent for specific queries): ï  Vocabulary search The words and patterns present in the query are isolated and searched in the vocabulary. Notice that phrases and proximity queries are split into single words. ï  Retrieval of occurrences The lists of the occurrences of all the words found are retrieved. ï  Manipulation of occurrences The occurrences are processed to solve phrases, proximity, or Boolean operations.   If block addressing is used it may be necessary to directly search the text to find the information missing from the occurrences (e.g., exact word positions to form phrases). Hence, searching on an inverted index always starts in the vocabulary. Because of this it is a good idea to have it in a separate file. It is possible that this file fits in main memory even for large text collections. Single-word queries can be searched using any suitable data structure to speed up the search, such as hashing, tries, or B-trees. The first two give O(m) search cost (independent of the text size). However, simply storing the words in lexicographical order is cheaper in space and very competitive in performance, since the word can be binary searched at O(logn) cost. Prefix and range queries can also be solved with binary search, tries, or B-trees, but not with hashing. If the query is formed by single words, then the process ends by delivering the list of occurrences (we may need to make a union of many lists if the pattern mat dies many words). 196        INDEXING AND SEARCHING Context queries are more difficult to solve with inverted indices. Each element must be searched separately and a list (in increasing positional order) generated for each one. Then, the lists of all elements are traversed in synchronization to find places where all the words appear in sequence (for a phrase) or appear close enough (for proximity). If one list is much shorter than the others, it may be better to binary search its elements into the longer lists instead of performing a linear merge. It is possible to prove using Zipf s law that this is normally the case. This is important because the most time-demanding operation on inverted indices is the merging or intersection of the lists of occurrences. If the index stores character positions the phrase query cannot allow the separators to be disregarded, and the proximity has to be defined in terms of character distance. Finally, note that if block addressing is used it is necessary to traverse the blocks for these queries, since the position information is needed. It is then better to intersect the lists to obtain the blocks which contain all the searched words and then sequentially search the context query in those blocks as explained in section 8.5. Some care has to be exercised at block boundaries, since they can split a match. This part of the search, if present, is also quite time consuming. Using Heaps1 and the generalized Zipf s laws, it has been demonstrated that the cost of solving queries is sublinear in the text size, even for complex queries involving list merging. The time complexity is 0(na), where a depends on the query and is close to 0.4..0.8 for queries with reasonable selectivity. Even if block addressing is used and the blocks have to be traversed, it is possible to select the block size as an increasing function of n, so that not only does the space requirement keep sublinear but also the amount of text traversed in all useful queries is also sublinear. Practical figures show, for instance, that both the space requirement and the amount of text traversed can be close to O(n0"85). Hence, inverted indices allow us to have sublinear search time at sublinear space requirements. This is not possible on the other indices. Search times on our reference machine for a full inverted index built on 250 Mb of text give the following results: searching a simple word took 0.08 seconds, while searching a phrase took 0.25 to 0.35 seconds (from two to five words).
mir-0138	8.2.2    Construction Building and maintaining an inverted index is a relatively low cost task. In principle, an inverted index on a text of n characters can be built in O(n) time. All the vocabulary known up to now is kept in a trie data structure, storing for each word a list of its occurrences (text positions). Each word of the text is read and searched in the trie. If it is not found, it is added to the trie with an empty list of occurrences. Once it is in the trie, the new position is added to the end of its list of occurrences. Figure 8.3 illustrates this process. Once the text is exhausted, the trie is written to disk together with tiie list of occurrence.  It is good practice to split the index into two files.  In the INVERTED FILES         197 1            6       9    11               17 19          24       28          33                  40            46         50          55           60 This   is   a  text.     A  text   has  many   words.     Words   are  made    from   letters. Text Vocabulary trie Figure 8.3    Building an inverted index for the sample text. first file, the lists of occurrences are stored contiguously. In this scheme, the file is typically called a 'posting file'. In the second file, the vocabulary is stored in lexicographical order and, for each word, a pointer to its list in the first file is also included. This allows the vocabulary to be kept in memory at search time in many cases. Further, the number of occurrences of a word can be immediately known from the vocabulary with little or no space overhead. We analyze now the construction time under this scheme. Since in the trie 0(1) operations are performed per text character, and the positions can be inserted at the end of the lists of occurrences in 0(1) time, the overall process is O(n) worst-case time. However, the above algorithm is not practical for large texts where the index does not fit in main memory. A paging mechanism will severely degrade the performance of the algorithm. We describe an alternative which is faster in practice. The algorithm already described is used until the main memory is exhausted (if the trie takes up too much space it can be replaced by a hash table or other structure). When no more memory is available, the partial index J2 obtained up to now is written to disk and erased from main memory before continuing with the rest of the text. Finally, a number of partial indices Iz exist on disk. These indices are then merged in a hierarchical fashion. Indices I\ and I2 are merged to obtain the index J1..2; /.3 and I4 produce i"3..4; and so on. The resulting partial indices are now approximately twice the size. When all the indices at this level have been merged in this way, the merging proceeds at the next level, joining the index I\ ,2 with the index J3..4 to form /1..4. This is continued until there is just one index comprising the whole text, as illustrated in Figure 8.4. Merging two indices consists of merging the sorted vocabularies, and whenever the same word appears in both indices, merging both lists of occurrences. By construction, the occurrences of the smaller-numbered index are before those of the larger-numbered index, and therefore the lists are just concatenated. This is a very fast process in practice, and its complexity is O(n\ +n2K where ii\ and fi-gt; are the sizes of the indices. 198        INDEXING AND SEARCHING I-1..8 Level 4 (final index) I-1..4		I-5..8 Level 3 Level 2 Level 1 (initial dumps) Figure 8.4 Merging the partial indices in a binary fashion. Rectangles represent partial Indices, while rounded rectangles represent merging operations. The numbers inside the merging operations show a possible merging order. The total time to generate the partial indices is O(n) as before. The number of partial indices is O(n/M). Each level of merging performs a linear process over the whole index (no matter how it is split into partial indices at this level) and thus its cost is O(n). To merge the 0{n/M) partial indices, Iog2(n/A/) merging levels are necessary, and therefore the cost of this algorithm is O(nlog(n/M)). More than two indices can be merged at once. Although this does not change the complexity, it improves efficiency since fewer merging levels exist. On the other hand, the memory buffers for each partial index to merge will be smaller and hence more disk seeks will be performed. In practice it is a good idea to merge even 20 partial indices at once. Real times to build inverted indices on the reference machine are between 4-8 Mb/min for collections of up to 1 Gb (the slowdown factor as the text grows is barely noticeable). Of this time, 20-30% is spent on merging the partial indices. To reduce build-time space requirements, it is possible to perform the merging in-place. That is, when two or more indices are merged, write the result in the same disk blocks of the original indices instead of on a new file. It is also a good idea to perform the hierarchical merging as soon as the files are generated (e.g.. collapse /) and /2 into J1gt;gt;2 as soon as I2 is produced). This also reduces space requirements because the vocabularies are merged and redundant words are eliminated (there is no redundancy in the occurrences). The vocabulary can be a significative part of the smaller partial indices, since they represent a small text. This algorithm changes very little if block addressing is used. Index maintenance is also cheap. Assume that a new text of size n* is added to the database, Tlip inverted index for the new text is built and then both indices are merged OTHER INDICES FOR TEXT         199 as is done for partial indices. This takes O(n -f n' log(n'JM)). Deleting text can be done by an O(n) pass over the index eliminating the occurrences that point inside eliminated text areas (and eliminating words if their lists of occurrences disappear in the process).
mir-0140	8.3.1    Suffix Trees and Suffix Arrays Inverted indices assume that the text can be seen as a sequence of words. This restricts somewhat the kinds of queries that can be answered. Other queries such as phrases are expensive to solve. Moreover, the concept of word does not exist in some applications such as genetic databases. In this section we present suffix arrays. Suffix arrays are a space efficient implementation of suffix trees. This type of index allows us to answer efficiently more complex queries. Its main drawbacks are its costly construction process, that the text must be readily available at query time, and that the results are not delivered in text position order. This structure can be used to index only words (without stopwords) as the inverted index as well as to index any text character. This makes it suitable for a wider spectrum of applications, such as genetic databases. However, for word-based applications, inverted files perform better unless complex queries are an important issue. This index sees the text as one long string. Each position in the text is considered as a text suffix (i.e., a string that goes from that text position to the end of the text). It is not difficult to see that two suffixes starting at different positions are lexicographically different (assume that a character smaller than all the rest is placed at the end of the text). Each suffix is thus uniquely identified by its position. Not all text positions need to be indexed. Index points are selected from the text, which point to the beginning of the text positions which will be retrievable. For instance, it is possible to index only word beginnings to have a functionality similar to inverted indices. Those elements which are not index points are not retrievable (as in an inverted index it is not possible to retrieve the middle of a word). Figure 8.5 illustrates this. Structure In essence, a suffix tree is a trie data structure built over all the suffixes of the text. The pointers to the suffixes are stored at the leaf nodes. To improve space utilization, this trie is compacted into a Patricia tree. This involves compressing unary paths, i.e. paths where each node has just one child. An indication of the next character position to consider is stored at the nodes which root a compressed path. Once unary paths are not present the tree has O(n) nodes instead of the worst-case O(n2) of the trie (see Figure 8.6). 200        INDEXING AND SEARCHING This is a text. A text has many words.  Words are made from letters. Text text. A text has many words. Words are made from letters. text has many words. Words are made from letters. many words. Words are made from letters. words. Words are made from letters. Words are made from letters. made from letters. letters. Suffixes Figure 8.5    The sample text with the index points of interest marked.   Below, the suffixes corresponding to those index points. I	6	9	11	17	19	24	28	33	40	46	50	55	60 This	is	a	text.	A	text	has	many	words.	Words	are	made	from	letters. Suffix Trie Suffix Tree Text Figure 8.6    The suffix trie and suffix tree for the sample text. The problem with this structure is its space. Depending on the implementation, each node of the trie takes 12 to 24 bytes, and therefore even if only word beginnings are indexed, a space overhead of 120% to 240% over the text size is produced. Suffix arrays provide essentially the same functionality as suffix trees with much less space requirements. If the leaves of the suffix tree are traversed in left-to-right order (top to bottom in our figures), all the suffixes of the text are retrieved in lexicographical order. A suffix array is simply an array containing all the pointers to the text suffixes listed in lexicographical order, as shown in Figure 8,7. Since they store one pointer per indexed suffix, the space requirements OTHER INDICES FOR TEXT        201 1	6	9	11	17	19	24	28	33	40	46	50	55	60 This	is	a	text.	A	text	has	many	words.	Words	are	made	from	letters. Text I 60 I 50 j 28 I 19 I 11  I 40 I 33 I    Suffix Array Figure 8.7    The suffix array for the sample text. 1	6	9	11	17	19	24	28	33	40	46	50	55	60 This	is	a	text.	A	text	has	many	words.	Words	are	made	from	letters. Text fiett      | /    1 text     | /     | word    I ,    \       Supra-lndex y      .   y      ... 60 I 50 1 28    19 1 11 I 40 1 33 1           Suffix Array Figure 8.8    A supra-index over our suffix array. One out of three entries are sampled, keeping their first four characters. The pointers (arrows) are in fact unnecessary. are almost the same as those for inverted indices (disregarding compression techniques), i.e. close to 40% overhead over the text size. Suffix arrays are designed to allow binary searches done by comparing the contents of each pointer. If the suffix array is large (the usual case), this binary search can perform poorly because of the number of random disk accesses. To remedy this situation, the use of supra-indices over the suffix array has been proposed. The simplest supra-index is no more than a sampling of one out of b suffix array entries, where for each sample the first £ suffix characters are stored in the supra-index. This supra-index is then used as a first step of the search to reduce external accesses. Figure 8.8 shows an example. This supra-index does not in fact need to take samples at fixed intervals, nor to take samples of the same length. For word-indexing suffix arrays it has been suggested that a new sample could be taken each time the first word of the suffix changes, and to store the word instead of I characters. This is exactly the same as having a vocabulary of the text plus pointers to the array. In fact, the only important difference between this structure and an inverted index is that the occurrences of each word in an inverted index are sorted by text position, while in a suffix array they are sorted lexicographically by the text following the word. Figure 8.9 illustrates this relationship. The extra space requirements of supra-indices are modest. In particular, it is clear that the space requirements of the suffix array with a vocabulary supra-index are exactly the same as for inverted indices (except for compression, as we see later). 202       INDEXING AND SEARCHING 1	6	9	11	17	19	24	28	33	40	46	50	55	60 This	is	a	text.	A	text	has	many	words.	Words	are	made	from	letters. Text letters	\	made      | I	many	/	text		words Vocabulary Supra-lndex 33  I     Suffix Array 33      40       Inverted List Figure 8.9    Relationship between our inverted list and suffix array with vocabulary supra-index. Searching If a suffix tree on the text can be afforded, many basic patterns such as words, prefixes, and phrases can be searched in O(m) time by a simple trie search. However, suffix trees are not practical for large texts, as explained. Suffix arrays, on the other hand, can perform the same search operations in O(logn) time by doing a binary search instead of a trie search. This is achieved as follows: the search pattern originates two 'limiting patterns5 Pi and P2, so that we want any suffix S such that Pi lt; S lt; iV We binary search both limiting patterns in the suffix array. Then, all the elements lying between both positions point to exactly those suffixes that start like the original pattern (i.e., to the pattern positions in the text). For instance, in our example of figure 8.9, in order to find the word 'text1 we search, for 'text' and ktexu\ obtaining the portion of the array that contains the pointers 19 and 11. Ail these queries retrieve a subtree of the suffix tree or an interval of the suffix array. The results have to be collected later, which may imply sorting them in ascending text order. This is a complication of suffix trees or arrays with respect to inverted indices. Simple phrase searching is a good case for these indices. A simple phrase of words can be searched as if it was a simple pattern. This is because the suffix tree/array sorts with respect to the complete suffixes and not only their first word. A proximity search, on the other hand, has to be solved element-wise. The matches for each element must be collected and sorted and then they have to be intersected as for inverted files. The binary search performed on suffix arrays, unfortunately, is done on disk, where the accesses to (random) text positions force a seek operation which spans the disk tracks containing the text. Since a random seek is O(n) in'size, this makes the search cost O(n logn) time. Supra-indices are used as a first step in any binary search operation to alleviate this problem. To avoid performing O(log n) random accesses to the ivxt on disk (and to the suffix array on disk), the search starts in the supra-index, which usually fits in main memory (text samples OTHER INDICES FOR TEXT        203 included). After this search is completed, the suffix array block which is between the two selected samples is brought into memory and the binary search is completed (performing random accesses to the text on disk). This reduces disk search times to close to 25% of the original time. Modified binary search techniques that sacrifice the exact partition in the middle of the array taking into account the current disk head position allow a further reduction from 40% to 60%. Search times in a 250 Mb text in our reference machine are close to 1 second for a simple word or phrase, while the part corresponding to the accesses to the text sums up 0.6 seconds. The use of supra-indices should put the total time close to 0.3 seconds. Note that the times, although high for simple words, do not degrade for long phrases as with inverted indices. Construction in Main Memory A suffix tree for a text of n characters can be built in O(n) time. The algorithm, however, performs poorly if the suffix tree does not fit in main memory, which is especially stringent because of the large space requirements of the suffix trees. We do not cover the linear algorithm here because it is quite complex and only of theoretical interest. We concentrate on direct suffix array construction. Since the suffix array is no more than the set of pointers lexicographically sorted, the pointers are collected in ascending text order and then just sorted by the text they point to. Note that in order to compare two suffix array entries the corresponding text positions must be accessed. These accesses are basically random. Hence, both the suffix array and the text must be in main memory. This algorithm costs O(n log n) string comparisons. An algorithm to build the suffix array in O(nlogn) character comparisons follows. All the suffixes are bucket-sorted in O(n) time according to the first letter only. Then, each bucket is bucket-sorted again, now according to their first two letters. At iteration i, the suffixes begin already sorted by their 22""1 first letters and end up sorted by their first 2% letters. As at each iteration the total cost of all the bucket sorts is O(n), the total time is O(nlogn), and the average is O(n log log n) (since O(logn) comparisons are necessary on average to distinguish two suffixes of a text). This algorithm accesses the text only in the first stage (bucket sort for the first letter). In order to sort the strings in the i-th iteration, notice that since all suffixes are sorted by their first 2i~"1 letters, to sort the text positions To... and IL, in the suffix array (assuming that they are in the same bucket, i.e., they share their first 2i~1 letters), it is enough to determine the relative order between text positions Ta+2*~i and T^*-1 m the current stage of the search. This can be done in constant time by storing the reverse permutation. We do not enter here into further detail. Construction of Suffix Arrays for Large Texts There is still the problem that large text databases will not fit in main memory. It could be possible to apply an external memory sorting algorithm. However, 204        INDEXING AND SEARCHING each comparison involves accessing the text at random positions on the disk. This will severely degrade the performance of the sorting process. We explain an algorithm especially designed for large texts. Split the text into blocks that can be sorted in main memory. Then, for each block, build its suffix array in main memory and merge it with the rest of the array already built for the previous text. That is: ï  build the suffix array for the first block, ï  build the suffix array for the second block, ï  merge both suffix arrays, ï  build the suffix array for the third block, ï  merge the new suffix array with the previous one, ï  build the suffix array for the fourth block, ï  merge the new suffix array with the previous one, ï  ... and so on. The difficult part is how to merge a large suffix array (already built) with the small suffix array (just built). The merge needs to compare text positions which are spread in a large text, so the problem persists. The solution is to first determine how many elements of the large array are to be placed between each pair of elements in the small array, and later use that information to merge the arrays without accessing the text. Hence, the information that we need is how many suffixes of the large text lie between each pair of positions of the small suffix array. We compute counters that store this information. The counters are computed without using the large suffix array. The text corresponding to the large array is sequentially read into main memory. Each suffix of that text is searched in the small suffix array (in main memory). Once we find the inter-element position where the suffix lies, we just increment the appropriate counter. Figure 8.10 illustrates this process. We analyze this algorithm now. If there is O(M) main memory to index, then there will be O(nfAl) text blocks. Each block is merged against an array of size O(n), where all the O(n) suffixes of the large text are binary searched in the small suffix array. This gives a total CPU complexity of O(n2 log(M)/M). Notice that this same algorithm can be used for index maintenance. If a new text of size n' is added to the database, it can be split into blocks as before and merged block-wise into the current suffix array. This will take O(nn'\og{M)/M). To delete some text it suffices to perform an O(n) pass over the array eliminating all the text positions which lie in the deleted areas. As can be seen, the construction process is in practice more costly for suffix arrays than for inverted files. The construction of the supra-index consists of a fast final sequential pass over the suffix array. Indexing times for 250 Mb of text are close to 0.8 Mb/niin on the reference machine. This is five to ten times slower than the construction of inverted indices. OTHER INDICES FOR TEXT        205 (a)  small text 1 small suffix array  (b) small text 1 small suffix array r T counters long text (c) small text small suffix array r counters long suffix array final suffix array Figure 8.10    A step of the suffix array construction for large texts: (a) the local suffix array is built, (b) the counters are computed, (c) the suffix arrays are merged.
mir-0141	8.3.2    Signature Files Signature files are word-oriented index structures based on hashing. They pose a low overhead (10% to 20% over the text size), at the cost of forcing a sequential search over the index. However, although their search complexity is linear (instead of sublinear as with the previous approaches), its constant is rather low, which makes the technique suitable for not very large texts. Nevertheless, inverted files outperform signature files for most applications. Structure A signature file uses a hash function (or 'signature') that maps words to bit masks of B bits. It divides the text in blocks of b words each. To each text block of size 6, a bit mask of size B will be assigned. This mask is obtained by bitwise ORing the signatures of all the words in the text block. Hence, the signature file is no more than the sequence of bit masks of all blocks (plus a pointer to each block). The main idea is that if a word is present in a text block, then all the bits set in its signature are also set in the bit mask of the text block. Hence, whenever a bit is set in the mask of the query word and not in the mask of the text block, then the word is not present in the text block. Figure 8.11 shows an example. However, it is possible that all the corresponding bits are set even though the word is not there. This is called a false drop. The most delicate part of the design of a signature file is to ensure that the probability of a false drop is low enough while keeping the signature file as short as possible. The hash function is forced to deliver bit masks which have at least £ bits set. A good model assumes that £ bits are randomly set in the mask (with passible repetition).   Let a = £/B.   Since each of the b words sets £ bits at 206        INDEXING AND SEARCHING Block 1                         Block 2                         Block 3 Block 4 This is a text. A text has many words.  Words are made from letters. 000101		110101		100100		101101 Text Text signature h(text) = 000101 h(many) =110000 h( words) = 100100 h(made) =001100 h(letters) = 100001 Signature function Figure 8.11    A signature file for our sample text cut into blocks. random, the probability that a given bit of the mask is set in a word signature is 1 - (1 - l/B)M ´ 1 - e~ba. Hence, the probability that the £ random bits set in the query are also set in the mask of the text block is which is minimized for a = ln(2)/6. The false drop probability under the optimal selection £ = Bln(2)/b is (i/2ln´2))5/6 = 1/2*. Hence, a reasonable proportion B/b must be determined. The space overhead of the index is approximately (1/80) x (B/b) because B is measured in bits and b in words. Then, the false drop probability is a function of the overhead to pay. For instance, a 10% overhead implies a false drop probability close to 2%, while a 20% overhead errs with probability 0.046%. This error probability corresponds to the expected amount of sequential searching to perform while checking if a match is a false drop or not. Searching Searching a single word is carried out by hashing it to a bit mask W, and then comparing the bit masks B% of all the text blocks. Whenever (W  B{ = W), where  is the bitwise AND, all the bits set in W are also set in B% and therefore the text block may contain the word. Hence, for all candidate text blocks, an online traversal must be performed to verify if the word is actually there. This traversal cannot be avoided as in inverted files (except if the risk of a false drop is accepted). No other types of patterns can be searched in this scheme. On the other hand, the scheme is more efficient to search phrases and reasonable proximity queries. This is because all the words must be present in a block in order for that block to hold the phrase or the proximity query. Hence, the bitwise OR of all the query masks is searched, so that all their bits must be present. This BOOLEAN QUERIES        207 reduces the probability of false drops. This is the only indexing scheme which improves in phrase searching. Some care has to be exercised at block boundaries, however, to avoid missing a phrase which crosses a block limit. To allow searching phrases of j words or proximities of up to j words, consecutive blocks must overlap in j words. If the blocks correspond to retrieval units, simple Boolean conjunctions involving words or phrases can also be improved by forcing all the relevant words to be in the block. We were only able to find real performance estimates from 1992, run on a Sun 3/50 with local disk. Queries on a small 2.8 Mb database took 0.42 seconds. Extrapolating to today's technology, we find that the performance should be close to 20 Mb/sec (recall that it is linear time), and hence the example of 250 Mb of text would take 12 seconds, which is quite slow. Construction The construction of a signature file is rather easy. The text is simply cut in blocks, and for each block an entry of the signature file is generated. This entry is the bitwise OR of the signatures of all the words in the block. Adding text is also easy, since it is only necessary to keep adding records to the signature file. Text deletion is carried out by deleting the appropriate bit masks. Other storage proposals exist apart from storing all the bit masks in sequence. For instance, it is possible to make a different file for each bit of the mask, i.e. one file holding all the first bits, another file for all the second bits, etc. This reduces the disk times to search for a query, since only the files corresponding to the £ bits which are set in the query have to be traversed.
mir-0142	8.4    Boolean Queries We now cover set manipulation algorithms.   These algorithms are used when operating on sets of results, which is the case in Boolean queries. Boolean queries are described in Chapter 4, where the concept of query syntax tree is defined. Once the leaves of the query syntax tree are solved (using the algorithms to find the documents containing the basic queries given), the relevant documents must be worked on by composition operators. Normally the search proceeds in three phases: the first phase determines which documents classify, the second determines the relevance of the classifying documents so as to present them appropriately to the user, and the final phase retrieves the exact positions of the matches to highlight them in those documents that the user actually wants to see. This scheme avoids doing unnecessary work on documents which will not classify at last (first phase), or will not be read at last (second phase). However, some phases can be merged if doing the extra operations is not expensive. Some phases may not be present at all in some scenarios. 208        INDEXING AND SEARCHING AND                                                        AND lt;ªgt;     /    \           /    \ 146        OR                       146      23467 (b) AND 246       237 OR 2   4         OR 2   4         OR 3   4         OR 4 3       4                     3       4                     7       6 Figure 8.12   Processing the internal nodes of the query syntax tree.    In (a) full evaluation is used. In (b) we show lazy evaluation in more detailOnce the leaves of the query syntax tree find the classifying sets of documents, these sets are further operated by the internal nodes of the tree. It is possible to algebraically optimize the tree using identities such as a OR (a AND b) = a, for instance, or sharing common subexpressions, but we do not cover this issue here. As all operations need to pair the same document in both their operands, it is good practice to keep the sets sorted, so that operations like intersection, union, etc. can proceed sequentially on both lists and also generate a sorted list. Other representations for sets not consisting of the list of matching documents (such as bit vectors) are also possible. Under this scheme, it is possible to evaluate the syntax tree in full or lazy form. In the full evaluation form, both operands are first completely obtained and then the complete result is generated. In lazy evaluation, results are delivered only when required, and to obtain that result some data is recursively required to both operands. Full evaluation allows some optimizations to be performed because the sizes of the results are known in advance (for instance, merging a very short list against a very long one can proceed by binary searching the elements of the short list in the long one). Lazy evaluation, on the other hand, allows the application to control when to do the work of obtaining new results, instead of blocking it for a long time. Hybrid schemes are possible, for example obtain all the leaves at once and then proceed in lazy form. This may be useful, for instance, to implement some optimizations or to ensure that all the accesses to the index are sequential (thus reducing disk seek times). Figure 8.12 illustrates this. The complexity of solving these types of queries, apart from the cost of obtaining the results at the leaves, is normally linear in the total size of all the intermediate results. This is why this time may dominate the others, when there are huge intermediate results. This is more noticeable to the user when the final result is small. SEQUENTIAL SEARCHING aldla|blrlal 209 |a b r a c  a d abracadabra Figure 8.13    Brute-force search algorithm for the pattern 'abracadabra.'   Squared areas show the comparisons performed.
mir-0143	8.5    Sequential Searching We now cover the algorithms for text searching when no data structure has been built on the text. As shown, this is a basic part of some indexing techniques as well as the only option in some cases. We cover exact string matching in this section. Later we cover matching of more complex patterns. Our exposition is mainly conceptual and the implementation details are not shown (see the Bibliographic Discussion at the end of this chapter for more information). The problem of exact string matching is: given a short pattern P of length m and a long text T of length n, find all the text positions where the pattern occurs. With minimal changes this problem subsumes many basic queries, such as word, prefix, suffix, and substring search. This is a classical problem for which a wealth of solutions exists. We sketch the main algorithms, and leave aside a lot of the theoretical work that is not competitive in practice. For example, we do not include the Karp-Rabin algorithm, which is a nice application of hashing to string searching, but is not practical. We also briefly cover multipattern algorithms (that search many patterns at once), since a query may have many patterns and it may be more efficient to retrieve them all at once. Finally, we also mention how to do phrases and proximity searches. We assume that the text and the pattern are sequences of characters drawn from an alphabet of size lt;r, whose first character is at position 1. The average-case analysis assumes random text and patterns.
mir-0144	8.5.1    Brute Force The brute-force (BF) algorithm is the simplest possible one. It consists of merely trying all possible pattern positions in the text. For each such position, it verifies whether the pattern matches at that position. See Figure 8.13. Since there are O(n) text positions and each one is examined at O(m) worst-case cost, the worst-case of brute-force searching is 0(mn). However, its average 210        INDEXING AND SEARCHING case is O(n) (since on random text a mismatch is found after (9(1) comparisons on average). This algorithm does not need any pattern preprocessing. Many algorithms use a modification of this scheme. There is a window of length m which is slid over the text. It is checked whether the text in the window is equal to the pattern (if it is, the window position is reported as a match). Then, the window is shifted forward. The algorithms mainly differ in the way they check and shift the window.
mir-0145	8.5.2    Knuth-Morris-Pratt The KMP algorithm was the first with linear worst-case behavior, although on average it is not much faster than BF. This algorithm also slides a window over the text, However, it does not try all window positions as BF does. Instead, it reuses information from previous checks. After the window is checked, whether it matched the pattern or not, a number of pattern letters were compared to the text window, and they all matched except possibly the last one compared. Hence, when the window has to be shifted, there is a prefix of the pattern that matched the text. The algorithm takes advantage of this information to avoid trying window positions which can be deduced not to match. The pattern is preprocessed in O(m) time and space to build a table called next The next table at position j says which is the longest proper prefix of Pi..j-i which is also a suffix and the characters following prefix and suffix are different. Hence j ó next[j] ¶+1 window positions can be safely skipped if the characters up to j ó 1 matched, and the j-th did not. For instance, when searching the word 'abracadabra/ if a text window matched up to 'abracab,' five positions can be safely skipped since next[7] = 1. Figure 8.14 shows an example. The crucial observation is that this information depends only on the pattern, because if the text in the window matched up to position j ó 1, then that text is equal to the pattern. The algorithm moves a window over the text and a pointer inside the window. Each time a character matches, the pointer is advanced (a match is reported if the pointer reaches the end of the window). Each time a character is not matched, the window is shifted forward in the text, to the position given by next but the pointer position in the text does not change. Since at each text comparison the window or the pointer advance by at least one position, the algorithm performs at most 2n comparisons (and at least n). The Aho-Corasick algorithm can be regarded as an extension of KMP in matching a set of patterns. The patterns are arranged in a trie-like data structure. Each trie node represents having matched a prefix of some pattern(s). The next function is replaced by a more general set of failure transitions. Those transitions go between nodes of the trie. A transition leaving from a node representing the prefix x leads to a node representing a prefix t/, such that y is the longest prefix in the set of patterns which is also a proper suffix of x. Figure 8.15 illustrates this. SEQUENTIAL SEARCHING        211 next=   000010100004         ,, a   b   r    a    c    a   d) a   b   r (a a b r a c a d ibracadabr Figure 8.14 KMP algorithm searching 'abracadabra.' On the left, an illustration of the next function. Notice that after matching 'abracada' we do not try to match the last 'a' with the first one since what follows cannot be a 'b.' On the right, a search example. Grayed areas show the prefix information reused. Figure 8.15    Aho-Corasick trie example for the set 'hello,' 'elbow' and 'eleven' showing only one of all the failure transitions. This trie, together with its failure transitions, is built in O(m) time and space (where m is the total length of all the patterns). Its search time is O{n) no matter how many patterns are searched. Much as KMP, it makes at most 2n inspections.
mir-0146	8.5.3    Boyer-Moore Family BM algorithms are based on the fact that the check inside the window can proceed backwards. When a match or mismatch is determined, a suffix of the pattern has been compared and found equal to the text in the window. This can be used in a way very similar to the next table of KMP, i.e. compute for every pattern position j the next-to-last occurrence of Pj..m inside P. This is called the kmatch heuristic/ This is combined with what is called the 'occurrence heuristic' It states that the text character that produced the mismatch (if a mismatch occurred) has to be aligned with the same character in the pattern after the shift. The heuristic which gives the longest shift is selected. For instance, assume that 'abracadabra" is searched in a text which starts with kabracababra/ After matching the suffix kabra" the underlined text character 'b1 will cause a mismatch. The match heuristic states that since "abra" was matched a shift of 7 is safe. The occurrence heuristic states that since the underlined *b* must match the pattern, a shift of 5 is safe. Hence, the pattern is 212        INDEXING AND SEARCHING Figure 8.16 BM algorithm searching 'abracadabra.' Squared areas show the comparisons performed. Grayed areas have already been compared (but the algorithm compares them again). The dashed box shows the match heuristic, which was not chosen. shifted by 7. See Figure 8.16. The preprocessing time and space of this algorithm is O(m + a). Its search time is O(nlog(m)/ra) on average, which is 'sublinear1 in the sense that not all characters are inspected. On the other hand, its worst case is 0{mn) (unlike KMP, the old suffix information is not kept to avoid further comparisons). Further simplifications of the BM algorithm lead to some of the fastest algorithms on average. The Simplified BM algorithm uses only the occurrence heuristic. This obtains almost the same shifts in practice. The BM-Horspool (BMH) algorithm does the same, but it notices that it is not important any more that the check proceeds backwards, and uses the occurrence heuristic on the last character of the window instead of the one that caused the mismatch. This gives longer shifts on average. Finally, the BM-Sunday (BMS) algorithm modifies BMH by using the character following the last one, which improves the shift especially on short patterns. The Commentz-Walter algorithm is an extension of BM to multipattern search. It builds a trie on the reversed patterns, and instead of a backward window check, it enters into the trie with the window characters read backwards. A shift function is computed by a natural extension of BM. In general this algorithm improves over Aho-Corasick for not too many patterns.
mir-0147	8.5.4    Shift-Or Shift-Or is based on hit-parallelism. This technique involves taking advantage of the intrinsic parallelism of the bit operations inside a computer word (of w bits). By cleverly using this fact, the number of operations that an algorithm performs can be cut by a factor of at most w. Since in current architectures w is 32 or 64. the speedup is very significant in practice. The Shift-Or algorithm uses bit-parallelism to simulate the operation of a non-deterministic automaton that searches the pattern in the text (see Figure 3.17). As this automaton is simulated in time O(mn), the Shift-Or algorithm achieves O(ninfir) worst-case time (optimal speedup). The algorithm first builds a table B which for each character stores a bit mask hm...bi.  The mask in B[c] has the i-th bit set to zero if and only if SEQUENTIAL		SEARCHING			213 gt;x	-N        C      ï-	gt;M	--n   d   /ó				a óªï	© B[a} =	1	0	0	1	0	1	0	1	0	0	1 B[b] =	0	1	0	0	0	0	0	0	1	0	0 SW =	0	0	1	0	0	0	0	0	0	1	0 B[c] =	0	0	0	0	1	0	0	0	0	0	0 B[d] =	0	0	0	0	0	0	1	0	0	0	0 B\*] =	0	0	0	0	0	0	0	0	0	0	0 Figure 8.17 Non-deterministic automaton that searches 'abracadabra,1 and the associated B table. The initial self-loop matches any character. Each table column corresponds to an edge of the automaton. pi = c (see Figure 8.17). The state of the search is kept in a machine word D = dm...di, where d{ is zero whenever the state numbered i in Figure 8.17 is active. Therefore, a match is reported whenever dm is zero. In the following, we use to denote the bitwise OR and c' to denote the bitwise AND. D is set to all ones originally, and for each new text character T3, D is updated using the formula D'   lt;-    (£gt;´ 1)   |   B[Tj] (where 'lt;lt;' means shifting all the bits in D one position to the left and setting the rightmost bit to zero). It is not hard to relate the formula to the movement that occurs in the non-deterministic automaton for each new text character. For patterns longer than the computer word (i.e., m gt; ir), the algorithm uses \m/w'] computer words for the simulation (not all them are active all the time). The algorithm is O(n) on average and the preprocessing is O(m + a) time and O(cr) space. It is easy to extend Shift-Or to handle classes of characters by manipulating the B table and keeping the search algorithm unchanged. This paradigm also can search a large set of extended patterns, as well as multiple patterns (where the complexity is the same as before if we consider that m is the total length of all the patterns).
mir-0148	8.5.5    Suffix Automaton The Backward DAWG matching (BDM) algorithm is based on a suffix automaton. A suffix automaton on a pattern P is an automaton that recognizes all the suffixes of P. The non-deterministic version of this automaton has a very regular structure and is shown in Figure 8.18. The BDlVf algorithm converts this automaton to deterministic. The size and construction time of this automaton is O(m). This is basically the preprocessing effort of the algorithm. Each path from the initial node to any internal 214        INDEXING AND SEARCHING ~~**"(r)-1-----------t----------t-----------1-----------1-----------1-----------t-----------1-----------1-----------1------------1------------* Figure 8.18 A non-deterministic suffix automaton. Dashed lines represent e-transitions (i.e., they occur without consuming any input). I is the initial state of the automaton. X| X           XX Figure 8-19 The BDM algorithm for the pattern 'abracadabra.' The rectangles represent elements compared to the text window. The Xs show the positions where a pattern prefix was recognized. node represents a substring of the pattern.   The final nodes represent pattern suffixes. To search a pattern P, the suffix automaton of Pr (the reversed pattern) is built. The algorithm searches backwards inside the text window for a substring of the pattern P using the suffix automaton. Each time a terminal state is reached before hitting the beginning of the window, the position inside the window is remembered. This corresponds to finding a prefix of the pattern equal to a suffix of the window (since the reverse suffixes of Pr are the prefixes of P). The last prefix recognized backwards is the longest prefix of P in the window. A match is found if the complete window is read, while the check is abandoned when there is no transition to follow in the automaton. In either case, the window is shifted to align with the longest prefix recognized. See Figure 8.19. This algorithm is O(rnn) time in the worst case and 0{n log(m)/m) on average. There exists also a multipattern version of this algorithm called MultiBDM, which is the fastest for many patterns or very long patterns. BDM rarely beats the best BM algorithms. However, a recent bit-parallel implementation called BNDN1 improves over BM in a wide range of cases. This algorithm simulates the non-deterministic suffix automaton using bit-parallelism. The algorithm supports some extended patterns and other applications mentioned in Shift-Or, while keeping more efficient than Shift-Or.
mir-0149	8.5.6    Practical Comparison Figure 8.20 shows a practical comparison between string matching algorithms run on our reference machine. The values are correct within b% of accuracy with a 9f/4 confidence interval. We tested English text from the TREC collection, DNA (corresponding to Ii.iiiflueiizae*) and random text uniformly generated over 64 letters. The patterns were randomly selected from the text except for random PATTERN MATCHING        215 text, where they were randomly generated. We tested over 10 Mb of text and measured CPU time. We tested short patterns on English and random text and long patterns on DNA, which are the typical cases. We first analyze the case of random text, where except for very short patterns the clear winners are BNDM (the bit-parallel implementation of BDM) and the BMS (Sunday) algorithm. The more classical Boyer-Moore and BDM algorithms are also very close. Among the algorithms that do not improve with the pattern length, Shift-Or is the fastest, and KMP is much slower than the naive algorithm. The picture is similar for English text, except that we have included the Agrep software in this comparison, which worked well only on English text. Agrep turns out to be much faster than others. This is not because of using a special algorithm (it uses a BM-family algorithm) but because the code is carefully optimized. This shows the importance of careful coding as well as using good algorithms, especially in text searching where a few operations per text character are performed. Longer patterns are shown for a DNA text. BNDM is the fastest for moderate patterns, but since it does not improve with the length after m gt; w, the classical BDM finally obtains better times. They are much better than the Boyer-Moore family because the alphabet is small and the suffix automaton technique makes better use of the information on the pattern. We have not shown the case of extended patterns, that is, where flexibility plays a role. For this case, BNDM is normally the fastest when it can be applied (e.g., it supports classes of characters but not wild cards), otherwise Shift-Or is the best option. Shift-Or is also the best option when the text must be accessed sequentially and it is not possible to skip characters.
mir-0150	8.5.7    Phrases and Proximity If a sequence of words is searched to appear in the text exactly as in the pattern (i.e., with the same separators) the problem is similar to that of exact search of a single pattern, by just forgetting the fact that there are many words. If any separator between words is to be allowed, it is possible to arrange it using an extended pattern or regular expression search. The best way to search a phrase element-wise is to search for the element which is less frequent or can be searched faster (both criteria normally match). For instance, longer patterns are better than shorter ones; allowing fewer errors is better than allowing more errors. Once such an element is found, the neighboring words are checked to see if a complete match is found. A similar algorithm can be used to search a proximity query.
mir-0151	8.6    Pattern Matching We present in this section the main techniques to deal with complex patterns. We divide it into two main groups: searching allowing errors and searching for extended patterns. 216        INDEXING AND SEARCHING 25 25-, 10   15   20   25   30 Figure 8.20 Practical comparison among algorithms. The upper left plot is for short patterns on English text. The upper right one is for long patterns on DNA. The lower plot is for short patterns on random text (on 64 letters). Times are in tenths of seconds per megabyte.
mir-0152	8.6.1    String Matching Allowing Errors This problem (called "approximate string matching')   can be stated as follows: given a short pattern P of length m, a long text T of length n, and a maximum allowed number of errors fc, find all the text positions where the pattern occurs with at most k errors. This statement corresponds to the Levenshtein distance. With minimal modifications it is adapted to searching whole wrords matching the pattern with k errors. This problem is newer than exact string matching, although there are already a number of solutions. We sketch the main approaches. Dynamic Programming The classical solution to approximate string matching is based on dynamic programming.   A matrix C[0..m,0..nj is filled column by column, where C[?\jj PATTERN MATCHING         217 represents the minimum number of errors needed to match Pi..$ to a suffix of Ti,,j. This is computed as follows C[0J] = 0 C[t,0] = i C[iJ]    =    if (Pi = Tj) then C[i-lJ~ I] else 1 + min(C[i - lJ],C[i,j - l],C[i - 1,j - 1]) where a match is reported at text positions j such that C[m,j] lt; k (the final positions of the occurrences are reported). Therefore, the algorithm is O(mn) time. Since only the previous column of the matrix is needed, it can be implemented in O(m) space. Its preprocessing time is O(m) . Figure 8.21 illustrates this algorithm. In recent years several algorithms have been presented that achieve O(kn) time in the worst case or even less in the average case, by taking advantage of the properties of the dynamic programming matrix (e.g., values in neighbor cells differ at most by one). Automaton It is interesting to note that the problem can be reduced to a non-deterministic finite automaton (NFA). Consider the NFA for k = 2 errors shown in Figure 8.22. Each row denotes the number of errors seen. The first one 0, the second one 1, and so on. Every column represents matching the pattern up to a given position. At each iteration, a new text character is read and the automaton changes its states. Horizontal arrows represent matching a character, vertical arrows represent insertions into the pattern, solid diagonal arrows represent replacements, and dashed diagonal arrows represent deletions in the pattern (they are ^-transitions). The automaton accepts a text position as the end of a match s	u	r	g	e	r	y 0	0	0	0	0	0	0	0 s	i	0	1	1	1	1	1	i u	2	1	0	1	2	2	2	2 r	3	2	1	0	1	2	2	3 V	4	3	2	1	1	2	3	3 e	5	4	3	2	2	1	2	3 y	6	5	4	3	3	2	2	2 Figure   8.21    The dynamic programming algorithm search  'survey'   in  the text 'surgery" with two errors. Bold entries indicate matching positions. 218        INDEXING AND SEARCHING 1 error 2 errors Figure 8.22 An NFA for approximate string matching of the pattern 'survey' with two errors. The shaded states are those active after reading the text 'surgery'. Unla-belled transitions match anv character. with k errors whenever the (fc -f l)-th rightmost state is active. It is not hard to see that once a state in the automaton is active, all the states of the same column and higher rows are active too. Moreover, at a given text character, if we collect the smallest active rows at each column, we obtain the current column of the dynamic programming algorithm. Figure 8.22 illustrates this (compare the figure with Figure 8.21). One solution is to make this automaton deterministic (DFA). Although the search phase is O(7i), the DFA can be huge. An alternative solution is based on bit-parallelism and is explained next. Bit-Parallelism Bit-parallelism has been used to parallelize the computation of the dynamic programming matrix (achieving average complexity O(kn/w)) and to parallelize the computation of the NFA (without converting it to deterministic), obtaining O(knwlw) time in the worst case. Such algorithms achieve O(n) search time for short patterns and are currently the fastest ones in many cases, running at 6 to 111 Mb per second on our reference machine. Filtering Finally, oilier approaches first filter the text, reducing the area where1 dynamic programming needs to be used. These algorithms achieve 'sublinear* expected time in many causes for low error ratios (i.e., not all text characters are inspected. PATTERN MATCHING        219 O(kn\oga{m)/m) is a typical figure), although the nitration is not effective for more errors. Filtration is based on the fact that some portions of the pattern must appear with no errors even in an approximate occurrence. The fastest algorithm for low error levels is based on filtering: if the pattern is split into /c +1 pieces, any approximate occurrence must contain at least one of the pieces with no errors, since k errors cannot alter all the k + 1 pieces. Hence, the search begins with a multipattern exact search for the pieces and it later verifies the areas that may contain a match (using another algorithm).
mir-0153	8.6.2    Regular Expressions and Extended Patterns General regular expressions are searched by building an automaton which finds all their occurrences in a text. This process first builds a non-deterministic finite automaton of size O(ra), where m is the length of the regular expression. The classical solution is to convert this automaton to deterministic form. A deterministic automaton can search any regular expression in O(n) time. However, its size and construction time can be exponential in m, i.e. O(m2m). See Figure 8.23. Excluding preprocessing, this algorithm runs at 6 Mb/sec in the reference machine. Recently the use of bit-parallelism has been proposed to avoid the construction of the deterministic automaton. The non-deterministic automaton is simulated instead. One bit per automaton state is used to represent whether the state is active or not. Due to the algorithm used to build the non-deterministic automaton, all the transitions move forward except for ^-transitions. The idea is that for each text character two steps are carried out. The first one moves forward, and the second one takes care of all the e-transitions. A function E from bit masks to bit masks is precomputed so that all the corresponding bits are moved according to the ^-transitions. Since this function is very large (i.e., 2m entries) its domain is split in many functions from 8- or 16-bit submasks to 7n-bit masks. This is possible because E(B\,..B3) ~ E{B\)\...\E(B3), where B% Figure 8.23    The non-deterministic (a) and deterministic (h) automata for the regular expression b b*   (b j b*a). 220        INDEXING AND SEARCHING are the submasks. Hence, the scheme performs |"m/8] or |~ra/16] operations per text character and needs [ra/8] 28 \m/w] or [ra/16]216|"m/it/] machine words of memory. Extended patterns can be rephrased as regular expressions and solved as before. However, in many cases it is more efficient to give them a specialized solution, as we saw for the extensions of exact searching (bit-parallel algorithms). Moreover, extended patterns can be combined with approximate search for maximum flexibility. In general, the bit-parallel approach is the best equipped to deal with extended patterns. Real times for regular expressions and extended pattern searching using this technique are between 2-8 Mb/sec.
mir-0154	8.6.3    Pattern Matching Using Indices We end this section by explaining how the indexing techniques we presented for simple searching of words can in fact be extended to search for more complex patterns. Inverted Files As inverted files are word-oriented, other types of queries such as suffix or substring queries, searching allowing errors and regular expressions, are solved by a sequential (i.e., online) search over the vocabulary. This is not too bad since the size of the vocabulary is small with respect to the text size. After either type of search, a list of vocabulary words that matched the query is obtained. All their lists of occurrences are now merged to retrieve a list of documents and (if required) the matching text positions. If block addressing is used and the positions are required or the blocks do not coincide with the retrieval unit, the search must be completed with a sequential search over the blocks. Notice that an inverted index is word-oriented. Because of that it is not surprising that it is not able to efficiently find approximate matches or regular expressions that span many words. This is a restriction of this scheme. Variations that are not subject to this restriction have been proposed for languages which do not have a clear concept of word, like Finnish. They collect text samples or n-grarns, which are fixed-length strings picked at regular text intervals. Searching is in genera! more powerful but more expensive. In a full-inverted index, search times for simple words allowing errors on 250 Mb of text took out reference machine from 0.6 to 0,85 seconds, while very complex expressions on extended patterns took from 0.8 to 3 seconds. As a comparison, the same collection cut in blocks of 1 Mb size takes more than 8 seconds for an approximate search with one error and more than 20 for two errors. PATTERN MATCHING        221 Suffix Trees and Suffix Arrays If the suffix tree indexes all text positions it can search for words, prefixes, suffixes and substrings with the same search algorithm and cost described for word search. However, indexing all positions makes the index 10 to 20 times the text size for suffix trees. Range queries are easily solved too, by just searching both extremes in the trie and then collecting all the leaves which lie in the middle. In this case the cost is the height of the tree, which is O(logn) on average (excluding the tasks of collecting and sorting the leaves). Regular expressions can be searched in the suffix tree. The algorithm simply simulates sequential searching of the regular expression. It begins at the root, since any possible match starts there too. For each child of the current node labeled by the character c, it assumes that the next text character is c and recursively enters into that subtree. This is done for each of the children of the current node. The search stops only when the automaton has no transition to follow. It has been shown that for random text only O(nQ;polylog(n)) nodes are traversed (for 0 lt; a lt; 1 dependent on the regular expression). Hence, the search time is sublinear for regular expressions without the restriction that they must occur inside a word. Extended patterns can be searched in the same way by taking them as regular expressions. Unrestricted approximate string matching is also possible using the same idea. We present a simplified version here. Imagine that the search is online and traverse the tree recursively as before. Since all suffixes start at the root, any match starts at the root too, and therefore do not allow the match to start later. The search will automatically stop at depth m -f- k at most (since at that point more than k errors have occurred). This implies constant search time if n is large enough (albeit exponential on m and k). Other problems such as approximate search of extended patterns can be solved in the same way, using the appropriate online algorithm. Suffix trees are able to perform other complex searches that we have not considered in our query language (see Chapter 4). These are specialized operations which are useful in specific areas. Some examples are: find the longest substring in the text that appears more than once, find the most common substring of a fixed size, etc. If a suffix array indexes all text positions, any algorithm that works on suffix trees at C(n) cost will work on suffix arrays at O(C(n) log n) cost. This is because the operations performed on the suffix tree consist of descending to a child node, which is done in O(l) time. This operation can be simulated in the suffix array in O(logn) time by binary searching the new boundaries (each suffix tree node corresponds to a string, which can be mapped to the suffix array interval holding ail suffixes starting with that string). Some patterns can be searched directly in the suffix array in O(logn) total search time without simulating the suffix tree. These are: word, prefix, suffix and subword search, as well as range search. However, again, indexing all text positions normally makes the suffix array 222        INDEXING AND SEARCHING size four times or more the text size. A different alternative for suffix arrays is to index only word beginnings and to use a vocabulary supra-index, using the same search algorithms used for the inverted lists.
mir-0155	8.7    Structural Queries The algorithms to search on structured text (see Chapter 4) are largely dependent on each model. We extract their common features in this section. A first concern about this problem is how to store the structural information. Some implementations build an ad hoc index to store the structure. This is potentially more efficient and independent of any consideration about the text. However, it requires extra development and maintenance effort. Other techniques assume that the structure is marked in the text using 'tags' (i.e., strings that identify the structural elements). This is the case with HTML text but not the case with C code where the marks are implicit and are inherent to C. The technique relies on the same index to query content (such as inverted files), using it to index and search those tags as if they were words. In many cases this is as efficient as an ad hoc index, and its integration into an existing text database is simpler. Moreover, it is possible to define the structure dynamically, since the appropriate tags can be selected at search time. For that goal, inverted files are better since they naturally deliver the results in text order, which makes the structure information easier to obtain. On the other hand, some queries such as direct ancestry are hard to answer without an ad hoc index. Once the content and structural elements have been found by using some index, a set of answers is generated. The models allow further operations to be applied on tiiose answers, such as 'select all areas in the left-hand argument which contain an area of the right-hand argument/ This is in general solved in a way very similar to the set manipulation techniques already explained in section 8.4. However, the operations tend to be more complex, and it is not always possible to find an evaluation algorithm which has linear time with respect to the size of the intermediate results. It is worth mentioning that some models use completely different algorithms, such as exhaustive search techniques for tree pattern matching. Those problems are NP-complete in many cases.
mir-0156	8.8    Compression In this section we discuss the issues of searching compressed text directly and of searching compressed indices. Compression is important when available storage is a limiting factor, as is the case of indexing the Web. Searching and compression were traditionally regarded as exclusive operations. Texts which were not to be searched could be compressed, and to search COMPRESSION        223 a compressed text it had to be decompressed first. In recent years, very efficient compression techniques have appeared that allow searching directly in the compressed text. Moreover, the search performance is improved, since the CPU times are similar but the disk times are largely reduced. This leads to a win-win situation. Discussion on how common text and lists of numbers can be compressed has been covered in Chapter 7.
mir-0157	8.8.1    Sequential Searching A few approaches to directly searching compressed text exist. One of the most successful techniques in practice relies on Huffman coding taking words as symbols. That is, consider each different text word as a symbol, count their frequencies, and generate a Huffman codefor the words. Then, compress the text by replacing each word with its code. To improve compression/decompression efficiency, the Huffman code uses an alphabet of bytes instead of bits. This scheme compresses faster and better than known commercial systems, even those based on Ziv-Lempel coding. Since Huffman coding needs to store the codes of each symbol, this scheme has to store the whole vocabulary of the text, i.e. the list of all different text words. This is fully exploited to efficiently search complex queries. Although according to Heaps' law the vocabulary (i.e., the alphabet) grows as 0{n) for 0 lt; (3 lt; 1, the generalized Zipf's law shows that the distribution is skewed enough so that the entropy remains constant (i.e., the compression ratio will not degrade as the text grows). Those laws are explained in Chapter 6. Any single-wrord or pattern query is first searched in the vocabulary. Some queries can be binary searched, while others such as approximate searching or regular expression searching must traverse sequentially all the vocabulary. This vocabulary is rather small compared to the text size, thanks to Heaps' law. Notice that this process is exactly the same as the vocabulary searching performed by inverted indices, either for simple or complex pattern matching. Once that search is complete, the list of different words that match the query is obtained. The Huffman codes of all those wrords are collected and they are searched in the compressed text. One alternative is to traverse byte-wise the compressed text and traverse the Huffman decoding tree in synchronization, so that each time that a leaf is reached, it is checked whether the leaf (i.e., word) was marked as 'matching' the query or not. This is illustrated in Figure 8.24. Boyer-Moore filtering can be used to speed up the search. Solving phrases is a little more difficult. Each element is searched in the vocabulary. For each word of the vocabulary we define a bit mask. We set the /-th bit in the mask of all words which match with the i-th element of the phrase query. This is used together with the Shift-Or algorithm. The text is traversed byte-wise, and only when a leaf is reached, does the Shift-Or algorithm consider that a new text symbol has been read, whose bit mask is that of the leaf (see Figure 8.24). This algorithm is surprisingly simple and efficient. 224        INDEXING AND SEARCHING EH cm cm cm cm m cm cm cm cm Huffman tree cm Vocabulary      Marks cm cm cm cm cm cm n cm Huffman tree rrooi rrfoi ["Tool CZH Vocabulary      Marks Figure 8.24 On the left, searching for the simple pattern 'rose' allowing one error. On the right, searching for the phrase 'ro* rose is,' where 'ro*' represents a prefix search. This scheme is especially fast when it comes to solving a complex query (regular expression, extended pattern, approximate search, etc.) that would be slow with a normal algorithm. This is because the complex search is done only in the small vocabulary, after which the algorithm is largely insensitive to the complexity of the originating query. Its CPU times for a simple pattern are slightly higher than those of Agrep (briefly described in section 8.5.6). However, if the I/O times are considered, compressed searching is faster than all the online algorithms. For complex queries, this scheme is unbeaten by far. On the reference machine, the CPU times are 14 Mb/sec for any query, while for simple queries this improves to 18 Mb/sec if the speedup technique is used. Agrep, on the other hand, runs at 15 Mb/sec on simple searches and at 1-4 Mb/sec for complex ones. Moreover, I/O times are reduced to one third on the compressed text.
mir-0158	8.8.2    Compressed Indices Inverted Files Inverted files are quite amenable to compression. This is because the lists of occurrences are in increasing order of text position. Therefore, an obvious choice is to represent the differences between the previous position and the current one. These differences can be represented using less space by using techniques that favor small numbers (see Chapter 7). Notice that, the longer the lists, the smaller the differences. Reductions in 909? for block-addressing indices with blocks of 1 Kb size have been reported. It is important to notice that compression does not necessarily degrade time performance. Most of the time spent in answering a query is in the disk transfer. Keeping the index compressed allows the transference of less data, and it may be worth the CPU work (if decompressing. Notice also that the lists of COMPRESSION        225 occurrences are normally traversed in a sequential manner, which is not affected by a differential compression. Query times on compressed or decompressed indices are reported to be roughly similar. The text can also be compressed independently of the index. The text will be decompressed only to display it, or to traverse it in case of block addressing. Notice in particular that the online search technique described for compressed text in section 8.8.1 uses a vocabulary. It is possible to integrate both techniques (compression and indexing) such that they share the same vocabulary for both tasks and they do not decompress the text to index or to search. Suffix Trees and Suffix Arrays Some efforts to compress suffix trees have been pursued. Important reductions of the space requirements have been obtained at the cost of more expensive searching. However, the reduced space requirements happen to be similar to those of uncompressed suffix arrays, which impose much smaller performance penalties. Suffix arrays are very hard to compress further. This is because they represent an almost perfectly random permutation of the pointers to the text. However, the subject of building suffix arrays on compressed text has been pursued. Apart from reduced space requirements (the index plus the compressed text take less space than the uncompressed text), the main advantage is that both index construction and querying almost double their performance. Construction is faster because more compressed text fits in the same memory space, and therefore fewer text blocks are needed. Searching is faster because a large part of the search time is spent in disk seek operations over the text area to compare suffixes. If the text is smaller, the seeks reduce proportionally. A compression technique very similar to that shown in section 8.8.1 is used. However, the Huffman code on words is replaced by a Hu-Tucker coding. The Hu-Tucker code respects the lexicographical relationships between the words, and therefore direct binary search over the compressed text is possible (this is necessary at construction and search time). This code is suboptimal by a very small percentage (2-3% in practice, with an analytical upper bound of 5%). Indexing times for 250 Mb of text on the reference machine are close to 1.6 Mb/min if compression is used, while query times are reduced to 0.5 seconds in total and 0.3 seconds for the text alone. Supra-indices should reduce the total search time to 0.15 seconds. Signature Files There are many alternative ways to compress signature files.   All of them are based on the fact that only a few bits are set in the whole file. It is then possible 226 INDEXING AND SEARCHING to use efficient methods to code the bits which are not set, for instance run-length encoding. Different considerations arise if the file is stored as a sequence of bit masks or with one file per bit of the mask. They allow us to reduce space and hence disk times, or alternatively to increase B (so as to reduce the false drop probability) keeping the same space overhead. Compression ratios near 70% are reported.
mir-0159	8.9    Trends and Research Issues In this chapter we covered extensively the current techniques of dealing with text retrieval. We first covered indices and then online searching. We then reviewed set manipulation, complex pattern matching and finally considered compression techniques. Figure 8.25 summarizes the tradeoff between the space needed for the index and the time to search one single word. O.ln Space Complexity Suffix tries Indexed search Suffix trees (full inversion) Suffix arrays Hybrid solutions /       (block addressing) Inverted files "x Sequential search I Boyer-Moore and BDM families^ó    KMP +  Shift-or Brute force m     m log n   n"*         - -~O.ln rnn     Time Complexity Figure 8.25    Tr.tdruiF of Index spare versus word searching time. BIBLIOGRAPHIC DISCUSSION        227 Probably the most adequate indexing technique in practice is the inverted file. As we have shown throughout the chapter, many hidden details in other structures make them harder to use and less efficient in practice, as well as less flexible for dealing with new types of queries. These structures, however, still find application in restricted areas such as genetic databases (for suffix trees and arrays, for the relatively small texts used and their need to pose specialized queries) or some office systems (for signature files, because the text is rarely queried in fact). The main trends in indexing and searching textual databases today are ï  Text collections are becoming huge. This poses more demanding requirements at all levels, and solutions previously affordable are not any more.   On the other hand, the speed of the processors and the relative slowness of external devices have changed what a few years ago were reasonable options (e.g., it is better to keep a text compressed because reading less text from disk and decompressing in main memory pays off). ï  Searching is becoming more complex.   As the text databases grow and become more heterogeneous and error-prone, enhanced query facilities are required, such as exploiting the text structure or allowing errors in the text. Good support for extended queries is becoming important in the evaluation of a text retrieval system. ï  Compression is becoming a star in the field. Because of the changes mentioned in the time cost of processors and external devices, and because of new developments in the area, text retrieval and compression are no longer regarded as disjoint activities.   Direct indexing and searching on compressed text provides better (sometimes much better) time performance and less space overhead at the same time. Other techniques such as block addressing trade space for processor time.
mir-0160	8.10    Bibliographic Discussion A detailed explanation of a full inverted index and its construction and querying process can be found in [26]. This work also includes an analysis of the algorithms on inverted lists using the distribution of natural language, The in-place construction is described in [572], Another construction algorithm is presented in [341]. The idea of block addressing inverted indices was first presented in a system called Glimpse [540], which also first exposed the idea of performing complex pattern matching using the vocabulary of the inverted index. Block addressing indices are analyzed in [42], where some performance improvements are proposed. The variant that indexes sequences instead of words has been implemented in a system called Grampse, which is described in [497]. Suffix arrays were presented in [538] together with the algorithm to build them in O(n log n) character comparisons. They were independently discovered 228        INDEXING AND SEARCHING by [309] under the name of TAT arrays.' The algorithm to build large suffix arrays is presented in [311]. The use of supra-indices over suffix array is proposed in [37], while the modified binary search techniques to reduce disk seek time are presented in [56]. The linear-time construction of suffix trees is described in [780]. The material on signature files is based on [243]. The different alternative ways of storing the signature file are explained in [242]. The original references for the sequential search algorithms are: KMP [447], BM [110], BMH [376], BMS [751], Shift-Or [39], BDM [205] and BNDM [592]. The multipattem versions are found in [9, 179], and MultiBDM in [196]. Many enhancements of bit-parallelism to support extended patterns and allow errors are presented in [837]. Many ideas from that paper were implemented in a widely distributed software for online searching called Agrep [836]. The reader interested in more details about sequential searching algorithms may look for the original references or in good books on algorithms such as [310, 196]. One source for the classical solution to approximate string matching is [716]. An O(kn) worst-case algorithm is described in [480]. The use of a DFA is proposed in [781]. The bit-parallel approach to this problem started in [837], although currently the fastest bit-parallel algorithms are [583] and [43]. Among all the filtering algorithms, the fastest one in practice is based on an idea presented in [837], later enhanced in [45], and finally implemented in [43]. A good source from which to learn about regular expressions and building a DFA is [375]. The bit-parallel implementation of the NFA is explained in [837]. Regular expression searching on suffix trees is described in [40], while searching allowing errors is presented in [779]. The Huffman coding was first presented in [386], while the word-oriented alternative is proposed in [571]. Sequential searching on text compressed using that technique is described in [577]. Compression used in combination with inverted files is described in [850], with suffix trees in [430], with suffix arrays in [575], and with signature files in [243, 242]. A good general reference on compression is [78].
mir-0162	9.1     Introduction The volume of electronic text available online today is staggering. By many accounts, the World Wide Web alone contains over 200 million pages of text, comprising nearly 500 gigabytes of data. Moreover, the Web (see Chapter 13) has been growing at an exponential rate, nearly doubling in size every six months. Large information service providers, such as LEXIS-NEXIS (see Chapter 14), have amassed document databases that reach into the terabytes. On a slightly smaller scale, the largest corporate intranets now contain over a million Web pages. Even private collections of online documents stored on personal computers are growing larger as disk space becomes cheaper and electronic content becomes easier to produce, download, and store. As document collections grow larger, they become more expensive to manage with an information retrieval system. Searching and indexing costs grow with the size of the underlying document collection; larger document collections invariably result in longer response times. As more documents are added to the system, performance may deteriorate to the point where the system is no longer usable. Furthermore, the economic survival of commercial systems and Web search engines depends on their ability to provide high query processing rates. In fact, most of a Web search company's gross income comes from selling 'advertising impressions' (advertising banners displayed at the user's screen) whose number is proportional to the number of query requests attended. To support the demanding requirements of modern search environments, we must turn to alternative architectures and algorithms. In this chapter we explore parallel and distributed information retrieval techniques. The application of parallelism can greatly enhance our ability to scale traditional information retrieval algorithms and support larger and larger document collections. We continue this introduction with a review of parallel computing and parallel program performance measures. In section 9.2 we explore techniques for 229 230        PARALLEL AND DISTRIBUTED IR implementing information retrieval algorithms on parallel platforms, including inverted file and signature file methods. In section 9.3, we turn to distributed information retrieval and approaches to collection partitioning, source selection, and distributed results merging (often called collection fusion). We discuss future trends in section 9.4, and conclude with a bibliographic discussion in section 9.5.
mir-0163	9.1.1    Parallel Computing Parallel computing is the simultaneous application of multiple processors to solve a single problem, where each processor works on a different part of the problem. With parallel computing, the overall time required to solve the problem can be reduced to the amount of time required by the longest running part. As long as the problem can be further decomposed into more parts that will run in parallel, we can add more processors to the system, reduce the time required to solve the problem, and scale up to larger problems. Processors can be combined in a variety of ways to form parallel architectures. Flynn [259] has defined a commonly used taxonomy of parallel architectures based on the number of the instruction and data streams in the architecture. The taxonomy includes four classes: ï  SISD single instruction stream, single data stream ï  SIMD single instruction stream, multiple data stream ï  MISD multiple instruction stream, single data stream ï  MIMD multiple instruction stream, multiple data stream. The SISD class includes the traditional von Neumann [134] computer running sequential programs, e.g., uniprocessor personal computers. SIMD computers consist of AT processors operating on N data streams, with each processor executing the same instruction at the same time.   Machines in this class are often massively parallel computers with many relatively simple processors, a communication network between the processors, and a control unit that supervises the synchronous operation of the processors, e.g., the Thinking Machines CM-2. The processors may use shared memory, or each processor may have its own local memory. Sequential programs require significant modification to make effective use of a SIMD architecture, and not all problems lend themselves to a SIMD implementation. MISD computers use N processors operating on a single data stream in shared memory. Each processor executes its own instruct ion stream, such that multiple operations are performed simultaneously on the same data item. MISD architectures are relatively rare. Systolic arrays are the best known example. MIMD is the most general and most popular class of parallel architectures. A MIMD computer contains N processors, A" instruction streams, and Ar data streams.   The processors are similar to those used in a SISD computer; each INTRODUCTION        231 processor has its own control unit, processing unit, and local memory.f MIMD systems usually include shared memory or a communication network that connects the processors to each other. The processors can work on separate, unrelated tasks, or they can cooperate to solve a single task, providing a great deal of flexibility. MIMD systems with a high degree of processor interaction are called tightly coupled, while systems with a low degree of processor interaction are loosely coupled. Examples of MIMD systems include multiprocessor PC servers, symmetric multiprocessors (SMPs) such as the Sun HPC Server, and scalable parallel processors such as the IBM SP2. Although MIMD typically refers to a single, self-contained parallel computer using two or more of the same kind of processor, MIMD also characterizes distributed computing architectures. In distributed computing, multiple computers connected by a local or wide area network cooperate to solve a single problem. Even though the coupling between processors is very loose in a distributed computing environment, the basic components of the MIMD architecture remain. Each computer contains a processor, control unit, and local memory, and the local or wide area network forms the communication network between the processors. The main difference between a MIMD parallel computer and a distributed computing environment is the cost of interprocessor communication, which is considerably higher in a distributed computing environment. As such, distributed programs are usually coarse grained, while programs running on a single parallel computer tend to be finer grained. Granularity refers to the amount of computation relative to the amount of communication performed by the program. Coarse-grained programs perform large amounts of computation relative to communication; fine-grained programs perform large amounts of communication relative to computation. Of course, an application may use different levels of granularity at different times to solve a given problem.
mir-0164	9.1.2    Performance Measures When we employ parallel computing, we usually want to know what sort of performance improvement we are obtaining over a comparable sequential program running on a uniprocessor.   A number of metrics are available to measure the performance of a parallel algorithm. One such measure is the speedup obtained with the parallel algorithm relative to the best available sequential algorithm for solving the same problem, defined as: __ Running time of best available sequential algorithm Running time of parallel algorithm I The processors used in a MIMD system may be identical to those used in SISD systems, or they may provide additional functionality, such as hardware cache coherence for shared memory. 232        PARALLEL AND DISTRIBUTED IR Ideally, when running a parallel algorithm on N processors, we would obtain perfect speedup, or S = N. In practice, perfect speedup is unattainable either because the problem cannot be decomposed into N equal subtasks, the parallel architecture imposes control overheads (e.g., scheduling or synchronization), or the problem contains an inherently sequential component. Amdahl's law [18] states that the maximal speedup obtainable for a given problem is related to /, the fraction of the problem that must be computed sequentially. The relationship is given by: Another measure of parallel algorithm performance is efficiency, given by: where S is speedup and N is the number of processors. Ideal efficiency occurs when 0=1 and no processor is ever idle or performs unnecessary work. As with perfect speedup, ideal efficiency is unattainable in practice. Ultimately, the performance improvement of a parallel program over a sequential program should be viewed in terms of the reduction in real time required to complete the processing task combined with the additional monetary cost associated with the parallel hardware required to run the parallel program. This gives the best overall picture of parallel program performance and cost effectiveness.
mir-0166	9.2.1    Introduction We can approach the development of parallel information retrieval algorithms from two different directions. One possibility is to develop new retrieval strategies that directly lend themselves to parallel implementation. For example, a text search procedure can be built on top of a neural network. Neural networks (see Chapter 2) are modeled after the human brain and solve problems using a large number of nodes (neurons), each of which has a set of inputs, a threshold, and an output. The output of one node is connected to the input of one or more other nodes, with the boundaries of the network defining the initial input and final output of the system. A node's output value is determined by a weighted function of the node's inputs and threshold. A training procedure is used to learn appropriate settings for the weights and thresholds in the network. Computation proceeds by applying input values to the network, computing each active node's output value, and conditioning these values through the network until the final output values are obtained. Neural networks naturally lend themselves to parallel implementation on SIMD hardware. The challenge with this approach is to PARALLEL IR        233 define the retrieval task in such a way that it maps well onto the computational paradigm. The other possibility is to adapt existing, well studied information retrieval algorithms to parallel processing. This is the approach that we will consider throughout the rest of this chapter. The modifications required to adapt an existing algorithm to parallel implementation depend on the target parallel platform. We will investigate techniques for applying a number of retrieval algorithms to both MIMD and SIMD architectures. Since parallel information retrieval is still very much an active research area, few approaches have fallen out as accepted standard techniques. We will, therefore, present a sampling of the work that has been done and avoid preferring one technique over another.
mir-0167	9.2.2     MIMD Architectures MIMD architectures offer a great deal of flexibility in how parallelism is defined and exploited to solve a problem. The simplest way in which a retrieval system can exploit a MIMD computer is through the use of multitasking. Each of the processors in the parallel computer runs a separate, independent search engine. The search engines do not cooperate to process individual queries, but they may share code libraries and data cached by the file system or loaded into shared memory. The submission of user queries to the search engines is managed by a broker, which accepts search requests from the end users and distributes the requests among the available search engines. This is depicted in Figure 9.1. As more processors are added to the system, more search engines may be run and more search requests may be processed in parallel, increasing the throughput of the system. Note, however, that the response time of individual queries remains unchanged. In spite of the simplicity of this approach, care must be taken to properly balance the hardware resources on the system. In particular, as the number of processors grows, so must the number of disks and I/O channels. Unless the entire retrieval index fits in main memory, the search processes running on the different processors will perform I/O and compete for disk access. A bottleneck at the disk will be disastrous for performance and could eliminate the throughput gains anticipated from the addition of more processors. In addition to adding more disks to the computer, the system administrator must properly distribute the index data over the disks. Disk contention will remain as long as two search processes need to access index data stored on the same disk. At one extreme, replicating the entire index on each disk eliminates disk contention at the cost of increased storage requirements and update complexity. Alternatively, the system administrator may partition and replicate index data across the disks according to profile information; heavily accessed data is replicated while less frequently accessed data is distributed randomly. Yet another approach is to install a disk array, or RAID [165], and let the operating system handle partitioning the index. Disk arrays can provide low latency and high throughput disk access by striping files across many disks. 234 PARALLEL AND DISTRIBUTED IR User Query ^		User Query Broker   Result Result Figure 9.1    Parallel multitasking on a MIMD machine. To move beyond multitasking and improve query response time, the computation required to evaluate a single query must be partitioned into subtasks and distributed among the multiple processors, as shown in Figure 9.2. In this configuration the broker and search processes run in parallel on separate processors as before, but now they all cooperate to evaluate the same query. High level processing in this system proceeds as follows. The broker accepts a query from the end user and distributes it among the search processes. Each of the search processes then evaluates a portion of the query and transmits an intermediate result back to the broker. Finally, the broker combines the intermediate results into a final result for presentation to the end user. Since IR computation is typically characterized by a small amount of processing per datum applied to a large amount of data, how to partition the computation boils down to a question of how to partition the data. Figure 9.3 presents a high level view of the data processed by typical search algorithms (see Chapter 8). Each row represents a document, djlt; and each column represents an indexing item, kt. Here, k{ may be a term, phrase, concept, or a more abstract indexing item such as a dimension in an LSI vector or a bit in a document signature. The entries in the matrix, wlnJ, are (possibly binary) weights, indicating if and to what degree indexing item i is assigned to document j. The indexing item weights associated with a particular document form a vector, d3 = (u'i,j.....Wtj)- During search, a query is also represented as a vector of indexing item weights, q = {iv\^___tr^9), and the search algorithm scores each document by applying a matching function F(dj,lt;[) = sim(dj,q). This high level data representation reveals two possible methods for partitioning the data. The first method, document partitioning, slices the data matrix horizontally, dividing the documents among the subtasks. The X documents in the collection are distributed across the P processors in the system. PARALLEL IR 235 Subquery/ Results User Query Search Process Result Search Process Search Process Figure 9.2    Partitioned parallel processing on a MIMD machine. Indexing Items k\         k2       ...        ki       ...	kt D    d\ 2   d2 u	W\ i         W2 1        ï ï ï         Wi 1        ïï ï IV x 2        ^2 2        ï ï ï         ^-72 2        ï ï ï	Wt 1 y^t 2 m      , e     ^*? n	^1 7         y^2 j        ï ' ï         ^i-iJ         " * * t     """ S      ´N Figure 9.3    Basic data elements processed by a search algorithm. creating P subcollections of approximately N/P documents each. During query processing, each parallel process (one for each processor) evaluates the query on the subcollection of N/P documents assigned to it, and the results from each of the subcollections are combined into a final result list. The second method, term partitioning, slices the data matrix vertically, dividing the Indexing items among the P processors such that the evaluation procedure for each document is spread over multiple processors in the system. Below we consider both of these partitioning schemes for each of the three main index structures. Inverted Files We first discuss inverted files for systems that employ document partitioning. Following that, we cover systems that employ term partitioning. Tiiere are two approaches to document partitioning in systems that use inverted files, namely, logical document partitioning and physical document partitioning. 236        PARALLEL AND DISTRIBUTED IR Dictionary Inverted List Term / term /   PO  PI  PZ  P3      Figure 9.4    Extended dictionary entry for document partitioning. Logical Document Partitioning In this case, the data partitioning is done logically using essentially the same basic underlying inverted file index as in the original sequential algorithm (see Chapter 8). The inverted file is extended to give each parallel process (one for each processor) direct access to that portion of the index related to the processor's subcollection of documents. Each term dictionary entry is extended to include P pointers into the corresponding inverted list, where the j-th pointer indexes the block of document entries in the inverted list associated with the subcollection in the j-th processor. This is shown in Figure 9.4, where the dictionary entry for term i contains four pointers into term fs inverted list, one for each parallel process (P = 4). When a query is submitted to the system, the broker (from Figure 9.2) first ensures that the necessary term dictionary and inverted file entries are loaded into shared memory, where all of the parallel processes can access a single shared copy. The broker then initiates P parallel processes to evaluate the query. Each process executes the same document scoring algorithm on its document subcollection, using the extended dictionary to access the appropriate entries in the inverted file. Since all of the index operations during query processing are read-only, there is no lock contention among the processes for access to the shared term dictionary and inverted file. The search processes record document scores in a single shared array of document score accumulators and notify the broker when they have completed. Updates to the accumulator array do not produce lock contention either since the subcollections scored by the different search processes are mutually exclusive. After all of the search processes have finished, the broker sorts the array of document score accumulators and produces the final ranked list of documents. PARALLEL IR        237 At inverted file construction time, the indexing process for logically partitioned documents can exploit the parallel processors using a variant of the indexing scheme described by Brown [123] (see Chapter 8). First, the indexer partitions the documents among the processors. Next, it assigns document identifiers such that all identifiers in partition i are less than all identifiers in partition i + 1. The indexer then runs a separate indexing process on each processor in parallel Each indexing process generates a batch of inverted lists, sorted by indexing item. After all of the batches have been generated, a merge step is performed to create the final inverted file. Since the inverted lists in each batch are sorted the same way, a binary heap-based priority queue is used to assemble the inverted list components from each batch that correspond to the current indexing item. The components are concatenated in partition number order to produce a final inverted list and a dictionary entry for the indexing item is created that includes the additional indexing pointers shown in Figure 9.4. Physical Document Partitioning In this second approach to document partitioning, the documents are physically partitioned into separate, self-contained subcollections, one for each parallel processor. Each subcollection has its own inverted file and the search processes share nothing during query evaluation. When a query is submitted to the system, the broker distributes the query to all of the parallel search processes. Each parallel search process evaluates the query on its portion of the document collection, producing a local, intermediate hit-list. The broker then collects the intermediate hit-lists from all of the parallel search processes and merges them into a final hit-list. The P intermediate hit-lists can be merged efficiently using a binary heap-based priority queue [188]. A priority queue of n elements has the property that element i is greater than elements 2i and 2i 4-1, where i ranges from 1 to n. A priority queue is not fully sorted, but the maximal element is always immediately available (i.e., in 6(1) time) and can be extracted in O(logn) time. Inserting an element into a priority queue can be done in O(logra) time as well. To merge the intermediate hit-lists, a priority queue of P elements is created with the first entry from each intermediate hit-list inserted into the queue in O(Plog P) time. To generate the final (and global) hit-list with the top k retrieved documents (in a global ranking), k elements are extracted from the priority queue. As each element is extracted from the priority queue, the intermediate hit-list from which the element was originally drawn inserts a new element into the priority queue. The P intermediate hit-lists can be merged into a final hit-list of A^ elements in O((P^k) log P) time. The merge procedure just described assumes that the parallel search processes produce globally consistent document scores, i.e., document scores that can be merged directly. Depending on the ranking algorithm in use, each parallel search process may require global term statistics in order to produce globally consistent document scores. There are two basic approaches to collect information on global term statistics. The first approach is to compute global term statistics at indexing time and store these statistics with each of the subcollec238        PARALLEL AND DISTRIBUTED IR tions. The second approach is for the query processing to proceed in two phases. During the first phase, the broker collects subcollection term statistics from each of the search processes and combines them into global term statistics. During the second phase, the broker distributes the query and global term statistics to the search processes and query evaluation proceeds as before. The first solution offers better query processing performance at the expense of more complex indexing, while the second solution allows subcollections to be built and maintained independently at the expense of doubling communication costs during query evaluation. To build the inverted files for physically partitioned documents, each processor creates, in parallel, its own complete index corresponding to its document partition. If global collection statistics are stored in the separate term dictionaries, then a merge step must be performed that accumulates the global statistics for all of the partitions and distributes them to each of the partition dictionaries. Logical document partitioning requires less communication than physical document partitioning with similar parallelization, and so is likely to provide better overall performance. Physical document partitioning, on the other hand, offers more flexibility (e.g., document partitions may be searched individually) and conversion of an existing IR system into a parallel IR system is simpler using physical document partitioning. For either document partitioning scheme, threads provide a convenient programming paradigm for creating the search processes, controlling their operation, and communicating between them. Threads are natively supported in some modern programming languages (e.g., Java [491]) and well supported in a standard way in others (e.g., POSIX threads in C/C++). Thread packages allow programmers to develop parallel programs using high level abstractions of concurrent execution, communication, and synchronization. The compiler and runtime system then map these abstractions to efficient operating system services and shared memory operations. Term Partitioning When term partitioning is used with an inverted file-based system, a single inverted file is created for the document collection (using the parallel construction technique described above for logical document partitioning) and the inverted lists are spread across the processors. During query evaluation, the query is decomposed into indexing items and each indexing item is sent to the processor that holds the corresponding inverted list. The processors create hit-lists with partial document scores and return them to the broker. The broker then combines the hit-lists according to the semantics of the query. For Boolean queries, the hit-lists are unioned, intersected, or subtracted as appropriate. For ranked free text queries, the hit-lists contain term scores that must be combined according to the semantics of the ranking formula. In comparison, document partitioning affords simpler inverted index construction and maintenance than term partitioning. Their relative performance during query processing was shown by Jeorig and Omieiinski [404] to depend on term distributions. Assuming each processor has its own I/O channel and disks, when term distributions in the documents and the queries are more skewed. PARALLEL IR        239 document partitioning performs better. When terms are uniformly distributed in user queries, term partitioning performs better. For instance, using TREC data, Ribeiro-Neto and Barbosa [673, 57] have shown that term partitioning might be twice as fast with long queries and 5-10 times faster with very short (Web-like) queries. Suffix Arrays We can apply document partitioning to suffix arrays in a straightforward fashion. As with physical document partitioning for inverted files, the document collection is divided among the P processors and each partition is treated as an independent, self-contained collection. The system can then apply the suffix array construction techniques described in Chapter 8 to each of the partitions, with the enhancement that all of the partitions are indexed concurrently. During search, the broker broadcasts the query to all of the search processes, collects the intermediate results, and merges the intermediate results into a final hit-list. If all of the documents will be kept in a single collection, we can still exploit the parallel processors to reduce indexing time. An interesting property of the suffix array construction algorithm for large texts (described in Chapter 8) is that each of the merges of partial indices is independent. Therefore all of the O((n/M)2) merges may be run in parallel on separate processors. After all merges are complete, the counters for each partial index must be accumulated and the final index merge may be performed. Term partitioning for a suffix array amounts to distributing a single suffix array over multiple processors such that each processor is responsible for a lexicographical interval of the array. During query processing, the broker distributes the query to the processors that contain the relevant portions of the suffix array and merges the results. Note that when searching the suffix array, all of the processors require access to the entire text. On a single parallel computer with shared memory (e.g., an SMP system), this is not a problem since the text may be cached in shared memory. This may be a problem, however, if shared memory is not available and communication costs are high, as is the case in a distributed system (e.g., a network of workstations). Signature Files To implement document partitioning in a system that uses signature files, the documents are divided among the processors as before and each processor generates signatures for Its document partition. At query time, the broker generates a signature for the query and distributes it to all of the parallel processors. Each processor evaluates the query signature locally as If Its document partition was a separate, self-contained collection. Then the results axe sent to the broker, which combines them into a final hit-list for the user. For Boolean queries, the final result is simply a union of the results returned from each processor.  For 240        PARALLEL AND DISTRIBUTED IR ranked queries, the ranked hit-lists are merged as described above for inverted file implementations. To apply term partitioning in a signature file-based system, we would have to use a bit-sliced signature file [627] and partition the bit slices across the processors. The amount of sequential work required to merge the intermediate results from each of the processors and eliminate false drops, however, severely limits the speedup 5 available with this organization. Accordingly, this organization is not recommended.
mir-0168	9.2.3    SSMD Architectures SIMD architectures lend themselves to a more restricted domain of problems than MIMD architectures. As such, SIMD computers are less common than MIMD computers. Perhaps the best known example of the SIMD architecture is the Thinking Machines CM-2, which has been used to support both signature file- and inverted file-based information retrieval algorithms. Each processing element in the CM-2 has a 1 bit arithmetic logic unit (ALU) and a small amount of local memory. The processing elements execute local and non-local parallel instructions. A local parallel instruction causes each processing element to perform the same operation in unison on data stored in the element's local memory. A non-local parallel instruction involves communication between the processing elements and includes operations such as summing the components of a vector or finding a global maximum. The CM-2 uses a separate front-end host computer to provide an interface to the back-end parallel processing elements. The front-end controls the loading and unloading of data in the back-end and executes serial program instructions, such as condition and iteration statements. Parallel macro instructions are sent from the front-end to a back-end microcontroller, which controls the simultaneous execution of the instruction on a set of back-end processing elements. The CM-2 provides a layer of abstraction over the back-end processors, called virtual processors. One or more virtual processors map to a single physical processor. Programs express their processing needs in terms of virtual processors, and the hardware maps virtual processor operations onto physical processors. A physical processor must sequentially perform the operations for each of its virtual processors. The ratio of virtual to physical processors is called the virtual processing ratio, VP. As VP increases, an approximately linear increase in running time occurs. Signature Files The most natural application of a SIMD computer in IR is to support signature files. Recall from Chapter 8 the basic search process for signature files. First, the search system constructs a signature for the query terms. Next, the system compares the query signature with the signature of every document in the collection and marks documents with matching signatures as potentially relevant. PARALLEL IR        241 probe_doc   (P_bit Doc_sig[],   char *term) { int       i; P__int Doc.match; Doc_match = 1; for   (i ´ 0;   i  lt; numjiashes;   i++)   i Doc.match = Doc_sig[hash  (i,   term)]; } return Doc_match; Figure 9.5    probe_doc. Finally, the system scans the full text of potentially relevant documents to eliminate false drops, ranks the matching documents, and returns the hit-list to the user. If the probability of false drops is acceptably low, the full text scan may be eliminated. Also, if the system is processing Boolean queries, it may need to generate more than one signature for the query and combine the intermediate results of each signature according to the operators used in the query. Stanfill [741] shows how this procedure can be adapted to the CM-2 (or any similar SIMD machine). The core of the procedure is the subroutine shown in Figure 9.54 This routine probes the document signature Doc.sig for the given query word term by applying each of the signature hash functions to term and ANDing together the corresponding bits in Doc.sig. The result of the AND operation is stored in Doc_match. If Doc_match is 1, term is present in Doc.sig; if Doc_match is 0, term is absent. Both Doc_sig and Doc_match are parallel variables, such that each virtual processor operates in parallel on its own copy of the variables. By loading the entire signature file onto the back-end virtual processors, all of the document signatures can be searched in parallel. This procedure must be enhanced under the following condition. If the number of words in a document |d| exceeds the number of words W that can be inserted into a document signature, then the document must be segmented into I^l/IF segments and represented by |d|/W signatures. In this case, the probe_doc routine is applied to all signatures for a document and an OR is taken over the individual signature results to obtain the final result for the document. If the false drop probability warrants scanning the full text of the documents. only those segments with matching signatures need be scanned. As soon as a qualifying segment is found, the entire document is marked as a match for the query. % The algorithms shown in this chapter are presented using a C-like pseudo-code.   Parallel data type names begin with a capital kP\ 242        PARALLEL AND DISTRIBUTED IR bool_search r	'PJbit Doc_sig[] , bquery_t query) switch (query.op) { case AND return	(bool_search (Doc_sig, query,argl)  bool_search (Doc_sig, query.arg2)); case OR: return	(bool_search (Doc_sig, query.argl) I! bool_search (Doc_sig, query.arg2)); case NOT return	(!bool_search (Doc_sig, query.argl)); case WORD: return gt; }	(probe_doc (Doc.sig, query.argl)); Figure 9.6    bool_search. A general Boolean retrieval system can be implemented on top of probe_doc with the recursive procedure shown in Figure 9.6. Here bquery.t is a recursive data type that contains two arguments and an operator. If the operator is NOT or WORD, then the second argument in the bquery_t is empty. The final return value is stored in a parallel Boolean variable, which indicates for each document whether or not that document satisfies the Boolean query. Again, if the probability of false drops associated with the signature scheme is acceptably low, the set of matching documents may be returned immediately. Otherwise, the system must perform further processing on the text of each matching document to eliminate false drops. If weights are available for the query terms, it is possible to build a ranking retrieval system on top of the parallel signature file search process. Query term weights could be supplied by the end-user when the query is created, or they could be assigned by the system using a collection statistic such as idf (see Chapter 2). The algorithm in Figure 9.7 shows how to use probe_doc to build a ranking system. In rank_search, the wqueryjt data type contains an array of query terms and an array of weights associated with those terms. First, all documents that contain the current term are identified with probe_doc. Next, the score for each of those documents is updated by adding the weight associated with the current query term (the where clause tests a parallel variable expression and activates only those processors that satisfy the expression). After all query terms have been processed, the parallel variable Doc.score contains the rank scores for all of the documents. The final step in the processing of a weighted query is to rank the scored documents by sorting and returning the top k hits. This can be accomplished in PARALLEL IR        243 rank_search (P_bit Doc_sig[], wquery_t query) { int    i; P_float Doc.score; P_bool Doc_match; Doc_score = 0; for  (i = 0;   i lt; query.num_terms;   i++)   { Docjaatch = probe_doc   (Doc_sig,  query.terms[i]): where  (Docjmatch)   { Doc_score += query.weights[i]; return  (Doc_score); gt; Figure 9.7    rank.search. a number of ways. One possibility is to use the global ranking routine provided by the CM-2, which takes a parallel variable and returns 0 for the largest value, 1 for the next largest value, etc. Applying this routine to Doc_score yields the ranked documents directly. If the number of hits returned is much less than the number of documents in the collection (k ´ JV), the global ranking function performs more work than necessary. An alternative is for the retrieval system to use the global maximum routine in an iterative process of identification and extraction. During each iteration, the system applies the global maximum routine to Doc_score to identify the current top ranked document. The document is added to the hit-list and its score in Doc_score is set to ó 1. After k iterations, the top k hits will have been entered on the hit-list. The techniques just described assume that the entire signature file fits in main memory. If this is not the case, additional steps must be taken to process the entire document collection. A straightforward approach is to process the collection in batches. A batch consists of as many document signatures as will fit in main memory at one time. Each batch is read into memory and scored using one of the above algorithms. The intermediate results are saved in an array of document scores. After all batches have been processed, the array of document scores is ranked and the final hit-list is generated. In general, processing the collection in batches performs poorly due to the I/O required to read in each batch. The performance penalty imposed by the I/O can be reduced by processing multiple queries on each batch, such that the I/O costs are amortized over multiple queries. This helps query processing throughput, but does nothing to improve query processing response time. An alternative to processing in batches is to use a parallel hit-sliced sig-nature file, proposed by Panagopoulos and Faloutsos [627] (see Chapter 8). 244        PARALLEL AND DISTRIBUTED IR docs 0 110 11 10 0 10 0 1110 10 0 10 0 0 0 rfoc5     110    0    0     1 Figure 9.8    Document signatures. Figure 9.8 shows a matrix representation of the signatures for a small document collection (N = 5). In a traditional signature file, each row of the matrix, or document signature, is stored contiguously. In a bit-sliced signature file, each column of the matrix, or bit-slice, is stored contiguously. A bit-slice is a vertical slice through the matrix, such that bit-slice i contains the i-th bit from every document signature. With this organization, the retrieval system can load just those bit-slices required by the query terms in question. Note that the file offset of bit-slice i (starting with 0) is z*iV bits, and the length of each bit-slice is iV bits. When using a bit-sliced signature file, each virtual processor is still responsible for scoring a single document. A virtual processor's local memory is used to store the bits from each bit-slice that correspond to the processor's document. A bit-slice, therefore, is distributed across the virtual processors with one bit at each processor. The set of bits across the virtual processors that corresponds to a single bit-slice is called a frame. The total number of frames is F = M/N, where M is the size of memory in bits available for storing bit-slices. When F lt; W (W is the number of bit-slices in the file), the system employs a frame replacement policy to determine which bit-slices must be resident to process the query. The frame replacement policy may simply fetch all of the bit-slices that correspond to the query terms, or it may analyze the query and identify a subset of bit-slices that, when evaluated, still provides an acceptably low false drop probability. To search the bit-sliced signature file, we must make a few modifications to our basic query processing procedures. First, the frame replacement routine must be run at the start of processing a query to insure that the required bit-slices are resident. Second, the signature hash functions must be updated to return a frame index rather than a signature bit index. The frame index is the index of the frame that contains the bit-slice corresponding to the previously computed signature bit index. Finally, the parallel bit array, Doc_sig, passed into probe^doc is replaced with the parallel bit array Frames, which provides eacii virtual processor access to its frames. Panagopoulos and Faloutsos [627] analyze the performance of the parallel bit-sliced signature file and show that query response times of under 2 seconds can be achieved on a 128 Gb database on the CM-2. Although this technique addresses the issue of query response time on large document collections, it defeats one of the often claimed advantages of the signature file organization, namely, that indexing new documents is straightforward. In a traditional signature file PARALLEL IR        245 organization, new document signatures may simply be appended to the signature file. With a bit-sliced signature file, the signature file must be inverted, resulting in update costs similar to that of an inverted file. inverted Files While the adaptation of signature file techniques to SIMD architectures is rather natural, inverted files are somewhat awkward to implement on SIMD machines. Nevertheless, Stanfill et al. [744, 740] have proposed two adaptations of inverted files for the CM-2. Recall from Chapter 8 the structure of an inverted list. In its simplest form, an inverted list contains a posting for each document in which the associated term appears. A posting is a tuple of the form (^, dj), where kz is a term identifier and dj is a document identifier. Depending on the retrieval model, postings may additionally contain weights or positional information. If positional information is stored, then a posting is created for each occurrence of k7 in dj. The first parallel inverted file implementation for the CM-2 uses two data structures to store the inverted file: a postings table and an index. The postings table contains the document identifiers from the postings and the index maps terms to their corresponding entries in the postings table. Before the postings are loaded into these structures, they are sorted by term identifier. The document identifiers are then loaded into the postings table in this sorted order, filling in a series of rows of length P, where P is the number of processors in use. The postings table is treated as a parallel array, where the array subscript selects a particular row, and each row is spread across the P processors. For each term, the index stores the locations of the first and last entries in the postings table for the set of document identifiers associated with the term. Figure 9.9 shows a small document collection, the raw postings, and the resulting postings table and index. For example, to find the documents that contain the term "piggy/ we look up 'piggy' in the index and determine that the postings table entries from row 1, position 3, to row 2, position 1, contain the corresponding document identifiers, or 0, 1, and 2. At search time these data structures are used to rank documents as follows. First, the retrieval system loads the postings table onto the back-end processors. Next, the system iterates over the query terms. For each query term, an index lookup returns the range of postings table entries that must be processed. The search system then iterates over the rows included in this range. For each row, the processors that contain entries for the current term are activated and the associated document identifiers are used to update the scores of the corresponding documents. Document scores are built up in accumulators (called mailboxes by Stanfill), which are allocated in a parallel array similar to the postings table. To update the accumulator for a particular document, we must determine the accumulator's row and position within the row. For convenience, well assume that this information (rather than document identifiers) is stored in the postings table. Furthermore, 246        PARALLEL AND DISTRIBUTED IR Documents This   little  piggy went to market. This  little  piggy stayed home. This  little   piggy had roast beef. Postings beef	2 had	2 home	1 little	0 little	1 little	2 market	0 piggy	0 piggy	1 piggy	2 roast	2 stayed	1 this	0 this	1 this	2 to	0 went	0 Index Term	First		Last Row	Pos.	Row	Pos. beef	0	0	0	0 had	0	1	0	1 home	0	2	0	2 little	0	3	1	1 market	1	2	1	2 piggy	1	3	2	1 roast	2	2	2	2 stayed	2	3	2	3 this	3	0	3	2 to	3	3	3	3 went	4	0	4	0 Postings			Table 2	2	1	0 1	2	0	0 1	2	2	1 0	1	2	0 0 Figure 9.9    Parallel inverted file. we'll assume that weights have been associated with each posting and stored in the postings table. The complete algorithm for scoring a weighted term is showTn in Figure 9.10. The score_term routine assumes that the index lookup for the query term has been done and the results were stored in term. The routine iterates over each row of postings associated with the term and determines which positions to process within the current row. Position is a parallel integer constant where the first instance contains 0, the second instance contains 1, etc., and the last instance contains N-PROCS ó 1. It is used in the where clause to activate the appropriate processors based on the positions of interest in the current row. The left-indexing performed on Doc.score at the end of the routine provides access to a particular instance of the parallel variable. This operation is significant because it involves communication between the processors. Posting weights must be shipped from the processor containing the posting to the processor containing the accumulator for the corresponding document. After the system has processed all of the query terms with score_term, it ranks the documents based on their scores and returns the top k documents. It is expensive to send posting weights to accumulators on different processors. To address this problem, Stanfill [740] proposed the partitioned postings PARALLEL IR        247 score_term  (P_float Doc_score[],  P_posting PostingG, term_t term) { int         i; int         first_pos; int         last_pos; P_int      Doc_row; P_int      Doc_pos; P.float Weight; for   (i = term.first_row;   i  lt;= term.last_row; first_pos =  (i === term.first_row ? term.first_pos   :   0); last_pos =  (i == term.last_row ? term.last_pos   :   N_PR0CS  -1); where   (Position gt;= first_pos  Position lt;= last.pos)   -C Doc_row = Posting[i].row; Doc_pos = Posting[i].pos; Weight = term.weight  * Posting[i].weight; [Doc_pos]Doc_score[Doc_row]   += Weight; Figure 9.10    score_term. file, which eliminates the communication required in the previous algorithm by storing the postings and accumulator for a given document on the same processor. There are two tricks to accomplishing this. First, as the postings are loaded into the postings table, rather than working left to right across the rows and filling each row before starting with the next one, the postings are added to the column that corresponds to the processor where the associated document will be scored. This ensures that all of the postings associated with a document will be loaded onto the same processor as the document's accumulator. Figure 9.11 (a) shows how the postings from Figure 9.9 would be loaded into a table for two processors, with documents 0 and 1 assigned to processor 0 and document 2 assigned to processor 1. Figure 9.11(a) also demonstrates a problem with this scheme. The postings for the term this are skewed and no longer span consecutive rows. To handle this situation, we apply the second trick of the partitioned postings file, which is to segment the postings such that every term in segment i is lexicographically less than or equal to every term in segment i 4-1. This is shown in Figure 9.11(b) using segments of three rows. Note how some segments may need to be padded with blank space in order to satisfy the partitioning constraints. 248        PARALLEL AND DISTRIBUTED IR home	1	beef	2 little	0	had	2 little	1	little	2 market	0	piggy	2 piggy	0	roast	2 piggy	1	this	2 stayed	1 this	0 this	1 to	0 went	0 home 1 little 0 little         1	beef 2 had 2 little      2 market     0 piggy o piggy     i	piggy    2 roast      2 stayed 1 this 0 this          1	this        2 to 0 went         0 (a) (b) Figure 9.11    Skewed and partitioned postings. Index First	Last Terni	Partition	Partition	Tag beef	0	0	0 had	0	0	1 home	0	0	2 little	0	0	3 market	1	1	0 piggy	1	1	1 | roast	1	1	2 stayed	2	2	0 this		2	1 to	3	3	0 went	3	3	1 Postings			Table 2	1	0	0 3	0	1	0 3	1	3	0 0	0	1	0 1	0	2	0 1	1 0	1	1	0 1	0 1	1 0	0 1	0 Figure 9.12   Partitioned postings file. The postings table and index undergo a few more modifications before reaching their final form, shown in Figure 9.12. First, term identifiers in the postings are replaced by term tags. The system assigns tags to terms such that no two terms in the same partition share the same tag. Second, document identifiers in the postings are replaced by document row numbers, where the row number Identifies which row contains the accumulator for the document. Since the accumulator is at the same position (i.e., processor) as the posting, the row number is sufficient to identify the document. Finally, the index is modified to record the .starting partition, ending partition, and tag for each term. DISTRIBUTED IR        249 ppf_score_term (P_float Doc_score [] , P_posting Posting [], term_t term) / \ int    i; P_int  Doc_row; P.float Weight; for (i - term.first_part	* N_R0WS; i lt; (term.last_part	+ 1) * N_R0WS; i++) { where (Posting[i].tag	== term.tag) { Doc_row = Posting [i]	.row; Weight = term.weight	* Posting[i].weight; Doc_score[Doc_row] + } } }	= Weight; Figure 9.13    ppf_s core-term. The modified term scoring algorithm is shown in Figure 9.13. Here NJLQWS is the number of rows per partition. The algorithm iterates over the rows of postings that span the term's partitions and activates the processors with matching postings. Each active processor extracts the document row from the posting, calculates the term weight, and updates the document's score. After all query terms have been processed, the system ranks the documents and returns the top k. Stanfill [740] shows that the partitioned postings file imposes a space overhead of approximately 1/3 the original text (of which 10-20% is wasted partition padding) and can support sub 2-second query response times on a terabyte of text using a 64K processor CM-2.
mir-0170	9.3.1    Introduction Distributed computing is the application of multiple computers connected by a network to solve a single problem. A distributed computing system can be viewed as a MIMD parallel processor with a relatively slow inter-processor communication channel and the freedom to employ a heterogeneous collection of processors in the system. In fact, a single processing node in the distributed system could be a parallel computer in its own right. Moreover, if they all support the same public interface and protocol for invoking their services, the computers in the system may be owned and operated by different parties. Distributed systems typically consist of a set of server processes, each running on a separate processing node, and a designated broker process responsible 250        PARALLEL AND DISTRIBUTED IR for accepting client requests, distributing the requests to the servers, collecting intermediate results from the servers, and combining the intermediate results into a final result for the client. This computation model is very similar to the MIMD parallel processing model shown in Figure 9.2. The main difference here is that the subtasks run on different computers and the communication between the subtasks is performed using a network protocol such as TCP/IP [176] (rather than, for example, shared memory-based inter-process communication mechanisms). Another significant difference is that in a distributed system it is more common to employ a procedure for selecting a subset of the distributed servers for processing a particular request rather than broadcasting every request to every server in the system. Applications that lend themselves well to a distributed implementation usually involve computation and data that can be split into coarse-grained operations with relatively little communication required between the operations. Parallel information retrieval based on document partitioning fits this profile well. In section 9.2.2 we saw how document partitioning can be used to divide the search task up into multiple, self-contained subtasks that each involve extensive computation and data processing with little communication between them. Moreover, documents are almost always grouped into collections, either for administrative purposes or to combine related documents into a single source. Collections, therefore, provide a natural granularity for distributing data across servers and partitioning the computation. Note that since term partitioning imposes greater communication overhead during query processing, it is rarely employed in a distributed system. To build a distributed IR system, we need to consider both engineering issues common to many distributed systems and algorithmic issues specific to information retrieval. The critical engineering issues involve defining a search protocol for transmitting requests and results; designing a server that can efficiently accept a request, initiate a subprocess or thread to service the request, and exploit any locality inherent in the processing using appropriate caching techniques; and designing a broker that can submit asynchronous search requests to multiple servers in parallel and combine the intermediate results into a final end user response. The algorithmic issues include how to distribute documents across the distributed search servers, how to select which servers should receive a particular search request, and how to combine the results from the different servers. The search protocol specifies the syntax and semantics of messages transmitted between clients and servers, the SÄaquence of messages required to establish a connection and carry out a search operation, and the underlying transport mechanism for sending messages (e.g., TCP/IP). At a minimum, the protocol should allow a client to: obtain information about a search server, e.g., a list of databases available for searching at the server and possibly statistics associated with the databases; DISTRIBUTED IR        251 Æ submit a search request for one or more databases using a well defined query language; Æ receive search results in a well denned format; ï retrieve items identified in the search results. For closed systems consisting of homogeneous search servers, a custom search protocol may be most appropriate, particularly if special functionality (e.g., encryption of requests and results) is required. Alternatively, a standard protocol may be used, allowing the system to interoperate more easily with other search servers. The Z39.50 [606] standard (see Chapter 4) for client/server information retrieval defines a widely used protocol with enough functionality to support most search applications. Another proposed protocol for distributed, heterogeneous search, called STARTS (Stanford Proposal for Internet Meta-Searching) [317], was developed at Stanford University in cooperation with a consortium of search product and service vendors. STARTS was designed from scratch to support distributed information retrieval and includes features intended to solve the algorithmic issues related to distributed IR, such as merging results from heterogeneous sources. The other engineering issues related to building efficient client/server systems have been covered extensively in the literature (see, for example, Comer and Stevens [176] and Zomaya [852]). Rather than review them here, we continue with a more detailed look at the algorithmic issues involved in distributed IR.
mir-0171	9.3.2    Collection Partitioning The procedure used to assign documents to search servers in a distributed IR system depends on a number of factors. First, we must consider whether or not the system is centrally administered. In a system comprising independently administered, heterogeneous search servers, the distributed document collections will be built and maintained independently. In this case, there is no central control of the document partitioning procedure and the question of how to partition the documents is essentially moot. It may be the case, however, that each independent search server is focused on a particular subject area, resulting in a semantic partitioning of the documents into distributed collections focused on particular subject areas. This situation is common in meta search systems that provide centralized access to a variety of back-end search service providers. When the distributed system is centrally administered, more options are available. The first option is simple replication of the collection across all of the search servers. This is appropriate when the collection is small enough to fit on a single search server, but high availability and query processing throughput are required. In this scenario, the parallelism in the system is being exploited via multitasking (see Figure 9.1) and the broker's job is to route queries to the search servers and balance the loads on the servers. 252        PARALLEL AND DISTRIBUTED IR Indexing the documents is handled in one of two ways. In the first method, each search server separately indexes its replica of the documents. In the second method, each server is assigned a mutually exclusive subset of documents to index and the index subsets are replicated across the search servers. A merge of the subsets is required at each search server to create the final indexes (which can be accomplished using the technique described under Document Partitioning in section 9.2.2). In either case, document updates and deletions must be broadcast to all servers in the system. Document additions may be broadcast, or they may be batched and partitioned depending on their frequency and how quickly updates must be reflected by the system. The second option is random distribution of the documents. This is appropriate when a large document collection must be distributed for performance reasons but the documents will always be viewed and searched as if they are part of a single, logical collection. The broker broadcasts every query to all of the search servers and combines the results for the user. The final option is explicit semantic partitioning of the documents. Here the documents are either already organized into semantically meaningful collections, such as by technical discipline, or an automatic clustering or categorization procedure is used to partition the documents into subject specific collections.
mir-0172	9.3.3    Source Selection Source selection is the process of determining which of the distributed document collections are most likely to contain relevant documents for the current query, and therefore should receive the query for processing. One approach is to always assume that every collection is equally likely to contain relevant documents and simply broadcast the query to all collections. This approach is appropriate when documents are randomly partitioned or there is significant semantic overlap between the collections. When document collections are partitioned into semantically meaningful collections or it is prohibitively expensive to search every collection every time, the collections can be ranked according to their likelihood of containing relevant documents. The basic technique is to treat each collection as if it were a single large document, index the collections, and evaluate the query against the collections to produce a ranked listing of collections. We can apply a standard cosine similarity measure using a query vector and collection vectors. To calculate a term weight in the collection vector using tf-idf style weighting (see Chapter 2), term frequency tf-hJ is the total number of occurrences of term i in collection j, and the inverse document frequency idft for term i is log(JV/nj), where N is the total number of collections and nt is the number of collections in which terin i appears. A danger of this approach is that although a particular collection may receive a high query relevance score, there may not be individual documents within the collection that receive a high query relevance score, essentially resulting in a false drop and unnecessary work to score the collection. Moffat and Zobel [574] DISTRIBUTED IR        253 propose avoiding this problem by indexing each collection as a series of blocks, where each block contains B documents. When B equals 1, this is equivalent to indexing all of the documents as a single, monolithic collection. When B equals the number of documents in each collection, this is equivalent to the original solution. By varying £?, a tradeoff is made between collection index size and likelihood of false drops. An alternative to searching a collection index was proposed by Voorhees [792], who proposes using training queries to build a content model for the distributed collections. When a new query is submitted to the system, its similarity to the training queries is computed and the content model is used to determine which collections should be searched and how many hits from each collection should be returned.
mir-0173	9.3.4    Query Processing Query processing in a distributed IR system proceeds as follows: (1)  Select collections to search. (2)  Distribute query to selected collections. (3)  Evaluate query at distributed collections in parallel. (4)  Combine results from distributed collections into final result. As described in the previous section, Step 1 may be eliminated if the query is always broadcast to every document collection in the system. Otherwise, one of the previously described selection algorithms is used and the query is distributed to the selected collections. Each of the participating search servers then evaluates the query on the selected collections using its own local search algorithm. Finally, the results are merged. At this point we have covered everything except how to merge the results. There are a number of scenarios. First, if the query is Boolean and the search servers return Boolean result sets, all of the sets are simply unioned to create the final result set. If the query involves free-text ranking, a number of techniques are available ranging from simple/naive to complex/accurate. The simplest approach is to combine the ranked hit-lists using round robin interleaving. This is likely to produce poor quality results since hits from irrelevant collections are given status equal to that of hits from highly relevant collections. An improvement on this process is to merge the hit-lists based on relevance score. As with the parallel process described for Document Partitioning in section 9.2.2, unless proper global term statistics are used to compute the document scores, we may get incorrect results. If documents are randomly distributed such that global term statistics are consistent across all of the distributed collections, the merging based on relevance score is sufficient to maintain retrieval effectiveness. If, however, the distributed document collections are 254        PARALLEL AND DISTRIBUTED IR semantically partitioned or maintained by independent parties, then reranking must be performed. Callan [139] proposes reranking documents by weighting document scores based on their collection similarity computed during the source selection step. The weight for a collection is computed as w = 1+ | C \ -(s ó s)/s, where | C | is the number of collections searched, s is the collection's score, and s is the mean of the collection scores. The most accurate technique for merging ranked hit-lists is to use accurate global term statistics. This can be accomplished in one of a variety of ways. First, if the collections have been indexed for source selection, that index will contain global term statistics across all of the distributed collections. The broker can include these statistics in the query when it distributes the query to the remote search servers. The servers can then account for these statistics in their processing and produce relevance scores that can be merged directly. If a collection index is unavailable, query distribution can proceed in two rounds of communication. In the first round, the broker distributes the query and gathers collection statistics from each of the search servers. These statistics are combined by the broker and distributed back to the search servers in the second round. Finally, the search protocol can require that search servers return global query term statistics and per-document query term statistics [317, 441]. The broker is then free to rerank every document using the query term statistics and a ranking algorithm of its choice. The end result is a hit-list that contains documents from the distributed collections ranked in the same order as if all of the documents had been indexed in a single collection.
mir-0174	9.3.5    Web Issues Information retrieval on the World Wide Web is covered extensively in Chapter 13. For completeness, we briefly mention here how parallel and distributed information retrieval applies to the Web. The most direct application is to gather all of the documents on the Web into a single, large document collection. The parallel and distributed techniques described above can then be used directly as if the Web were any other large document collection. This is the approach currently taken by most of the popular Web search services. Alternatively, we can exploit the distributed system of computers that make up the Web and spread the work of collecting, organizing, and searching all of the documents. This is the approach taken by the Harvest system [108]. Harvest comprises a number of components for gathering, summarizing, replicating, distributing, and searching documents. User queries are processed by brokers, which collect and refine information from gatherers and other brokers. The information at a particular broker is typically related to a restricted set of topics, allowing users to direct their queries to the most appropriate brokers. A central broker registry helps users find the best brokers for their queries (see Figure 13.4). TRENDS AND RESEARCH ISSUES        255
mir-0175	9.4    Trends and Research Issues Parallel computing holds great potential for tackling the performance and scale issues associated with the large and growing document collections currently available online. In this chapter we have surveyed a number of techniques for exploiting modern parallel architectures. The trend in parallel hardware is the development of general MIMD machines. Coincident with this trend is the availability of features in modern programming languages, such as threads and associated synchronization constructs, that greatly facilitate the task of developing programs for these architectures. In spite of this trend, research in parallel IR algorithms on MIMD machines is relatively young, with few standard results to draw on. Much of the early work in parallel IR was aimed at supporting signature files on SIMD architectures. Although SIMD machines are well suited to processing signature files, both SIMD machines and signature files have fallen out of favor in their respective communities. SIMD machines are difficult to program and are well suited to a relatively small class of problems. As Chapter 8 points out, signature files provide poor support for document ranking and hold few, if any, advantages over inverted files in terms of functionality, index size, and processing speed [851]. Distributed computing can be viewed as a form of MIMD computing with relatively high interprocessor communication costs. Most of the parallel IR algorithms discussed in this chapter, however, have a high ratio of computation to communication, and are well suited to both symmetric multiprocessor and distributed implementations. In fact, by using an appropriate abstraction layer for inter-process communication, we can easily implement a parallel system that works well on both multiprocessor and distributed architectures with relatively little modification. Many challenges remain in the area of parallel and distributed text retrieval. While we have presented a number of approaches in this chapter, none stand out as the definitive solution for building parallel or distributed information retrieval systems. In addition to the continued development and investigation of parallel indexing and search techniques for systems based on inverted files and suffix arrays, two specific challenges stand out. The first challenge is measuring retrieval effectiveness on large text collections. Although we can easily measure the speedup achieved by a given parallel system, measuring the quality of the results produced by that system is another story. This challenge, of course, is not unique to parallel IR systems. Large collections pose problems particularly when it comes to generating relevance judgments for queries. The pooling techniques used in TREC (see Chapter 3) may not work. There, ranked result lists are combined from multiple systems to produce a relatively small set of documents for human evaluation. The assumption is that most, if not all, of the relevant documents will be included in the pool. With large collections, this assumption may not hold. Moreover, it is unclear how important recall is in this context. The second significant challenge is interoperability, or building distributed IR systems from heterogeneous components. The need for distributed systems 256        PARALLEL AND DISTRIBUTED IR comprising heterogeneous back-end search servers is clear from the popularity of meta search services on the Web. The functionality of these systems is limited, however, due to the lack of term statistics from the back-end search servers, which would otherwise allow for accurate reranking and result list merging. Moreover, each search server employs its own, custom query language, opening up the possibility that the original intent of the query is lost when it is translated to the back-end query languages. Protocol standardization efforts, such as STARTS [317], attempt to address these problems, but commitment to these standards by the entire community of search providers is required.
mir-0176	9.5    Bibliographic Discussion A thorough overview of parallel and distributed computing can be found in the Parallel and Distributed Computing Handbook [852], edited by Albert Zomaya. Many interesting research papers specific to parallel and distributed information systems can be found in the proceedings of the IEEE International Conference on Parallel and Distributed Information Systems. Stanfill et al [742, 744, 740] are responsible for much of the early work using massively parallel hardware (in particular, the Thinking Machines Connection Machine) to solve IR problems. Pogue and Willet [645] also explored massively parallel IR using the ICL Distributed Array Processor. Salton and Buckley [701] provide some interesting comments on the early implementations of parallel IR, challenging both their speed and effectiveness. Lu et al [524] analyze how to properly scale SMP hardware for parallel IR and emphasize the importance of proper hardware balance. Investigations into parallel and distributed inverted file implementations have been performed by Tomasic and Garcia-Molina [762, 763, 764], Jeong and Omiecinski [404], and Ribeiro-Neto and Barbosa [673]. Parallel and distributed algorithms for suffix array construction and search have been explored by Navarro et al. [591]. Given P processors and total text of size n, they obtain average indexing times that are O(n/P logn) CPU time and 0{n/P) communication time. Macleod et al [535] offer a number of strategies and tips for building distributed information retrieval systems. Cahoon and McKinley [137] analyze the performance of the Inquery distributed information retrieval system. Source selection and collection fusion issues have been investigated by Gra-vano et al using the G1OSS system [318], Voorhees et al [792], Callan et al [139], Moffat arid Zobel [574], Viles and French [787], and others. Acknowledgements The author gratefully acknowledges the support of IBM.
mir-0178	10.1    Introduction This chapter discusses user interfaces for communication between human information seekers and information retrieval systems. Information seeking is an imprecise process. When users approach an information access system they often have only a fuzzy understanding of how they can achieve their goals. Thus the user interface should aid in the understanding and expression of information needs. It should also help users formulate their queries, select among available information sources, understand search results, and keep track of the progress of their search. The human-computer interface is less well understood than other aspects of information retrieval, in part because humans are more complex than computer systems, and their motivations and behaviors are more difficult to measure and characterize. The area is also undergoing rapid change, and so the discussion in this chapter will emphasize recent developments rather than established wisdom. The chapter will first outline the human side of the information seeking process and then focus on the aspects of this process that can best be supported by the user interface. Discussion will encompass current practice and technology, recently proposed innovative ideas, and suggestions for future areas of development. Section 10.2 outlines design principles for human-computer interaction and introduces notions related to information visualization, section 10.3 describes information seeking models, past and present. The next four sections describe user interface support for starting the search process, for query specification, for viewing retrieval results in context, and for interactive relevance feedback. The last major section, section 10.8, describes user interface techniques to support the information access process as a whole. Section 10.9 speculates on future developments and Section 10.10 provides suggestions for further reading. Figure 10.1 presents the flow of the chapter contents. 257 258        USER INTERFACES AND VISUALIZATION Introduction HCI Background The Information Access Process Starting Points Query Specification Context Using Relevance Judgements Interface Support 4 Conclusions Figure 10.1     The flow of this chapter's contents.
mir-0179	10.2    Human-Computer Interaction What makes an effective human-computer interface? Ben Shneiderman, an expert in the field, writes [725, p. 10]: Well designed, effective computer systems generate positive feelings of success, competence, mastery, and clarity in the user community. When an interactive system is well-designed, the interface almost disappears, enabling users to concentrate on their work, exploration, or pleasure. As steps towards achieving these goals, Shneiderman lists principles for design of user interfaces. Those which are particularly important for information access include (slightly restated): provide informative feedback, permit easy reversal of actions, support an internal locus of control, reduce working memory load, and provide alternative interfaces for novice and expert users. Each of these principles should be instantiated differently depending on the particular interface application.  Below we discuss those principles that are of special interest to information access systems.
mir-0180	10.2.1    Design Principles Offer informative feedback This principle is especially important for information access interfaces. In this chapter we will see current ideas about how to provide HUMAN-COMPUTER INTERACTION        259 users with feedback about the relationship between their query specification and documents retrieved, about relationships among retrieved documents, and about relationships between retrieved documents and metadata describing collections. If the user has control of how and when feedback is provided, then the system provides an internal locus of control Reduce working memory load. Information access is an iterative process, the goals of which shift and change as information is encountered. One key way information access interfaces can help with memory load is to provide mechanisms for keeping track of choices made during the search process, allowing users to return to temporarily abandoned strategies, jump from one strategy to the next, and retain information and context across search sessions. Another memory-aiding device is to provide browsable information that is relevant to the current stage of the information access process. This includes suggestions of related terms or metadata, and search starting points including lists of sources and topic lists. Provide alternative interfaces for novice and expert users. An important tradeoff in all user interface design is that of simplicity versus power. Simple interfaces are easier to learn, at the expense of less flexibility and sometimes less efficient use. Powerful interfaces allow a knowledgeable user to do more and have more control over the operation of the interface, but can be time-consuming to learn and impose a memory burden on people who use the system only intermittently. A common solution is to use a 'scaffolding' technique [684]. The novice user is presented with a simple interface that can be learned quickly and that provides the basic functionality of the application, but is restricted in power and flexibility. Alternative interfaces are offered for more experienced users, giving them more control, more options, and more features, or potentially even entirely different interaction models. Good user interface design provides intuitive bridges between the simple and the advanced interfaces. Information access interfaces must contend with special kinds of simplicity/power tradeoffs. One such tradeoff is the amount of information shown about the workings of the search system itself. Users who are new to a system or to a particular collection may not know enough about the system or the domain associated with the collection to make choices among complex features. They may not know how best to weight terms, or in the case of relevance feedback, not know what the effects of reweighting terms would be. On the other hand, users that have worked with a system and gotten a feeling for a topic are likely to be able to choose among suggested terms to add to their query in an informed manner. Determining how much information to show the user of the system is a major design choice in information access interfaces.
mir-0181	10.2.2    The Role of Visualization The tools of computer interface design are familiar to most computer users today: windows, menus, icons, dialog boxes, and so on. These make use of bitmapped display and computer graphics to provide a oiore accessible Interface 260        USER INTERFACES AND VISUALIZATION than command-line-based displays. A less familiar but growing area is that of information visualization, which attempts to provide visual depictions of very large information spaces. Humans are highly attuned to images and visual information [769, 456, 483]. Pictures and graphics can be captivating and appealing, especially if well designed. A visual representation can communicate some kinds of information much more rapidly and effectively than any other method. Consider the difference between a written description of a person's face and a photograph of it, or the difference between a table of numbers containing a correlation and a scatter plot showing the same information. The growing prevalence of fast graphics processors and high resolution color monitors is increasing interest in information visualization. Scientific visualization, a rapidly advancing branch of this field, maps physical phenomena onto two- or three-dimensional representations [433]. An example of scientific visualization is a colorful image of the pattern of peaks and valleys on the ocean floor; this provides a view of physical phenomena for which a photograph cannot (currently) be taken. Instead, the image is constructed from data that represent the underlying phenomena. Visualization of inherently abstract information is more difficult, and visualization of textually represented information is especially challenging. Language is our main means of communicating abstract ideas for which there is no obvious physical manifestation. What does a picture look like that describes negotiations over a trade agreement in which one party demands concessions on environmental policies while the other requires help in strengthening its currency? Despite the difficulties, researchers are attempting to represent aspects of the information access process using information visualization techniques. Some of these will be described later in this chapter. Aside from using icons and color highlighting, the main information visualization techniques include brushing and Unking [233, 773], panning and zooming [71], focus-plus-context [502], magic leimts [95], and the use of animation to retain context and help make occluded information visible [676, 143]. These techniques support dynamic, interactive use. Interactivity seems to be an especially important property for visualizing abstract information, although it has not played as large a role within scientific visualization. Brushing and linking refers to the connecting of two or more views of the same data, such that a change to the representation in one view affects the representation in the other views as well. For example, say a display consists of two parts: a histogram and a list of titles. The histogram shows, for a set of documents, how many documents were published each year. The title list shows the titles for the corresponding documents. Brushing and linking would allow the user to assign a color, say red, to one bar of the histogram, thus causing the titles in the list display that were published during the corresponding year to also be highlighted in red. Panning and zooming refers to the actions of a movie camera that can scan sideways across a scene (panning) or move in for a closeup or back away to get a wider view (zooming).   For example, text clustering can be used to show a HUMAN-COMPUTER INTERACTION        261 top-level view of the main themes in a document collection (see Figures 10.7 and 10.8). Zooming can be used to move 'closer,' showing individual documents as icons, and then zoom in closer still to see the text associated with an individual document. When zooming is used, the more detail that is visible about a particular item, the less can be seen about the surrounding items. Focus-plus-context is used to partly alleviate this effect. The idea is to make one portion of the view ó the focus of attention ó larger, while simultaneously shrinking the surrounding objects. The farther an object is from the focus of attention, the smaller it is made to appear, like the effect seen in a fisheye camera lens (also in some door peepholes). Magic lenses are directly manipulable transparent windows that, when overlapped on some other data type, cause a transformation to be applied to the underlying data, thus changing its appearance (see Figure 10.13). The most straightforward application of magic lenses is for drawing tasks, and it is especially useful if used as a two-handed interface. For example, the left hand can be used to position a color lens over a drawing of an object. The right hand is used to mouse-click on the lens, thus causing the appearance of the underlying object to be transformed to the color specified by the lens. Additionally, there are a large number of graphical methods for depicting trees and hierarchies, some of which make use of animation to show nodes that would otherwise be occluded (hidden from view by other nodes) [286, 364, 407, 478, 676]. It is often useful to combine these techniques into an interface layout consisting of an overview plus details [321, 644]. An overview, such as a table-of-contents of a large manual, is shown in one window. A mouse-click on the title of the chapter causes the text of the chapter itself to appear in another window, in a linking action (see Figure 10.19). Panning and zooming or focus-plus-context can be used to change the view of the contents within the overview window.
mir-0182	10.2.3    Evaluating Interactive Systems From the viewpoint of user interface design, people have widely differing abilities, preferences, and predilections. Important differences for information access interfaces include relative spatial ability and memory, reasoning abilities, verbal aptitude, and (potentially) personality differences [227, 725]. Age and cultural differences can contribute to acceptance or rejection of interface techniques [557]. An interface innovation can be useful and pleasing for some users, and foreign and cumbersome for others. Thus software design should allow for flexibility in interaction style, and new features should not be expected to be equally helpful for all users. An important aspect of human-computer interaction is the methodology for evaluation of user interface techniques. Precision and recall measures have been widely used for comparing the ranking results of non-Interactive systems, but are less appropriate for assessing interactive systems [470]. The standard evaluations 262        USER INTERFACES AND VISUALIZATION emphasize high recall levels; in the TREC tasks systems are compared to see how well they return the top 1000 documents (see chapter 3). However, in many interactive settings, users require only a few relevant documents and do not care about high recall to evaluate highly interactive information access systems, useful metrics beyond precision and recall include: time required to learn the system, time required to achieve goals on benchmark tasks, error rates, and retention of the use of the interface over time. Throughout this chapter, empirical results of user studies are presented whenever they are available. Empirical data involving human users is time consuming to gather and difficult to draw conclusions from. This is due in part to variation in users' characteristics and motivations, and in part to the broad scope of information access activities. Formal psychological studies usually only uncover narrow conclusions within restricted contexts. For example, quantities such as the length of time it takes for a user to select an item from a fixed menu under various conditions have been characterized empirically [142], but variations in interaction behavior for complex tasks like information access are difficult to account for accurately. Nielsen [605] advocates a more informal evaluation approach (called heuristic evaluation) in which user interface affordances are assessed in terms of more general properties and without concern about statistically significant results.
mir-0183	10.3    The Information Access Process A person engaged in an information seeking process has one or more goals in mind and uses a search system as a tool to help achieve those goals. Goals requiring information access can range quite widely, from finding a plumber to keeping informed about a business competitor, from writing a publishable scholarly article to investigating an allegation of fraud. Information access tasks are used to achieve these goals. These tasks span the spectrum from asking specific questions to exhaustively researching a topic. Other tasks fall between these two extremes. A study of business analysts [614] found three main kinds of information seeking tasks: monitoring a well known topic over time (such as researching competitors' activities each quarter), following a plan or stereotyped series of searches to achieve a particular goal (such as keeping up to date on good business practices), and exploring a topic in an undirected fashion (m when getting to know an unfamiliar industry). Although the goals differ, there is a common core revolving around the information seeking component, which is our focus here.
mir-0184	10.3.1    Models of interaction Most accounts of the information access process assume an interaction cycle consisting of query specification, receipt and examination of retrieval results, and then either stopping or reformulating the query and repeating the process THE INFORMATION ACCESS PROCESS        263 until a perfect result set is found [700, 726]. In more detail, the standard process can be described according to the following sequence of steps (see Figure 10.2): (1)  Start with an information need. (2)  Select a system and collections to search on. (3)  Formulate a query. (4)  Send the query to the system. (5)  Receive the results in the form of information items. (6)  Scan, evaluate, and interpret the results. (7)  Either stop, or, (8)  Reformulate the query and go to step 4. This simple interaction model (used by Web search engines) is the only model that most information seekers see today. This model does not take into account the fact that many users dislike being confronted with a long disorganized list of retrieval results that do not directly address their information needs. It also contains an underlying assumption that the user's information need is static and the information seeking process is one of successively refining a query until it retrieves all and only those documents relevant to the original information need. Information Need Query Send to System Reformulate Receive Results Evaluate Results No ^ Done? \ Stop Figure 10.2 processes. A simplified diagram of the standard model of the information access 264        USER INTERFACES AND VISUALIZATION In actuality, users learn during the search process. They scan information, read the titles in result sets, read the retrieved documents themselves, viewing lists of topics related to their query terms, and navigating within hyperlinked Web sites. The recent advent of hyperlinks as a pivotal part of the information seeking process makes it no longer feasible to ignore the role of scanning and navigation within the search process itself. In particular, today a near-miss is much more acceptable than it was with bibliographic search, since an information seeker using the Web can navigate hyperlinks from a near-miss in the hopes that a useful page will be a few links away. The standard model also downplays the interaction that takes place when the user scans terms suggested as a result of relevance feedback, scans thesaurus structures, or views thematic overviews of document collections. It de-emphasizes the role of source selection, which is increasingly important now that, for the first time, tens of thousands of information collections are immediately reachable for millions of people. Thus, while useful for describing the basics of information access systems, this simple interaction model is being challenged on many fronts [65, 614, 105, 365, 192]. Bates [65] proposes the 'berry-picking' model of information seeking, which has two main points. The first is that, as a result of reading and learning from the information encountered throughout the search process, the users' information needs, and consequently their queries, continually shift. Information encountered at one point in a search may lead in a new, unanticipated direction. The original goal may become partly fulfilled, thus lowering the priority of one goal in favor of another. This is posed in contrast to the assumption of 'standard' information retrieval that the user's information need remains the same throughout the search process. The second point is that users' information needs are not satisfied by a single, final retrieved set of documents, but rather by a series of selections and bits of information found along the way. This is in contrast to the assumption that the main goal of the search process is to hone down the set of retrieved documents into a perfect match of the original information need. The berry-picking model is supported by a number of observational studies [236, 105], including that of O'Day and Jeffries [614]. They found that the information seeking process consisted of a series of interconnected but diverse searches on one problem-based theme. They also found that search results for a goal tended to trigger new goals, and hence search in new directions, but that the context of the problem and the previous searches was carried from one stage of search to the next. They also found that the main value of the search resided in the accumulated learning and acquisition of information that occurred during the search process, rather than in the final results set. Thus, a user interface for information access should allow users to reassess their goals and adjust their search strategy accordingly. A related situation occurs when users encounter a 'trigger' that causes them to pursue a different strategy temporarily, perhaps to return to the current unfinished activity at a later time. An implication of these observations is that the user interface should support search strategies by making it easy to follow trails with unanticipated results. This can be accomplished in part by supplying ways to record the progress THE INFORMATION ACCESS PROCESS        265 of the current strategy and to store, find, and reload intermediate results, and by supporting pursuit of multiple strategies simultaneously. The user interface should also support methods for monitoring the status of the current strategy in relation to the user's current task and high-level goals. One way to cast the activity of monitoring the progress of a search strategy relative to a goal or subgoal is in terms of a cost/benefit analysis, or an analysis of diminishing returns [690]. This kind of analysis assumes that at any point in the search process, the user is pursuing the strategy that has the highest expected utility. If, as a consequence of some local tactical choices, another strategy presents itself as being of higher utility than the current one, the current one is (temporarily or permanently) abandoned in favor of the new strategy. There are a number of theories and frameworks that contrast browsing, querying, navigating, and scanning along several dimensions [75, 159, 542, 804]. Here we assume that users scan information structure, be it titles, thesaurus terms, hyperlinks, category labels, or the results of clustering, and then either select a displayed item for some purpose (to read in detail, to use as input to a query, to navigate to a new page of information) or formulate a query (either by recalling potential words or by selecting categories or suggested terms that have been scanned). In both cases, a new set of information is then made viewable for scanning. Queries tend to produce new, ad hoc collections of information that have not been gathered together before, whereas selection retrieves information that has already been composed or organized. Navigation refers to following a chain of links, switching from one view to another, toward some goal, in a sequence of scan and select operations. Browsing refers to the casual, mainly undirected exploration of information structures, and is usually done in tandem with selection, although queries can also be used to create subcollections to browse through. An important aspect of the interaction process is that the output of one action should be easily used as the input to the next.
mir-0185	10.3.2    Non-Search Parts of the Information Access Process The O'Day and Jeffries study [614] found that information seeking is only one part of the full work process their subjects were engaged in. In between searching sessions many different kinds of work was done with the retrieved information, including reading and annotating [617] and analysis. O'Day and Jeffries examined the analysis steps in more detail, finding that 80% of this work fell into six main types: finding trends, making comparisons, aggregating information, identifying a critical subset, assessing, and interpreting. The remaining 20% consisted of cross-referencing, summarizing, finding evocative visualizations for reports, and miscellaneous activities. The Sensemaking work of Russell et al. [690] also discusses information work as a process in which information retrieval plays only a small part. They observe that most of the effort made in Sensemaking is in the synthesis of a good representation, or ways of thinking about, the problem at hand. They describe the process of formulating and crystallizing the important concepts for a given task. 266        USER INTERFACES AND VISUALIZATION From these observations it is convenient to divide the entire information access process into two main components: search/retrieval, and analysis /synthesis of results. User interfaces should allow both kinds of activity to be tightly interwoven, However, analysis/synthesis are activities that can be done independently of information seeking, and for our purposes it is useful to make a distinction between the two types of activities.
mir-0186	10.3.3    Earlier Interface Studies The bulk of the literature on studies of human-computer information seeking behavior concerns information intermediaries using online systems consisting of bibliographic records (e.g., [546, 707, 104]), sometimes with costs assessed per time unit. Unfortunately, many of the assumptions behind those studies do not reflect the conditions of modern information access [335, 222]. The differences include the following: ï  The text being searched now is often full text rather than bibliographic citations.   Because users have access to full text, rather than document surrogates, it is more likely that simple queries will find relevant answers directly as part of the search process. ï  Modern systems use statistical ranking (which is more effective when abstracts and full text are available than when only titles and citations are available) whereas most studies were performed on Boolean systems. ï  Much of modern searching is done by end users, many new to online searching, rather than professional intermediaries, which were the focus of many of the earlier studies. ï  Tens of thousands of sources are now available online on networked information systems, and many are tightly coupled via hyperlinks, as opposed to being stored in separate collections owned by separate services.   Earlier studies generally used systems in which moving from one collection to another required prior knowledge of the collections and considerable time and effort to switch. A near miss is much more useful in this hyperlinked environment than in earlier systems, since hyperlinks allow users to navigate from the near miss directly to the source containing information of interest. In a card catalog environment, where documents are represented as isolated units, a near miss consists of finding a book in the general area of interest and then going to the bookshelf in the library to look for related books, or obtaining copies of many issues of a journal and scanning for related articles. ï  Finally, most users have access to bit-mapped displays allowing for direct manipulation, or at least form fiUin. Most earlier studies arid bibliographic systems were implemented on TTY displays, which require command-line based syntax and do a poor job of retaining context. STARTING POINTS        267 Despite these significant differences, some general information seeking strategies have been identified that seem to transfer across systems. Additionally, although modern systems have remedied many of the problems of earlier online public access catalogs, they also introduce new problems of their own.
mir-0187	10.4    Starting Points Search interfaces must provide users with good ways to get started. An empty screen or a blank entry form does not provide clues to help a user decide how to start the search process. Users usually do not begin by creating a long, detailed expression of their information need. Studies show that users tend to start out with very short queries, inspect the results, and then modify those queries in an incremental feedback cycle [22]. The initial query can be seen as a kind of 'testing the water' to see what kinds of results are returned and get an idea of how to reformulate the query [804, 65]. Thus, one task of an information access interface is to help users select the sources and collections to search on. For example, there are many different information sources associated with cancer, and there are many different kinds of information a user might like to know about cancer. Guiding the user to the right set of starting points can help with the initial problem formulation. Traditional bibliographic search assumes that the user begins by looking through a list of names of sources and choosing which collections to search on, while Web search engines obliterate the distinctions between sources and plunge the user into the middle of a Web site with little information about the relationship of the search hit to the rest of the collection. In neither case is the interface to the available sources particularly helpful. In this section we will discuss four main types of starting points: lists, overviews, examples, and automated source selection.
mir-0188	10.4.1    Lists of Collections Typical online systems such as LEXIS-NEXIS require users to begin any inquiry with a scan through a long list of source names and guess which ones will be of interest. Usually little information beyond the name of the collection is provided online for these sources (see Figure 10.3). If the user is not satisfied with the results on one collection, they must reissue the query on another collection. Frequent searchers eventually learn a set of sources that axe useful for their domains of interest, either through experience, formal training, or recommendations from friends and colleagues. Often-used sources can be stored on a 'favorites1 list, also known as a bookmark list or a hotlist on the Web. Recent research explores the maintenance of a personalized information profile for users or work groups, based on the kinds of information they've used in the past [277]. However, when users want to search outside their domains of expertise, a list of familiar sources is not sufficient. Professional searchers such as librarians 268        USER INTERFACES AND VISUALIZATION % Boolean ] B. FREESTYLE ] h Legal Services) Lgok in Source Favorite Sources _, BYTE(NEWS,BYTE1 Computer/Communication N ews, Current(CM PCO M ,CU R NWS ] 7] Computer/Communication Stones[CMPCOMALLNWS)  - Sign ontol£XIS-M=M$ to continue pTe\4otis reseatcKstaft print rgquects ached Figure 10,3     The LEXIS-NEXIS source selection screen. learn through experience and years of training which sources are appropriate for various information needs. The restricted nature of traditional interfaces to information collections discourages exploration and discovery of new useful sources. However, recently researchers have devised a number of mechanisms to help users understand the contents of collections as a way of getting started in their search.
mir-0189	10.4.2    Overviews Faced with a large set of text collections, how can a user choose which to begin with? One approach is to study an overview of the contents of the collections. An overview can show the topic domains represented within the collections, to help users select or eliminate sources from consideration. An overview can help users get started, directing them into general neighborhoods, after which they can navigate using more detailed descriptions. Shneiderman [724] advocates an interaction model in which the user begins with an overview of the information to be worked with, then pans and zooms to find areas of potential interest, and then view details. The process is repeated as often as necessary. Three types of overviews are discussed in this subsection. The first is display and navigation of large topical category hierarchies associated with the documents of a collection. The second is automatically derived overviews, usually created by unsupervised clustering techniques on the text of documents, that attempt to extract overall characterizing themes from collections. The third type STARTING POINTS        269 of overview is that created by applying a variant of co-citation analysis on connections or links between different entities within a collection. Other kinds of overviews are possible, for example, showing graphical depictions of bookshelves or piles of books [681, 46]. Category or Directory Overviews There exist today many large online text collections to which category labels have been assigned. Traditional online bibliographic systems have for decades assigned subject headings to books and other documents [752]. MEdigital libraryINE, a large collection of biomedical articles, has associated with it Medical Subject Headings (MeSH) consisting of approximately 18,000 categories [523]. The Association for Computing Machinery (ACM) has developed a hierarchy of approximately 1200 category (keyword) labels.f Yahoo! [839], one of the most popular search sites on the World Wide Web, organizes Web pages into a hierarchy consisting of thousands of category labels. The popularity of Yahoo! and other Web directories suggests that hierarchically structured categories are useful starting points for users seeking information on the Web. This popularity may reflect a preference to begin at a logical starting point, such as the home page for a set of information, or it may reflect a desire to avoid having to guess which words will retrieve the desired information. (It may also reflect the fact that directory services attempt to cull out low quality Web sites.) The meanings of category labels differ somewhat among collections. Most are designed to help organize the documents and to aid in query specification. Unfortunately, users of online bibliographic catalogs rarely use the available subject headings [335, 222], Hancock-Beaulieu and Drabenstott and Weller, among others, put much of the blame on poor (command line-based) user interfaces which provide little aid for selecting subject labels and require users to scroll through long alphabetic lists. Even with graphical Web interfaces, finding the appropriate place within a category hierarchy can be a time-consuming task, and once a collection has been found using such a representation, an alternative means is required for searching within the site itself. Most interfaces that depict category hierarchies graphically do so by associating a document directly with the node of the category hierarchy to which it has been assigned. For example, clicking on a category link in Yahoo! brings up a list of documents that have been assigned that category label Conceptually, the document is stored within the category label. When navigating the results of a search in Yahoo!, the user must look through a list of category labels and guess which one is most likely to contain references to the topic of interest. A wrong path requires backing up and trying again, and remembering which pages contain which information. If the desired information is deep in the hierarchy, or t http://www.acm.org/class 270        USER INTERFACES AND VISUALIZATION Figure 10.4 [453]. The MeSHBrowse interface for viewing category labels hierarchically not available at all, this can be a time-consuming and frustrating process. Because documents are conceptually stored 'inside' categories, users cannot create queries based on combinations of categories using this interface. It is difficult to design a good interface to integrate category selection into query specification, in part because display of category hierarchies takes up large amounts of screen space. For example, Internet Grateful Medt is a Web-based service that allows an integration of search with display and selection of MeSH category labels. After the user types in the name of a potential category label, a long list of choices is shown in a page. To see more information about a given label the user selects a link (e.g., Radiation Injuries). The causes the context of the query to disappear because a new Web page appears showing the ancestors of the term and its immediate descendants. If the user attempts to see the siblings of the parent term (Wounds and Injuries) then a new page appears that changes the context again. Radiation Injuries appears as one of many siblings and its children can no long be seen. To go back to the query, the illustration of the category hierarchy disappears. The MeSHBrowse system [453] allows users to interactively browse a subset of semantically associated links in the MeSH hierarchy. From a given starting point, clicking on a category causes the associated categories to be displayed In a two-dimensional tree representation. Thus only the relevant subset of the http: // iga. sin, sib. go*?: 80/ STARTING POINTS 271 + HIBROWSE foi EMBASE Add a View  Anatomy		Organisms   Disease  Diagnosis   Therapy				Chernov	sarKlDfugs   Pharmacology '?#*´( Arid* ortfaic							em M Broaden}	Close	Ma 1266i|physical disease    £			:©?       1			jmpy AND child   [ï£			12661|diild AND therapy  \~. C3	313	abdominal diseas		1		2468	therapy (in geij*;			8411      child (in genera.". 189	abnormal body bu				11	acupuncture       __		la	1      brain damaged ch: D	34	breast disease				1976	biological thei		la	15      handicapped chili £3	2209	cardiovascular d.		£3		724	cancer therapy		Cj	3696      infant £d	248	connective tissui				2	computer assist		i	1855      preschool child £3	2395	digestive system				481	conseruatiue tt			2371      school child £3	774	ear nose  throat lt;		D		116	counseling £3	1195	endocrine disease				58	detoxification £3	848	eye disease		£3		171	disease control D	764	head and neck di:				8467	drug therapy £3	2648	hematologic dise.		2j		8282	drug  therapy ß	376	mouth  disease				53	adjuvant thei CD £3	3236	neurologic  disealt;				88 300	antiDiotic pi antibfptic tl anticoagulant anticonuulsai 2527	respiratory trad				19 60 £]	1509	skin disease				15	antihyperten; £]	48	soft tissue dise.				**6	antimicrobia. £]	56	thorax disease				72	bone marrow ' £d	1537	urogenital tract				15	chelation thi 1	chemical sym| ¶j		14	chenoprophyl. £3		629	chemotherapy 4		5	diuretic thei ^i Sviewf** Figure 10.5     The HiBrowse interface for viewing category labels hierarchically and according to facets [646]. hierarchy is shown at one time, making browsing of this very large hierarchy a more tractable endeavor. The interface has the space limitations inherent in a two-dimensional hierarchy display and does not provide mechanisms for search over an underlying document collection. See Figure 10.4. The HiBrowse system [646] represents category metadata more efficiently by allowing users to display several different subsets of category metadata simultaneously. The user first selects which attribute type (or facet, as attributes are called in this system) to display. For example, the user may first choose the ïphysical disease' value for the Disease facet. The categories that appear one level below this are shown along with the number of documents that contain each category. The user can then select other attribute types, such as Therapy and Groups (by age). The number of documents that contain attributes from all three types are shown. If the user now selects a refinement of one of the categories, such as the 'child' value for the Groups attribute, then the number of documents that contain all three selected facet types are shown. At the same time, the number of documents containing the subcategories found below 'physical disease" and 'therapy (general)' are updated to reflect this more restricted specification. See Figure 10.5. A problem with the HiBrowse system is that it requires users to navigate through tiie category hierarchy, rather than specify queries directly. In other words, query specification is not tightly coupled with display of category metadata. As a solution to some of these problems, the Cat-a-Cone interface [358' will be described in section 10.8. 272        USER INTERFACES AND VISUALIZATION Automatically Derived Collection Overviews Many attempts to display overview information have focused on automatically extracting the most common general themes that occur within the collection. These themes are derived via the use of unsupervised analysis methods, usually variants of document clustering. Clustering organizes documents into groups based on similarity to one another; the centroids of the clusters determine the themes in the collections. The Scatter/Gather browsing paradigm [203, 202] clusters documents into topically-coherent groups, and presents descriptive textual summaries to the user. The summaries consist of topical terms that characterize each cluster generally, and a set of typical titles that hint at the contents of the cluster. Informed by the summaries, the user may select a subset of clusters that seem to be of most interest, and recluster their contents. Thus the user can examine the contents of each subcollection at progressively finer granularity of detail. The reclustering is computed on-the-fly; different themes are produced depending on the documents contained in the subcollection to which clustering is applied. The choice of clustering algorithm influences what clusters are produced, but no one algorithm has been shown to be particularly better than the rest when producing the same number of clusters [816]. A user study [640] showed that the use of Scatter/Gather on a large text collection successfully conveys some of the content and structure of the corpus. However, that study also showed that Scatter/Gather without a search facility was less effective than a standard similarity search for finding relevant documents for a query. That is, subjects allowed only to navigate, not to search over, a hierarchical structure of clusters covering the entire collection were less able to find documents relevant to the supplied query than subjects allowed to write queries arid scan through retrieval results. It is possible to integrate Scatter/Gather with conventional search technology by applying clustering on the results of a query to organize the retrieved documents (see Figure 10.6). An offline experiment [359] suggests that clustering may be more effective if used in this manner. The study found that documents relevant to the query tend to fall mainly into one or two out of five clusters, if the clusters are generated from the top-ranked documents retrieved in response to the query. The study also showed that precision and recall were higher within the best cluster than within the retrieval results as a whole. The implication is that a user might save time by looking at the contents of the cluster with the highest proportion of relevant documents and at the same time avoiding those clusters with mainly non-relevant documents. Thus, clustering of retrieval results may be useful for helping direct users to a subset of the retrieval results that contain a large proportion of the relevant documents. General themes do seem to arise from document clustering, but the themes are highly dependent on the makeup of the documents within the clusters [$59, 357]. The unsupervised nature of clustering can result in a display of topics at varying levels of description. For example, clustering a collection of documents about computer science might result in clusters containing documents about STARTING POINTS        273 0  Star-Spstttgtei Banner, The O  Klt;y, Francis Smtt G  FortMcHettiy O  AIiH :     f * / i***i'ª*'´**^^ O  Blimp, Bleat O  Stanwyck, Bartam O  Berle^MMtoit O  2Sutor, Addph J k O star O Galaxy* The O O intostdlar mattar Figure 10.6     Display of Scatter/Gather clustering retrieval results [203]. artificial intelligence, computer theory, computer graphics, computer architecture, programming languages, government, and legal issues. The latter two themes are more general than the others, because they are about topics outside the general scope of computer science. Thus clustering can results in the juxtaposition of very different levels of description within a single display. Scatter/Gather shows a textual representation of document clusters. Researchers have developed several approaches to map documents from their high dimensional representation in document space into a 2D representation in which each document is represented as a small glyph or icon on a map or within an abstract 2D space. The functions for transforming the data into the lower dimensional space differ, but the net effect is that each document is placed at one point in a scatter-plot-like representation of the space. Users are meant to detect themes or clusters in the arrangement of the glyphs. Systems employing such graphical displays include BEAD [156], the Galaxy of News [671], and ThemeScapes [821]. The TherneScapes view imposes a three-dimensional representation on the results of clustering (see Figure 10.7). The layout makes use of "negative space' to help emphasize the areas of concentration where the clusters occur. Other systems display inter-document similarity hierarchically [529, 14], while still others display retrieved documents in networks based on inter-document similarity [262, 761]. Kohonen's feature map algorithm has been use^d to create maps that graphically characterize the overall content of a document collection or subcollection [520, 163] (see Figure 10.8). The regions of the 2D map vary in size and shape corresponding to how frequently documents assigned to the corresponding themes occur within the collection. Regions are characterized by single words or phrases. 274        USER INTERFACES AND VISUALIZATION Figure 10.7     A three-dimensional overview based on document clustering [821]. and adjacency of regions is meant to reflect semantic relatedriess of the themes within the collection. A cursor moved over a document region causes the titles of the documents most strongly associated with that region to be displayed in a pop-up window. Documents can be associated with more than one region. Evaluations of Graphical Overviews Although intuitively appealing, graphical overviews of large document spaces have yet to be shown to be useful and understandable for users. In fact, evaluations that have been conducted so far provide negative evidence as to their usefulness. One study found that for non-expert users the results of clustering were difficult to use, and that graphical depictions (for example, representing clusters with circles and lines connecting documents) were much harder to use than textual representations (for example, showing titles and topical words, as in Scatter "'Gather,?, because documents' contents are difficult to discern without actually reading some text [443]. Another recent study compared the Kohonen feature map overview representation on a browsing task to that of Yahoo! [163]. For one (if the tasks, subjects were asked to find an interesting" Web page within the entertainment category of Yahoo! and of an organization of the same Web pages into a Ko-lionen map layout. The experiment varied whether subjects started in Yahoo! or in the t*raphical map. After completion of the browing task, subjects were asked to attempt to repeat the browse using the other tool. For the subjects that STARTING POINTS 275 tRfiBKt ´*		J;			¶rtii		C84 ..	¶am				1				: t +"SGf´IN£ +dª  HIM						1		ftTffcf				ªª lt;	-ttCSCT w h - lt; ?¶WCHIW			?ª	**			:v. * |-------	3 'i.	+t j. LET				1H ICi	**"   .,^ ,m  Figure 10.8      A two-dimensional overview created using a Kohonen feature map learning algorithm on Web pages having to do with the topic Entertainment [163]. began with the Kohonen map visualization, 11 out of 15 found an interesting page within ten minutes.  Eight of these were able to find the same page using Yahoo!. Of the subjects who started with Yahoo!, 14 out of 16 were able to find interesting home pages. However, only two of the 14 were able to find the page in the graphical map display! This is strong evidence against the navigability of the display and certainly suggests that the simple label view provided by Yahoo! is more useful. However, the map display may be more useful if the system is modified to tightly integrate querying with browsing. The subjects did prefer some aspects of the map representation. In particular, some liked the ease of being able to jump from one area to another without having to back up as is required in Yahoo!, and some liked the fact that the maps have varying levels of granularity. The subjects disliked several aspects of the display. The experimenters found that some subjects expressed a desire for a visible hierarchical organization, others wanted an ability to zoom in on a sub-area to get more detail, and some users disliked having to look through the entire map to find a theme, desiring an alphabetical ordering instead. Many found the single-term labels to be misleading, in part because they were ambiguous (one region called 'BILL* was thought to correspond to a person's name rather than count ing money). The authors concluded that tills interface is more appropriate for casual browsing than for search. In general, unsupervised thematic overviews are perhaps must useful for giving users a "gist' of the kinds of information that can be 276        USER INTERFACES AND VISUALIZATION found within the document collection, but generally have not been shown to be helpful for use in the information access process. Co-citation Clustering for Overviews Citation analysis has long been recognized as a way to show an overview of the contents of a collection [812]. The main idea is to determine 'centrally-located' documents based on co-citation patterns. There are different ways to determine citation patterns: one method is to measure how often two articles are cited together by a third. Another alternative is to pair articles that cite the same third article. In both cases the assumption is that the paired articles share some commonalities. After a matrix of co-citations is built, documents are clustered based on the similarity of their co-citation patterns. The resulting clusters are interpreted to indicate dominant themes within the collection. Clustering can focus on the authors of the documents rather than the contents, to attempt to identify central authors within a field. This idea has recently been implemented using Web-based documents in the Referral Web project [432]. The idea has also been applied to Web pages, using Web link structure to identify major topical themes among Web pages [485, 639]. A similar idea, but computed a different way, is used to explicitly identify pages that act as good starting points for particular topics (called "authority pages' by Kleinberg [444]).
mir-0190	10.4.3    Examples, Dialogs, and Wizards Another way to help users get started is to start them off with an example of interaction with the system. This technique is also known as retrieval by reformulation. An early version of this idea is embodied in the Rabbit system [818] which provides graphical representations of example database queries.  A general framework for a query is shown to the user who then modifies it to construct a partially complete description of what they want. The system then shows an example of the kind of information available that matches this partial description. For instance, if a user searching a computer products database indicates an interest in disks, an example item is retrieved with its disk descriptors filled in. The user can use or modify the displayed descriptors, and iterate the procedure. The idea of retrieval by reformulation has been developed further and extended to the domains of user interface development [581] and software engineering [669]. The Helgon system [255] is a modern variant of this idea applied to bibliographic database information. In Helgon, users begin by navigating a hierarchy of topics from which they select structured examples, according to their interests. If a feature of an example is inappropriately set, the user can modify the feature to Indicate how it would appear in the desired information. Unfortunately, in tests with users, the system was found to be problematic. Users had problems with the organization of the hierarchy, and found that searching for a useful example by critiquing an existing one to be tedious. This result STARTING POINTS        277 underscores an unfortunate difficulty with examples and dialogues: that of getting the user to the right starting dialogue or the right example strategy becomes a search problem in itself. (How to index prior examples is studied extensively in the case-based reasoning (CBR) literature [492, 449].) A more dynamic variation on this theme is the interactive dialog. Dialog-based interfaces have been explored since the early days of information retrieval research, in an attempt to mimic the interaction provided by a human search intermediary (e.g., a reference librarian). Oddy did early work in the THOMAS system, which provided a question and answer session within a command-line-based interface [615]. More recently, Belkin et al have defined quite elaborate dialog interaction models [75] although these have not been assessed empirically to date. The digital libraryITE system interface [192] uses an animated focus-plus-context dialog as a way to acquaint users with standard sequences of operations within the system. Initially an outline of all of the steps of the dialog is shown as a list. The user can expand the explanation of any individual step by clicking on its description. The user can expand out the entire dialog to see what questions are coming next, and then collapse it again in order to focus on the current tactic. A more restricted form of dialog that has become widely used in commercial products is that of the wizard. This tool helps users in time-limited tasks, but does not attempt to overtly teach the processes required to complete the tasks. The wizard presents a step-by-step shortcut through the sequence of menu choices (or tactics) that a user would normally perform in order to get a job done, reducing user input to just a few choices with default settings [636]. A recent study [145] found wizards to be useful for goals that require many steps, for users who lack necessary domain knowledge (for example, a restaurant owner installing accounting software), and when steps must be completed in a fixed sequence (for example, a procedure for hiring personnel). Properties of successful wizards included allowing users to rerun a wizard and modify their previous work, showing an overview of the supported functions, and providing lucid descriptions and understandable outcomes for choices. Wizards were found not to be helpful when the interface did not solve a problem effectively (for example, a commercial wizard for setting up a desktop search index requests users to specify how large to make the index, but supplies no information about how to make this decision). Wizards were also found not to be helpful when the goal was to teach the user how to use the interface, and when the wizard was not user-tested. It maybe the case that information access is too variable a process for the use of wizards. A guided tour leads a user through a sequence of navigational choices through hypertext links, presenting the nodes in a logical order for some goal. In a dynamic tour, only relevant nodes are displayed, as opposed to the static case where the author decides what is relevant before the users have even formulated their queries [329]. A recent application is the Walden Paths project which enables teachers to define instructional!}* useful paths through pagers found nn the Web [289].   This approach has not been commonly used to date for 278        USER INTERFACES AND VISUALIZATION information access but could be a promising direction for acquainting users with search strategies in large hyperlinked systems.
mir-0191	10.4.4    Automated Source Selection Human-computer interfaces for helping guide users to appropriate sources is a wide open area for research. It requires both eliciting the information need from users and understanding which needs can be satisfied by which sources. An ambitious approach is to build a model of the source and of the information need of the user and try to determine which fit together best. User modeling systems and intelligent tutoring systems attempt to do this both for general domains [204, 814] and for online help systems [378]. A simpler alternative is to create a representation of the contents of information sources and match this representation against the query specification. This approach is taken by G1OSS, a system which tries to determine in advance the best bibliographic database to send a search request to, based on the terms in the query [765]. The system uses a simple analysis of the combined frequencies of the query words within the individual collections. The SavvySearch system [383] takes this idea a step further, using actions taken by users after a query to decide how to decrease or increase the ranking of a search engine for a particular query (see also Chapter 13). The flip side to automatically selecting the best source for a query is to automatically send a query to multiple sources and then combine the results from the various systems in some way. Many metasearch engines exist on the Web. How to combine the results effectively is an active area of research, sometimes known as collection fusion [63, 767, 388].
mir-0192	10,5    Query Specification To formulate a query, a user must select collections, metadata descriptions, or information sets against which the query is to be matched, and must specify words, phrases, descriptors, or other kinds of information that can be compared to or matched against the information in the collections. As a result, the system creates a set of documents, metadata, or other information type that match the query specification in some sense and displays the results to the user in some form. Shneiderman [725] identifies five primary human-computer interaction styles. These are: command language, form fillin, menu selection, direct manipulation, and natural language.^ Each technique lias been used in query specification interfaces and each has advantages and disadvantages. These are described below in the context of Boolean query specification. ß  This list limits non-visual modalities, such as audio. QUERY SPECIFICATION        279
mir-0193	10.5.1    Boolean Queries In modern information access systems the matching process usually employs a statistical ranking algorithm. However, until recently most commercial full-text systems and most bibliographic systems supported only Boolean queries. Thus the focus of many information access studies has been on the problems users have in specifying Boolean queries. Unfortunately, studies have shown time and again that most users have great difficulty specifying queries in Boolean format and often misjudge what the results will be [111, 322, 558, 841]. Boolean queries are problematic for several reasons. Foremost among these is that most people find the basic syntax counter-intuitive. Many English-speaking users assume everyday semantics are associated with Boolean operators when expressed using the English words AND and OR, rather than their logical equivalents. To inexperienced users, using AND implies the widening of the scope of the query, because more kinds of information are being requested. For instance, 'dogs and cats' may imply a request for documents about dogs and documents about cats, rather than documents about both topics at once. "Tea or coffee' can imply a mutually exclusive choice in everyday language. This kind of conceptual problem is well documented [111, 322, 558, 841]. In addition, most query languages that incorporate Boolean operators also require the user to specify complex syntax for other kinds of connectors and for descriptive metadata. Most users are not familiar with the use of parentheses for nested evaluation, nor with the notions associated with operator precedence. By serving a massive audience possessing little query-specification experience, the designers of World Wide Web search engines have had to come up with more intuitive approaches to query specification. Rather than forcing users to specify complex combinations of ANDs and ORs, they allow users to choose from a selection of common simple ways of combining query terms, including "all the words' (place all terms in a conjunction) and 'any of the words1 (place all terms in a disjunction). Another Web-based solution is to allow syntactically-based query specification, but to provide a simpler or more intuitive syntax. The '-}-' prefix operator gained widespread use with the advent of its use as a mandatory specifier in the Altavista Web search engine. Unfortunately, users can be misled to think it is an infix AND rather than a prefix mandatory operator, and thus assume that 'cat + dog1 will only retrieve articles containing both terms (where in fact this query requires dog but allows cat to be optional). Another problem with pure Boolean systems is they do not rank the retrieved documents according to their degree of match to the query. In the pure Boolean framework a document either satisfies the query or it does not. Commercial systems usually resort to ordering documents according to some kind of descriptive metadata, usually in reverse chronological order. (Since these systems usually index timely data corresponding to newspaper and news wires, date of publication is often one of the most salient features of the document.) Web-based systems usually rank order the results of Boolean queries using statistical algorithms and Web-specific heuristics. 280        USER INTERFACES AND VISUALIZATION 10.5.2    From Command Lines to Forms and Menus Aside from conceptual misunderstandings of the logical meaning of AND and OR, another part of the problem with pure Boolean query specification in online bibliographic systems is the arbitrariness of the syntax and the contextlessness nature of the TTY-based interface in which they are predominantly available. Typically input is typed at a prompt and is of a form something like the following: COMMAND ATTRIBUTE value  {BOOLEAN-OPERATOR  ATTRIBUTE value}* e.g., FIND PA darwin AND TW species OR TW descent or FIND TW Mt St. Helens AND DATE 1981 (These examples are derived from the syntax of the telnet interface to the University of California Melvyl system [526].) The user must remember the commands and attribute names, which are easily forgotten between usages of the system [553]. Compounding this problem, despite the fact that the command languages for the two main online bibliographic systems at UC Berkeley have different but very similar syntaxes, after more than ten years one of the systems still reports an error if the author field is specified as PA instead of PN, as is done in the other system. This lack of flexibility in the syntax is characteristic of interfaces designed to suit the system rather than its users. The new Web-based version of Melvyl || provides form fillin and menu selection so the user no longer has to remember the names and types of attributes available. Users select metadata types from listboxes and attributes are shown explicitly, allowing selection as an alternative to specification. For example, the 'search type1 field is adjacent to an entry form in which users can enter keywords, and a choice between AND and NOT is provided adjacent to a list of the available document types (editorial, feature, etc.). Only the metadata associated with a given collection is shown in the context of search over that collection. (Unfortunately the system is restricted to searching over only one database at a time. It does however provide a mechanism for applying a previously executed search to a new database.) See Figure 10.9. The Web-based version of Melvyl also allows retention of context between searches, storing prior results in tables and hyper linking these results to lists containing the retrieved bibliographic information. Users can also modify any of the previously submitted queries by selecting a checkbox beside the record of the query. The graphical display makes explicit and immediate many of the powerful options of the system that most users would not learn using the command-line version of the interface. Bit-mapped displays are an improvement over command-line interface, but do not solve all the problems. For example, a blank entry form is in some ways http:/ /www.melvyl.ucop.edu/ ï$*$%***   Jk Looafio´|http //192 35 215185/nWmwcgi home QUERY SPECIFICATION        281 ;Database Current Contents Author Search: Current Contents database Author                     ............................       ........................ IPersonal Profile* Off Jswanson,  d. Options and Limits Another Author] and J| j Journal Title     fand~]ß| T (eg.jones, ed) (e g , Wilson, r) (eg, daedalus or jama) Any words lt;"" Escact beginning lt;" Complete title Location Send questions, comments, or suggestions to rndwl@,www mdwl ucop edu MehrylÆ is a registered trademark of The Regents of the University of California Figure 10.9     A view of query specification in the Web-based version of the Melvyi bibliographic catalog. Copyright © 1998, The Regents of the University of California. not much better than a TTY prompt, because it does not provide the user with clues about what kinds of terms should be entered.
mir-0194	10.5.3    Faceted Queries Yet another problem with Boolean queries is that their strict interpretation tends to yield result sets that are either too large, because the user includes many terms in a disjunct, or are empty, because the user conjoins terms in an effort to reduce the result set. This problem occurs in large part because the user does not know the contents of the collection or the role of terms within the collection. A common strategy for dealing with this problem, employed in systems with command-line-based interfaces like DIALOG'S, is to create a series of short queries, view the number of documents returned for each, and combine those queries that produce a reasonable number of results. For example, in DIALOG, each query produces a resulting set of documents that is assigned an identifying name. Rather than returning a list of titles themselves, DIALOG shows the set number with a listing of the number of matched documents. Titles can be shown by specifying the set number and issuing a command to show the titles. Document sets that are not empty can be referred to by a set name and combined with AND operations to produce new sets. If this set in turn is too small, the user can back up and try a different combination of sets, and this process is repeated in pursuit of producing a reasonably sized document set. This kind of query formulation is often called a faceted query, to indicate that the user's query is divided into topics or facets, each of which should be 282        USER INTERFACES AND VISUALIZATION present in the retrieved documents [553, 348]. For example, a query on drugs for the prevention of osteoporosis might consist of three facets, indicated by the disjuncts (osteoporosis OR 'bone loss') (drugs OR Pharmaceuticals) (prevention OR cure) This query implies that the user would like to view documents that contain all three topics. A technique to impose an ordering on the results of Boolean queries is what is known as post-coordinate or quorum-level ranking [700, Ch. 8]. In this approach, documents are ranked according to the size of the subset of the query terms they contain. So given a query consisting of 'cats,' 'dogs,' 'fish,' and 'mice/ the system would rank a document with at least one instance of 'cats/ wdogs/ and 'fish1 higher than a document containing 30 occurrences of 'cats' but no occurrences of the other terms. Combining faceted queries with quorum ranking yields a situation intermediate between full Boolean syntax and free-form natural language queries. An interface for specifying this kind of interaction can consist of a list of entry lines. The user enters one topic per entry line, where each topic consists of a list of semantically related terms that are combined in a disjunct. Documents that contain at least one term from each facet are ranked higher than documents containing terms only from one or a few facets. This helps ensure that documents which contain discussions of several of the user's topics are ranked higher than those that contain only one topic. By only requiring that one term from each facet be matched, the user can specify the same concept in several different ways in the hopes of increasing the likelihood of a match. If combined with graphical feedback about which subsets of terms matched the document, the user can see the results of a quorum ranking by topic rather than by word. Section 10.6 describes the TileBars interface which provides this type of feedback. Tiiis idea can be extended yet another step by allowing users to weight each facet. More likely to be readily usable, however, is a default weighting in which the facet listed highest is assigned the most weight, the second facet is assigned less weight, and so on, according to some distribution over weights.
mir-0195	10.5.4    Graphical Approaches to Query Specification Direct manipulation interfaces provide an alternative to command-line syntax. The properties of direct manipulation are [725, p.205]: (1) continuous representation of the object of interest, (2) physical actions or button presses instead of complex syntax, and (3) rapid incremental reversible operations whose impact on the object of interest is immediately visible. Direct manipulation interfaces often evoke enthusiasm from users, and for this reason alone it is worth exploring their use. Although they are not without drawbacks, they are easier to use than other methods for many users in many contexts. QUERY SPECIFICATION        283 Search for any documents in "HCI Bibliography" containing either Query and Boolean, or Graphical, Searching and Browsing, but not Rankino ÆHCI Bibliography match the selected query Graphical  Presentation of Boolean Expressions  in a    fl.  Richard Query Processing   In a Heterogeneous Retrieval Heta    Patricia Simpson On Extending the Uector- Space Oodel  for Boolean Qu    S.  K. ti.  Uong,  U    Slarko,  U,  U    Ragbavon, PEN    lleng fi Direct rtanipulot Ion Interface for Boolean Inform    Peter G.  flnlck,  Jeffrey 0.   Brennon,  Rex H,  "lynn,  Dawi J Figure 10.10 tion [417]. The VQuery Venn diagram visualization for Boolean query specificaSeveral variations of graphical interfaces, both directly manipulable and static, have been developed for simplifying the specification of Boolean syntax. User studies tend to reveal that these graphical interfaces are more effective in terms of accuracy and speed than command-language counterparts. Three such approaches are described below. Graphical depictions of Venn diagrams have been proposed several times as a way to improve Boolean query specification. A query term is associated with a ring or circle and intersection of rings indicates conjunction of terms. Typically the number of documents that satisfy the various conjuncts are displayed within the appropriate segments of the diagram. Several studies have found such interfaces more effective than their command-language-based syntax [417, 368, 558]. Hertzum and Prokjaer found that a simple Venn diagram representation produced faster and more accurate results than a Boolean query syntax. However, a problem with this format is the limitations on the complexity of the expression. For example, a maximum of three query terms can be ANDed together in a standard Venn diagram. Innovations have been designed to get around this problem, as seen in the VQuery system [417] (see Figure 10.10). In VQuery, a direct manipulation interface allows users to assign any number of query terms to ovals. If two or more ovals are placed such that they overlap with one another, and if the user selects the area of their intersection, an AND is implied among those terms. (In Figure 10.10, the term "Query' is conjoined with 'Boolean'.) If the user selects outside the area of intersection but within the ovals, an OR is implied among the corresponding terms. A NOT operation 284        USER INTERFACES AND VISUALIZATION Figure 10.11     The filter-flow visualization for Boolean query specification [841]. is associated with any term whose oval appears in the active area of the display but which remains unselected (in the figure, NOT 'Ranking' has been specified). An active area indicates the current query; all groups of ovals within the active area are considered part of a conjunction. Ovals containing query terms can be moved out of the active area for later use. Young and Shneiderman [841] found improvements over standard Boolean syntax by providing users with a direct manipulation filter-flow model. The user is shown a scrollable list of attribute types on the left-hand side and selects attributes from another list of attribute types shown across the top of the screen. Clicking on an attribute name causes a list box containing values for those attributes to be displayed in the main portion of the screen. The user then selects which values of the attributes to let the flow go through. Placing two or more of these attributes in sequence creates the semantics of a conjunct over the selected values. Placing two or more of these in parallel creates the semantics of a disjunct. The number of documents that match the query at each point is indicated by the width of the 'water* flowing from one attribute to the next. (See Figure 10.11. () A conjunct can reduce the amount of flow. The items that match the full query are shown on the far right-hand side. A user study found that fewer errors were made using the filter flow model than a standard SQL database query. However, the examples and study pertain only to database querying rather than information access, since the possible query terms for information access cannot he represented realistically in a scrollable list. This interface could perhaps be modified to better suit information access applications by having the user supply initial query terms, and using the attribute selection facility to show those terms QUERY SPECIFICATION        285 STARS:Query Reformulation Workspace File      Terms Help from	I! tape bu---------II	under scratch tape version 5 JDl Apply Changes  j| Display Tides | Figure 10.12 [21]. A block-oriented diagram visualization for Boolean query specification that are conceptually related to the query terms. Another alternative is to use this display as a category metadata selection interface (see Section 10.4). Anick et al [21] describe another innovative direct manipulation interface for Boolean queries. Initially the user types a natural language query which is automatically converted to a representation in which each query term is represented within a block. The blocks are arranged into rows and columns (See Figure 10.12). If two or more blocks appear along the same row they are considered to be ANDed together. Two or more blocks within the same column are ORed. Thus the user can represent a technical term in multiple ways within the same query, providing a kind of faceted query interface. For example, the terms 'version 5\ "version 5-0', and "vS1 might be shown in the same column. Users can quickly experiment with different combinations of terms "within Boolean queries simply by activating and deactivating blocks. This facility also allows users to have multiple representations of the same term in different places throughout the display, thus allowing rapid feedback on the consequences of specifying various combinations of query terms. Informal evaluation of the system found that users were able to learn to manipulate the interface quickly and enjoyed using it. It was not formally compared to other interaction techniques [21]. This interface provides a kind of query preview: a low cost, rapid turnaround visualization of the results of many variations on a query [643]. Another example of query previewing can be found in some help systems, which show all the words in the index whose first letters match the characters that the user has typed so far. The more characters typed, the fewer possible matches become available. The HiBrowse system described above (646] also provides a kind of preview for viewing category hierarchies and facets, showing how many documents would be matched if a category one level below the current one were selected. It perhaps could be improved by showing the consequences of more combinations of categories in an animated manner. If based on prior action and interests of the user, query previewing may become more generally applicable for information access interfaces. 286 USER INTERFACES AND VISUALIZATION Average annual pay, 1991 ?  Anaheim, CA Long Beach* CA ?  Los Angeles, CA D Riverside, CA ¶ San Diego, C A Santa Ana, CA  n Figure  10.13 Fishkin). A magic lens interface for query specification (courtesy  of Ken A final example of a graphical approach to query specification is the use of niagic lenses. Fishkin and Stone have suggested an extension to the usage of this visualization tool for the specification of Boolean queries [256]. Information is represented as lists or icons within a 2D space. Lenses act as filters on the document set. (See Figure 10.13.) For example, a word can be associated with a transparent lens. When this lens is placed over an iconic representation of a set of documents, it can cause all documents that do not contain a given word to disappear. If a second lens representing another word is then laid over the first, the lenses combine to act as a conjunction of the two words with the document set, hiding any documents that do not contain both words. Additional information can be adjusted dynamically, such as a minimum threshold for how often the term occurs in the documents, or an on-off switch for word stemming. For example. Figure 10.13 shows a disjunctive query that finds cities with relatively low housing prices or high annual salaries. One lens 'calls out' a clump of southern California cities, labeling each. Above that is a lens screening for cities with average house price below $194,821 (the data is from 1990), and above this one is a leiib screening for cities with average annual pay above $28,477. This approach, while promising, has not been evaluated in an information access setting.
mir-0196	10.5.5    Phrases and Proximity hi general, proximity iiifurmation can be quite1 effective at improving precision of  On the Web, the difference between a single-word query and a two-word QUERY SPECIFICATION        287 exact phrase match can mean the difference between an unmanageable mess of retrieved documents and a short list with mainly relevant documents. A large number of methods for specifying phrases have been developed. The syntax in LEXIS-NEXIS requires the proximity range to be specified with an infix operator. For example, 'white w/3 house' means 'white within 3 words of house, independent of order.' Exact proximity of phrases is specified by simply listing one word beside the other, separated by a space. A popular method used by Web search engines is the enclosure of the terms between quotation marks. Shneiderman et al. [726] suggest providing a list of entry labels, as suggested above for specifying facets. The difference is, instead of a disjunction, the terms on each line are treated as a phrase. This is suggested as a way to guide users to more precise query specification. The disadvantage of these methods is that they require exact match of phrases, when it is often the case (in English) that one or a few words comes between the terms of interest. For example, in most cases the user probably wants 'president' and 'lincoln' to be adjacent, but still wants to catch cases of the sort 'President Abraham Lincoln.' Another consideration is whether or not stemming is performed on the terms included in the phrase. The best solution may be to allow users to specify exact phrases but treat them as small proximity ranges, with perhaps an exponential fall-off in weight in terms of distance of the terms. This has been shown to be a successful strategy7 in non-interactive ranking algorithms [174]. It has also been shown that a combination of quorum ranking of faceted queries with the restriction that the facets occur within a small proximity range can dramatically improve precision of results [356, 566].
mir-0197	10.5.6    Natural Language and Free Text Queries Statistical ranking algorithms have the advantage of allowing users to specify queries naturally, without having to think about Boolean or other operators. But they have the drawback of giving the user less feedback about and control over the results. Usually the result of a statistical ranking is the listing of documents and the association of a score, probability, or percentage beside the title. Users are given little feedback about why the document received the ranking it did and what the roles of the query terms are. This can be especially problematic if the user is particularly interested in one of the query terms being present. One search strategy that can help with this particular problem with statistical ranking algorithms is the specification of 'mandatory' terms within the natural language query. This in effect helps the user control which terms are considered important, rather than relying on the ranking algorithm to correctly weight the query terms. But knowing to include a mandatory specification requires the user to know about a particular command and how it works. The preceding discussion assumes that a natural language query entered by the user is treated as a bag of words, with stopwords removed, for the purposes of document match. However, some systems attempt to parse natural language queries in order to extract concepts to match against concepts in the 288        USER INTERFACES AND VISUALIZATION text collection [399, 552, 748]. Alternatively, the natural language syntax of a question can be used to attempt to answer the question. (Question answering in information access is different than that of database management systems, since the information desired is encoded within the text of documents rather than specified by the database schema.) The Murax system [463] determines from the syntax of a question if the user is asking for a person, place, or date. It then attempts to find sentences within encyclopedia articles that contain noun phrases that appear in the question, since these sentences are likely to contain the answer to the question. For example, given the question Who was the Pulitzer Prize-winning novelist that ran for mayor of New York City?,' the system extracts the noun phrases 'Pulitzer Prize,' 'winning novelist,' kmayor,' and 'New York City.' It then looks for proper nouns representing people's names (since this is a 'who' question) and finds, among others, the following sentences: The Armies of the Night (1968), a personal narrative of the 1967 peace march on the Pentagon, won Mailer the Pulitzer Prize and ª        the National Book Award. In 1969 Mailer ran unsuccessfully as an independent candidate for mayor of New York City. Thus the two sentences link together the relevant noun phrases and the system hypothesizes (correctly) from the title of the article in which the sentences appear that Norman Mailer is the answer. Another approach to automated question answering is the FAQ finder system which matches question-style queries against question-answer pairs on various topics [130]. The system uses a standard IR search to find the most likely FAQ (frequently asked questions) files for the question and then matches the terms in the question against the question portion of the question-answer pairs. A less automated approach to question answering can be found in the Ask Jeeves system [34]. This system makes use of hand-picked Web sites and matches these to a predefined set of question types. A user^s query is first matched against the question types. The user selects the most accurate rephrase of their question and this in turn is linked to suggested Web sites. For example, the question "Who is the leader of Sedan?1 is mapped into the question type 4Who is the head of state of X (Sudan)?1 where the variable is replaced by a listbox of choices, with Sudan the selected choice in this case. This is linked to a Web page that lists current heads of state. The system also automatically substitutes in the name 'Sudan* in a query against that Web page, thus bringing the answer directly to the user's attention. The question is also sent to standard Web search engines. However, a system is only as good as its question templates. For example a question "Where can I find reviews of spas in Calistoga?1 matches the question * Where can 1 find X (reviews) of activities for children aged Y (1)?' and "Where can I find a concise encyclopedia article on X (hot springs)?' CONTEXT        289
mir-0198	10.6    Context This section discusses interface techniques for placing the current document set in the context of other information types, in order to make the document set more understandable. This includes showing the relationship of the document set to query terms, collection overviews, descriptive metadata, hyperlink structure, document structure, and to other documents within the set.
mir-0199	10.6.1    Document Surrogates The most common way to show results for a query is to list information about documents in order of their computed relevance to the query. Alternatively, for pure Boolean ranking, documents are listed according to a metadata attribute, such as date. Typically the document list consists of the document's title and a subset of important metadata, such as date, source, and length of the article. In systems with statistical ranking, a numerical score or percentage is also often shown alongside the title, where the score indicates a computed degree of match or probability of relevance. This kind of information is sometimes referred to as a document surrogate. See Figure 10.14 from [824]. Some systems provide users with a choice between a short and a detailed view. The detailed view typically contains a summary or abstract. In bibliographic systems, the author-written or service-written abstract is shown. Web search engines automatically generate excerpts, usually extracting the first few lines of non-markup text in the Web page. In most interfaces, clicking on the document's title or an iconic representation of the document shown beside the title will bring up a view of the document itself, either in another window on the screen, or replacing the listing of search results. (In traditional bibliographic systems, the full text was unavailable online, and only bibliographic records could be readily viewed.)
mir-0200	10.6.2    Query Term Hits Within Document Content In systems in which the user can view the full text of a retrieved document, it is often useful to highlight the occurrences of the terms or descriptors that match those of the user's query. It can also be useful for the system to scroll the view of the document to the first passage that contains one or more of the query terms, and highlight the matched terms in a contrasting color or reverse video. This display is thought to help draw the user's attention to the parts of the document most likely to be relevant to the query. Highlighting of query terms lias been found time and again to be a useful feature for information access interfaces [481], [542, p.31]. Color highlighting has also recently been found to be useful for scanning lists of bibliographic records [52]. 290        USER INTERFACES AND VISUALIZATION Jjfc yi#i^!http//w mm   tuiuimm    help    feebmok. COMPUTER SCIEN TECHNICAL REPOR mm 1 WiltY RESULTS'                   ****** ^ q^^ ignore upper/bwer case differences, ignore word endings Terms  must appear within the same report 1         W^ Jk  Your quay contained mixed-case letters, wen though your preferences are to ignore upper/lower case differences Word count Swaieou 301 Penults for the query Swunsan (more than 50 documents matched the query) 1 Set I Mto I 3] ^ S      Tecteacal R eport CMU/SEI-87-TR-2 The Effect of Software Support Needs on the Department of Defense '   '            Software Acquisition Policy Part 1 A Framework for Analyzing Legal Issues Anne C Martin and Kevin M Deasy The Effect of Software Support Needs on the Department of Defense Software Acquisition Poll £ [ft R Eh      AN IMPROVED TREATMENT OF EXTERNAL BOUNDARY FOR THREE-DIMENSIONAL FLOW COMPUTATIONS? Semyon V Tsynkovy Veer N Vatsaz NASA Langley Research Center, Hampton, VA Abstract We present an innovative numerical approach for setting highly accurate nonlocal boundary conditions at the external computational 112 0 ß      ^to^ Aeronautics and Space AdmtnistrationLangley Research Center? Hampton, Virginia 23681 -2199NASA Technical Paper 36 31 Multistage Schemes With MulUgnd for Eulerand Havier-Stokes Equations Components and AnaiysisP C SframonLajsgiey Research Center ? Hampton, VirgtneEli TurkelTel-Aviv Umversit | !f1 ril f%      A distributed Garbage Collection Algonthm Terence Cntchlow UUCS-92-11 Department of Computer Science Uraveraty of Utah Salt Lake City, UT 84112 USA July 30, 1992 Abstract Concurrent Scheme extends the Scheme programming language, providing parallel program execution on a distributed network The Figure 10.14     An example of a ranked list of titles and other document surrogate information [824]. KWIC A facility related to highlighting is the key word-in-context (KWIC) document surrogate. Sentence fragments, full sentences, or groups of sentences that contain query terms are extracted from the full text and presented for viewing along with other kinds of surrogate information (such as document title and abstract). Note that a KWIC listing is different than an abstract. An abstract summarizes the main topics of the document but might not contain references to the terms within the query. A KWIC extract shows sentences that summarize the ways the query terms are used within the document. This display can show not only which subsets of query terms occur in the retrieved documents, but also the context they appeal in with respect to one another. Tradeoff decisions must be made between how many lines of text to show and which lines to display, It is not known which contexts are best selected for viewing but results from text summarization research suggest that the best fragments to show art8 those that appear near the beginning of the document and that routaiu the largest subset of query terms [464]. If users have specified which CONTEXT        291 terms are more important than others, then those fragments containing important terms should be shown before those that contain only less important terms. However, to help retain coherence of the excerpts, selected sentences should be shown in order of their occurrence in the original document, independent of how many search terms they contain. The KWIC facility is usually not shown in Web search result display, most likely because the system must have a copy of the original document available from which to extract the sentences containing the search terms. Web search engines typically only retain the index without term position information. Systems that index individual Web sites can show KWIC information in the document list display. TileBars A more compact form of query term hit display is made available through the TileBars interface. The user enters a query in a faceted format, with one topic per line. After the system retrieves documents (using a quorum or statistical ranking algorithm), a graphical bar is displayed next to the title of each document showing the degree of match for each facet. TileBars thus illustrate at a glance which passages in each article contain which topics - and moreover, how frequently each topic is mentioned (darker squares represent more frequent matches). Each document is represented by a rectangular bar. Figure 10.15 shows an example. The bar is subdivided into rows that correspond to the query facets. The top row of each TileBar corresponds to 'osteoporosis/ the second row to 'prevention,' and the third row to 'research.1 The bar is also subdivided into columns, where each column refers to a passage within the document. Hits that overlap within the same passage are more likely to indicate a relevant document than hits that are widely dispersed throughout the document [356]. The patterns are meant to indicate whether terms from a facet occur as a main topic throughout the document, as a subtopic, or are just mentioned in passing. The darkness of each square corresponds to the number of times the query occurs in that segment of text; the darker the square the greater the number of hits. White indicates no hits on the query term. Thus, the user can quickly see if some subset of the terms overlap in the same segment of the document. (The segments for this version of the interface are fixed blocks of 100 tokens each.) The first document can be seen to have considerable overlap among the topics of interest towards the middle, but not at the beginning or the end (the actual end is cut off). Thus it most likely discusses topics in addition to research into osteoporosis. The second through fourth documents, which are considerably shorter, also have overlap among all terms of interest, and so are also probably of interest to the user. (The titles help to verify this.) The next three documents are all long, and from the TileBars we can tell they discuss research and prevention, but do not even touch on osteoporosis, and so probably are not of interest. Because the TileBars interface-* allows the user to specify the query in terms 292        USER INTERFACES AND VISUALIZATION User Query {Enter wtttds for different topk	son different Unas.)	Ron Search 1	NwQ	oerj	'......I		Quit osteoporosis prevention research    '  '       " '       """'		Search limit v 50 v Number of Ousters: v	´¶ 100 3 v	4	250 ? 5	V V	500. *   V	IOC 10 FR88513-0157 AP: Groups Seek $1 Billion a Year for Aging Research SJMN: WOMEN'S HEALTH LEGISLATION PROPOSED Clj AP: Older Athletes Run For Science                                        \ FR: Committee Meetings FR: October Advisory Committees; Meetings FR8S120-0046                                                                        I FR: Chronic Disease Burden and Prevention Models; Program * AP: Survey Says Experts Split on Diversion of Funds for AIDS I I FR: Consolidated Delegations of Authority for Policy Develop* J SJMN: RESEARCH FOR BREAST CANCER IS STUCK IN PiL sr: Figure 10.15     An example of the TileBars retrieval results visualization [355]. of facets, where the terms for each facet are listed on an entry line, a color can be assigned to each facet. When the user displays a document with query term hits, the user can quickly ascertain what proportion of search topics appear in a passage based only on how many different highlight colors are visible. Most systems that use highlighting use only a single color to bring attention to all of the search terms. It would be difficult for users to specify in advance which patterns of term hits they are interested in. Instead, TileBars allows users to scan graphic representations and recognize which documents are and are not of interest. It may be the case that TileBars may be most useful for helping users discard mislead-ingly interesting documents, but only preliminary studies have been conducted to date. Passages can correspond to paragraphs or sections, fixed sized units of arbitrary length, or to automatically determined multiparagraph segments [355]. SeeSoft The SeeSoft visualization [232] represents text in a manner resembling columns of newspaper text, with one line' of text on each horizontal line of the strip. (See Figure 10.16.) The representation is compact and aesthetically pleasing. Graphics are used to abstract away the details, providing an overview showing the amount and shape of the text. Color highlighting is used to pick out various attributes, such as where a particular word appears in the text. Details of a smaller portion of the display can be viewed via a pop-up window; the overview CONTEXT        293 shows more of the text but in less detail. Figure 10.16 An example of the SeeSoft visualization for showing locations of characters within a text [232]. SeeSoft was originally designed for software development, in which a line of text is a meaningful unit of information. (Programmers tend to place each individual programming statement on one line of text.) Thus SeeSoft shows attributes relevant to the programming domain, such as which lines of code were modified by which programmer, and how often particular lines have been modified, and how many days have elapsed since the lines were last modified. The SeeSoft developers then experimented with applying this idea to the display of text, although this has not been integrated into an information access system. Color highlighting is used to show which characters appear where in a book of fiction, and which passages of the Bible contain references to particular people and items. Note the use of the abstraction of an entire line to stand for a single word such as a character's name (even though though this might obscure a tightly interwoven conversation between two characters).
mir-0201	10.6.3    Query Term Hits Between Documents Other visualization ideas have been developed to show a different kind of information about the relationship between query terms and retrieved documents. Rather than showing how query terms appear within individual documents, as is done in KWIC interfaces and TileBars, these systems display an overview or summary of the retrieved documents according to which subset of query terms they contain. The following subsections describe variations on this idea. 294        USER INTERFACES AND VISUALIZATION Figure 10.17     A sketch of the InfoCrystai retrieval results display [738]. InfoCrystal The InfoCrystai shows how many documents contain each subset of query terms [738]. This relieves the user from the need to specify Boolean ANDs and ORs in their query, while still showing which combinations of terms actually appear in documents that were ordered by a statistical ranking (although beyond four terms the interface becomes difficult to understand). The InfoCrystai allows visualization of all possible relations among N user-specified 'concepts' (or Boolean keywords). The InfoCrystai displays, in a clever extension of the Venn diagram paradigm, the number of documents retrieved that have each possible subset of the N concepts. Figure 10.17 shows a sketch of what the InfoCrystai might display as the result of a query against four keywords or Boolean phrases, labeled A, B, C. and D. The diamond in the center indicates that one document was discovered that contains all four keywords. The triangle marked with *12" indicates that 12 documents were found containing attributes A, B, and D, and so on. The InfoCrystai does not show proximity among the terms within the documents, nor their relative frequency. So a document that contains dozens of hits on 'volcano' and 'lava' and one hit on 'Mars' will be grouped with documents that contain mainly hits on 'Mars' but just one mention each of 'volcano* and iava." authoring CONTEXT        295 navigation l hypertext-engineering knowledge representation usabili1y-links-and-fbtion                             .   _        u               ._....._.                  __^ ^                        '  ^^                    ïï^"'¶'     ^                 ó------fmplernentationsand-in1er1aces application Figure 10.18     An example of the VIBE retrieval results display [452]. VIBE and Lyberworld Graphical presentations that operate on similar principles are VIBE [452] and Lyberworld [363]. In these displays, query terms are placed in an abstract graphical space. After the search, icons are created that indicate how many documents contain each subset of query terms. The subset status of each group of documents is indicated by the placement of the icon. For example, in VIBE a set of documents that contain three out of five query terms are shown on an axis connecting these three terms, at a point midway between the representations of the three query terms in question. (See Figure 10.18.) Lyberworld presents a 3D version of this idea. Lattices Several researchers have employed a graphical depiction of a mathematical lattice for the purposes of query formulation, where the query consists of a set of constraints on a hierarchy of categories (actually, semantic attributes in these systems) [631, 147]. This is one solution to the problem of displaying documents in terms of multiple attributes; a document containing terms A, B, C, and D could be placed at a point in the lattice with these four categories as parents. However, if such a representation were to be applied to retrieval results instead of query formulation, the lattice layout would in most cases be too complex to allow for readability. None of the displays discussed in this subsection have been evaluated for effectiveness at improving query specification or understanding of retrieval results, but they are intriguing ideas and perhaps are useful in conjunct ion with other displays. 296        USER INTERFACES AND VISUALIZATION  ¶3m ___ d Figure 10.19     The SuperBook interface for showing retrieval results on a large manual in context [481]. 10.6.4    SuperBook: Context via Table of Contents The SuperBook system [481, 229, 230] makes use of the structure of a large document to display query term hits in context. The table of contents (TOC) for a book or manual are shown in a hierarchy on the left-hand side of the display, and full text of a page or section is shown on the right-hand side. The user can manipulate the table of contents to expand or contract the view of sections and subsections. A focus-plus-context mechanism is used to expand the viewing area of the sections currently being looked at and compress the remaining sections. When the user moves the cursor to another part of the TOC, the display changes dynamically, making the new focus larger and shrinking down the previously observed sections. After the user specifies a query on the book, the search results are shown in the context of the table of contents hierarchy. (See Figure 10.19.) Those sections that contain search hits are made larger and the others are compressed. The query terms that appear in chapter or section names are highlighted in reverse video. When the user selects a page from the table of contents view, the page itself is displayed on the right-hand side and the query terms within the page are highlighted in reverse video. The SuperBook designers created innovative techniques for evaluating its special features. Subjects were compared using this system against using paper (Iocunieiitatiori and against a more standard online infoririatiori access system. Subjects were also compared on different kinds of carefully selected tasks: browsing topics of interest, citation searching, searching to answer questions, and searching and browsing to write summary essays.   For most of the tasks CONTEXT        297 SuperBook subjects were faster and more accurate or equivalent in speed and accuracy to a standard system. When differences arose between SuperBook and the standard system, the investigators examined the logs carefully and hypothesized plausible explanations. After the initial studies, they modified SuperBook according to these hypotheses and usually saw improvements as a result [481]. The user studies on the improved system showed that users were faster and more accurate at answering questions in which some of the relevant terms were within the section titles themselves, but they were also faster and more accurate at answering questions in which the query terms fell within the full text of the document only, as compared both to a paper manual and to an interface that did not provide such contextualizing information. SuperBook was not faster than paper when the query terms did not appear in the document text or the table of contents. This and other evidence from the SuperBook studies suggests that query term highlighting is at least partially responsible for improvements seen in the system.
mir-0202	10.6.5    Categories for Results Set Context In section 10.4 we saw the use of category or directory information for providing overviews of text collection content. Category metadata can also be used to place the results of a query in context. For example, the original formulation of SuperBook allowed navigation within a highly structured document, a computer manual. The CORE project extended the main idea to a collection of over 1000 full-text chemistry articles. A study of this representation demonstrated its superiority to a standard search system on a variety of task types [228]. Since a table of contents is not available for this collection, context is provided by placing documents within a category hierarchy containing terms relevant to chemistry. Documents assigned a category are listed when that category is selected for more detailed viewing, and the categories themselves are organized into a hierarchy, thus providing a hierarchical view on the collection. Another approach to using predefined categories to provide context for retrieval results is demonstrated by the DynaCat system [650]. The DynaCat system organizes retrieved documents according to which types of categories, selected from the large MeSH taxonomy, are known in advance to be important for a given query type. DynaCat begins with a set of query types known to be useful for a given user population and collection. One query type can encompass many different queries. For example, the query type 'Treatment-Adverse Effects' covers queries such as 'What are the complications of a mastectomy?* as well as 'What are the side-effects of aspirin?1 Documents are organized according to a set of criteria associated with each query type. These criteria specify which types of categories that are acceptable to use for organizing the documents and consequently, which categories should be omitted from the display. Once categories have been assigned to the retrieved documents, a hierarchy is formed based on where the categories exist within MeSH. The algorithm selects only a subset of the category 298 USER INTERFACES AND VISUALIZATION i Query:Wttat are tite ways to frtevent breast cane* ! IBS  Betaffior mA Bhswiot i  MectoMsios (14 refs) ;        ï AtttenAe C9 rafggt; t Behavior (8,ittfg), ï Psychology, Social (3 ªflg) Bsoclwiiiical Heta1toli$n, ami ;   HmttlUQB (5 yefg) ï Diet (5j£fs) Clemicals mA Drags Cß1Mß) ï Ahubo Acids, Peptiies, ´M ;        Proteins (£j£fa) ï Axitiiiieoplftstu: ani Immnikosuppiessivv	rr  Behawor anil Behavior Meclianisms £-        ;; Attitude ï Attltycfe to Heaftli ï Por La Vida intervention mode! for	5, cancer prevention in Latmas ï Breast cancer prevention education at a shopDina center in Israel" a student nurse community health proiect ï Future challenaes in secondary drevention of breast cancer for women at high risk ï A studv of diet and breast cancer prevention in Canada whv healthy wo m e n p a rti c i p -ate i n c o ntr o 1! e cl tri a S s ï Knowledge, Attitudes, Practice ï Por La Vicla intervention mode! for ^                   c a n i e r p r eve nti o r"gt; i n Lati n a s 4 Figure 10.20     The DynaCat interface for viewing category labels that correspond to query types [650]. labels that might be assigned to the document to be used in the organization. Figure 10.20 shows the results for a query on breast cancer prevention. The interface is tiled into three windows. The top window displays the user's query and the number of documents found. The left window shows the categories in the first two levels of the hierarchy, providing a table of contents view of the organization of search results. The right pane displays ail the categories in the hierarchy and the titles of the documents that belong in those categories. An obstacle to using category labels to organize retrieval results is the requirement of precompiled knowledge about which categories are of interest for a particular user or a particular query type. The SONIA system [692] circumvents this problem by using a combination of misupervised and supervised methods to organize a set of documents. The misupervised method (document clustering similar to Scatter/Gat her) imposes an initial organization on a user's personal information collection or on a set of documents retrieved as the result of a query. The user can then invoke a direct manipulation interface to make adjustoients to tliis Initial clustering, causing it to align more closely with their preferences (IwaiLse uiLsupervised methods do not usually produce an organization that n´Tespond.s to a human-derived category structure [857]). The resulting organization is then used to train a supervised text categorization algorithm which automatically classifies any new documents that are added to the collection. As the collection grows it can be periodically reorganized by rerunning the clustering algorithm and redoing the manual adjustments. CONTEXT        299
mir-0203	10.6.6    Using Hyperlinks to Organize Retrieval Results Although the SuperBook authors describe it as a hypertext system, it is actually better thought of as a means of showing search results in the context of a structure that users can understand and view all at once. The hypertext component was not analyzed separately to assess its importance, but it usually is not mentioned by the authors when describing what is successful about their design. In fact, it seems to be responsible for one of the main problems seen with the revised version of the system ó that users tend to wander off (often unintentionally) from the pages they are reading, thus causing the time spent on a given topic to be longer for SuperBook in some cases. (Using completion time to evaluate users on browsing tasks can be problematic, however, since by definition browsing is a casual, unhurried process [804].) This wandering may occur in part because SuperBook uses a non-standard kind of hypertext, in which any word is automatically linked to occurrences of the same word in other parts of the document. This has not turned out to be how hypertext links are created in practice. Today, hyperlinked help systems and hyperlinks on the Web make much more discriminating use of hyperlink connections (in part since they are usually generated by an author rather than automatically). These links tend to be labeled in a somewhat meaningful manner by their surrounding context. Back-of-the-book indexes often do not contain listings of every occurrence of a word, but rather to the more important uses or the beginnings of series of uses. Automated hypertext linking should perhaps be based on similar principles. Additionally, at least one study showed that users formed better mental models of a small hypertext system that was organized hierarchically than one that allowed more flexible access [226]. Problems relating to navigation of hypertext structure have long been suspected and investigated in the hypertext literature [181, 551, 440, 334]. More recent work has made better use of hyperlink information for providing context for retrieval results. Some of this work is described below. Cha-Cha: SuperBook on the Web The Cha-Cha intranet search system [164] extends the SuperBook idea to a large heterogeneous Web site such as might be found in an organization's intranet. Figure 10.21 shows an example. This system differs from SuperBook in several ways. On most Web sites there is no existing real table of contents or category structure, and an intranet like those found at large universities or large corporations is usually not organized by one central unit. Cha-Cha uses link structure present within the site to create what is intended to be a meaningful organization on top of the underlying chaos. After the user issues a query, the shortest paths from the root page to each of the search hits are recorded and a subset of these are selected to be shown as a hierarchy, so that each hit is shown only once. (Users can begin with a query, rather than with a table of contents view.) If a user does not know to use the term 'health center* but instead queries on 'medical center/ if "medical* appears as a term in a document within 300        USER INTERFACES AND VISUALIZATION srvldte^ vj  JjSf University Health Services Health Services for Faculty and Staff ^ Other Programs Available to Faculty and Staff... Colleges and Schools w School of Social Welfare: Home Page ? Programs, Curricula, and Courses ? MSW PROGRAM Field Work Agencies SWF ? The Letters  Science WWW Home Page "* Departments  Divisions * Townsend Center for the Humanities, UC Berkeley 1 September Townsend Center Newsletter i bioethics                \\ The........UC Berkeley Libraries. w UC Berkeley Libraries '*' Health Sciences Information Service B HSIS Medical Informatics w Center for Southeast Asia Studies ffi CSEAS Newsletter.Upcommg Events, Spring 1996 Policies and Guidelines for Web Publishing at "r Image/Multimedia Database Resources 1 Medical Ima,qe D^afragjI 1-20 of SS5 matches UstViewR Next* Page Summary jg Health Net Health Net HealthNet Health Care......University Health Services (UHS) at the University of California at Berkeley offers general medical office visits, physical therapy, and laboratory services to faculty and staff who are HealthNet members and have selected a Personal Care Physician (PCP) at the Tang Center.......Hospitalization: If you need to be hospitalized, in most cases you will be cared for at Alta Bates Medical Center by a physician affiliated with Alta Bates.......Tittle is active in quality assurance activities at University Health Services where he has been a physician since 1977, He received his medical degree from Stanford University in 1973 and specialized in Internal Medicine during his residencies at Pacific Medical Center and UCS... http://www.uhs. berkeley.edu/FacStaff/healthNet.htm (Sizes 10K) Figure 10.21     The Cha-Cha interface for showing Web intranet search results in context displaying results on the query 'medical centre'[164]. the health center part of the Web, the home page (or starting point) of this center will be presented as well as the more specific hits. Users can then either query or navigate within a subset of sites if they wish. The organization produced by this simple method is surprisingly comprehensible on the UC Berkeley site. It seems especially useful for providing the information about the sources (the Web server) associated with the search hits, whose titles are often cryptic. The AMIT system [826] also applies the basic ideas behind SuperBook to the Web, but focuses on a single-topic Web site, which is likely to have a more reasonable topic structure than a complex intranet. The link structure of the Web site is used as contextualizing information but all of the paths to a given, document are shown and focus-plus-context is used to emphasize subsets of the document space. The WebTOC system [585] is similar to AMIT but focuses on showing the structure and number of documents within each Web subhierarchy, and is not tightly coupled with search. CONTEXT        301 j j; File  Layout View  Operations  Fetch  Help Figure 10.22    Example of a Web subset visualized by Mapuccino (courtesy of M. Jacovi, B. Shaul and Y. Maarek). Mapuccino: Graphical Depiction of Link Structure The Mapuccino system (formerly WebCutter) [527] allows the user to issue a query on a particular Web site. The system crawls the site in real-time, checking each encountered page for relevance to the query. When a relevant page is found, the weights on that page's outlinks are increased. Thus, the search is based partly on an assumption that relevant pages will occur near one another in the Web site. The subset of the Web site that has been crawled is depicted graphically in a nodes-and-links view (see Figure 10.22). This kind of display does not provide the user with information about what the contents of the pages are, but rather only shows their link structure- Other researchers have also investigated spreading activation among hypertext links as a way to guide an information retrieval system, e.g., [278, 555].
mir-0204	10.6.7    Tables Tabular display is another approach for showing relationships among retrieval documents. The Envision system [273] allows the user to organize results according to metadata such as author or date along the X and Y-axes, and uses graphics to show values for attributes associated with retrieved documents within each cell (see Figure 10.23). Color, shape, and size of an iconic representation of a document are used to show the computed relevance, the type of document, or 302 USER INTERFACES AND VISUALIZATION Key constraints governing human 1983 Tho Psychology of Human-Computer Interaction 1904    Human Liadts and  tha VDT Coadjutor Intotiaco Figure 10.23     The Envision tabular display for graphically organizing retrieved documents [270]. other attributes. Clicking on an icon brings up more information about the document in another window. Like the WebCutter system, this view provides few cues about how the documents are related to one another in terms of their content or meaning. The SenseMaker system also allows users to group documents into different views via a table-like display [51], including a Scatter/Gather [203] style view. Although tables are appealing, they cannot show the intersections of many different attributes; rather they are better for pairwise comparisons. Another problem with tables for display of textual Information is that very little information can be fitted on a screen at a time, making comparisons difficult. The Table Lens [666] is an innovative interface for viewing and interactively reorganizing very large tables of information (see Figure 10.24). It uses focus-plus-context to fit hundreds of rows of information in a space occupied by at most two dozen rows in standard spreadsheets. And because it allows for rapid reorganization via sorting of columns, users can quickly switch from a view focused around one kind of metadata to another. For example, first sorting documents by rank and then by author name can show the relative ranks of different articles by the same author. A re-sort by date can show patterns in relevance scores with respect to date of publication. This rapid re-sorting capability helps circumvent the problems associated with the fact that tables cannot show many simultaneous intersections. Another variation on the table theme Is that seen in the Perspective Wall [530] in which a focuH-plas-coritext display is used to center information currently USING RELEVANCE JUDGEMENTS 303 68490CL   _____l 287658 427500.______M79550 L1617B4 Figure 10.24     The TableLens visualization [666]. of interest in the middle of the display, compressing less important information into the periphery on the sides of the wall. The idea is to show in detail the currently most important information while at the same time retaining the context of the rest of the information. For example, if viewing documents in chronological order, the user can easily tell if they are currently looking at documents in the beginning, middle, or end of the time range. These interfaces have not been applied to information access tasks. The problem with such displays when applied to text is that they require an attribute that can be shown according to an underlying order, such as date. Unfortunately, information useful for organizing text content, such as topic labels, does not have an inherent meaningful order. Alphabetical order is useful for looking up individual items, but not for seeing patterns across items according to adjacency, as in the case for ordered data types like dates and size.
mir-0205	10.7    Using Relevance Judgements An import ant part of the information access process is query reformulation, and a proven effective technique for query reformulation is relevance feedback. In its original form, relevance feedback refers to an interaction cycle in which the user selects a small set of documents that appear to be relevant to the query, and the system then uses features derived from these selected relevant documents to revise the original query. This revised query is then executed and a new set of documents is returned. Documents from the original set can appear in the new results 304        USER INTERFACES AND VISUALIZATION list, although they are likely to appear in a different rank order. Relevance feedback in its original form has been shown to be an effective mechanism for improving retrieval results in a variety of studies and settings [702, 343, 127]. In recent years the scope of ideas that can be classified under this term has widened greatly. Relevance feedback introduces important design choices, including which operations should be performed automatically by the system and which should be user initiated and controlled. Bates discusses this issue in detail [66], asserting that despite the emphasis in modern systems to try to automate the entire process, an intermediate approach in which the system helps automate search at a strategic level is preferable. Bates suggests an analogy of an automatic camera versus one with adjustable lenses and shutter speeds. On many occasions, a quick, easy method that requires little training or thought is appropriate. At other times the user needs more control over the operation of the machinery, while still not wanting to know about the low level details of its operation. A related idea is that, for any interface, control should be described in terms of the task being done, not in terms of how the machine can be made to accomplish the task [607]. Continuing the camera analogy, the user should be able to control the mood created by the photograph, rather than the adjustment of the lens. In information access systems, control should be over the kind of information returned, not over which terms are used to modify the query. Unfortunately it is often quite difficult to build interfaces to complex systems that behave in this manner.
mir-0206	10.7.1    Interfaces for Standard Relevance Feedback A standard interface for relevance feedback consists of a list of titles with checkboxes beside the titles that allow the user to mark relevant documents. This can imply either that unmarked documents are not relevant or that no opinion has been made about unmarked documents, depending on the system. Another option is to provide a choice among several checkboxes indicating relevant or not relevant (with no selection implying no opinion). In some cases users are allowed to indicate a value on a relevance scale [73]. Standard relevance feedback algorithms usually do not perform better given negative relevance judgement evidence [225], but machine learning algorithms can take advantage of negative feedback [629, 460]. After the user has made a set of relevance judgements and issued a search command, the system can either automatically reweight the query and re-execute the search, or generate a list of terms for the user to select from in order to augment the original query. (See Figure 10.25, taken from [448].) Systems usually do not suggest terms to remove from the query. After the query is re-executed, a new list of titles is shown. It can be helpful to retain an indicator such as a marked checkbox beside the documents that the usc^r has already judged. A difficult design decision concerns whether or not to show documents that the user lias already viewed towards the top of the ranked list 111 Repeatedly showing the same set of documents at the top may inconvenience a user who is trying to create a large set of relevant documents. USING RELEVANCE JUDGEMENTS 305 |ResetAll|      |UNDQ LAST RUN QUERV|      |Show Search Topic Textj Enter (next) quei ªACE B2 DETROIT óGeneral Motors Corp said it Is recalling 62,000 1988-89 model cars equipped with Its high-tech Quad 4 iglne To fU defective fuel lines (Inked To 24 engine fires CM said the 1988-89 Pontiac Grand Am, Oldsmoblle Cutlass Mais and Buick Skylark cars equipped with the 16-valve, ur-cylmder Quad 4 engine have fuel lines that could cracK separate from the engines Although GM has received jports of 24 fires caused by leaks attributable to the faulty fuel I ines a spokesman says the company knows of no Injuries resulting from the Incidents GM sold about 312,000 cars equipped with Quad 4 engines in the 1988-89 model years another action GM said it is recalling about 3 200 of 990 Oldsmoblle Cutlass Calais and Buick Skylark models  ix fuel-line defects on three engines the Quad 4, 3 3-liter V-6 and? 5-liter four cylinder GM Isn't aware of any fires or injuries related to the fuel Jm´ problems in this group of cars, the spokesman said an repairs will be done free of charge to owners, the company said Separately, the U S sales arm of Volkswagen AC'S Audi subsidiary said it is recalling 1 600 1990-model Audi 80 90 ind Coupe Qurtro luxury csrs to replace d defective bolt in fie assembly that locks The STeering when the car Is parked 'he defective bolt could break causing the steering wheel to Figure 10.25     An example of an interface for relevance feedback [448]. but at the same time, this can serve as feedback indicating that the revised query does not downgrade the ranking of those documents that have been found especially important. One solution is to retain a separate window that shows the rankings of only the documents that have not been retrieved or ranked highly previously. Another solution is to use smaller fonts or gray-out color for the titles of documents already seen. Creating multiple relevance judgements is an effortful task, and the notion of relevance feedback is unfamiliar to most users. To circumvent these problems, Web-based search engines have adopted the terminology of 'more like this1 as a simpler way to indicate that the user is requesting documents similar to the selected one. This 4one-click' interaction method is simpler than standard relevance feedback dialog which requires users to rate a small number of documents and then request a reranking. Unfortunately, in most cases relevance feedback requires many relevance judgements in order to work well. To partly alleviate this problem, Aalbersberg [1] proposes incremental relevance feedback which works well given only one relevant document at a time and thus can be used to hide the two-step procedure from the user.
mir-0207	10.7.2    Studies of User Interaction with Relevance Feedback Systems Standard relevance feedback assumes the user is involved in the interaction by specifying the relevant documents.   In some interfaces users are also able to 306        USER INTERFACES AND VISUALIZATION select which terms to add to the query. However, most ranking and reweighting algorithms are difficult to understand or predict (even for the creators of the algorithms!) and so it might be the case that users have difficulties controlling a relevance feedback system explicitly. A recent study was conducted to investigate directly to what degree user control of the feedback process is beneficial. Koenemann and Belkin [448] measured the benefits of letting users 'under the hood' during relevance feedback. They tested four cases using the Inquery system [772]: ï  Control No relevance feedback; the subjects could only reformulate the query by hand. ï  Opaque The subjects simply selected relevant documents and saw the revised rankings. ï  Transparent The subjects could see how the system reformulated the queries (that is, see which terms were added ó the system did not reweight the subjects1 query terms) and the revised rankings. ï  Penetrable The system is stopped midway through the reranking process. The subjects are shown the terms that the system would have used for opaque and transparent query reformulation.    The subjects then select which, if any, of the new terms to add to the query.   The system then presents the revised rankings. The 64 subjects were much more effective (measuring precision at a cutoff of top 5, top 10, top 30, and top 100 documents) with relevance feedback than without it. The penetrable group performed significantly better than the control, with the opaque and transparent performances falling between the two in effectiveness. Search times did not differ significantly among the conditions, but there were significant differences in the number of feedback iterations. The subjects in the penetrable group required significantly fewer iterations to achieve better queries (an average of 5.8 cycles in the penetrable group, 8.2 cycles in the control group, 7.7 cycles in the opaque group, and surprisingly, the transparent group required more cycles, 8.8 on average). The average number of documents marked relevant ranged between 11 and 14 for the three conditions. All subjects preferred relevance feedback over the baseline system, and several remarked that they preferred the lazy' approach of selecting suggested terms over having to think up their own. An observational study on a TTY-based version of an online catalog system [338] also found that users performed better using a relevance feedback mechanism that allowed manual selection of terms. However, a later observational study did not find overall success with this form of relevance feedback [337]. The authors attribute these results to a poor design of a new graphical interface. These* results may also be due to the fact that users often selected only one relevant document before performing the feedback operation, although then' were using a system optimized from multiple document selection. USING RELEVANCE JUDGEMENTS         307
mir-0208	10.7.3    Fetching Relevant Information in the Background Standard relevance feedback is predicated on the goal of improving an ad hoc query or building a profile for a routing query. More recently researchers have begun developing systems that monitor users' progress and behavior over long interaction periods in an attempt to predict which documents or actions the user is likely to want in future. These systems are called semi-automated assistants or recommender 'agents,' and often make use of machine learning techniques [565]. Some of these systems require explicit user input in the form of a goal statement [406] or relevance judgements [629], while others quietly record users' actions and try to make inferences based on these actions. A system developed by Kozierok and Maes [460, 536] makes predictions about how users will handle email messages (what order to read them in, where to file them) and how users will schedule meetings in a calendar manager application. The system 'looks over the shoulder7 of the users, recording every relevant action into a database. After enough data has been accumulated, the system uses a nearest-neighbors method [743] to predict a user's action based on the similarity of the current situation to situations already encountered. For example, if the user almost always saves email messages from a particular person into a particular file, the system can offer to automate this action the next time a message from that person arrives [536]. This system integrates learning from both implicit and explicit user feedback. If a user ignores the system's suggestion, the system treats this as negative feedback, and accordingly adds the overriding action to the action database. After certain types of incorrect predictions, the system asks the user questions that allow it to adjust the weight of the feature that caused the error. Finally, the user can explicitly train the system by presenting it with hypothetical examples of input-action pairs. Another system, Syskill and Webert [629], attempts to learn a user profile based on explicit relevance judgements of pages explored while browsing the Web. In a sense this is akin to standard relevance feedback, except the user judgements are retained across sessions and the interaction model differs: as the user browses a new Web page, the links on the page are automatically annotated as to whether or not they should be relevant to the user's interest. A related system is Letizia [518], whose goal is to bring to the user's attention a percentage of the available next moves that are most likely to be of interest, given the user's earlier actions. Upon request, Letizia provides recommendations for further action on the user's part, usually in the form of suggestions of links to follow when the user is unsure what to do next. The system monitors the user's behavior while navigating and reading Web pages, and concurrently evaluates the links reachable from the current page. The system uses only implicit feedback. Thus, saving a page as a bookmark is taken as strong positive evidence for the terms in the corresponding Web page. Links skipped are taken as negative support for the information reachable from the link. Selected links can indicate positive or negative evidence, depending on how much time the user spends on the resulting page and whether or not the decision to leave a page quickly is later reversed.  Additionally, the evidence for user interest remains persistent across 308        USER INTERFACES AND VISUALIZATION browsing sessions. Thus, a user who often reads kayaking pages is at another time reading the home page of a professional contact and may be alerted to the fact that the colleague's personal interests page contains a link to a shared hobby. The system uses a best-first search strategy and heuristics to determine which pages to recommend most strongly. A more user-directed approach to prefetching potentially relevant information is seen in the Butterfly system [531]. This interface helps the user follow a series of citation links from a given reference, an important information seeking strategy [66]. The system automatically examines the document the user is currently reading and prefetches the bibliographic citations it refers to. It also retrieves lists of articles that cite the focus document. The underlying assumption is that the services from which the citations are requested do not respond immediately. Rather than making the user wait during the delay associated with each request, the system handles many requests in parallel and the interface uses graphics and animations to show the incrementally growing list of available citations. The system does not try to be clever about which cites to bring first; rather the user can watch the 'organically' growing visualization of the document and its citations, and based on what looks relevant, direct the system as to which parts of the citation space to spend more time on.
mir-0209	10.7.4    Group Relevance Judgements Recently there has been much interest in using relevance judgements from a large number of different users to rate or rank information of general interest [672], Some variations of this social recommendation approach use only similarity among relevance judgements by people with similar tastes, ignoring the representation of the information being judged altogether. This has been found highly effective for rating information in which taste plays a major role, such as movie and music recommendations [720]. More recent work has combined group relevance judgements with content information [64],
mir-0210	10.7.5    Pseudo-Relevance Feedback At the far end of the system versus user feedback spectrum is what is informally known as pseudo-relevance feedback. In this method, rather than relying on the user to choose the top k relevant documents, the system simply assumes that its top-ranked documents are relevant, and uses these documents to augment the query with a relevance feedback ranking algorithm. This procedure has been found to be highly effective in some settings [760, 465, 12], most likely those in which the original query statement is long and precise. An intriguing extension to this idea is to use the output of clustering of retrieval results as the input to a relevance feedback mechanism, either by having the user or the system select the cluster to be used [359], but this idea has not yet been evaluated. INTERFACE SUPPORT FOR THE SEARCH PROCESS        309
mir-0211	10.8    Interface Support for the Search Process The user interface designer must make decisions about how to arrange various kinds of information on the computer screen and how to structure the possible sequences of interactions. This design problem is especially daunting for a complex activity like information access. In this section we discuss design choices surrounding the layout of information within complex information systems, and illustrate the ideas with examples of existing interfaces. We begin with a discussion of very simple search interfaces, those used for string search in 'find' operations, and then progress to rnultrwindow interfaces and sophisticated workspaces. This is followed by a discussion of the integration of scanning, selecting, and querying within information access interfaces and concludes with interface support for retaining the history of the search process.
mir-0212	10.8.1    Interfaces for String Matching A common simple search need is that of the 'find' operation, typically run over the contents of a document that is currently being viewed. Usually this function does not produce ranked output, nor allow Boolean combinations of terms; the main operation is a simple string match (without regular expression capabilities). Typically a special purpose search window is created, containing a few simple controls (e.g., case-sensitivity, search forward or backward). The user types the query string into an entry form and string matches are highlighted in the target document (see Figure 10.26). The next degree of complexity is the 'find' function for searching across small collections, such as the files on a personal computer's hard disk, or the history list of a Web browser. This type of function is also usually implemented as a simple string match. Again, the controls and parameter settings are shown at the top of a special purpose search window and the various options are set via checkboxes and entry forms. The difference from the previous example is that a results list is shown within the search interface itself (see Figure 10.27). A common problem arises even in these very simple interfaces. An ambiguous state occurs in which the results for an earlier search are shown while the user is entering a new query or modifying the previous one. If the user types in jhrcrr-aticn                [                                I    FindNxt   \ Direction Figure 10.26     An example of a simple interface for string matching, from Netscape Communicator 4.05. 310        USER INTERFACES AND VISUALIZATION Search for items in the Htap List where; Search [Title ]Contains j*J jberkeleii gear     j Help Title Location J FirstVisfted] LastVisftedjExpiration   | Visit..T _ Searching UC... "The'lJCBerkeir"" Berkeley Pledge 1998Berkeleya...  BerkeleyanArc...  Berkeley /Pr...  Berkeleyan/Pr...  02-25-98 Berkel...  UC Berkeley Dir...  UC Berkeley Dir... http: //library, berkele... http: //www. urel. berk.. http: //www. urel. berk.. http://www.urel.berk.. http://www.urel.berk.. http: //www. urel. berk.. http: //www. urel. berk.. http: //www-resource.. http:// www. berkeley... 7/7/1998... 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1  hours ago 2 hours ago 7/22/199... 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours ago 1 hours aqo 8/27/199... '8/27/1917 8/27/199... 8/27/199... 8/27/199... 8/27/199... 8/27/199... 8/27/199... 8/27/199... 8/27/199... 60 U ......"or 1 3  ~ 1 3 1 7 55 4  2 A Figure 10.27     An example of an string matching over a list, in this case, a history of recently viewed Web pages, from Netscape Communicator 4.05. new terms and but then does not activate the search, the interface takes on a potentially misleading state, since a user could erroneously assume that the old search hits shown correspond to the newly typed-in query. One solution for this problem is to clear the results list as soon as the user begins to type in a new query. However, the user may want to refer to terms shown in the search results to help reformulate the query, or may decide not to issue the new query and instead continue with the previous results. These goals would be hampered by erasing the current result set as soon as the new query is typed. Another solution is to bring up a new window for every new query. However, this requires the user to execute an additional command and can lead to a proliferation of windows. A third, probably more workable solution, is to automatically "stack1 the queries and results lists in a compact format and allow the user to move back and forth among the stacked up prior searches. Simple interfaces like these can be augmented with functionality that can greatly aid initial query formulation. Spelling errors are a major cause of void result sets. A spell-checking function that suggests alternatives for query terms that have low frequency in the collection might be useful at this stage. Another option is to suggest thesaurus terms associated with the query terras at the time the query terms are entered. Usually these kinds of information are shown after the query is entered and documents have been retrieved, but an alternative is to provide this information as the user enters the query, in a form of query preview. INTERFACE SUPPORT FOR THE SEARCH PROCESS        311
mir-0213	10.8.2    Window Management For search tasks more complex than the simple string matching find operations described above, the interface designer must decide how to lay out the various choices and information displays within the interface. As discussed above, traditional bibliographic search systems use TTY-based command-line interfaces or menus. When the system responds to a command, the new results screen obliterates the contents of the one before it, requiring the user to remember the context. For example, the user can usually see only one level of a subject hierarchy at a time, and must leave the subject view in order to see query view or the document view. The main design choices in such a system are in the command or menu structure, and the order of presentation of the available options. In modern graphical interfaces, the windowing system can be used to divide functionality into different, simultaneously displayed views [582]. In information access systems, it is often useful to link the information from one window to the information in another, for example, linking documents to their position in a table of contents, as seen in SuperBook. Users can also use the selection to cut and paste information from one window into another, for example, copy a word from a display of thesaurus terms and paste the word into the query specification form. When arranging information within windows, the designer must choose between a monolithic display, in which all the windows are laid out in predefined positions and are all simultaneously viewable, tiled windows, and overlapping windows. User studies have been conducted comparing these options when applied to various tasks [725, 96]. Usually the results of these studies depend on the domain in which the interface is used, and no clear guidelines have yet emerged for information access interfaces. The monolithic interface has several advantages. It allows the designer to control the organization of the various options, makes all the information simultaneously viewable, and places the features in familiar positions, making them easier to find. But monolithic interfaces have disadvantages as well. They often work best if occupying the full viewing screen, and the number of views is inherently limited by the amount of room available on the screen (as opposed to overlapping windows which allow display of more information than can fit on the screen at once). Many modern work-intensive applications adopt a monolithic design, but this can hamper the integration of information access with other work processes such as text editing and data analysis. Plaisant et al. [644] discuss issues relating to coordinating information across different windows to providing overview plus details. A problem for any Information access Interface is an inherent limit in how many kinds of information can be shown at once. Information access systems must always reserve room for a text display area, and this must take up a significant proportion of screen space in order for the text to be legible. A tool within a paint program, for example, can be made quite small while nevertheless remaining recognizable and usable. For legibility reasons, it is difficult to compress many of the information displays needed for an Information access system (such 312        USER INTERFACES AND VISUALIZATION as lists of thesaurus terms, query specifications, and lists of saved titles) in this manner. Good layout, graphics, and font design can improve the situation; for example, Web search results can look radically different depending on spacing, font, and other small touches [580]. Overlapping windows provide flexibility in arrangement, but can quickly lead to a crowded, disorganized display. Researchers have observed that much user activity is characterized by movement from one set of functionally related windows to another. Bannon et al. [54] define the notion of a workspace ó the grouping together of sets of windows known to be functionally related to some activity or goal ó arguing that this kind of organization more closely matches users' goal structure than individual windows [96]. Card et al. [140] also found that window usage could be categorized according to a 'working set' model. They looked at the relationship between the demands of the task and the number of windows in use, and found the largest number of individual windows were in use when users transitioned from one task to another. Based on these and other observations, Henderson and Card [420] built a system intended to make it easier for users to move between 'multiple virtual workspaces" [96]. The system uses a 3D spatial metaphor, where each workspace is a 'room,' and users transition between workspaces by 'moving' through virtual doors. By 'traveling' from one room to the next, users can change from one work context to another. In each work context, the application programs and data files that are associated with that work context are visible and readily available for reopening and perusal. The workspace notion as developed by Card et al. also emphasizes the importance of having sessions persist across time. The user should be able to leave a room dedicated to some task, work on another task, and three days later return to the first room and see all of the applications still in the same state as before. This notion of bundling applications and data together for each task has since been widely adopted by window manager software in workstation operating system interfaces. Elastic windows [428] is an extension to the workspace or rooms notion to the organization of 2D tiled windows. The main idea is to make the transition easier from one role or task to another, by adjusting how much of the screen real estate is consumed by the current role. The user can enlarge an entire group of windows with a simple gesture, and this resizing automatically causes the rest of the workspaces to reduce in size so they all still fit on the screen without overlap.
mir-0214	10.8.3    Example Systems The following sections describe the information layout and management approaches taken by several modern information access interfaces. The InfoGrid Layout The InfoGrid system [667] is a typical example of a monolithic layout for an information access interface.   The layout assumes a large display is available INTERFACE SUPPORT FOR THE SEARCH PROCESS         313 Search Parameters		Property Sheet Document Text Control Panel	Thumbnail Images Holding Area		Search Paths Control Panel Table of Contents	TOC Subset Document Text Search Parameters Figure 10.28     Diagrams of monolithic layouts for information access interfaces. and is divided into a left-hand and right-hand side (see Figure 10.28). The left-hand side is further subdivided into an area at the top that contains structured entry forms for specifying the properties of a query, a column of iconic controls lining the left side, and an area for retaining documents of interest along the bottom. The main central area is used for the viewing of retrieval results, either as thumbnail representations of the original documents, or derived organizations of the documents, such as Scatter/Gather-style cluster results. Users can select documents from this area and store them in the holding area below or view them in the right-hand side. Most of the right-hand side of the display is used for viewing selected documents, with the upper portion showing metadata associated with the selected document. The area below the document display is intended to show a graphical history of earlier interactions. Designers must make decisions about which kinds of information to show in the primary view(s). If InfoGrid were used on a smaller display, either the document viewing area or the retrieval results viewing area would probably have to be shown via a pop-up overlapping window; otherwise the user would have to toggle between the two views. If the system were to suggest terms for relevance feedback, one of the existing views would have to be supplanted with this information or a pop-up window would have to be used to display the candidate terms. The system does not provide detailed information for source selection, although this could be achieved in a very simple way with a pop-up menu of choices from the control panel. The SuperBook Layout The layout of the InfoGrid is quite similar to that of SuperBook (see section 10.6). The main difference is that SuperBook retains the table of contents-like display in the main left-hand pane, along with indicators of how many documents containing search hits occur in each level of the outline. Like InfoGrid, the main pane of the right-hand side is used to display selected documents. Query 314        USER INTERFACES AND VISUALIZATION formulation is done just below the table of contents view (although in earlier versions this appeared in a separate window). Terms related to the user's query are shown in this window as well Large images appear in pop-up overlapping windows. The SuperBook layout is the result of several cycles of iterative design [481]. Earlier versions used overlapping windows instead of a monolithic layout, allowing users to sweep out a rectangular area on the screen in order to create a new text box. This new text box had its own set of buttons that allowed users to jump to occurrences of highlighted words in other documents or to the table of contents. SuperBook was redesigned after noting results of experimental studies [350, 532] showing that users can be more efficient if given fewer, well chosen interaction paths, rather than allowing wide latitude (A recent study of auditory interfaces found that although users were more efficient with a more flexible interface, they nevertheless preferred the more rigid, predictable interface [801]). The designers also took careful note of log files of user interactions. Before the redesign, users had to choose to view the overall frequency of a hit, move the mouse to the table of contents window, click the button and wait for the results to be updated. Since this pattern was observed to occur quite frequently, in the next version of the interface, the system was redesigned to automatically perform this sequence of actions immediately after a search was run. The SuperBook designers also attempted a redesign to allow the interface to fit into smaller displays. The redesign made use of small, overlapping windows. Some of the interaction sequences that were found useful in this more constrained environment were integrated into later designs for large monolithic displays. The digital library1TE Interface The digital library1TE system [193, 192] makes a number of interesting design choices. It splits functionality into two parts: control of the search process and display of results . The control portion is a graphical direct manipulation display with animation (see Figure 10.29). Queries, sources, documents, and groups of retrieved documents are represented as graphical objects.   The user creates a query by filling out the editable fields within a query constructor object.   The system manufactures a query object, which is represented by a small icon which can be dragged and dropped onto iconic representations of collections or search services. If a service is active, it responds by creating an empty results set object and attaching the query to this. A set of retrieval results is represented as a circular pool, and documents within the result set are represented as icons distributed along the perimeter of the pool. Documents can be dragged out of the results set pool and dropped into other services, such as a document summarizer or a language translator. Meanwhile, the user can make a copy of the query icon and drop it onto another search service. Placing the mouse over the iconic representation of the query causes a 'tool-tips* window to pop up to show the contents of the underlying query. Queries can be stored and reused at a later time, thusgt; facilitating retention of previously successful search strategies. INTERFACE SUPPORT FOR THE SEARCH PROCESS        315 Figure 10.29     The digital libraryITE interface [193]. A flexible interface architecture frees the user from the restriction of a rigid order of commands. On the other hand, as seen in the SuperBook discussion, such an architecture must provide guidelines, to help get the user started, give hints about valid ways to proceed, and prevent the user from making errors. The graphical portion of the digital libraryITE interface makes liberal use of animation to help guide the user. For example, if the user attempts to drop a query in the document summarizer icon ó an illegal operation ó rather than failing and giving the user an accusatory error message [185], the system takes control of the object being dropped, refusing to let it be placed on the representation for the target application, and moves the object left, right, and left again, mimicking a cshake-the-head-no' gesture. Animation is also used to help the user understand the state of the system, for example, in showing the progress of the retrieval of search results by moving the result set object away from the service from which it was invoked. digital libraryITE uses a separate Web browser window for the display of detailed information about the retrieved documents, such as their bibliographic citations and their full text. The browser window is also used to show Scatter/Gather-style cluster results and to allow users to select documents for relevance feedback. Earlier designs of the system attempted to incorporate text display into the direct manipulation portion, but this was found to be infeasible because of the space required [192]. Thus, digital libraryITE separates the control portion of the information access process from the scanning and reading portion. This separation allows for reusable query construction and service selection, while at the same time allowing for a legible view of documents and relationships among retrieved documents. The selection in the display view is linked to the graphical control portion, so a document viewed in the display could be used as part of a query in a query constructor. 316        USER INTERFACES AND VISUALIZATION digital libraryITE also incorporates the notion of a workspace, or 'workcenter,' as it is known in this system. Different workspaces are created for different kinds of tasks. For example, a workspace for buying computer software can be equipped with source icons representing good sources of reviews of computer software, good Web sites to search for price information and link to the user's online credit service. The SketchTrieve Interface The guiding principle behind the SketchTrieve interface [365] is the depiction of information access as an informal process, in which half-finished ideas and partly explored paths can be retained for later use, saved and brought back to compare to later interactions, and the results can be combined via operations on graphical objects and connectors between them. It has been observed [584, 722] that users use the physical layout of information within a spreadsheet to organize information. This idea motivates the design of SketchTrieve, which allows users to arrange retrieval results in a side-by-side manner to facilitate comparison and recombination (see Figure 10.30). The notion of a canvas or workspace for the retention of the previous context should be adopted more widely in future. Many issues are not easily solved, such as how to show the results of a set of interrelated queries, with minor modifications based on query expansion, relevance feedback, and other forms of modification.  One idea is to show sets of related retrieval results as a stack of 1 John a-.:-- .5 wortt!.: Jotno ar 'iM f _*t.ps pro^t    : .1 h jv.*i:_: ': be lock::!.;:* Figure 10.30     The SketchTYieve interface [365]. INTERFACE SUPPORT FOR THE SEARCH PROCESS        317 cards within a folder and allow the user to extract subsets of the cards and view them side by side, as is done in SketchTrieve, or compare them via a difference operation.
mir-0215	10.8.4    Examples of Poor Use of Overlapping Windows Sometimes conversion from a command-line-based interface to a graphical display can cause problems. Hancock-Beaulieu et al [337] describe poor design decisions made in an overlapping windows display for a bibliographic system. (An improvement was found with a later redesign of the system that used a monolithic interface [336].) Problems can also occur when designers make a literal' transformation from a TTY interface to a graphical interface. The consequences can be seen in the current LEXIS-NEXIS interface, which does not make use of the fact that window systems allow the user to view different kinds of information simultaneously. Instead, despite the fact that it occupies the entire screen, the interface does not retain window context when the user switches from one function to another. For example, viewing a small amount of metadata about a list of retrieved titles causes the list of results to disappear, rather than overlaying the information with a pop-up window or rearranging the available space with resizable tiles. Furthermore, this metadata is rendered in poorly-format ted ASCII instead of using the bit-map capabilities of a graphical interface. When a user opts to see the full text view of a document, it is shown in a small space, a few paragraphs at a time, instead of expanding to fill the entire available space.
mir-0216	10.8.5    Retaining Search History Section 10.3 discusses information seeking strategies and behaviors that have been observed by researchers in the field. This discussion suggests that the user interface should show what the available choices are at any given point, as well as what moves have been made in the past, short-terrn tactics as well as longer-term strategies, and allow the user to annotate the choices made and information found along the way. Users should be able to bundle search sessions as well as save individual portions of a given search session, and flexibly access and modify each. There is also increasing interest in incorporating personal preference and usage information both into formulation of queries and use of the results of search [277]. For the most part these strategies are not supported well in current user interfaces; however some mechanisms have been introduced that begin to address these needs. In particular, mechanisms to retain prior history of the search are useful for these tasks. Some kind of history mechanism has been made available in most search systems in the past. Usually these consist of a list of the commands executed earlier. More recently, graphical history has been introduced, that allows tracking of commands and results as well. Kim and Hirtle 318        USER INTERFACES AND VISUALIZATION Figure 10.31     The VISAGE interaction history visualization [685]. [440] present a summary of graphical history presentation mechanisms. Recently, a graphical interface that displays Web page access history in a hierarchical structure was found to require fewer page accesses and require less time when returning to pages already visited [370]. An innovation of particular interest for information access interfaces is exemplified by the saving of state in miniature form in a 'slide sorter' view as exercised by the VISAGE system for information visualization [685] (see Figure 10.31). The VISAGE application has the added advantage of being visual in nature and so individual states are easier to recognize. Although intended to be used as a presentation creation facility, this interface should also be useful for retaining search action history.
mir-0217	10.8.6    Integrating Scanning, Selection, and Querying User interfaces for information access in general do not do a good job of supporting strategies, or even of sequences of movements from one operation to the next. Even something as simple as taking the output of retrieval results from one query and using them as input to another query executed in a later search session is not well supported in most interfaces. Hertzum and Frokjaer [368] found that users preferred an integration of scanning and query specification in their user interfaces. They did not, however, observe better results with such interactions. They hypothesized that if interactions are too unrestricted this can lead to erroneous or wasteful behavior, and interaction between two different modes requires more guidance. This suggests that more flexibility is needed, but within constraints (this argument was also made in the discussion of the SuperBook system in section 10.6). There are exceptions. The new Web version of the Melyvl system provides ways to take the output of one query and modify it later for re-execution (see Figure 10.32), The workspace-based systems such as digital libraryITE and Rooms allow storage and reuse of previous state. However, these systems do not integrate the general search process well with scanning and selection of information from auxiliary structures. Scanning, selection, and querying needs to be better integrated in general This discussion will conclude with an example of an interface that does attempt to tightly couple querying and browsing. Help INTERFACE SUPPORT FOR THE SEARCH PROCESS 319 Personal Profile. Off [search ! personal author swanson, d j personal author swanson, d [and] title words j literature personal author swanson, d [ancQ information Item display: [Short Find^ore  j    Find Fewer  |     Diete Search    j Send questions, comments, or suggestions to iridvyl@www mejyyl ucop edu MelvylÆ is a registered trademark of The Regents of the University of California Figure 10.32     A view of query history revision in the Web-based version of the Melvyl bibliographic catalog. Copyright ©, The Regents of the University of California. The Cat-a-Cone interface integrates querying and browsing of very large category hierarchies with their associated text collections. The prototype system uses 3D-f animation interface components from the Information Visualizer [144], applied in a novel way, to support browsing and search of text collections and their category hierarchies. See Figure 10.33. A key component of the interface is the separation of the graphical representation of the category hierarchy from the graphical representation of the documents. This separation allows for a fluid, flexible interaction between browsing and search, and between categories and documents. It also provides a mechanism by which a set of categories associated with a document can be viewed along with their hierarchical context. Another key component of the design is assignment of first-class status to the representation of text content. The retrieved documents are stored in a 3D-hanimation book representation [144] that allows for compact display of moderate numbers of documents. Associated with each retrieved document is a page of links to the category hierarchy and a page of text showing the document contents. The user can "ruffle' the pages of the book of retrieval results and see corresponding changes in the category hierarchy, which is also represented in 3D+animation. All and only those parts of the category space that reflect the semantics of the retrieved document are shown with the document. The system allows for several different kinds of starting points. Users can start by typing in a name of a category and seeing which parts of the category hierarchy match it. For example, Figure 10.34 shows the results of searching on 320        USER INTERFACES AND VISUALIZATION I". Mir j´wMei ´´ªï n ïuftM-ntKigittanrKilirc  i1   f ! lt;tªp          ftgt;*k          itit.-i'          % jrª            Hti?          J[*n ª-         Vii Figure 10.33     The Cat-a-Cone interface for integrating category and text scanning and search [358]. 'Radiation' over the MeSH terms in this sub collection. The word appears under four main headings (Physical Sciences, Diseases, Diagnostics, and Biological Sciences). The hierarchy immediately shows why "Radiation' appears under Diseases ó as part of a subtree on occupational hazards. Now the user can select one or more of these category labels as input to a query specification. Another way the user can start is by simply typing in a free text query into an entry label. This query is matched against the collection. Relevant documents are retrieved and placed in the book format. When the user "opens* the book to a retrieved document, the parts of the category hierarchy that correspond to the retrieved documents are shown in the hierarchical representation. Thus, multiple intersecting categories can be shown simultaneously. In their hierarchical context. Thus, this interface fluidly combines large, complex metadata, starting points, scanning, and querying Into one Interface. The interface allows for a kind of relevance feedback, by suggesting additional categories that are related to the documents that have been retrieved. This interaction model is similar to that proposed by [5]. Recall the evaluation of the Kohonen feature map representation discussed in section 10.4. The experimenters found that some users expressed a desire for a visible hierarchical organization, others wanted an ability to zoom in on a subarea to get more detail and some* users disliked having to look through the entire map to find a theme, desiring an alphabetical ordering instead. The subjects liked the ease of being able to jump from one area to another without TRENDS AND RESEARCH ISSUES        321 EZEZJ-SH3  Figure 10.34     An interface for a starting point for searching over category labels [358]. having to back up (as is required in Yahoo!) and liked the fact that the maps have varying levels of granularity. These results all support the design decisions made in the Cat-a-Cone. Hierarchical representation of term meanings is supported, so users can choose which level of description is meaningful to them. Furthermore, different levels of description can be viewed simultaneously, so more familiar concepts can be viewed in more detail, and less familiar at a more general level An alphabetical ordering of the categories coupled with a regular expression search mechanism allows for straightforward location of category labels. Retrieved documents are represented as first-class objects, so full text is visible, but in a compact form. Category labels are disambiguated by their ancestor/descendant/sibling representation. Users can jump easily from one category to another and can in addition query on multiple categories simultaneously (something that is not a natural feature of the maps). The Cat-a-Cone has several additional advantages as well. such as allowing a document to be placed at the intersection of several categories, and explicitly linking document contents with the category representation.
mir-0218	10.9    Trends and Research Issues The importance of human computer interaction is receiving increasing recognition within the field of computer science [5871.  As should be evident from the 322        USER INTERFACES AND VISUALIZATION contents of this chapter, the role of the user interface in the information access process has only recently begun to receive the attention it deserves. Research in this area can be expected to increase rapidly, primarily because of the rise of the Web. The Web has suddenly made vast quantities of information available globally, leading to an increase in interest in the problem of information access. This has lead to the creation of new information access paradigms, such as the innovative use of relevance feedback seen in the Amazon.com interface. Because the Web provides a platform-independent user interface, investment in better user interface design can have an impact on a larger user population than before. Another trend that can be anticipated is an amplified interest in organization and search over personal information collections. Many researchers are proposing that in future a person's entire life will be recorded using various media, from birth to death. One motivation for this scenario is to enable searching over everything a person has ever read or written. Another motivation is to allow for searching using contextual clues, such as cfmd the article I was reading in the meeting I had on May 1st with Pam and Hal'. If this idea is pursued, it will require new, more sophisticated interfaces for searching and organizing a huge collection of personal information. There is also increasing interest in leveraging the behavior of individuals and groups, both for rating and assessing the quality of information items, and for suggesting starting points for search within information spaces. Recommender systems can be expected to increase in prevalence and diversity. User interfaces will be needed to guide users to appropriate recommended items based on their information needs. The field of information visualization needs some new ideas about how to display large, abstract information spaces intuitively. Until this happens, the role of visualization in information access will probably be primarily confined to providing thematic overviews of topic collections and displaying large category hierarchies dynamically. Breakthroughs in information visualization can be expected to have a strong impact on information access systems.
mir-0219	10.10    Bibliographic Discussion The field of human-computer interaction is a broad one, and this chapter touches on only a small subset of pertinent issues. For further information, see the excellent texts on user interface design by Shneidennan [725], information seeking behavior by Marchionini [542], and digital libraries by Lesk [501]. An excellent book on visual design is that of Mullet and Sano [580]. Tufte has written thought-provoking and visually engaging books on the power of information visualization [769. 770] and a collection of papers on information visualization has been edited by Card Vf ui [Ul], This chapter has discussed many ideas fur improving the human-computer interaction  experience  for  information  seekers.      This  is  the  most   rapidly BIBLIOGRAPHIC DISCUSSION        323 developing area of information access today, and improvements in the interface are likely to lead the way toward better search results and better-enabled information creators and users. Research in the area of human-computer interaction is difficult because the field is relatively new, and because it can be difficult to obtain strong results when running user studies. These challenges should simply encourage those who really want to influence the information access systems of tomorrow. AcknowSedgements The author gratefully acknowledges the generous and helpful comments on the contents of this chapter by Gary Marchionini and Ben Shneiderman, the excellent administrative assistance of Barbara Goto, and the great faith and patience of Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
mir-0221	11.1    Introduction The need for an integrated management for multimedia data is rapidly growing in several application environments such as offices, CAD/CAM applications, and medical applications. For this reason, multimedia information systems are widely recognized to be one of the most promising fields in the area of information management. The most important characteristic of a multimedia information system is the variety of data it must be able to support. Multimedia systems must have the capability to store, retrieve, transport, and present data with very heterogeneous characteristics such as text, images (both still and moving), graphs, and sound. For this reason, the development of a multimedia system is considerably more complex than a traditional information system. Conventional systems only deal with simple data types, such as strings or integers. On the contrary, the underlying data model, the query language, and the access and storage mechanisms of a multimedia system must be able to support objects with a very complex structure. The need then arises for developing Multimedia Information Retrieval (Multimedia IB. for short) systems specifically for handling multimedia data. Traditional IR systems (see Chapter 2) only deal with textual unstructured data; therefore, they are unable to support the mix of structured and unstructured data, and different kinds of media, typical of a Multimedia IR system. For instance, a traditional IR system does not support metadata information such as that provided by database schema, which is a fundamental component in a database management system (DBMS). On the other hand, Multimedia IR systems require some form of database schema because several multimedia applications need to structure their data at least partially. However, the notion of schema may need to be weakened with respect to the traditional notion to ensure a higher degree of flexibility in structuring data.   Moreover, 325 326        MULTIMEDIA IR: MODELS AND LANGUAGES a Multimedia IR system requires handling metadata which is crucial for data retrieval, whereas traditional IR systems do not have such requirement. The architecture of a Multimedia IR system depends on two main factors: first, the peculiar characteristics of multimedia data, and second, the kinds of operations to be performed on such data. In what follows, we briefly deal with both these aspects. Data Modeling A Multimedia IR system should be able to represent and store multimedia objects in a way that ensures their fast retrieval. The system should be therefore able to deal with different kinds of media and with semi-structured data, i.e., data whose structure may not match, or only partially match, the structure prescribed by the data schema. In order to represent semi-structured data, the system must typically extract some features from the multimedia objects. A related issue is how these features are extracted and efficiently maintained by the system. Data Retrieval The main goal of a Multimedia IR system is to efficiently perform retrieval based on user requests, exploiting not only data attributes, as in traditional DBMSs, but also the content of multimedia objects. This poses several interesting challenges, due to the heterogeneity of data, the fuzziness of information, the loss of information in the creation of indexes, and the need of an interactive refinement of the query result. Data retrieval relies on the following basic steps: (1)   Query specification.   In this step, the user specifies the request.   The query interface should allow the user to express fuzzy predicates for proximity searches (for example, "Find all images similar to a car"), content-based predicates (for example, 'Find multimedia objects containing an apple'), conventional predicates on the object attributes (for example, conditions on the attribute 'color of an image, such as 'Find all red images"), and structural predicates (for example, vFind ail multimedia objects containing a video clip'). (2)  Query processing and optimization. Similarly to traditional systems, the query is parsed and compiled into an internal form. In generating this internal representation, the query is also optimized, choosing the best evaluation plan. Note that, due to the presence of fuzzy terms, content-based predicates, and structural predicates, query processing is a very complex activity. A great amount of work has been done on query processing both in traditional [402] and spatial databases [247, 82, 118, 361, 623].  However, little work lias been done on query processing strategies for multimedia databases. The main problem is the heterogeneity of data: different query processing strategies, one for each data type, should be combined together in some wav. INTRODUCTION        327 (3)  Query answer. The retrieved objects are returned to the user in decreasing order of relevance. Relevance is measured as a distance function from the query object to the stored ones. (4)  Query iteration. In traditional DBMSs, the query process ends when the system returns the answer to the user.  In a Multimedia IR system, due to the inevitable lack of precision in the user request, the query execution is iterated until the user is satisfied.   At each iteration the user supplies the system with additional information by which the request is refined, reducing or increasing the number of returned answers. From the previous discussion it follows that a Multimedia IR system differs from a traditional IR system in two main aspects. First, the structure of multimedia objects is more complex than the structure of typical textual data, handled by traditional IR systems. This complexity requires the integration of traditional IR technology with the technology of multimedia database management systems to adequately represent, manage, and store multimedia objects. Note that the use of a DBMS also provides update functionalities and transaction management which are in general not covered by typical IR systems. Second, object retrieval is mainly based on a similarity approach. Moreover, the objects retrieved by a query are usually returned to the user in a ranked form. These aspects are successfully handled by IR techniques (see Chapter 2). However, IR systems have initially been developed to support libraries of articles, journals, and encyclopedic knowledge bases (see Chapter 2). In those systems, the fundamental unit is the textual document Thus, the techniques developed for traditional IR systems should be extended to deal with documents containing other media. Multimedia IR systems should therefore combine both the DBMS and the IR technology, to integrate the data modeling capabilities of DBMSs with the advanced and similarity-based query capabilities of IR systems. The resulting system will be able to answer attribute-based queries as well as content-based queries. The whole architecture of the resulting system, in particular the query optimizer, must take this aspect into account in order to efficiently support user requests. In this chapter, we discuss modeling and query language issues for multimedia objects, pointing out the differences and the analogies between a traditional IR system and a multimedia one. Problems related to feature extraction and searching are covered by Chapter 12. The first part of the chapter is devoted to the presentation of the most relevant models proposed in the literature for multimedia data, with particular attention to commercial proposals. The second part of the chapter investigates the peculiarities of multimedia query languages with respect to traditional ones. Then, as an example, two different language proposals are presented. Also in this case, we focus on commercial proposals and we discuss how the new standard SQL3 could be iLsed to deal with multimedia data retrieval. 328        MULTIMEDIA IR: MODELS AND LANGUAGES
mir-0222	11.2    Data Modeling As we have already remarked, the complex nature of multimedia data may benefit from the use of DBMS functions for data representation and querying. However, the integration of multimedia data in a traditional DBMS is not an easy task. Indeed, traditional DBMSs are mainly targeted to support conventional data. Multimedia data is inherently different from conventional data. The main difference is that information about the content of multimedia data are usually not encoded into attributes provided by the data schema (structured data). Rather, text, image, video, and audio data are typically unstructured. Therefore, specific methods to identify and represent content features and semantic structures of multimedia data are needed. Another distinguishing feature of multimedia data is its large storage requirements. One single image usually requires several Kbytes of storage, whereas a single second of video can require several Mbytes of storage. Moreover, the content of multimedia data is difficult to analyze and compare, in order to be actively used during query processing. Addressing data modeling issues in the framework of Multimedia IR systems entails two main tasks. First, a data model should be defined by which the user can specify the data to be stored into the system. Such a data model should have the ability of an integrated support for both conventional and multimedia data types and should provide methods to analyze, retrieve, and query such data. Second, the system should provide a model for the internal representation of multimedia data. The definition of such a model is crucial for the efficiency of query processing. As far as the first aspect is concerned, a promising technology with respect to the modeling requirements of multimedia data is the object-oriented one [89]. The richness of the data model provided by OODBMSs makes them more suitable than relational DBMSs for modeling both multimedia data types and their semantic relationships. Moreover, the concept of class can be naturally used to define ad hoc data types for multimedia data in that a class is characterized by both a set of attributes and a set of operations that can be performed on these attributes. Classes can, moreover, be related into inheritance hierarchies, thus allowing the definition of a multimedia class as a specialization of one or more superclasses. However, the performance of OODBMs in terms of storage techniques, query processing, and transaction management is not comparable to that of relational DBMSs. Another drawback of OODBMs is that they are highly non-standard. Indeed, even though a standard language has been defined by the Object Database Management Group (ODMG), very few systems support it. For all the above reasons, a lot of effort lias been devoted to the extension of the relational model with capabilities for modeling complex objects, typical of the object-oriented context. The goal of the so-called object-relational technology is to extend the relational model with the ability of representing complex data types by maintaining, at the same time, the performance and the simplicity of relational DBMSs and related query languages. The possibility of defining abstract data types inside the relational model allows one to define ad hoc data types for multimedia data. For instance, such data typ**s ran provide support for DATA MODELING        329 content-dependent queries. In the following section, we will give some examples of such extensions. The second problem related to data modeling is how multimedia data are represented inside the system. Due to the particular nature of multimedia data, it is not sufBcient to describe it through a set of attributes as usually done with traditional data. Rather, some information should be extracted from the objects and used during query processing. The extracted information is typically represented as a set of features; each multimedia object is therefore internally represented as a list of features, each of which represents a point in a multidimensional space. Multi-at tribute access methods can then be used to index and search for them (see Chapter 12). Features can be assigned to multimedia objects either manually by the user, or automatically by the system. In general, a hybrid approach is used, by which the system determines some of the values and the user corrects or augments them. In both cases, values assigned to some specific features, such as the shape of an image or the style of an audio object, are assigned to the object by comparing the object with some previously classified objects. For instance, to establish whether an image represents a car or a house, the shape of the image is compared with the shapes of already classified cars and houses before taking a decision. Finally, it is important to recall that feature extraction cannot be precise. Therefore, a weight is usually assigned to each feature value representing the uncertainty of assigning such a value to that feature. For example, if we are 80% sure that a shape is a square, we can store this value together with the recognized shape. From the previous discussion, it follows that data modeling in a Multimedia IR system is an articulated activity that must take into account both the complex structure of data and the need of representing features extracted from multimedia objects. In the following, we give a brief overview of some proposals to model multimedia data. We start by reviewing the support for multimedia data provided by commercial DBMSs. Then, as an example of a research proposal we survey the data model developed in the context of the MULTOS project.
mir-0223	11.2.1    Multimedia Data Support In Commercial DBMSs Most current relational DBMSs support variable-length data types which can be used to represent multimedia data. The way these data are supported by commercial DBMSs is mostly non-standard in that each DBMS vendor uses different names for such data types and provides support for different operations on them. For example, the Oracle DBMS provides the VARCHAR2 data type to represent variable length, character strings. The maximum length of VARCHAR2 data is 4000 bytes. The RAW and LONG RAW data types are used for data that is not to be interpreted by Oracle. These data types can be used to store graphics, sounds, or unstructured objects. LOB data types can be used to store Large unstructured data OBjects up to four gigabytes in size. BLOBs are used to store unstructured Binary Large OBjects, whereas CLOBs are used to store Character Large OBject data. 330        MULTIMEDIA IR: MODELS AND LANGUAGES The Sybase SQL server supports IMAGE and TEXT data types to store images and unstructured text, respectively, and provides a limited set of functions for their searching and manipulation. However, the support provided by the above mentioned data types is very limited in that the DBMS does not provide any interpretation of the data content. Moreover, operations that can be performed on such data by means of the built-in functions provided by the DBMS are very simple. As we have already remarked, most commercial relational DBMSs vendors are investing a lot of effort in extending the relational model with the capability of modeling complex objects, typical of the object-oriented context. Such efforts have given rise to the upcoming SQL3 standard. Prom a data modeling point of view, the major improvement provided by SQL3 with respect to its predecessor SQL-92, is the support for an extensible type system. Extensibility of the type system is achieved by providing constructs to define user-dependent abstract data types, in an object-oriented like manner. In SQL3, each type specification consists of both attribute and function specifications. A strong form of encapsulation is provided, in that attribute values can only be accessed by using some system functions. Moreover, user-defined functions can be either visible from any object or only visible in the object they refer to. Both single and multiple inheritance can be defined among user-defined types and dynamic late binding is provided [89]. SQL3 also provides three types of collection data types: sets, multisets, and lists. The elements of a collection must have compatible types. Several system-defined operations are provided to deal with collections. Besides the definition of user-dependent abstract data types, SQL3 provides a restricted form of object identifier that supports sharing and avoids data duplication. Although SQL3 has not yet been officially published, most commercial products have already implemented their proprietary versions of SQL3. An example in such direction is the data cartridges provided by Oracle for multimedia data handling, or the data blades supported by Illustra.f Oracle provides data cartridges for text, spatial data, image, audio and video data. To give a concrete example, OracleS provides a ConText cartridge, which is a text management solution combining data management capabilities of a traditional DBMS with advanced text retrieval and natural-language process technology. The ConText cartridge supports the most popular document formats, including ASCII, MS Word, and HTML. One of the most relevant feature of the ConText cartridge is its ability to find documents about a specific topic I thus providing a form of content-based retrieval). Content-based queries on text documents can be combined with traditional queries in the same SQL statement and can be efficiently executed due to the use of indexing techniques specific for texts. Such techniques are based on the notion of inverted files (see Chapter 8) which map a given word to the documents containing it, thus allowing a fast retrieval of all the documents containing a particular word. t  Ilhistni 'Aits acquired by Infoniiix in DATA MODELING        331 Illustra provides 3D and 2D spatial data blades for modeling spatial data. The supported data types include boxes, vectors, quadrangles, etc., and examples of supported operations are INTERSECT, CONTAINS, OVERLAPS, CENTER, and so on. Spatial data blades also implement R-trees for performing efficient spatial queries [330, 717]. The text data blade provides data types for representing unstructured text and performing content-based queries. For example, the method ContainWords can be used to search for all the documents containing a particular word. Moreover, Illustra supports a data blade which can be used to query images by content. The object-relational technology and its extensive type system is now starting to be widely used both in industrial and research projects. An example of this trend is the La Scala archive project, currently under development at the Laboratorio di Informatica Musicale of the University of Milano [254]. The goal of this project is the development of the multimedia archive of Teatro alia Scala, one of the best known musical temples of the world, using the Oracle technology and the related data cartridges. The system is organized around La Scala nights. Each night encompasses the phonic items, score, and other graphical and video items related to the performance. When a new performance has to be prepared, the musicians can easily access all the materials (such as CD-ROMs, video, photos, and scores) of previous editions of the same performance. Accessing such information has required the development of ad hoc cartridges to represent and query non-conventional data. For instance, we are currently developing a data cartridge that allows content-based queries on music scores. We apply pattern matching techniques to music scores to enable the user to sing a few bars into a microphone linked to the computer and see all the music scores containing a piece of music close to the one being sung. Users can then view the retrieved musical graphic scores, or excerpts from them, and simultaneously play the corresponding music. As an example of a data model suitable for a multimedia environment, in the following we consider the data model developed in the context of the MULTOS project [759].
mir-0224	11.2.2    The MULTOS Data Model MULTOS (KIULTimedia Office Server) is a multimedia document^ server with advanced document retrieval capabilities, developed in the context of an ESPRIT project in the area of Office Systems [759]. MULTOS is based on a client/server architecture. Three different types of document servers are supported: current servers^ dynamic servers, and archive servers, which differ in storage capacity and document retrieval speed. Such servers support filing and retrieval of multimedia objects based on document collections, document types, document attributes, document text, and images. |  As MULTOS deals with office services, in the following we use the words object and document agt;, svnonvnious. 332        MULTIMEDIA IR: MODELS AND LANGUAGES The MULTOS data model allows the representation of high level concepts present in the documents contained in the database, the grouping of documents into classes of documents having similar content and structure, and the expression of conditions on free text. Each document is described by a logical structure, a layout structure, and a conceptual structure. The logical structure determines arrangements of logical document components (e.g., title, introduction, chapter, section, etc.). The layout structure deals with the layout of the document content and it contains components such as pages, frames, etc. The conceptual structure allows a semantic-oriented description of the document content as opposed to the syntax-oriented description provided by the logical and layout structure. The conceptual structure has been added to provide support for document retrieval by content. MULTOS provides a formal model, based on a data structuring tool available in semantic data models, to define the document conceptual structure. The logical and layout structures are defined according to the ODA document representation [398]. Documents having similar conceptual structures are grouped into conceptual types. In order to handle types in an effective manner, conceptual types are maintained in a hierarchy of generalization, where a subtype inherits from its supertypes the conceptual structure and can then refine it. Types can be strong or weak. A strong type completely specifies the structure of its instances. A weak type, on the other hand, partially specifies the structure of its instances. Moreover, components of unspecified type (called spring component types) can appear in a document definition. Example 1 The conceptual structure of the type Generic .Letter is shown in Figure 11.1. The node Letter_Body is a spring conceptual component. The complete conceptual structure in Figure 11.2 corresponds to the type BusinessJProductJLetter. This type has been obtained from Generic. Letter by specialization of Letter JBody into a complex conceptual component, defined as an aggregation of five conceptual components. According to the conceptual model the document ^peBusiness_Prodnct_Letter is linked to the document type GenericXetter by an kis-a' relationship. In this example, the iJt' symbol attached to the Receiver component means that it is multivalued. Notice also that the Name and the Address appear in two subtrees having as roots the conceptual components Receiver and Sender, respectively. For document retrieval, conceptual types play the role of the database schema which enables the use of efficient access structures. Moreover, conceptual types are the basis for formulating queries at an abstract level. MULTOS also provides a sophisticated approach to deal with image data. First, an image analysis process is performed, consisting of two phases: low level image analysis and high level image analysis. During the low level image analysis phase, the basic objects composing a given image and their relative positions are identified. The high level image analysis phase deals with image interpretation according to the Dempster-Shaffer theory of evidence [60, 312;. DATA MODELING        333 Document Place Date Receiver+            Sender Name     Address                     Name Street        City     Country      Street        City      Country Figure 11.1     Conceptual structure of the type Generic_Letter. Document ^-ó----^ Place                   Date                 Receiver+           lender Name      Address                     Name      Address Street        City     Country      Street        City     Country LetterJBody CompanyJLogo Image       ProductJPresentation    \i/ Texl            Product_Description Text Signature Product_Cost Text Figure 11.2     Complete conceptual structure of the type Business-Product -Letter. At the end of the image analysis process, images are described in terms of the objects recognized, with associated belief and plausibility values, and the classes to which they belong. The information is then exploited in image access. Image access information is stored in an image header, associated with the image file. Access structures are then built for a fast access to image headers. Two types of index are constructed: ï  Object index. For each object a list is maintained. Each element of the lists is a pair (BI,IMH), where IMH is a pointer to the header of the image containing the object, and BI is the associated belief interval, representing the probability that the image considered really contains the object. ï  Cluster index. For each image class, a list of pairs (MFJMH) is maintained.  IMH is a pointer to an image header corresponding to an image with a non-null degree of membership to the class, and MF is the value of the membership degree.  The membership degree of an image to a given class is computed by comparing the image interpretation resulting from the analysis phase, with the class description, using techniques analogous to the ones used in text IR systems [698] (see Chapter 6). 334        MULTIMEDIA IR: MODELS AND LANGUAGES
mir-0225	11.3    Query Languages Queries in relational or object-oriented database systems are based on an exact match mechanism, by which the system is able to return exactly those tuples or objects satisfying some well specified criteria given in the query expression and nothing more. In general, query predicates specify which values the object attributes must contain. Because of the semi-structured nature of multimedia objects, the previous approach is no longer adequate in a Multimedia IR system. In this context, the user should still be able to query the content of multimedia objects by specifying values of semantic attributes but he/she should also be able to specify additional conditions about the content of multimedia data. Thus, the exact match is only one of the possible ways of querying multimedia objects. More often, a similarity-based approach is applied that considers both the structure and the content of the objects. Queries of the latter type are called content-based queries since they retrieve multimedia objects depending on their global content. Information on the global content of an object is not represented as attribute values in the database system. Rather, as we have already remarked in section 11.2, a set of information, called features, is extracted and maintained for each object. When the query is submitted, the features of the query object are matched with respect to the features of the objects stored in the database and only the objects that are more similar to the query one are returned to the user (see Chapter 12). The characteristics of content-based query processing impacts the definition of a multimedia query language and, in general, of the user interface. In particular, in designing a multimedia query language, three main aspects require attention: ï  How the user enters his/her request to the system, i.e., which interfaces are provided to the user for query formulation. ï  Which conditions on multimedia objects can be specified in the user request.  The conditions that can be expressed depend on the support the system provides for content-based retrieval (see Chapter 12). ï  How uncertainty, proximity, and weights impact the design of the query language. In the following, we discuss the above aspects in detail. Then, we present two examples of multimedia query languages. First, we illustrate how traditional relational query languages can be extended to deal with multimedia data, discussing the main characteristics of the upcoming SQL3 query language. Then. as an example of a research proposal, we introduce4 the query language supported by MULTOS (see section 11.2.2). QUERY LANGUAGES        335
mir-0226	11.3.1    Request Specification Two different interfaces can be presented to the user for querying multimedia objects. The first type of interface is based on browsing and navigation. Usually, due to the complex structure of multimedia objects, it may be useful to let users browse and navigate inside the structure of multimedia objects to locate the desired objects. Such an approach is typically used in CAD/CAM/CASE environments due to the complex structure of the objects under consideration. Navigation, however, is not always the best way to find multimedia objects, in that it may be heavily time consuming when the object desired is deeply nested. The second approach for selecting objects is therefore based, as traditionally in DBMSs, on specifying the conditions the objects of interest must satisfy, by means of queries. Queries, in turn, can be specified in two different ways: the first, typical of a traditional database context, is to enter the query by using a specific query language. However, in some cases (especially when images and audio data are considered), a query by example approach is preferred. Under this approach, queries are specified by using actual data inside a visual environment; the user provides the system with an object example that is then used to retrieve all the stored objects similar to the given one. For example, the user may choose a house and pose the query: 'Retrieve all houses of similar shape and different color.' This approach requires the use of a GUI environment where the user can pick examples and compose the query object. In order to pick examples, the system must supply some domains, i.e., sets of typical values, one for each object feature (see section 11.2).
mir-0227	11.3.2    Conditions on Multimedia Data Multimedia query languages should provide predicates for expressing conditions on the attributes, the content, and the structure of multimedia objects.   In general, query predicates can be classified into three different groups: ï  Attribute predicates concern the attributes (i.e., the structured content) of multimedia objects. ï  Structural predicates concern the structure of the data being considered. ï  Semantic predicates concern the semantic and unstructured content of the data involved. By the term attribute predicates we mean predicates against traditional attributes, i.e., attributes for which an exact value is supplied for each object. Examples of attributes are the speaker of an audio object, the size of an object, or its type. By querying these predicates, the system applies an exact-match retrieval, using the same techniques as traditional DBMSs. Structural predicates concern the structure of multimedia objects. Such predicates can be answered by using some form of metadata [99, 442; and 336        MULTIMEDIA IR: MODELS AND LANGUAGES information about the database schema. With respect to traditional databases, structural queries play a fundamental role in multimedia query processing, due to the complex structure of multimedia objects. An example of use of a structural predicate is the query: 'Find all multimedia objects containing at least one image and a video clip.' On the other hand, semantic predicates concern the semantic content of the queried data, depending on the features that have been extracted and stored for each multimedia object. An example of a semantic query is 'Find all the objects containing the word OFFICE.' Note that the word 'OFFICE' may appear either in a textual component of the object or as a text attribute of some image components. The query 'Find all the red houses' is a query on the image content. This query can be executed only if color and shape are features that have been previously extracted from images. Current systems support semantic predicates only with respect to specific features, such as the color, the shape, the texture, and sometimes the motion. For example, QBIC allows the retrieval of images with similar shapes or similar textures with respect to the object example specified in the query [257]. More innovative approaches include the Name-it project, whose aim is to process a video clip and automatically associate spoken or typed names with their corresponding faces [708]. The main difference between attribute predicates and semantic predicates is that, in the latter case, an exact match cannot be applied. This means that there is no guarantee that the objects retrieved by this type of predicate are 1009? correct or precise. In general, the result of a query involving semantic predicates is a set of objects, each of which has an associated degree of relevance with respect to the query. The user can subsequently select the better matches and submit the query again. Structural and semantic predicates can also refer to spatial or temporal properties of multimedia objects. Spatial semantic predicates specify conditions about the relative positions of a set of objects in an image or a video. Examples of spatial semantic predicates are: contain, intersect, is contained in, is adjacent to. Temporal semantic predicates are mainly related to continuous media, like audio and video. They allow one to express temporal relationships among the various frames of a single audio or video. For example, the query ¶"Fiuci all the objects that contain an audio component, where the hint of the discussion is first policy, and then economy' is a temporal audio query. From the point of view of structural predicates, spatial and temporal predicates can be used to specify temporal synchronization properties and spatial layout properties for the presentation of multimedia objects [87, 88]. For instance, in the query: 'Find all the objects containing an image overlapping the associated text', a spatial structural predicate Is used to impose a condition on the spatial layout of the retrieved objects. Analogously, the query: 'Find all the objects in which a jingle is played for the duration of an linage display1 is an example of a structural temporal query. Note, moreover, that temporal and spatial predicates can be combined to express more articulated requirements. An example Is the query: "Find all the objects in which the logo of a car company QUERY LANGUAGES        337 is displayed and, when it disappears, a graphic showing the increases in the company sales is shown in the same position where the logo was,' Due to the complex structure of multimedia objects, all the previous types of predicates can refer either to the whole object or, if the underlying data model supports complex object representation, to some subcomponents of the object. In the last case, the query language must also be able to navigate the object structure. A typical example in this direction is represented by path expressions in object-oriented systems [89].
mir-0228	11.3.3    Uncertainty, Proximity, and Weights in Query Expressions As we have already remarked, the execution of a content-dependent query returns a set of relevant objects. An interesting aspect in designing a multimedia query language is how it is possible to specify the degree of relevance of the retrieved objects. In general, this can be done in three different ways: ï  By using some imprecise terms and predicates, such as normal, unacceptable, typical. Each of those terms does not represent a precise value but a set of possible acceptable values with respect to which the attribute or the feature has to be matched. ï  By specifying particular proximity predicates.  In this case, the predicate does not represent a precise relationship between objects or between attributes/features and values. Rather, the relationship represented is based on the computation of a semantic distance between the query object and the stored ones, on the basis of the extracted features. The Nearest object search is an example of proximity predicate, by which the user requests all the objects which are closest or within a certain distance of a given object. Indexing support for this kind of query is discussed in Chapter 12. ï  By assigning each condition or term a given weighty specifying the degree of precision by which a condition must be verified by an object. For example, the query 'Find all the objects containing an image representing a screen (HIGH) and a keyboard (LOW)' [657], can be used to retrieve all the objects containing an image representing a screen and a keyboard. However, the objects containing only a screen are also retrieved and returned to the user, after the ones containing both the screen and the keyboard, since the condition imposing the containment of a keyboard is weaker than the condition imposing the containment of a screen. The use of imprecise terms and relationships, as well as the use of weights, allows the user to drive the similarity-based selection of relevant objects. The corresponding query is executed by assigning some importance and preference values to each predicate and term. Then* objects are retrieved and presented to the user as an ordered list. This ordering is given by a score associated with each object, giving a measure of the matching degree between the object and 338        MULTIMEDIA IR: MODELS AND LANGUAGES the query. The computation of the score is based on probabilistic models, using the preference values assigned to each predicate.
mir-0229	11.3.4    Some Proposals In the following we briefly survey some query languages supporting retrieval of multimedia objects. In order to describe how standard languages are evolving to support multimedia applications, we first describe the facilities provided by the upcoming standard SQL3 to support such kinds of applications. Then, we present the query language supported by the MULTOS system [90], introduced in section 11.2.2. The SQL3 Query Language As we have seen in section 11.2.1, the extensible type system and in general the ability to deal with complex objects make SQL3 suitable for modeling multimedia data. From the query language point of view, the major improvements of SQL3 with respect to SQL-92 can be summarized as follows: ï  Functions and stored procedures.  SQL3 allows the user to integrate external functionalities with data manipulation. This means that functions of an external library can be introduced into a database system as external functions. Such functions can be either implemented by using an external language, and in this case SQL3 only specifies which is the language and where the function can be found, or can be directly implemented by using SQL3 itself. In this way, impedance mismatch between two different programming languages and type systems is avoided. Of course, this approach requires an extension of SQL with imperative programming languages constructs. ï  Active database facilities. Another important property of SQL3 is the support of active rules, by which the database is able to react to some system- or user-dependent events by executing specific actions.   Active rules, or triggers, are very useful to enforce integrity constraints. From the multimedia perspective point of view, the aspects described make SQL3 suitable for being used as an interface language for multimedia applications, in particular, the ability to deal with external functions and user-defined data types enables the language to deal with objects with a complex structure, as multimedia objects. Note that, without this characteristic, the ability to deal with BLOB would have been useless since it reduces the view of multimedia data to single large uninterpreted data values, which are not adequate for the rich semantics of multimedia data. By the use of triggers, spatial and temporal constraints can be enforced, thus preserving the database consistency. Finally, as SQL3 is a widespread standard, it allows one to model multimedia objects in the framework of a well understood technology. QUERY LANGUAGES        339 Though the above facilities make SQL3 suitable for use as an interface for multimedia applications, there are also some limitations. The main drawback is related to retrieval support and, as a consequence, optimization. Indeed, no IR techniques are integrated into the SQL3 query processor. This means that the ability to perform content-based search is application dependent. As a consequence, objects are not ranked and are therefore returned to the application as a unique set. Moreover, specialized indexing techniques can be used but they are not transparent to the user. Bearing in mind the previous limitations, several projects have already been started with the aim of integrating SQL3 with IR facilities. An example of such a project is represented by SQL/MM Pull Text [190]. Text is in this case considered as a nested sequence of words, sentences, and paragraphs. In order to precisely capture the structure and the meaning of the words, SQL/MM Full Text is also able to view the text as a tree structure entity. The structure of this entity is controlled by a grammar. These facilities allow one to easily express queries to perform selection on the basis of the text content and/or text structure. There have also been several proposals for introducing spatial data types and predicates into the SQL framework. Among them, we recall Probe [623], Spatial SQL [231], Pictorial SQL [687], and QBE [418]. The MULTOS Query Language The development of the MULTOS query language has been driven by a number of requirements: first, it should be possible to easily navigate through the document structure. Path-names can be used for this purpose. Path-names can be total, if the path identifies only one component, or partial, if several components are identified by the path. Path-names are similar to object-oriented path expressions. Queries both on the content and on document structure must be supported. Query predicates on complex components must be supported. In this case, the predicate applies to all the document subcomponents that have a type compatible with the type required by the query. This possibility is very useful when a user does not recall the structure of a complex component. In general, a MULTOS query has the form: FIND DOCUMENTS VERSION version-clause SCOPE scope-clause TYPE type-clause WHERE condition-clause WITH component where: The version-clause specifies which versions of the documents should be considered by the query. 340        MULTIMEDIA IR: MODELS AND LANGUAGES Æ The scope-clause restricts the query to a particular set of documents. This set of documents is either a user-defined document collection or a set of documents retrieved by a previous query. ï  The type-clause allows the restriction of a query to documents belonging to a prespecifled set of types. The conditions expressed by the condition-clause only apply to the documents belonging to these types and their subtypes. When no type is specified, the query is applied to all document types. Æ The condition-clause is a Boolean combination of simple conditions (i.e., predicates) on documents components. Predicates are expressed on conceptual components of documents. Conceptual components are referenced by path-names. The general form of a predicate is: component restriction where component is a path-name and restriction is an operator followed by an expression. ï  The with-clause allows one to express structural predicates. Component is a path-name and the clause looks for all documents structurally containing such a component. Different types of conditions can be specified in order to query different types of media. In particular, MULTOS supports three main classes of predicates: predicates on data attributes, on which an exact match search is performed; predicates on textual components, determining all objects containing some specific strings; and predicates on images, specifying conditions on the image content. Image predicates allow one to specify conditions on the class to which an image should belong or conditions on the existence of a specified object within an image and on the number of occurrences of an object within an image. The following example illustrates the basic features of the MULTOS query language. Example 2  Consider the conceptual structure GenericJLetter, presented in example 1.  The following is an example of query: FIND DOCUMENT VERSIONS LAST WHERE Document.Date gt;  1/1/1998 AND (*Sendei\Name = "Olivetti" OR *Product_Presentation CONTAINS  "Olivetti")  AMD *ProdTict_Description CONTAINS  "Personal Computer11  AND (?Address.Country =  "Italy"  OR TEXT CONTAINS  "Italy")  AND WITH *Company_Logo. According to this query, the user looks for the last version of all documents, dated after January 1998, containing a company logo, having the word 'Olivetti' either as sender name or in the product presentation (which is a textual component), with the word 'Personal Computer' in the product description section TRENDS AND RESEARCH ISSUES        341 (which is another textual component) and with the word 'Italy' either constituting the country in the address or contained in any part of the entire document. Symbol '*' indicates that the path-name is not complete, that is, it could identify more than one component. The query language provided by MULTOS also supports the specification of imprecise queries that can be used when the user has an uncertain knowledge about the content of the documents he/she is seeking [657]. Such uncertainty is expressed by associating both a preference and an importance value with the attributes in the query. Such values are then used for ranking the retrieved documents. The following example illustrates the discussion. Example 3  The query: FIND DOCUMENT VERSIONS LAST WHERE (Document.Date BETWEEN   (12/31/1998,1/31/98)   PREFERRED BETWEEN   (2/1/1998,2/15/98)   ACCEPTABLE)   HIGH AND (*Sender.Name =  "Olivetti"  OR *Product_Presentation CONTAINS "Olivetti") HIGH AND (*ProductJ)escription CONTAINS "Personal Computer") HIGH AND (*ProductJDescription CONTAINS "good ergonomics") LOW AND (?Address.Country = "Italy" OR TEXT CONTAINS "Italy") HIGH AND WITH *CompanyJLogo HIGH (IMAGE MATCHES screen HIGH keyboard HIGH AT LEAST 2 floppy .drives LOW)  HIGH finds the last versions of all documents written in January, but possibly even at the beginning of February 1998, containing a company logo, having the word 'Olivetti' either as sender name or in the product presentation, with the word 'Personal Computer' in the product description section, and with the word 'Italy' either constituting the country in the address or contained in any part of the entire document. Personal Computers are described in the product description section as products having good ergonomics. Moreover, the document should contain a picture of the Personal Computer, complete with screen and keyboard, with at least two floppy drives. The value fL0W3 associated with the condition on 'good ergonomics7 indicates that the user formulating the query is not completely sure about this description of PC. By contrast, he/she is sure of all the conditions whose associated value is HIGH.7
mir-0230	11.4    Trends and Research Issues In this chapter we have discussed the main issues in developing a Multimedia IR system. We have observed that only the integration of DBMS and IR technologies provides the ability to represent, store, and manipulate multimedia data and, at the same time, to retrieve those data by applying content-based searches. 342        MULTIMEDIA IR: MODELS AND LANGUAGES We then discussed the main issues arising in defining a data model for multimedia data. Since multimedia data has, in general, a complex structure, the data model must be able to reflect and manage this complexity. Object-oriented or object-relational data models represent the right technology for multimedia data representation. Additional relevant requirements include the support of semi-structured data and metadata. Another important requirement is the ability to internally represent the content of multimedia data in a way that ensures fast retrieval of the stored data and efficient processing of content-based queries. To achieve this goal, semantic features can be extracted from the data, stored inside the system, and used during query processing. The second topic discussed in this chapter is related to multimedia query languages. We observed that a multimedia query language is characterized by the type of interface presented to the user and the types of predicates it allows in a query. Such predicates are used to perform content-based searches and to let the user drive the selection of relevant objects. Examples of commercial and prototype systems have been discussed, with respect to the data modeling and query language capabilities. Several aspects require further investigation. For example, even though SQL3 supports multimedia data representation, it cannot be taken as the basis for the definition of a Multimedia IR system. Additional research is needed to integrate SQL3 with specific language constructs and underlying techniques to perform information retrieval and query optimization. Another topic is related to XML (see Chapter 6), the new standard format for data on the Web [304]. XML is a text-based format, providing a standard data model to encode the content, the semantics, and the schema of ordinary documents, structured records, and metacontent information about a Web site. The extension of such a standard to support multimedia data and content-based queries is an important research direction. A further direction concerns the techniques for ranking the objects returned by a partial-match query. Such ranking usually only takes into account the degree of similarity of the objects retrieved with the query request. However, other factors can be considered, such as the profile of the user submitting the query, or the history of the previous queries specified by the user. Taking into account these aspects is very important, since it gives rise to a customized ranking which is closer to the user needs.
mir-0231	11.5    Bibiographic Discussion As we have seen, due to their complex nature, the object-oriented paradigm seems the right approach to model multimedia data.    Details about object-oriented database models and architectures can be found in [89].   The object database standard, as defined by the Object Database Management Group, is presented in [150]. On the research side, several models have been proposed for multimedia BIBIOGRAPHIC DISCUSSION        343 data. Such proposals range from data models suitable for a particular media type, like data models for videos [211, 238, 297, 621], data models for images [170] or models for spatial data [623], to general-purpose multimedia data models [169, 296, 397, 545, 759, 827]. Issues related to the definition and the classification of metadata in the multimedia context are extensively discussed in [99, 442]. Among the systems supporting similarity-based queries, we recall QBIC [257], Name-It [708], QBE [418], Probe [623], and PICQUERY [418]. For additional details about video and image multimedia databases we refer the reader to [405] and [438], respectively. Details about modeling and architectural aspects of the MULTOS system can be found in [759].
mir-0233	12.1    Introduction The problem we focus on here is the design of fast searching methods that will search a database of multimedia objects to locate objects that match a query object, exactly or approximately. Objects can be two-dimensional color images, gray-scale medical images in 2D or 3D (e.g., MRI brain scans), one-dimensional time series, digitized voice or music, video clips, etc. A typical query by content would be, e.g., 'zn a collection of color photographs, find ones with the same color distribution as a sunset photograph.' Specific applications include image databases; financial, marketing and production time series; scientific databases with vector fields; audio and video databases; DNA/Genorne databases; etc. In such databases, typical queries would be ''find companies whose stock prices move similarly," or 'find images that look like a sunset^ or cfind medical X-rays that contain something that has the texture of a tumor.'' Searching for similar patterns in such databases as the above is essential, because it helps in predictions, computer-aided medical diagnosis and teaching, hypothesis testing and, in general, in 'data mining' [8] and rule discovery. Of course, the distance of two objects has to be quantified. We rely on a domain expert to supply such a distance function Definition      Given two objects, O\ and 02, the distance (= dissimilarity) of the two objects is denoted by Tgt;(OuO2)                                                                                    (12.1) For example, if the objects are two (equal-lengt.h) time series, the distance V{) could be their Euclidean distance (the root of the sum of squared differences). Similarity queries can been classified into two categories; 345 346        MULTIMEDIA IR: INDEXING AND SEARCHING ï  Whole match Given a collection of N objects Oi, 0%,..., On and a query object Q, we want to find those data objects that are within distance s from Q. Notice that the query and the objects are of the same type: for example, if the objects are 512 x 512 gray-scale images, so is the query. ï  Sub-pattern match Here the query is allowed to specify only part of the object. Specifically, given N data objects (e.g., images) Oi, O2, ï ï ï, On, a query (sub-)object Q and a tolerance £, we want to identify the parts of the data objects that match the query.   If the objects are, e.g., 512x512 gray-scale images (like medical X-rays), in this case the query could be, e.g., a 16x16 subpattern (e.g., a typical X-ray of a tumor). Additional types of queries include the cnearest neighbors" queries (e.g., 'find the five most similar stocks to IBM's stock9) and the 'all pairs'' queries or "spatial joins' (e.g., 'report all the pairs of stocks that are within distance e from each other'). Both the above types of queries can be supported by the approach we describe next. As we shall see, we reduce the problem into searching for multi-dimensional points, which will be organized in R-trees; in this case, nearest-neighbor search can be handled with a branch-and-bound algorithm and the spatial join query can be handled with recent, highly fine-tuned algorithms, as discussed in section 12.8. Thus, we do not focus on nearest-neighbor and 'all-pairs' queries. For all the above types of queries, the ideal method should fulfill the following requirements: ï  It should be fast. Sequential scanning and distance calculation with each and every object will be too slow for large databases. ï  It should be 'correct'  In other words, it should return all the qualifying objects, without missing any (i.e., no 'false dismissals'). Notice that 'false alarms" are acceptable, since they can be discarded easily through a postprocessing step.    Of course, as we see, e.g. in Figure 12.5, we try to keep their number low (but not necessarily minimal), so that the total response time is minimized. ï  The ideal method should require a small space overhead. ï  The method should be dynamic.  It should be easy to insert, delete, and update objects. As we see next, the heart of the presented 'GEMINI* approach is to use / feature extraction functions to map objects into points in /-dimensional space; thus, we can use highly fine-tuned database spatial access methods to accelerate the search. The remainder of the chapter is organized as follows. Section 12.2 gives some background material on past related work on spatial access methods. Section 12.3 describes the main ideas for GEMINI, a generic approach to indexing multimedia objects. Section 12.4 shows the application of the approach for ID time series indexing.   Section 12.5 gives another case study, for color images. BACKGROUND ó SPATIAL ACCESS METHODS        347 within the QBIC project. Section 12.6 presents 'FastMap', a method to do automatic feature extraction. Section 12.7 summarizes the conclusions and lists problems for future research and section 12.8 provides pointers to the related bibliography.
mir-0234	12.2    Background ó Spatial Access Methods As mentioned earlier, the idea is to map objects into points in /-D space, and to use multiattribute access methods (also referred to as spatial access methods or SAMs)  to cluster them and to search for them. Thus, a brief introduction to multidimensional indexing methods (or spatial access methods) is in order. The prevailing methods form three classes: (1) R*-trees and the rest of the R-tree family,   (2) linear quadtrees, and (3) grid-files. Several of these methods explode exponentially with the dimensionality, eventually reducing to sequential scanning. For linear quadtrees, the effort is proportional to the hypersurface of the query region [244]; the hypersurface grows exponentially with the dimensionality. Grid files face similar problems, since they require a directory that grows exponentially with the dimensionality. The R-tree-based methods seem to be most robust for higher dimensions, provided that the fanout of the R-tree nodes remains gt; 2. Below, we give a brief description of the R-tree method and its variants, since it is one of the typical representatives of spatial access methods. The R-tree represents a spatial object by its minimum bounding rectangle (MBR). Data rectangles are grouped to form parent nodes, which are recursively grouped, to form grandparent nodes and, eventually, a tree hierarchy. The MBR of a parent node completely contains the MBRs of its children; MBRs are allowed to overlap. Nodes of the tree correspond to disk pages. Disk pages, or 'disk blocks', are consecutive byte positions on the surface of the disk that are typically fetched with one disk access. The goal of the insertion, split, and deletion routines is to give trees that will have good clustering, with few, tight parent MBRs. Figure 12.1 illustrates data rectangles (in black), organized in an R-tree with fanout 3. Figure 12.2 shows the file structure for the same R-tree, where nodes correspond to disk pages. A range query specifies a region of interest, requiring all the data regions that Intersect it. To answer this query, we first retrieve a superset of the qualifying data regions: we compute the MBR of the query region, and then we recursively descend the R-tree, excluding the branches whose MBRs do not intersect the query MBR. Thus, the R-tree will give us quickly the data regions whose MBR intersects the MBR of the query region. The retrieved data regions will be further examined for intersection with the query region. Algorithms for additional operations (nearest neighbor queries, spatial joins, insertions, and deletions) are more complicated and are still under research (see the Bibliographic Discussion). The original R-tree paper inspired much follow-up work, as described in 348        MULTIMEDIA IR: INDEXING AND SEARCHING -------1 T ii Figure 12.1 Data (dark rectangles) organized in an R-tree with fanout = 3. Solid, light-dashed, and heavy-dashed lines indicate parents, grandparents and great-grandparent (the root, in this example). Figure 12.2    The file structure for the R-tree of the previous figure (fanout = 3). section 12.8. It is important to highlight, however, that any spatial access method can be used (like i?*-trees, X-trees, SR-trees, and so on).
mir-0235	12.3    A Generic Multimedia Indexing Approach To illustrate the basic idea, we shall focus on 'whole match' queries.  For such queries the problem is defined as follows: ï  We have a collection of A" objects: Oi, O2, ..., O4\-. ï  The distance/dissimilarity between two objects (Oi^Oj) is given by the function T)(Ot,O3), which can be implemented as a (possibly, slow) pro-grain. ï   The user specifies a query object Q* and a tolerance e. A GENERIC MULTIMEDIA INDEXING APPROACH        349 Our goal is to find the objects in the collection that are within distance e from the query object. An obvious solution is to apply sequential scanning: For each and every object Oi (1 lt; i lt; JV), we can compute its distance from Q and report the objects with distance Tgt;(Q, Oi) lt; e. However, sequential scanning may be slow, for two reasons: (1)  The distance computation might be expensive. For example, as discussed in Chapter 8, the editing distance in DNA strings requires a dynamic programming algorithm, which grows like the product of the string lengths (typically, in the hundreds or thousands, for DNA databases). (2)  The database size N might be huge. Thus, we are looking for a faster alternative. The GEMINI (GEneric Multimedia object INdexIng) approach we present next, is based on two ideas, each of which tries to avoid each of the two disadvantages of sequential scanning: Æ a 'quick-and-dirty'  test,  to discard quickly the vast majority of nonqualifying objects (possibly, allowing some false alarms); Æ the use of spatial access methods, to achieve faster-than-sequential searching. The case is best illustrated with an example. Consider a database of time series, such as yearly stock price movements, with one price per day. Assume that the distance function between two such series S and Q is the Euclidean distance V(S,Q) = where S[i] stands for the value of stock S on the z-th day. Clearly, computing the distance of two stocks will take 365 subtractions and 365 squarings in our example. The idea behind the quick-and-dirty test is to characterize a sequence with a single number, which will help us discard many non-qualifying sequences. Such a number could be, e.g., the average stock price over the year. Clearly, if two stocks differ in their averages by a large margin, it is impossible that they will be similar. The converse is not true, which is exactly the reason we may have false alarms. Numbers that contain some information about a sequence (or a multimedia object, in general), will be referred to as 'features for the rest of this chapter. Using a good feature (like the 'average,' in the stock prices example), we can have a quick test, which will discard many stocks, with a single numerical comparison for each sequence (a big gain over the 365 subtractions and squarings that the original distance function requires). If using one feature is good, using two or more features might be even better, because they may reduce the number of false alarms (at the cost of 350        MULTIMEDIA IR: INDEXING AND SEARCHING making the quick-and-ciirty test a bit more elaborate and expensive). In our stock prices example, additional features might be, e.g., the standard deviation, or, even better, some of the discrete Fourier transform (DFT) coefficients, as we shall see in section 12.4. The end result of using / features for each of our objects is that we can map each object into a point in /-dimensional space. We shall refer to this mapping as T() (for T'eature): Definition     Let JF() be the mapping of objects to f-dimensional points, that is, T(O) will be the f-D point that corresponds to object O. This mapping provides the key to improve on the second drawback of sequential scanning: by organizing these f-D points into a spatial access method, we can cluster them in a hierarchical structure, like the R*-trees. Upon a query, we can exploit the i?*-tree, to prune out large portions of the database that are not promising. Thus, we do not even have to do the quick-and-dirty test on all of the f-D points! Figure 12.3 illustrates the basic idea: Objects (e.g., time series that are 365 points long) are mapped into 2D points (e.g., using the average and the standard deviation as features). Consider the 'whole match' query that requires all the objects that are similar to Sn within tolerance e: this query becomes an f-D sphere in feature space, centered on the image J-(Sn) of Sn. Such queries on multidimensional points is exactly what R-trees and other SAMs are designed to answer efficiently. More specifically, the search algorithm for a wThole match query is as follows: Feature2 /'   \ / 365 I   ,                                                                         .'                          Feature 1 I                                    365 Figure 12.3    Illustration of the basic idea: a database of sequences S\.....S.v; each sequence is mapped to a point in feature space; a query with tolerance s becomes a sphere of radius 5. A GENERIC MULTIMEDIA INDEXING APPROACH        351 Algorithm 1 Search: (1)  Map the query object Q into a point F(Q) in feature space. (2)  Using a spatial access method, retrieve all points within the desired tolerance e fromF(Q). (3)  Retrieve the corresponding objects, compute their actual distance from Q and discard the false alarms. Intuitively, the method has the potential to relieve both problems of the sequential scan, presumably resulting in much faster searches. The only step that we have to be careful with is that the mapping !F() from objects to /-D points does not distort the distances. Let V{) be the distance function of two objects, and Vfeature() be the (say, Euclidean) distance of the corresponding feature vectors. Ideally, the mapping should preserve the distances exactly, in which case the SAM will have neither false alarms nor false dismissals. However, requiring perfect distance preservation might be difficult. For example, it is not obvious which features we have to use to match the editing distance between two DNA strings. Even if the features are obvious, there might be practical problems: for example, in the stock price example, we could treat every sequence as a 365-dimensional vector; although in theory a SAM can support an arbitrary number of dimensions, in practice they all suffer from the 'dimensionality curse,' as discussed earlier. The crucial observation is that we can guarantee that there will be no false dismissals if the distance in feature space matches or underestimates the distance between two objects. Intuitively, this means that our mapping T() from objects to points should make things look closer (i.e., it should be a contractive mapping). Mathematically, let O\ and O2 be two objects (e.g., same-length sequences) with distance function V() (e.g., the Euclidean distance) and F(O\), F{02) be their feature vectors (e.g., their first few Fourier coefficients), with distance function Vfeaiure() (e.g., the Euclidean distance, again). Then we have: Lemma 12.1 (Lower Bounding) To guarantee no false dismissals for whole-match queries, the feature extraction function P() should satisfy the following formula: VfeatUre{HOi),F(O2)) lt; V{OUO2)                                            (12.3) As proved in [249], lower-bounding the distance works correctly for range queries.   Will it work for the other queries of interest, like "all pairs" and 'nearest neighbor' ones? The answer is affirmative in both cases. An "all pairs* query can easily be handled by a "spatial join" on the points of the feature space: using a similar reasoning as before, we see that the resulting set of pairs will be a superset of the qualifying pairs. For the nearest neighbor query, the following algorithm guarantees no false dismissals: (1) find the point T{P) that is the 352        MULTIMEDIA IR: INDEXING AND SEARCHING nearest neighbor to the query point !F(Q), (2) issue a range query, with query object Q and radius e = U(Q,P) (i.e., the actual distance between the query object Q and data object P). In conclusion, the GEMINI approach to indexing multimedia objects for fast similarity searching is as follows: Algorithm 2 (GEMINI) GEneric Multimedia object INdexIng approach: (1)  Determine the distance function Tgt;() between two objects. (2)  Find one or more numerical feature-extraction functions,   to provide a 'quick-and-dirty' test. (3)  Prove that the distance in feature space lower-bounds the actual distance Tgt;(), to guarantee correctness. (4)   Use a SAM (e.g., an R-tree), to store and retrieve the f-D feature vectors. The first two steps of GEMINI deserve some more discussion: the first step involves a domain expert. The methodology focuses on the speed of search only; the quality of the results is completely relying on the distance function that the expert will provide. Thus, GEMINI will return exactly the same response set (and therefore, the same quality of output, in terms of precision-recall) that would be returned by a sequential scanning of the database; the only difference is that GEMINI will be faster. The second step of GEMINI requires intuition and imagination. It starts by trying to answer the question (referred to as the 'feature-extracting' question for the rest of this chapter): 'Feature-extracting' question: If we are allowed to use only one numerical feature to describe each data object, what should this feature be? The successful answers to the above question should meet two goals: first, they should facilitate step 3 (the distance lower-bounding), and second, they should capture most of the characteristics of the objects. We give case studies of steps 2 and 3 of the GEMINI algorithm in the following sections. The first involves ID time series, and the second focuses on 2D color images. We shall see that the philosophy of the quick-and-dirty filter, in conjunction with the lower-bounding lemma, can lead to solutions to two problems: ï  the dimensionality curse (time series) ï  the 'cross-talk' of features (color images). For each case study, we first describe the objects and the distance function, then show how to apply the lower-bounding lemma, and finally give experimental results, on real or realistic data. ONE-DIMENSIONAL TIME SERIES        353
mir-0236	12.4    One-dimensional Time Series Here the goal is to search a collection of (equal-length) time series, to find the ones that are similar to a desirable series. For example, Hn a collection of yearly stock price movements, find the ones that are similar to IBM?
mir-0237	12.4.1    Distance Function According to GEMINI (algorithm 2), the first step is to determine the distance measure between two time series. A typical distance function is the Euclidean distance (equation 12.2), which is routinely used in financial and forecasting applications. Additional, more elaborate distance functions, that, for example, include time-warping, are discussed in section 12.8.
mir-0238	12.4.2    Feature Extraction and Lower-bounding Having decided on the Euclidean distance as the dissimilarity measure, the next step is to find some features that can lower-bound it. We would like a set of features that first, preserve/lower-bound the distance, and second, carry much information about the corresponding time series (so that the false alarms are few). The second requirement suggests that we use 'good' features, that have much discriminatory power. In the stock price example, a 'bad' feature would be, e.g., the first day's value: the reason being that two stocks might have similar first-day values, yet they may differ significantly from then on. Conversely, two otherwise similar sequences may agree everywhere, except for the first day's values. At the other extreme, we could use the values of all 365 days as features. However, although this would perfectly match the actual distance, it would lead to the 'dimensionality curse' problem. Clearly, we need some better features. Applying the second step of the GEMINI algorithm, we ask the feature-extracting question: 'If we are allowed to use only one feature from each sequence, what would this feature be?' A natural answer is the average. By the same token, additional features could be the average of the first half, of the second half, of the first quarter, etc. Or, in a more systematic way, we could use the coefficients of the Fourier transform, and, for our case, the Discrete Fourier Transform (DFT). For a signal x = [xl], i = 0,..., n -~ 1, let Xp denote the n-point DFT coefficient at the F-th frequency ) The third step of the GEMINI methodology is to show that the distance in feature space lower-bounds the actual distance. The solution is provided by ParsevaFs theorem, which states that the DFT preserves the energy of a signal, as well as the distances between two signals: )                                                                     (12.4) 354        MULTIMEDIA IR: INDEXING AND SEARCHING where X and Y are Fourier transforms of x and y respectively. Thus, if we keep the first /(/ lt; n) coefficients of the DFT as the features, we lower-bound the actual distance: £ \XF - YF\2 F=0 F=0 n-1 i=0 and finally Vfeature^), ?($))    lt;   VV)                                        (12-5) because we ignore positive terms from equation 12.2. Thus, there will be no false dismissals, according to lemma 12.1. Notice that the GEMINI approach can be applied with any orthonormal transform, such as, the Discrete Cosine Transform (DCT), the wavelet transform etc., because they all preserve the distance between the original and the transformed space. In fact, our response time will improve with the ability of the transform to concentrate the energy: the fewer the coefficients that contain most of the energy, the more accurate our estimate for the actual distance, the fewer the false alarms, and the faster our response time. Thus, the performance results presented next are just pessimistic bounds; better transforms will achieve even better response times. In addition to being readily available, (e.g., in 'Mathematical bS,' 'maple,1 inatlab' etc.), the DFT concentrates the energy in the first few coefficients, for a large class of signals, the colored noises. These signals have a skewed energy spectrum (O(F~~b), as follows: ï  For 6 = 2, we have the so-called random walks or brown noise, which model successfully stock movements and exchange rates (e.g., [541]). ï  With even more skewed spectrum (b gt; 2), we have the black noises [712]. Such signals model successfully, for example, the water level of rivers and the rainfall patterns as they vary over time [541]. ï  With 6 = 1, we have the pink noise.   BirkhofFs theory [712] claims that interesting' signals, such as musical scores and other works of art, consist of pink noise, whose energy spectrum follows OIF"1). The argument of the theory is that white noise with O(F∞) energy spectrum is completely unpredictable, while brown noise with O(F~2) energy spectrum is too predictable and therefore "boring." The energy spectrum of pink noise lies in between. ONE-DIMENSIONAL TIME SERIES        355 (a) time plot (lin-lin) (b) amplitude spectrum (log-log) Figure 12.4 (a) The Swiss-franc exchange rate (7 August 1990 to 18 April 1991 -first 3000 values out of 30,000) and (b) log-log amplitude of its Fourier transform, along with the 1/F line. As an illustration of the above observations, Figure 12.4(a) plots the movement of the exchange rate between the Swiss franc and the US dollar starting 7 August 1990 (3000 measurements); Figure 12.4(b) shows the amplitude of the Fourier coefficients as a function of the frequency F, as well as the 1/F line, in a logarithmic-logarithmic plot. Notice that, since it is successfully modeled as a random walk, the amplitude of the Fourier coefficients follow the 1/F line. The above data set is available through anonymous ftp from sfi.santafe.edu. In addition to ID signals (stock price movements and exchange rates), it is believed that several families of real n-D signals belong to the family of 'colored noises', with skewed spectrum. For example, 2D signals, like photographs, are far from white noise, exhibiting a few strong coefficients in the lower spatial frequencies. The JPEG image compression standard exploits this phenomenon, effectively ignoring the high frequency components of the discrete cosine transform, which is closely related to the Fourier transform. If the image consisted of white noise, no compression would be possible at all.
mir-0239	12.4.3    Experiments Performance results with the GEMINI approach on time series are reported in [6]. There, the method is compared to a sequential scanning method. The R*-tree was used for the spatial access method within GEMINI. The sequences were artificially generated random walks, with length n = 1024; their number A7 varied from 50 to 400. Figure 12.5 shows the break-up of the response time, as a function of the number / of DFT coefficients kept. The diamonds, triangles, and squares indicate total time, post-processing time, and i?*-tree time, respectively. Notice that, as we keep more features /, the i?*~tree becomes bigger and slower, but more accurate (fewer false alarms, and therefore shorter post-processing time). This tradeoff reaches an equilibrium for / = 2 or 3. For the rest of the experiments, the / = 2 Fourier coefficients were kept for indexing, resulting in a four-dimensional i?*-tree (two real numbers for each complex DFT coefficient). 356        MULTIMEDIA IR: INDEXING AND SEARCHING p; 80 63	lt; gt;-----------------^óó ? Search A   Post 0 Total 16 5 2                    3                    4 Number of Fourier Coefficients Figure 12.5    Breakup of the execution time, for range query (db size N = 400 sequences). 178	-		y'"  D GEMINI a Seq 80	 42	4 21 11	1            t		\                                              \ 50    100            200 Sequence Set Size 400 Figure 12.6    Search time per query vs.   number N of sequences, for whole-match queries; GEMINI (black line) and sequential scanning (gray line). Figure 12.6 shows the response time for the two methods (GEMINI and sequential scan), as a function of the number of sequences N. Clearly, GEMINI outperforms the sequential scanning. The major conclusions from the application of GEMINI on time series are the following: (1J GEMINI can be successfully applied to time series, and specifically to the ones that behave like 'colored noises' (stock prices movements, currency exchange rates, water level in rivers etc.). (2) For signals with skewed spectrum like the above ones, the minimum in the response time is achieved for a small number of Fourier coefficients (/ = 1,2,3).   Moreover, the minimum is rather flat, which implies that TWO-DIMENSIONAL COLOR IMAGES        357 a suboptimal choice for / will give search time that is close to the minimum. Thus, with the help of the lower-bounding lemma and the energy-concentrating properties of the DFT, we managed to avoid the 'dimensionality curse.' (3) The success in ID series suggests that GEMINI is promising for 2D or higher-dimensionality signals, if those signals also have skewed spectrum. The success of JPEG (that uses DOT) indicates that real images indeed have a skewed spectrum. Finally, the method has been extended to handle subpattern matching; for time sequences, the details are in [249]. We only mention the main idea here. Assuming that query patterns have length of at least iu, we preprocess every sequence of the database, by allowing a sliding window of length w at each and every possible position, and by extracting the / features for a given positioning of the window. Thus, every sequence becomes a trail in the /-dimensional feature space, which can be further approximated by a set of few MBRs that cover it. Representing each sequence by a few MBRs in feature space may allow false alarms, but no false dismissals. The approach can be generalized for subpattern matching in 2D signals (and, in general, in n-dimensional vector fields).
mir-0240	12.5    Two-dimensional Color Images GEMINI has also been applied for color images, within the QBIC project of IBM. The QBIC (Query By Image Content) project studies methods to query large online image databases using the images' content as the basis of the queries. Examples of the content include color, texture, shape, position, and dominant edges of image items and regions. Potential applications include medical ("Give me other images that contain a tumor with a texture like this one'), photojournalism ('Give me images that have blue at the top and red at the bottom"), and many others in art, fashion, cataloging, retailing, and industry. Here we will discuss methods on databases of still images, with two main datatypes: images' (= "scenes') and 'items.' A scene is a (color) image, and an item is a part of a scene, for example, a person, a piece of outlined texture, or an apple. Each scene has zero or more items. The identification and extraction of items is beyond the scope of this discussion (see [603] for more details). In this section we give an overview of the indexing aspects of QBIC, and specifically the distance functions and the application of the GEMINI approach. More details about the algorithms and the implementation of QBIC are in [257].
mir-0241	12.5.1    Image Features and Distance Functions We mainly focus on the color features, because color presents an interesting problem (namely, the % cross-talk" of features), which can be resolved by the GEMINI 358        MULTIMEDIA IR: INDEXING AND SEARCHING pixel count orange pink dark blue bright red light blue Figure 12.7    An example of a color histogram of a fictitious sunset photograph: many red, pink, orange, purple, and blue-ish pixels; few yellow, white, and green-ish ones. approach (algorithm 2). For color, we compute a /e-element color histogram for each item and scene, where k = 256 or 64 colors. Each component in the color histogram is the percentage of pixels that are most similar to that color. Figure 12.7 gives an example of such a histogram of a fictitious photograph of a sunset: there are many red, pink, orange, and purple pixels, but only a few white and green ones. Once these histograms are computed, one method to measure the distance between two histograms (A: x 1 vectors) x and y is given by xj - y3) (12.6) where the superscript t indicates matrix transposition, and the color-to-color similarity matrix A has entries ai3 which describe the similarity between color i and color j.
mir-0242	12.5.2    Lower-bounding In applying the GEMINI method for color indexing, there are two obstacles: first, the 'dimensionality curse' (k may be large, e.g. 64 or 256 for color features) and, most importantly, the quadratic nature of the distance function.  The distance function in the feature space involves cross-talk among the features (see equation 12.6), and thus it is a full quadratic form involving all cross terms. Not only is such a function much more expensive to compute than a Euclidean (or any Lp) distance, but it also precludes efficient implementation of commonly used spatial access methods. Figure 12.8 illustrates the situation. To compute the distance between the two color histograms x and q, the, e.g., bright-red component off has to be compared not only to the bright-red component of q, but also to the pink, orange, etc. components of q. TWO-DIMENSIONAL COLOR IMAGES        359 bright red pink orange _e.g., 64 colors. Figure 12.8    Illustration of the 'cross-talk' between two color histograms. To resolve the cross-talk problem, we try to apply the GEMINI approach (algorithm 2). The first step of the algorithm has been done: the distance function between two color images is given by equation 12.6, that is, V() = dhistO-The second step is to find one or more numerical features, whose Euclidean distance would lower-bound dhistO- Thus, we ask the feature-extracting question again: // we are allowed to use only one numerical feature to describe each color image, what should this feature be? Taking a cue from the previous section on time series, we can consider some average value, or the first few coefficients of the two-dimensional DFT transform. Since we have three color components, (e.g., Red, Green, and Blue), we could consider the average amount of red, green, and blue in a given color image. Notice that different color spaces (such as Munsell) can be used, with absolutely no change in our indexing algorithms. Thus, we continue the discussion with the RGB color space. This means that the color of an individual pixel is described by the triplet (R,G,B) (for cR'ed, 'G'reen, 4B'lue). The average color vector of an image or item x = {Ravg, GaVgy Bavg)t, is defined in the obvious way, with p=l p p=l p Bavg     = p=l where P is the number of pixels in the item, and R(p), G(p), and B(p) are the red, green and blue components (intensities, typically in the range 0-255) respectively of the p-th pixel. Given the average colors j1 and y of two items, we define davg() as the Euclidean distance between the three-dimensional average 360        MULTIMEDIA IR: INDEXING AND SEARCHING color vectors, d2avg{x,y) = (x-y)\x~y)                                                    (12.7) The third step of the GEMINI algorithm is to prove that our simplified distance davg() lower-bounds the actual distance dhistQ- Indeed, this is true, as an application of the so-called Quadratic Distance Bounding or QDB Theorem (see [244]). The result is that, given a color query, our retrieval proceeds by first filtering the set of images based on their average (R, G, B) color, then doing a final, more accurate matching using their full /c-element histogram. The resulting speedup is discussed next.
mir-0243	12.5.3    Experiments We now present experimental results [244] with GEMINI on color, using the bounding theorem, The experiments compare the relative performance (in terms of CPU time and disk accesses) between first, simple sequential evaluation of dhist f∞r a^ database vectors (referred to as 'naive'), and second, GEMINI. The experiments report the total and the CPU times required by the methods, by performing simulations on a database of N = 924 color image histograms, each of A; = 256 colors, of assorted natural images. Results are shown in Figure 12.9, which presents the total response time as a function of the selectivity (ratio of actual hits over the database size N). The figure also shows the CPU time for each method. Notice that, even for a selectivity of 5% (which would return ~ 50 images to the user), the GEMINI method is much faster than the straightforward, sequential computation of the histogram distances. In fact, it requires from a fraction of a second up to ´ 4 seconds, while the naive method requires consistently ´ 10 seconds. Moreover, notice that for larger databases, the naive method will have a linearly increasing response time. Thus, the conclusions are the following: ï  The GEMINI approach (i.e., the idea to extract some features for a quick-and-dirty test) motivated a fast method, using the average RGB distance; it also motivated a strong theorem (the so-called QDB theorem [244]) which guarantees the correctness in our case. ï  In addition to resolving the cross-talk problem, GEMINI solved the 'dimensionality curse' problem at no extra cost, requiring only / = 3 features, as opposed to k = 64 or 256 that dhtst{) required.
mir-0244	12.6    Automatic Feature Extraction GEMINI is useful for any setting that we can extract features from.   In fact, algorithms for automatic feature extraction methods exist, like the 'MultidimenTRENDS AND RESEARCH ISSUES        361 2000		Total ti CPUti CPU time for na	ne - GEMINI ------ me-GEMINI ó-  -we sequential ive sequential 0000			8000			6000 4000		___------óó	--------------------------' _------0 Figure 12.9    Response time vs. selectivity, for the sequential ('naive') retrieval and for GEMINI. sional Scaling' (MDS) and 'FastMap.' Extracting features not only facilitates the use of off-the-shelf spatial access methods, but it also allows for visual data mining: we can plot a 2D or 3D projection of the data set, and inspect it for clusters, correlations, and other patterns. Figure 12.10 shows the results of FastMap on 35 documents of seven classes, after deriving k = 3 features/dimensions. The classes include basketball reports ('Bbr'), abstracts of computer science technical reports (cAbs5), cooking recipes ('Rec'), and so on. The distance function was a decreasing function of the cosine similarity. The figure shows the 3D scatter-plot, (a) in its entirety and (b) after zooming into the center, to highlight the clustering abilities of FastMap. Notice that the seven classes are separated well, in only k = 3 dimensions.
mir-0245	12.7    Trends and Research Issues In this chapter wre focused on how to accelerate queries by content on image databases and, more general, on multimedia databases. Target queries are, e.g., "find images with a color distribution of a sunset photography or, 'find companies whose stock price moves similarly to a given company's stock." The method expects a distance function Tgt;() (given by domain experts), which should measure the dissimilarity between two images or objects O\* 02-We mainly examined whole match, range queries (that is, 'queries by example" where the user specifies the ideal object and asks for all objects that are within distance e from the ideal object). Extensions to other types of queries (nearest neighbors, all pairs and subpattern match) are briefly discussed. We focused on the GEMINI approach, which combines two ideas: ï The first Is to devise a kquick-and-dirty" test, which will eliminate several 362 MULTIMEDIA IR: INDEXING AND SEARCHING (a) (b) Figure 12.10    A collection of documents, after FastMap in 3-D space: (a) the whole collection and (b) magnification of the dashed box. non-qualifying objects. To achieve that, we should extract / numerical features from each object, which should somehow describe the object (for example, the first few DFT coefficients for a time sequence, or for a grayscale image). The key question to ask is 'If we are allowed to use only one numerical feature to describe each data object, what should this feature be?' ï The second idea is to further accelerate the search, by organizing these f-dimensional points using state-of-the art spatial access methods [400], like the i?*~trees. These methods typically group neighboring points together, thus managing to discard large unpromising portions of the address space early. The above two ideas achieve fast searching. Moreover, we need to consider the condition under which the above method will be not only fast, but also correct in the sense that it will not miss any qualifying object. Notice that false alarms are acceptable, because they can be discarded, in the obvious way. The answer is provided by the lower-bounding lemma, which intuitively states that the mapping J-() of objects to /-D points should make things look closer. In the rest of the chapter, we discussed how to apply GEMINI for a variety of environments, like ID time sequences and 2D color images. As discussed in the bibliographic notes, GEMINI has been applied to multiple other settings, like tumor-like shapes, time sequences with the time-warping distance function, 2D medical images, and so on. Moreover, it is one of the main reasons behind a strong recent interest on high-dimensionality index structures. With respect to future trends, probably the most notable and most challenging trend is data mining in multimedia and mixed-media data sets. For example, given a collection of medical records, with demographic data, text data BIBLIOGRAPHIC DISCUSSION        363 (like history), 2D images (like X-rays), and ID signals (electrocardiograms), we want to find correlations, clusters, patterns, and outliers. Successful detection of such patterns is the basis for forecasting, for hypothesis formation, anomaly detection, and several other knowledge discovery operations. GEMINI, insisting on turning every data type into a feature vector, should prove extremely useful: the reason is that it opens the door for off-the-shelf statistical and machine learning packages, which typically expect a set of vectors as input. Typical such packages are the 'Principal Component Analysis' (PCA, also known as 'Latent Semantic Indexing' (LSI), 'Karhunen-Loeve Transform' (KLT), and 'Singular Value Decomposition' (SVD)), Artificial Neural Networks, tree classifiers, to name a few.
mir-0246	12.8    Bibliographic Discussion Spatial Access Methods Structures and Algorithms For a recent, very thorough survey of spatial access methods, see [290]. For the introduction of R-trees, see the seminal paper by Guttman [330]. Among the numerous follow-up variations, the jR*-tree [69] seems to be one of the best performing methods, using the idea of deferred splitting with 'forced-reinsert,' thus achieving higher space utilization, and therefore more compact, shorter, and faster trees. Another strong contender is the Hilbert R-tree [427], which achieves even higher space utilization and often outperforms the JT-tree. A generalized framework and implementation for all these methods is the GiST tree [362] which is available, at the time of writing, at http://gist.cs.berkeley.edu:8000 /gist. With respect to algorithms, the range search is trivial in R-trees. Nearest neighbors queries require more careful record keeping, with a branch-and-bound algorithm (e.g., [686]). Spatial joins (e.g., 'find all pairs of points within distance £') have also attracted a lot of interest: see the filtering algorithms in [119] and the methods in [521] and [458]. Indexing high-dimensional address spaces has attracted a lot of recent interest: the TV-trees [519] adaptively use only a few of the available dimensions. The SR-trees [431] use spheres in conjunction to rectangles, as bounding regions. The more recent X-trees [83] gracefully switch to sequential scanning for extremely high dimensionalities. For the analysis of spatial access methods and selectivity estimation, the concept of "fractal dimension' has given very accurate results in every case it was tried: range queries [247], nearest neighbor queries [628], spatial joins [79]. quadtrees [245]. The idea behind the fractal dimension is to consider the intrinsic dimensionality of the given set of points. For example, consider the points on the diagonal of a 3D cube: their "embedding' dimensionality is E = 3: however, their intrinsic dimensionality is D = 1. Using the appropriate definition for the dimensionality, like the Hausdorff fractal dimension, or the correlation fractal 364        MULTIMEDIA IR: INDEXING AND SEARCHING dimension [712], it turns out that real data sets have a fractional dimensionality: the value is 1.1-1.2 for coastlines, ~2.7 for the brain surface of mammals, ´1.3 for the periphery of rain patches, ´1.7 for the end-points of road segments, to name but a few [247]. Metric Trees Finally, a class of access methods that operate on the distance function directly seems promising. These methods require only a distance function, and they typically build a cluster hierarchy, that is, a tree structure of 'spheres', which include the children spheres, and so on, recursively. This class includes the Burkhard-Keller methods [131], the Fixed-query trees [47], the GNAT trees [116], the MVP trees [112], and the M-trees [172]. The technology is still young: most of the above methods are designed for static data sets. On the positive side, they don't need feature extraction; on the negative side, they don't provide for visualization and data mining, like GEMINI and FastMap do (see Figure 12.10). Multimedia Indexing, DSP and Feature Extraction GEMINI ó Feature Extraction Probably the earliest paper that suggested feature extraction for fast indexing is [400], for approximate matching in shapes. The proof of the lower bounding lemma is in [249]. Algorithms for automatic feature extraction include the traditional, Multidimensional Scaling (MDS), see, e.g., [462]. MDS has attracted tremendous interest, but it is O(iV2), quadratic on the number of database objects N. Thus, it is impractical for large data sets. An O(N) alternative is the so-called FastMap [248], which was used to produce Figure 12.10. Time Sequences For additional, more elaborate distance functions, that include time-warping, see Chapter 8 or [706]. An indexing method with the time-warping distance function has recently been developed [840], using FastMap. For linear time sequence forecasting, see the classic book on the Box-Jenkins methodology [109]. For more recent, non-linear forecasting methods, see the intriguing volumes from the Santa-Fe Institute [149, 808]. Digital Signal Processing (DSP) Powerful tools for the analysis of time sequences and n-D signals in general include the traditional Fourier transform (see, e.g., [622]), the popular    discrete cosine transform, which is the basis for the JPEG image compression standard [802], and the more recent, and even more effective, wavelet transform (DWT) [689]. An excellent introduction to all these methods, as well as source code, is available in [051]. BIBLIOGRAPHIC DISCUSSION        365 Image Features and Similarity Functions There is a lot of work in machine vision on feature extraction and similarity measures. Classic references are e.g., [53, 224, 285]. A recent survey on image registration and image comparison methods is in [125]. The proof for quadratic distance bounding theorem of section 12.5 is in [244]. Other Applications of Multimedia Indexing There are numerous papers on indexing in multimedia databases. A small sample of them include the following: for time sequences allowing scaling or subpattern matching, see [305], [7], [246]. For voice and video see, e.g., [800]. For shapes see, e.g., [244]. For medical image databases see, e.g., [381], [454], [635]. For multimedia searching on the Web, see, e.g., [4, 733, 80, 714]. Data Mining Finally, there is a lot of work on traditional machine learning [565] and statistics (e.g., [408]).
mir-0248	13.1    Introduction The World Wide Web dates from the end of the 1980s [85] and no one could have imagined its current impact. The boom in the use of the Web and its exponential growth are now well known. Just the amount of textual data available is estimated to be in the order of one terabyte. In addition, other media, such as images, audio, and video, are also available. Thus, the Web can be seen as a very large, unstructured but ubiquitous database. This triggers the need for efficient tools to manage, retrieve, and filter information from this database. This problem is also becoming important in large intranets, where we want to extract or infer new information to support a decision process, a task called data mining. As mentioned in Chapter 1, we make the important distinction between data and information retrieval. We are interested in the latter case, in which the user searches for data that fulfills his information need. We focus on text, because although there are techniques to search for images and other non-textual data, they cannot be applied (yet) on a large scale. We also emphasize syntactic search. That is, we search for Web documents that have user-specified words or patterns in their text. As discussed in Chapter 2, such words or patterns may or may not reflect the intrinsic semantics of the text. An alternative approach to syntactic search is to do a natural language analysis of the text. Although the techniques to preprocess natural language and extract the text semantics are not new, they are not yet very effective and they are also too costly for large amounts of data. In addition, in most cases they are only effective with well structured text, a thesaurus, and other contextual information. There are basically three different forms of searching the Web, Two of them are well known and are frequently used. The first is to use search engines that index a portion of the Web documents as a full-text database. The second is to use Web directories, which classify selected Web documents by subject. The third and not yet fully available, is to search the Web exploiting its hyperlinkf f We will use hyperlink or link to denote a pointer (anchor) from a Web page to another Web page. 367 368        SEARCHING THE WEB structure. We cover all three forms of Web search here. We first discuss the challenges of searching the Web, followed by some Web statistics and models which can be used to understand the complexity of the problem. Next, we discuss in detail the main tools used today to search the Web. The discussion includes search engines, Web directories, hybrid systems, user interfaces, and searching examples. We continue with new query languages that exploit the graphical structure of the Web. Finally, we survey current trends and research issues. As Web research is a very dynamic field, we may have missed some important work, for which we apologize in advance.
mir-0249	13.2    Challenges We now mention the main problems posed by the Web. We can divide them in two classes: problems with the data itself and problems regarding the user and his interaction with the retrieval system. The problems related to the data are: Æ Distributed data: due to the intrinsic nature of the Web, data spans over many computers and platforms. These computers are interconnected with no predefined topology and the available bandwidth and reliability on the network interconnections varies widely. ï  High percentage of volatile data: due to Internet dynamics, new computers and data can be added or removed easily (it is estimated that 40% of the Web changes every month [424]). We also have dangling links and relocation problems when domain or file names change or disappear. ï  Large volume: the exponential growth of the Web poses scaling issues that are difficult to cope with. ï  Unstructured and redundant data: most people say that the Web is a distributed hypertext.  However, this is not exactly so.  Any hypertext has a conceptual model behind it, which organizes and adds consistency to the data and the hyperlinks. That is hardly true in the Web, even for individual documents. In addition, each HTML page is not well structured and some people use the term semi-structured data. Moreover, much Web data is repeated (mirrored or copied) or very similar. Approximately 30% of Web pages are (near) duplicates [120, 723].  Semantic redundancy can be even larger. ï  Quality of data: the Web can be considered as a new publishing medium. However, there is, in most cases, no editorial process. So, data can be false, invalid (for example, because it is too old), poorly written or, typically, with many errors from different sources (typos, grammatical mistakes, OCR errors, etc.). Preliminary studies show that the number of words with typos can range from 1 in 200 for common words to 1 in 3 for foreign surnames [588;. CHARACTERIZING THE WEB        369 Æ Heterogeneous data: in addition to having to deal with multiple media types and hence with multiple formats, we also have different languages and, what is worse, different alphabets, some of them very large (for example, Chinese or Japanese Kanji). Most of these problems (such as the variety of data types and poor data quality) are not solvable simply by software improvements. In fact, many of them will not change (and they should not, as in the case of language diversity!) because they are problems (also features) intrinsic to human nature. The second class of problems are those faced by the user during the interaction with the retrieval system. There are basically two problems: (1) how to specify a query and (2) how to interpret the answer provided by the system. Without taking into account the semantic content of a document, it is not easy to precisely specify a query, unless it is very simple. Further, even if the user is able to pose the query, the answer might be a thousand Web pages. How do we handle a large answer? How do we rank the documents? How do we select the documents that really are of interest to the user? In addition, a single document could be large. How do we browse efficiently in large documents? So, the overall challenge, in spite of the intrinsic problems posed by the Web, is to submit a good query to the search system, and obtain a manageable and relevant answer. Moreover, in practice we should try to achieve the latter goal even for poorly formulated queries. In the rest of this chapter, we use the term Web pages for HTML documents (HTML is described in Chapter 6). To denote all possible data types available on the Web, we use the term Web documents.
mir-0251	13.3.1    Measuring the Web Measuring the Internet and in particular the Web, is a difficult task due to its highly dynamic nature. Nowadays, there are more than 40 million computers in more than 200 countries connected to the Internet, many of them hosting Web servers. The estimated number of Web servers ranges from 2.4 million according to NetSizer [597] (November 1998) to over three million according to the Netcraft Web survey [596] (October 1998). This wide range might be explained when we consider that there are many Web sites that share the same Web server using virtual hosts, that not all of them are fully accessible, that many of them are provisional, etc. Other estimations were made by sampling 0.1% of all Internet numeric addresses obtaining about 2 million unique Web sites [619] or by counting domain names starting with www which in July 1998 were 780,000 according to the Internet Domain survey [599]. However, since not all Web servers have this prefix, the real number is even higher. Considering that in July 1998 the number of Internet hosts was estimated at 36.7 million [599], there is about one Web server per every ten computers connected to the 370 SEARCHING THE WEB Internet. The characterization of the Web is a new task of the Web Consortium [797]. In two interesting articles, already (sadly) outdated, Bray [114] and Woodruff et al. [834] studied different statistical measures of the Web. The first study uses 11 million pages while the second uses 2.6 million pages, with both sets gathered in November 1995. Their characterization of Web pages is partially reproduced in the following paragraphs. A first question is how many different institutions (not Web servers) maintain Web data. This number is smaller than the number of servers, because many places have multiple servers. The exact number is unknown, but should be more than 40% of the number of Web servers (this percentage was the value back in 1995). The exact number of Web pages is also not known. Estimates at the beginning of 1998 ranged from 200 to 320 million, with 350 million as the best current estimate (July 1998 [91]). The latter study used 20,000 random queries based on a lexicon of 400,000 words extracted from Yahoo!. Those queries were submitted to four search engines and the union of all the answers covered about 70% of the Web. Figure 13.1 gives an approximation of how the number of Web servers and the number of pages have changed in recent years. Between 1997 and 1998, the size of the Web doubled in nine months and is currently growing at a rate of 20 million pages per month. On the other hand, it is estimated that the 30,000 largest Web sites (about 1% of the Web) account for approximately 50% of all Web pages [619]. The most popular formats for Web documents are HTML, followed by GIF and JPG (both for images), ASCII text, and Postscript, in that order. The most popular compression tools used are GNU zip, Zip, and Compress. What is a typical HTML page? First, most HTML pages are not standard, meaning that they do not comply with all the HTML specifications.   In ad300 200 Number of Web pages (millions) _cl 19% 1997 1998 Figure 13.1    Approximate growth of the Web. CHARACTERIZING THE WEB        371 dition, although HTML is an instance of SGML, HTML documents seldom start with a formal document type definition. Second, they are small (around 5 Kbs on average with a median of 2 Kbs) and usually contain few images (between one and two on average with an average size of 14 Kb). The pages that have images use them for presentation issues such as colored bullets and lines. An average page has between five and 15 hyperlinks (more than eight links on average) and most of them are local (that is, they point to pages in their own Web server hierarchy). On average, no external server points to any given page (typically, there are only local links pointing to a given page). This is true even for home pages of Web sites. In fact, in 1995, around 80% of these home pages had fewer than ten external links pointing to each of them. The top ten most referenced sites are Microsoft, Netscape, Yahoo!, and top US universities. In these cases we are talking about sites which are referenced by at least 100,000 places. On the other hand, the site with most links to outside sites is Yahoo!. In some sense, Yahoo! and other directories are the glue of the Web. Without them we would have many isolated portions (which is the case with many personal Web pages). If we assume that the average HTML page has 5 Kb and that there are 300 million Web pages, we have at least 1.5 terabytes of text. This is consistent with other measures obtained from search engines. Note that this volume does not include non-textual documents. Regarding the languages used in Web pages, there have been three studies made. The first study was done by Funredes [637] from 1996 to 1998. It uses the AltaVista search engine and is based on searching different words in different languages. This technique might not be significant statistically, but the results are consistent with the second study wrhich was carried out by Alis Technology [11] and is based on automatic software that can detect the language used. One of the goals of the study was to test such software (done in 8000 Web servers). The last study was done by OCLC in June of 1998 [619] by sampling Internet numeric addresses and using the SILC language identification software. Table 13.1 gives the percentages of Web pages written in each language (with the exception of the OCLC data that counts Web sites), as well as the number of people (millions) who speak the language. The variations for Japanese might be due to an inability to detect pages written in Kanji. Some languages, in particular Spanish and Portuguese, are growing fast and will surpass French in the near future. The total number of languages exceeds 100.
mir-0252	13.3.2    Modeling the Web Can we model the document characteristics of the whole Web? Yes, as has already been discussed partially in Chapter 6. The Heaps' and Zipf's laws are also valid in the Web. In particular, the vocabulary grows faster (larger 3) and the word distribution should be more biased (larger 0). However, there are no experiments on large Web collections to measure these parameters. 372        SEARCHING THE WEB Language	Funredes	Alis Tech.	OCLC	Spoken by (1998, %)	(June 1997, %)	(June 1998, %	)      (millions) English	76.4	82.3	71	450 Japanese	4.8	1.6	4	126 German	4.4	4.0	7	118 French	2.9	1.5	3	122 Spanish	2.6	1.1	3	266 Italian	1.5	0.8	1	63 Portuguese	0.8	0.7	2	175 Table 13.1    Languages of the Web. 0 -1 11  1		;=; ~2 A j 1  (  - -4	All Files ------                      v'a   \     \ Image Files----                        w \     \ Audio Files----                          \\\      I -J	Video Files ¶                                           x ''   'i'         lt;     fllfll			Text Files - ,iaª.	-6 C	12        3        4        5        6        7        8 2                    4	6		log (File Size in Bytes) log(File Size) Figure 13.2 Left: Distribution for all file sizes (courtesy of M. Crovella, 1998). Right: Right tail distribution for different file types (from Crovella and Bestavros, 1996). All logarithms are in base 10. An additional model is related to the distribution of document sizes. According to this model, the document sizes are self-similar [201], that is, they have a large variance (a similar behavior appears in Web traffic). This can be modeled by two different distributions. The main body of the distribution follows a logarithmic normal distribution, such that the probability of finding a document of size x bytes is given by P(x) 1 -/x)2/2ct2 where the average (/i) and standard deviation (a) are 9.357 and 1.318, respectively [59;. Figure 13.2 (left) shows the size distribution of the experimental data. SEARCH ENGINES        373 The right tail of the distribution is iheavy-tailed.' That is, the majority of documents are small, but there is a non-trivial number of large documents. This is intuitive for image or video files, but it is also true for HTML pages. A good fit is obtained with the Pareto distribution where x is measured in bytes and k and a are parameters of the distribution [59] (see Figure 13.2 (right)). For text files, a is about 1.36, being smaller for images and other binary formats [201, 819]. Taking all Web documents into account, we get a = 1.1 and k = 9.3 Kb [58]. That is, 9.3 Kb is the cut point between both distributions, and 93% of all the files have a size below this value. In fact, for less than 50 Kb, images are the typical files, from 50 to 300 Kb we have an increasing number of audio files, and over that to several megabytes, video files are more frequent. The parameters of these distributions were obtained from a sample of more than 54,000 Web pages requested by several users in a period of two months of 1995. Recent data collected in 1998 show that the size distributions have the same form, but parameters change [58]. Related information can be found on Web benchmarks such as WebSpec96 and the Sun/Inktomi Inkbench [395].
mir-0253	13.4    Search Engines In this section we cover different architectures of retrieval systems that model the Web as a full-text database. One main difference between standard IR systems and the Wreb is that, in the Web, all queries must be answered without accessing the text (that is, only the indices are available). Otherwise, that would require either storing locally a copy of the Web pages (too expensive) or accessing remote pages through the network at query time (too slow). This difference has an impact on the indexing and searching algorithms, as well as on the query languages made available.
mir-0254	13.4.1    Centralized Architecture Most search engines use a centralized crawler-indexer architecture. Crawlers are programs (software agents) that traverse the Web sending new or updated pages to a main server where they are indexed. Crawlers are also called robots, spiders, wanderers, walkers, and knowbots.   In spite of their name, a crawler does not actually move to and run on remote machines, rather the crawler runs on a local system and sends requests to remote Web servers. The index is used in a centralized fashion to answer queries submitted from different places in the Web. Figure 13.3 shows the software architecture of a search engine based on the AltaVista architecture [17]. It has two parts: one that deals with the users, 374        SEARCHING THE WEB Query Engine   Interface  Users Crawler Indexer Web Figure 13.3    Typical crawler-indexer architecture. consisting of the user interface and the query engine and another that consists of the crawler and indexer modules. In 1998, the overall AltaVista system was running on 20 multi-processor machines, all of them having more than 130 Gb of RAM and over 500 Gb of disk space. Only the query engine uses more than 75% of these resources. The main problem faced by this architecture is the gathering of the data, because of the highly dynamic nature of the Web, the saturated communication links, and the high load at Web servers. Another important problem is the volume of the data. In fact, the crawler-indexer architecture may not be able to cope with Web growth in the near future. Particularly important is good load balancing between the different activities of a search engine, internally (answering queries and indexing) and externally (crawling). The largest search engines, considering Web coverage in June 1998, were AltaVista [17], HotBot [380], Northern Light [608], and Excite [240], in that order. According to recent studies, these engines cover 28-55% [749] or 14-34% [490] of all Web pages, whose number was estimated at over 300 million in 1998. Table 13.2 lists the most important search engines and their estimated sizes along with their corresponding URLs. Beware that some search engines are powered by the same internal engine. For example, HotBot, GoTo, and Microsoft are powered by Inktomi [395] and Magellan by Exciters internal engine. Up to date information can be found in [749, 609]. Most search engines are based in the United States and focus on documents in English. Nevertheless, there are search engines specialized in different countries and/or languages, which are able, for instance, to query and retrieve documents written in Kanji (Chinese, Japanese, and Korean). Also there are search engines that take other approaches, like Ask Jeeves! which simulates an interview [34] or DirectHit [215] which ranks the Web pages in the answer in order of their popularity. We should also mention those search engines aimed at specific topics, for example the Search Broker [537] which allows us to search in many specific topics and DejaNews [212] which searches the USENET archives. SEARCH ENGINES        375 Search engine	URL	Web pages indexed AltaVista	www.altavista.com	140 AOL Netfind	www.aol.com/netfind/	Excite	www.excite.com	55 Google	google.Stanford.edu	25 GoTo	goto.com	HotBot	www.hotbot.com	110 Infoseek	www.infoseek.com	30 Lycos	www.lycos.com	30 Magellan	www.mckinley.com	55 Microsoft	search.msn.com	_ NorthernLight	www. nlsearch. com	67 Web Crawler	www.webcrawler.com	2 Table 13.2    URLs and estimated size (millions) of the largest search engines (May 1998). There are also engines to retrieve specific Web pages such as personal or institutional home pages or specific objects such as electronic mail addresses, images, or software applets.
mir-0255	13.4.2    Distributed Architecture There are several variants of the crawler-indexer architecture. Among them, the most important is Harvest [108]. Harvest uses a distributed architecture to gather and distribute data, which is more efficient than the crawler architecture. The main drawback is that Harvest requires the coordination of several Web servers. The Harvest distributed approach addresses several of the problems of the crawler-indexer architecture, such as: (1) Web servers receive requests from different crawlers, increasing their load; (2) Web traffic increases because crawlers retrieve entire objects, but most of their content is discarded; and (3) information is gathered independently by each crawler, without coordination between all the search engines. To solve these problems, Harvest introduces two main elements: gatherers and brokers. A gatherer collects and extracts indexing information from one or more Web servers. Gathering times are defined by the system and are periodic (i.e. there are harvesting times as the name of the system suggests). A broker provides the indexing mechanism and the query interface to the data gathered. Brokers retrieve information from one or more gatherers or other brokers, updating incrementally their indices. Depending on the configuration of gatherers and brokers, different improvements on server load and network traffic can be 376 SEARCHING THE WEB Figure 13.4    Harvest architecture. achieved. For example, a gatherer can run on a Web server, generating no external traffic for that server. Also, a gatherer can send information to several brokers, avoiding work repetition. Brokers can also filter information and send it to other brokers. This design allows the sharing of work and information in a very flexible and generic manner. An example of the Harvest architecture is shown in Figure 13.4 [108]. One of the goals of Harvest is to build topic-specific brokers, focusing the index contents and avoiding many of the vocabulary and scaling problems of generic indices. Harvest includes a distinguished broker that allows other brokers to register information about gatherers and brokers. This is useful to search for an appropriate broker or gatherer when building a new system. The Harvest architecture also provides replicators and object caches. A replicator can be used to replicate servers, enhancing user-base scalability. For example, the registration broker can be replicated in different geographic regions to allow faster access. Replication can also be used to divide the gathering process between many Web servers. Finally, the object cache reduces network and server load, as well as response latency when accessing Web pages. More details on the system can be found in [108]. Currently, there are hundreds of Harvest applications on the Web (for example, the CIA, NASA, the US National Academy of Sciences, and the US Government Printing Office), as this software is on the public domain.! Netscape's Catalog Server is a commercial version of Harvest and Network Appliances* cache is a commercial version of the Harvest Cache. | Information is available at haxvest.transaxc.com. SEARCH ENGINES        377 Enter ranSyng key words In I^langu^e a [ flange of dales: Frem: if-;,J Count decumettts matching the bo To take advantage of advanced search features, please consult the Help section. Figure 13.5    Query interface for complex queries in AltaVista.
mir-0256	13.4.3    User Interfaces There are two important aspects of the user interface of search engines: the query interface and the answer interface (see also Chapter 10), The basic query interface is a box where one or more words can be typed. Although a user would expect that a given sequence of words represents the same query in all search engines, it does not. For example, in AltaVista a sequence of words is a reference to the union of all the Web pages having at least one of those words, while in HotBot it is a reference to the Web pages having all the words. Another problem is that the logical view of the text is not known, that is, some search engines use stopwords, some do stemming, and some are not case sensitive (see Chapter 7). All search engines also provide a query interface for complex queries as well as a command language including Boolean operators and other features, such as phrase search, proximity search, and wild cards. Figures 13.5 and 13.6 show the query interfaces for complex queries for the three largest search engines. They provide several filtering functions. The results can be filtered by additional words that must be present or absent from the answer or in a particular field such as the URL or title, language, geographic region or Internet domain, date range, or inclusion of specific data types such as images or audio. The answer usually consists of a list of the ten top ranked Web pages. Figure 13.7 shows the three top documents for the main four search engines for the query searching and Web and engine. Each entry in this list includes some information about the document it represents. Typically, the information includes the URL, size, the date when the page was indexed, and a couple of lines with its content (title plus first lines or selected headings or sentences). Some search engines allow the user to change the number of pages returned in the list and the amount of information per page, but in most cases this is fixed or limited to a few choices. The order of the list is typically by relevance, but sorting by URL or date is also available in some engines. In addition, most search engines also have an option to find documents similar to each Web page in the answer. 378        SEARCHING THE WEB k ttte $1 rated search engine Return to fewer Options LwkFor Sweh for pages                      L                                , mustcontsin   **|    tlie words nsptt not contain -ªI   tfaewds limit roisito to After ** i   erim S^7t^ Jtem  -ª´*     Jlt;´ª   JSMme Returner v pmmkfm JSSSSflfiSLJ Words an/where. Words m title. If Publication name'11 Words in URL p Select: vSpeaal Collection A World Wide Web vAll Sources Fill in. orte dat´ field oa both to nanowyoui results by daw Starttoteff  '"V'"^	End date ||^;i^-''' JSoit results by date SELECT SOWtCES    JA11 Sources below JJouªuliabUV´iws	-iMon- profit v/eb siVs UWews aatchiwes	-jEdw ational web sites JPeiisorial p^ges	J(Milgt;tajyv/sb sites JCorruruacttl web sit^s Docuroeats wicittjen in	Any language -JJ Alcountnes    -j ï ¶^SELECT SUBJECTS    JA11 S'jbie. ts SEARCH   dear setting Figure 13.6    Query interface for complex queries for HotBot (left) and NorthernLight (right). SEARCH ENGINES        379 ªgt; AltaVista round 3,156,580 Web pages for you.   Refine your search 1.  Welcome to PCfriend USA Searching Engine Web Site URL: '.'aw/ fvfrend net/menu 1 htm Last modified 23 -Feto SB - page see 628 bytes - in English [ Translate 1 2.  Searching Engine Home| TYP Databank! TVP Homepages| Net Trade Centerl Fair News| Leading Firms Business  Finance Database | New Media Database | World Trade Promotion.. URL: topi t'*n-c´niine wDm tw1searchfeincle;?lt;..htm Last modified 22-8ep~38 - page size 4K - in English f Translate 1 3. Searching Engine Welcome] ~ [Contact] ~ [Map] ~ [Search] Searching Engine - Here are some popi complete substring. Infoseek the.. URL: violet tele pitt edufeearch html Last modified 2 O-Jun-97 - psge see 12K - in English f Translate 1 Web Matches 49,690                                      1 - 10 mgt;S gt; Get the Top 10 IVfayt Vjsiterf Sites for "Searching Web Engine" welcome to sybilweb overview | about sybilweb | site map | search | help | contact Search Tips Answers to Frequently Asked Questions (FAQ) Search Corner Web Compass Canada Sybil's Search Engine overview sybilweb, trie Web component of Sybil's Search 99% http.//www sybilweb com/ See results from this site oojv Mte        Cte msam to gmpta Maintenance. Computers Aha, you have found the Complatinos S A , Costa Rica, Design, Submitting, Hosting, Maintenance, Translation, Links, Logo 99% http/Avww complatinos com/ See results from this yife only 3. ££3 Vu Search Engine - Yu Internet Pretragivac Srpsfci   Info  Add URL   Add E-mail  Business  Open Site  Dally News  Guide VuSearch Promo Advertising  Web Hosting Click! Wet) Search Enter keywords for searching Yu Web E-mail Search Enter keywords for searching E-mail Web Index Arts 97% http/Avwwyusearchcom/ See results from this site only Power Search found 113,731 items for v Special Collection   v World Wide Web   A AH Sources Q Documents that best match your search ' 79% - Directories  Lists: Internet Search Mechanisms Internet Search Mechanisms Harold Goldstein - dcbiker@gddray.. - Visit the Goldpages See Fossilized Insects, set your beading supplies and help save the., Date Not Available Commercial sit*: toftf J fdUnyBSiuti teuuMm 2, Internet Search h 79% - Directories  Lfets: Internet Search Mechanisms Internet Search Mechanisms Harold Goldstein - dcbfter@£oldray.,,. - Visit the Goldpages See Fossilized Insects, get your beading supplies and help save the... Date Not Available Commercial siW: http://goldray,cenv'seBrchcs.hta 3 NetVet Web Searching Web Picks 79% - Directories  Lists: NetVetWeb Searching Web Picks Search Tools This Site Other Veterirwy WWW Search Forms Other Search Engines Search NetVet mi fee Electronic Zoo! Other Veterinary. 01/07/98 Educational site: http7/net?etwustl.edH/ searchito Top 10 matchet. [12760 ktts AiªMtYÆttr .Susxits}.                                                   Show Titles only list by Web site 74% W3 Search Engine* - This documents collects some of the mostusenjl search engines available on the WWW. Omissions are the fault of the tmintainer. Suggestions for additions are welcome! Some interesting information sources are available only through specialized software. btt//t         dWtfckhUri udttcWnetafcKkxh 73% \yebjfflxiitCQ/ffl ; ^h^lt; gcurcjhi enitiean^ ^tabaaeMvigstiiffl, intarface^guid,.. ó wcbtaxixoni is a breakthrough navigation service designed to help Internet users conveniently search the World Wide Web. webtaxi.com enhances the existing capabilities of current versions of Netscape Navigator (2.0 and higher). This free service was developed to offer efficient point and click access to search engines, newsgroups and thousands of hard-to-reach databases, webtaxi.com provides... htS Jp            btd' 71% Free Software from AQL^nd PLS - The industry's leading, search software prodsicts are now free! nbsp; FLS's powerful search engine and products, accompanied by complete documentation, are available for download from this Web site free of charge.Gheck it oat. And check back frequently for updates on product and service off erings. tet/AwwplscV Figure 13.7    Output for the query searching and Web and engine for the four main search engines; from top to bottom: AltaVista, HotBot, NorthernLight, and Excite. 380        SEARCHING THE WEB The user can also refine the query by constructing more complex queries based on the previous answer. The Web pages retrieved by the search engine in response to a user query are ranked, usually using statistics related to the terms in the query. In some cases this may not have any meaning, because relevance is not fully correlated with statistics about term occurrence within the collection. Some search engines also taking into account terms included in metatags or the title, or the popularity of a Web page to improve the ranking. This topic is covered next.
mir-0257	13.4.4    Ranking Most search engines use variations of the Boolean or vector model (see Chapter 2) to do ranking. As with searching, ranking has to be performed without accessing the text, just the index. There is not much public information about the specific ranking algorithms used by current search engines. Further, it is difficult to compare fairly different search engines given their differences, and continuous improvements. More important, it is almost impossible to measure recall, as the number of relevant pages can be quite large for simple queries. Some inconclusive studies include [327, 498]. Yuwono and Lee [844] propose three ranking algorithms in addition to the classical tf~idf scheme (see Chapter 2). They are called Boolean spread, vector spread, and most-cited. The first two are the normal ranking algorithms of the Boolean and vector model extended to include pages pointed to by a page in the answer or pages that point to a page in the answer. The third, most-cited, is based only on the terms included in pages having a link to the pages in the answer. A comparison of these techniques considering 56 queries over a collection of 2400 Web pages indicates that the vector model yields a better recall-precision curve, with an average precision of 75%. Some of the newr ranking algorithms also use hyperlink information. This is an important difference between the Web and normal IR databases. The number of hyperlinks that point to a page provides a measure of its popularity and quality. Also, many links in common between pages or pages referenced by the same page often indicates a relationship between those pages. We now present three examples of ranking techniques that exploit these facts, but they differ in that two of them depend on the query and the last does not. The first is WebQuery [148], which also allows visual browsing of Web pages. WebQuery takes a set of Web pages (for example, the answer to a query) and ranks them based on how connected each Web page is. Additionally, it extends the set by finding Web pages that are highly connected to the original set. A related approach is presented by Li [512], A better idea is due to Kleinberg [444] and used in HITS (Hypertext Induced Topic Search). This ranking scheme depends on the query and considers the set of pages 5 that point to or are pointed by pages in the answer. Pages that have many links pointing to them in S are called authorities (that is. they should have relevant content). Pages that have many outgoing links are called hubs (they should point to similar content). A positive two-way feedback exists: SEARCH ENGINES        381 better authority pages come from incoming edges from good hubs and better hub pages come from outgoing edges to good authorities. Let H(p) and A(p) be the hub and authority value of page p. These values are defined such that the following equations are satisfied for all pages p: uES  I pó+u                                        v£S | vógt;p where H(p) and A(p) for all pages are normalized (in the original paper, the sum of the squares of each measure is set to one). These values can be determined through an iterative algorithm, and they converge to the principal eigenvector of the link matrix of S. In the case of the Web, to avoid an explosion of the size of 5, a maximal number of pages pointing to the answer can be defined. This technique does not work with non-existent, repeated, or automatically generated links. One solution is to weight each link based on the surrounding content. A second problem is that the topic of the result can become diffused. For example, a particular query is enlarged by a more general topic that contains the original answer. One solution to this problem is to analyze the content of each page and assign a score to it, as in traditional IR ranking. The link weight and the page score can be included on the previous formula multiplying each term of the summation [154, 93, 153]. Experiments show that the recall and precision on the first ten answers increases significantly [93]. The order of the links can also be used by dividing the links into subgroups and using the HITS algorithm on those subgroups instead of the original Web pages [153]. The last example is PageRank, which is part of the ranking algorithm used by Google [117]. PageRank simulates a user navigating randomly in the Web who jumps to a random page with probability q or follows a random hyperlink (on the current page) with probability 1 ó q. It is further assumed that this user never goes back to a previously visited page following an already traversed hyperlink backwards. This process can be modeled with a Markov chain, from where the stationary probability of being in each page can be computed. This value is then used as part of the ranking mechanism of Google. Let C(a) be the number of outgoing links of page a and suppose that page a is pointed to by pages p\ to pn. Then, the PageRank, PR(a) of a is defined as PR{a) = q + (l-q)J2 PR(Pi)/C(pi) where q must be set by the system (a typical value is 0.15). Notice that the ranking (weight) of other pages is normalized by the number of links in the page. PageRank can be computed using an iterative algorithm, and corresponds to the principal eigenvector of the normalized link matrix of the Web (which is the transition matrix of the Markov chain). Crawling the Web using this ordering has been shown to be better than other crawling schemes [168] (see next section). 382        SEARCHING THE WEB Therefore, to help ranking algorithms, page designers should include informative titles, headings, and meta fields, as well as good links. However, keywords should not be repeated as some search engines penalize repeating words (spam-ming). Using full terms instead of indirect ways to refer to subjects should also be considered.
mir-0258	13.4.5    Crawling the Web In this section we discuss how to crawl the Web, as there are several techniques. The simplest is to start with a set of URLs and from there extract other URLs which are followed recursively in a breadth-first or depth-first fashion. For that reason, search engines allow users to submit top Web sites that will be added to the URL set. A variation is to start with a set of populars URLs, because we can expect that they have information frequently requested. Both cases work well for one crawler, but it is difficult to coordinate several crawlers to avoid visiting the same page more than once. Another technique is to partition the Web using country codes or Internet names, and assign one or more robots to each partition, and explore each partition exhaustively. Considering how the Web is traversed, the index of a search engine can be thought of as analogous to the stars in an sky. What we see has never existed, as the light has traveled different distances to reach our eye. Similarly, Web pages referenced in an index were also explored at different dates and they may not exist any more. Nevertheless, when we retrieve a page, we obtain its actual content. How fresh are the Web pages referenced in an index? The pages will be from one day to two months old. For that reason, most search engines show in the answer the date when the page was indexed. The percentage of invalid links stored in search engines vary from 2 to 9%. User submitted pages are usually crawled after a few days or weeks. Starting there, some engines traverse the whole Web site, while others select just a sample of pages or pages up to a certain depth. Non-submitted pages will wait from weeks up to a couple of months to be detected. There are some engines that learn the change frequency of a page and visit it accordingly [175]. They may also crawl more frequently popular pages (for example, pages having many links pointing to them). Overall, the current fastest crawlers are able to traverse up to 10 million Web pages per day. The order in which the URLs are traversed is important. As already mentioned, the links in a Web page can be traversed breadth first or depth first. Using a breadth first policy, we first look at all the pages linked by the current page, and so on. This matches well Web sites that are structured by related topics. On the other hand, the coverage will be wide but shallow and a Web server can be bombarded with many rapid requests. In the depth first case, we follow the first link of a page and we do the same on that page until we cannot go deeper, returning recursively. This provides a narrow but deep traversal Only recently, some research on this problem has appeared [168], showing that good ordering schemes can make a difference if crawling better pages first (using the PageRank scheme mentioned above). SEARCH ENGINES        383 Due to the fact that robots can overwhelm a server with rapid requests and can use significant Internet bandwidth (in particular the whole bandwidth of small domains can be saturated), a set of guidelines for robot behavior has been developed [457]. For this purpose, a special file is placed at the root of every Web server indicating the restrictions at that site, in particular the pages that should not be indexed. Crawlers can also have problems with HTML pages that use frames (a mechanism to divide a page in two or more parts) or image maps (hyperlinks associated to images). In addition, dynamically generated pages cannot be indexed as well as password protected pages.
mir-0259	13.4.6    Indices Most indices use variants of the inverted file (see Chapter 8). In short, an inverted file is a list of sorted words (vocabulary), each one having a set of pointers to the pages where it occurs. Some search engines use elimination of stopwords to reduce the size of the index. Also, it is important to remember that a logical view of the text is indexed. Normalization operations may include removal of punctuation and multiple spaces to just one space between each word, uppercase to lowercase letters, etc. (see Chapter 7). To give the user some idea about each document retrieved, the index is complemented with a short description of each Web page (creation date, size, the title and the first lines or a few headings are typical). Assuming that 500 bytes are required to store the URL and the description of each Web page, we need 50 Gb to store the description for 100 million pages. As the user initially receives only a subset of the complete answer to each query, the search engine usually keeps the whole answer set in memory, to avoid having to recompute it if the user asks for more documents. State of the art indexing techniques can reduce the size of an inverted file to about 30% of the size of the text (less if stopwords are used). For 100 million pages, this implies about 150 Gb of disk space. By using compression techniques, the index size can be reduced to 10% of the text [825]. A query is answered by doing a binary search on the sorted list of words of the inverted file. If we are searching multiple words, the results have to be combined to generate the final answer. This step will be efficient if each word is not too frequent. Another possibility is to compute the complete answer while the user requests more Web pages, using a lazy evaluation scheme. More details on searching over an inverted file can be found in Chapter 8. Inverted files can also point to the actual occurrences of a word within a document (full inversion). However, that is too costly in space for the Web, because each pointer has to specify a page and a position inside the page (word numbers can be used instead of actual bytes). On the other hand, having the positions of the words in a page, we can answer phrase searches or proximity queries by finding words that are near each other in a page. Currently, some search engines are providing phrase searches, but the actual implementation is not known. Finding words which start with a given prefix requires two binary searches in the sorted list of words.    More complex searches, like words with errors, 384        SEARCHING THE WEB arbitrary wild cards or, in general, any regular expression on a word, can be performed by doing a sequential scan over the vocabulary (see Chapter 8). This may seem slow, but the best sequential algorithms for this type of query can search around 20 Mb of text stored in RAM in one second (5 Mb is more or less the vocabulary size for 1 Gb of text). Thus, for several gigabytes we can answer those queries in a few seconds. For the Web this is still too slow but not completely out of the question. In fact, using Heaps' law and assuming /? = 0.7 for the Web, the vocabulary size for 1 Tb is 630 Mb which implies a searching time of half a minute. Pointing to pages or to word positions is an indication of the granularity of the index. The index can be less dense if we point to logical blocks instead of pages. In this way we reduce the variance of the different document sizes, by making all blocks roughly the same size. This not only reduces the size of the pointers (because there are fewer blocks than documents) but also reduces the number of pointers because words have locality of reference (that is, all the occurrences of a non-frequent word will tend to be clustered in the same block). This idea was used in Glimpse [540] which is at the core of Harvest [108]. Queries are resolved as for inverted files, obtaining a list of blocks that are then searched sequentially (exact sequential search can be done over 30 Mb per second in RAM). Glimpse originally used only 256 blocks, which was efficient up to 200 Mb for searching words that were not too frequent, obtaining an index of only 2% of the text. By tuning the number of blocks and the block size, reasonable space-time trade-offs can be achieved for larger document collections (for more details see Chapter 8). These ideas cannot be used (yet) for the Web because sequential search cannot be afforded, as it implies a network access. However, in a distributed architecture where the index is also distributed, logical blocks make sense.
mir-0260	13.5    Browsing In this section we cover Web tools which are based on browsing and searching, in particular Web directories. Although the Web coverage provided by directories is very low (less than 1% of all Web pages), the answers returned to the user are usually much more relevant.
mir-0261	13.5.1    Web Directories The best and oldest example of a Web directory is Yahoo! [839], which is likely the most used searching tool.   Other large Web directories include eBLAST, LookSmart, Magellan, and NewHoo. Some of them are hybrids, because they also provide searches in the whole Web. Most search engines also provide subject categories nowadays, including AltaVista Categories, AOL Netfind, Excite Channels, HotBot, Infoseek, Lycos Subjects, and WebCrawler Select, are specific to some areas. For example, there are Web sites focused on business, news, BROWSING 385 Web directory	URL	Web sites	Categories eBLAST	www.eblast.com	125	_ LookSmart	www.looksmart.com	300	24 Lycos Subjects	a2z.lycos.com	50	Magellan	www.mckinley.com	60 NewHoo	www. lie who o. com	100	23 Netscape	www.netscape.com	-	Search.com	www.search.com	-	Snap	www.snap.com	-	Yahoo!	www.yahoo.com	750 Table 13.3    URLs, Web pages indexed and categories (both in thousands) of some Web directories (beginning of 1998). Arts  Humanities Automotive Business c Economy Computers ; Internet Education Employment Entertainment  Leisure Games Government Health  Fitness Hobbies c Interests Home Investing Kids  Family Life  Style Living Local News Oddities People Philosophy  Religion Politics Recreation Reference Regional Science Sz Technology Shopping  Services Social Science Society : Culture Sports Travel ; Tourism World Table 13.4    The first level categories in Web directories. and, in particular, research bibliography. Web directories are also called catalogs, yellow pages, or subject directories. Table 13.3 gives the URLs of the most important Web directories (not including the search engines already listed in section 13.4). Directories are hierarchical taxonomies that classify human knowledge. Table 13.4 shows the first level of the taxonomies used by Web directories (the number of first level categories ranges from 12 to 26). Some subcategories are also available in the main page of Web directories, adding around 70 more topics. The largest directory, Yahoo!, has close to one million pages classified, followed by LookSmart, which has about 24,000 categories in total.  Yahoo!  also offers 386        SEARCHING THE WEB 14 regional or country specialized directories in other languages including Chinese, Danish, French, German, Italian, Japanese, Korean, Norwegian, Spanish, and Swedish. In most cases, pages have to be submitted to the Web directory, where they are reviewed, and, if accepted, classified in one or more categories of the hierarchy. Although the taxonomy can be seen as a tree, there are cross references, so it is really a directed acyclic graph. The main advantage of this technique is that if we find what we are looking-for, the answer will be useful in most cases. On the other hand, the main disadvantage is that the classification is not specialized enough and that not all Web pages are classified. The last problem becomes worse every day as the Web grows. The efforts to do automatic classification, by using clustering or other techniques, are very old. However, up to now, natural language processing is not 100% effective in extracting relevant terms from a document. Thus, classification is done manually by a limited number of people. This is a potential problem with users having a different notion of categories than the manmade categorization. Web directories also allow the user to perform a search on the taxonomy descriptors or in the W7eb pages pointed to by the taxonomy. In fact, as the number of classified Wreb pages is small, we can even afford to have a copy of all pages. In that case they must be updated frequently, which may pose performance and temporal validity problems. In addition, most Web directories also send the query to a search engine (through a strategic alliance) and allow the whole Wreb to be searched.
mir-0262	13.5.2    Combining Searching with Browsing Usually, users either browse following hypertext links or they search a Web site (or the whole Web). Currently, in Web directories, a search can be reduced to a subtree of the taxonomy. However, the search may miss related pages that are not in that part of the taxonomy. Some search engines find similar pages using common words, but often this is not effective. WebGlimpse is a tool that tries to solve these problems by combining browsing with searching [539]. WebGlimpse attaches a small search box to the bottom of every HTML page, and allows the search to cover the neighborhood of that page or the whole site, without having to stop browsing. This is equivalent to following hypertext links that are constructed on the fly through a neighborhood search. WebGlimpse can be useful in building indices for personal Web pages or collections of favorite URLs. First, WebGlimpse indexes a Web site (or a collection of specific documents) and computes neighborhoods according to user specifications. As a result. WebGlimpse adds the search boxes to selected pages, collects remote pages that are relevant, and caches those pages locally. Later, the users can search in the neighborhood of a page using the search boxes. As the name suggests, WebGlimpse uses Glimpse as its search engine [540], The neighborhood of a Web page is defined as the set of Web pages that are reachable by a path of hypertext links within a maximum predefined distance. This distance can be set differently for local and remote pages. For example, it METASEARCHERS        387 can be unlimited locally, but be only three at any remote site. The neighborhood can also include all the subdirectories of the directory where the Web page is. The result is a graph of all the neighborhoods of the Web site or collection, and for each Web page, a file with all the Web pages in its neighborhood. When searching, any query in the whole index can be intersected with a neighborhood list, obtaining the relevant Web pages. A nice addition to WebGlimpse would be to visualize the neighborhoods. This problem is the topic of the next section.
mir-0263	13.5.3    Helpful Tools There are many software tools to help browsing and searching. Some of them are add-ons to browsers, such as Alexa [10]. Alexa is a free Web navigation service that can be attached as a toolbar at the bottom of any browser and accompanies the user in his surfing. It provides useful information about the sites that are visited, including their popularity, speed of access, freshness, and overall quality (obtained from votes of Alexa users). Alexa also suggests related sites helping one's navigation.   Another navigation service and searching guide is WebTaxi [805]. There are other tools that use visual metaphors, which can be broadly classified into two types: tools designed to visualize a subset of the Web and tools designed to visualize large answers. Both cases need to represent a large graph in a meaningful way. Specific commercial examples of tools to visualize Web subsets are Microsoft's SiteAnalyst (formerly from NetCarta), 3MAPA from Dynamic Diagrams, IBM's Mapuccino (formerly WebCutter [527], shown in Figure 10.22), SurfSerf, Merzscope from Merzcom, CLEARweb, Astra SiteManager, WebAn-alyzer from InContext, HistoryTree from SmartBrowser, etc. Non-commercial works include WebMap [220], Sitemap, Ptolomeaus, and many earlier research [234, 578, 564, 20]. We have not included more generic visualization software, where Web visualization is just a particular case, or other related visualization tools such as Web usage analysis [642, 294, 737]. Metaphors to visualize large answers are covered in Chapter 10. Visual tools are not yet deployed in the whole Web because there is no standard way of communicating visualizers and search engines. One possible approach is to use a markup language based on XML (see Chapter 6), as proposed in [15].
mir-0264	13.6    Metasearchers Metasearchers are Web servers that send a given query to several search engines, Web directories and other databases, collect the answers and unify them. Examples are Metacrawler [715] and SawySearch [383, 223]. The main advantages of metasearchers are the ability to combine the results of many sources and the fact that the user can pose the same query to various sources through a single common interface. Metasearchers differ from each other in how ranking 388        SEARCHING THE WEB Metasearcher	URL	Sources used Cyber 411	www.cyber411.com	14 Dogpile	www.dogpile.com	25 Highway 61	www. higliway61. com	5 Inference Find	www.infind.com	6 Mamma	www.mamma.coin	7 MetaCrawler	www.metacrawler.com	7 MetaFind	www.met af ind.c om	7 MetaMiner	www.miner.uol.com.br	13 MetaSearch	www.metasearch.com	_ SavvySearch	savvy.cs.colostate.edu:2000	gt;13 Table 13.5    URLs of metasearchers and number of sources that they use (October 1998). is performed in the unified result (in some cases no ranking is done), and how well they translate the user query to the specific query language of each search engine or Web directory (the query language common to all of them could be small). Table 13.5 shows the URLs of the main metasearch engines as well as the number of search engines, Web directories and other databases that they search. Metasearchers can also run on the client, for example, Copernic, EchoSearch, WebFerret, WebCompass, and WebSeeker. There are others that search several sources and show the different answers in separate windows, such as A1140ne, OneSeek, Proteus, and Search Spaniel. The advantages of metasearchers are that the results can be sorted by different attributes such as host, keyword, date, etc; which can be more informative than the output of a single search engine. Therefore browsing the results should be simpler. On the other hand, the result is not necessarily all the Web pages matching the query, as the number of results per search engine retrieved by the metasearcher is limited (it can be changed by the user, but there is an upper limit). Nevertheless, pages returned by more than one search engine should be more relevant. We expect that new metasearchers will do better ranking. A first step in this direction is the NEC Research Institute metasearch engine, Inquirus [488, 489]. The main difference is that Inquirus actually downloads and analyzes each Web page obtained and then displays each page, highlighting the places where the query terms were found. The results are displayed as soon as they are available in a progressive manner, otherwise the waiting time would be too long. This technique also allows non-existent pages or pages that have changed and do not contain the query any more to be discarded, and. more important, provides for better ranking than normal search engines. On the other hand, this inetajsoaivher is not available to the general public. FINDING THE NEEdigital libraryE IN THE HAYSTACK        389 Measure	Average value	Range Number of words	2.35	0 to 393 Number of operators	0.41	0 to 958 Repetitions of each query	3.97	1-1.5 million Queries per user session	2.02	1-173,325 Screens per query	1.39	1-78,496 Table 13.6    Queries on the Web: average values. The use of metasearchers is justified by coverage studies that show that a small percentage of Web pages are in all search engines [91]. In fact, fewer than 1% of the Web pages indexed by AltaVista, HotBot, Excite, and Infoseek are in all of those search engines. This fact is quite surprising and has not been explained (yet). Metasearchers for specific topics can be considered as software agents and are covered in section 13.8.2.
mir-0266	13.7.1    User Problems We have already glanced at some of the problems faced by the user when interacting with the query interfaces currently provided by search engines. First, the user does not exactly understand the meaning of searching using a set of words, as discussed in Chapter 10. Second, the user may get unexpected answers because he is not aware of the logical view of the text adopted by the system. An example is the use of uppercase letters when the search engine is not case sensitive. Hence, a word like 'Bank7 loses part of its semantics if we search for 'bank.' Simple experiments also show that due to typos or variations of a word, even if correctly capitalized, 10-20% of the matches can be lost. Similarly, foreign names or words that are difficult to spell may appear incorrectly which may result in a loss of up to 50% of the relevant answers, as mentioned in section 13.2. Another problem is that most users have trouble with Boolean logic. In natural language, sometimes we use 'and7 and %or' with different meaning depending on the context. For example, when choosing between two things, we use an exclusive "or,' which does not match the Boolean interpretation. Because of this, several studies show that around 80% of the queries do not use any Boolean or other operation. For these reasons many people have trouble using command query languages, and query forms should clearly specify which words must or must not be contained in a document that belongs to the answer. There are a few surveys and analyses of query logs with respect to the usage of search engines [647, 403, 728], The latter reference is based on 285 million user sessions containing 575 million queries.  Table 13.6 gives the main results 390        SEARCHING THE WEB of that study, carried out in September 1998. Some of the strange results might be due to queries done by mechanized search agents. The number of queries submitted per day to AltaVista is over 13 million. Users select a search engine mainly based on ease of use, speed, coverage, relevance of the answer, and habit. The main purposes are research, leisure, business, and education. The main problems found are that novice users do not know how to start and lack the general knowledge that would help in finding better answers. Other problems are that search engines are slow, that the answer is too large, not very relevant, and not always up to date. Also, most people do not care about advertising, which is one of the main sources of funding for search engines. When searching, 25% of the users use a single keyword, and on average their queries have only two or three terms. In addition, about 15% of the users restrict the search to a predefined topic and most of them (nearly 80%) do not modify the query. In addition, most users (about 85%) only look at the first screen with results and 64% of the queries are unique. Also, many words appear in the same sentence, suggesting that proximity search should be used. There are also studies about users1 demographics and software and hardware used.
mir-0267	13.7.2    Some Examples Now we give a couple of search examples. One problem with full-text retrieval is that although many queries can be effective, many others are a total deception. The main reason is that a set of words does not capture all the semantics of a document. There is too much contextual information (that can be explicit or even implicit) lost at indexing time, which is essential for proper understanding. For example, suppose that we want to learn an oriental game such as Shogi or Go. For the first case, searching for Shogi will quickly give us good Web pages where we can find what Shogi is (a variant of chess) and its rules. However, for Go the task is complicated, because unlike Shogi, Go is not a unique word in English (in particular, because uppercase letters are converted to lowercase letters, see Chapter 7). The problem of having more than one meaning for a word is called polysem,y. We can add more terms to the query, such as game and Japanese but still we are out of luck, as the pages found are almost all about Japanese games written in English where the common verb go is used. Another common problem comes from synonyms. If we are searching for a certain word, but a relevant page uses a synonym, we will not find it. The following example (taken from [152]) better explains the polysemy problem, where the ambiguity comes from the same language. Suppose that we want to find the running speed of the jaguar, a big South American cat. A first naive search in AltaVista would be jaguar speed. The results are pages that talk about the Jaguar car, an Atari video game, a US football team, a local network server, etc. The first page about the animal is ranked 183 and is a fable, without information about the speed. In a second try, we add the term cat. The answers are about the Clans Nova Cat and Smoke Jaguar, LMG Enterprises, fine cars, etc.    Only the page ranked FINDING THE NEEdigital libraryE IN THE HAYSTACK        391 25 has some information on jaguars but not the speed. Suppose we try Yahoo!. We look at 'Science:Biology:Zoology:Animals:Cats:Wild_Cats' and 'Science: Biology :Animal_Beliavior.' No information about jaguars there.
mir-0268	13.7.3    Teaching the User Interfaces are slowly improving in assisting the user with the task of acquiring a better grasp of what Web pages are being retrieved. Query forms must specify clearly if one or all the words must be in a page, which words should not be in a page, etc., without using a written Boolean query language. Second, users should try to give as many terms as possible, in particular terms that must be or should not be in the pages. In particular, a user should include all possible synonyms of a word. If the user can restrict the search to a field (for example, the page title) or limit some attribute (date, country), this will certainly reduce the size of the answer. In case of doubt, the user should remember to look at the help information provided by the search engine. If he cannot find where one of the relevant terms is in a page, he can use the Find option of the browser. Even if we are able to pose a good query, the answer can still be quite large. Considering that the visual tools mentioned before are not yet available for the general public, the user must learn from experience. There are many strategies for quickly finding relevant answers. If the user is looking for an institution, he can always try to guess the corresponding URL by using the www prefix followed by a guessed institution acronym or brief name and ending with a top level domain (country code or com, edu, org, gov for the US). If this does not work, the user can search the institution name in a Web directory. If we are looking for work related to a specific topic, a possible strategy is: (1) select an article relevant to the topic, if possible with non-common author surnames or title keywords (if it is not available, try any bibliographic database or a Web directory search for a first reference); and (2) use a search engine to find all Web pages that have all those surnames and keywords. Many of the results are likely to be relevant, because we can find: (a) newer papers that reference the initial reference, (b) personal Web pages of the authors, and most important, (c) pages about the topic that already contain many relevant references. This strategy can be iterated by changing the reference used as better references appear during the search. As mentioned at the beginning of this chapter, the Web poses so many problems, that it is easier and more effective to teach the user how to properly profit from search engines and Web directories, rather than trying to guess what the user really wants. Given that the coverage of the search engines is low, use several engines or a metasearcher. Also, remember that you have to evaluate the quality of each answer, even if it appears to be relevant. Remember that anybody can publish in the Web, and that does not mean that the data is correct or still valid. The lessons learned in the examples shown above are: (1) search engines still return too much hay together with the needle: and (2) Web directories do not have enough depth to find the needle. So, we can use the following rules of thumb: 392        SEARCHING THE WEB Æ Specific queries Look in an encyclopedia, that is the reason that they exist. In other words, do not forget libraries. Æ Broad queries Use Web directories to find good starting points. ï Vague queries Use Web search engines and improve the query formulation based on relevant answers.
mir-0269	13.8    Searching using Hyperlinks In this section we cover other paradigms to search the Web, which are based on exploiting its hyperlinks. They include Web query languages and dynamic searching. These ideas are still not widely used due to several reasons, including performance limitations and lack of commercial products.
mir-0270	13.8.1    Web Query Languages Up to this point, queries have been based on the content of each page. However, queries can also include the link structure connecting Web pages. For example, we would like to search for all the Web pages that contain at least one image and are reachable from a given site following at most three links. To be able to pose this type of query, different data models have been used. The most important are a labeled graph model to represent Web pages (nodes) and hyperlinks (edges) between Web pages, and a semi-structured data model to represent the content of Wreb pages. In the latter model, the data schema is not usually known, may change over time, may be large and descriptive, etc. [2, 129]. Although some models and languages for querying hypertext were proposed before the Web appeared [563, 72, 184], the first generation of Web query languages were aimed at combining content with structure (see also Chapter 4). These languages combine patterns that appear within the documents with graph queries describing link structure (using path regular expressions). They include W3QL [450], WebSQL [556, 33], WebLog [476], and WQL [511]. The second generation of languages, called Web data manipulation languages, maintain the emphasis on semi-structured data. However, they extend the previous languages by providing access to the structure of Web pages (the model also includes the internal structure) and by allowing the creation of new structures as a result of a query. Languages in this category include STRUQL [253], FLORID [373], and WebOQL [32]. All the languages mentioned are meant to be used by programs, not final users. Nevertheless, there are some examples of query interfaces for these languages. Web query languages have been extended to other Web tasks, such as extracting and integrating information from Web pages, and constructing and restructuring Web sites. More details about Web query languages can be found in the excellent survey by Florescu, Levy, and Mendelzon [258]. TRENDS AND RESEARCH ISSUES        393
mir-0271	13.8.2    Dynamic Search and Software Agents Dynamic search in the Web is equivalent to sequential text searching. The idea is to use an online search to discover relevant information by following links. The main advantage is that you are searching in the current structure of the Web, and not in what is stored in the index of a search engine. While this approach is slow for the entire Web, it might be used in small and dynamic subsets of the Web. The first heuristic devised was the fish search [113], which exploits the intuition that relevant documents often have neighbors that are relevant. Hence, the search is guided by following links in relevant documents. This was improved by shark search [366], which does a better relevance assessment of neighboring pages. This algorithm has been embedded in Mapuccino (see section 13.5.3), and Figure 10.22 shows a Web subset generated by this type of search. The main idea of these algorithms is to follow links in some priority, starting from a single page and a given query. At each step, the page with highest priority is analyzed. If it is found to be relevant, a heuristic decides to follow or not to follow the links on that page. If so, new pages are added to the priority list in the appropriate positions. Related work includes software agents for searching specific information on the Web [602, 477]. This implies dealing with heterogeneous sources of information which have to be combined. Important issues in this case are how to determine relevant sources (see also Chapters 9 and 15, as well as section 10.4.4) and and how to merge the results retrieved (the fusion problem). Examples are shopping robots such as Jango [401], Junglee [180], and Express [241].
mir-0272	13.9    Trends and Research Issues The future of the Web might surprise us, considering that its massive use started less than five years ago. There are many distinct trends and each one opens up new and particular research problems. What follows is a compilation of the major trends as we have perceived them. ï  Modeling: Special IR models tailored for the Web are needed [308, 155, 652].  As we have seen, Web user queries are different.  We also have the pull/push dichotomy: Will we search for information or will the information reach us? In both cases we need better search paradigms and better information filtering [782]. ï  Querying: Further work on combining structure and content in the queries is needed as well as new visual metaphors to pose those queries and visualize the answers [44]. Future query languages may include concept-based search and natural language processing, as well as searching by example (this implies document clustering and categorization on the Web [810,120, 157]). ï  Distributed architectures:   New distributed schemes to traverse and search the Web must be devised to cope with its growth.  This will have an impact on current crawling and indexing techniques, as well as caching 394        SEARCHING THE WEB techniques for the Web. Which will be the bottleneck in the future? Server capacity or network bandwidth? Æ Ranking: Better ranking schemes are needed, exploiting both content and structure (internal to a page and hyperlinks); in particular, combining and comparing query-dependent and independent techniques. One problem related to advertisements is that search engines may rank some pages higher due to reasons that are not based on the real relevance of a page (this is called the search engine persuasion problem in [543]). 9 Indexing: Which is the best logical view for the text? What should be indexed? How to exploit better text compression schemes to achieve fast searching and get lower network traffic? How to compress efficiently word lists, URL tables, etc. and update them without significant run-time penalty? Many implementation details must be improved. ï  Dynamic pages: A large number of Web pages are created on demand and current techniques are not able to search on those dynamic pages. This is called the hidden Web. Æ Duplicated data: Better mechanisms to detect and eliminate repeated Web pages (or pages that are syntactically very similar) are needed. Initial approaches are based on resemblance measures using document fingerprints [121, 120]. This is related to an important problem in databases: finding similar objects. ï  Multimedia:  Searching for non-textual objects will gain importance in the near future. There are already some research results in the literature [579, 80, 136]. ï  User interfaces: Better user interfaces are clearly needed.  The output should also be improved, for example allowing better extraction of the main content of a page or the formulation of content-based queries [766]. ï  Browsing:   More tools will appear, exploiting links, popularity of Web pages, content similarity, collaboration, 3D, and virtual reality [384, 638, 385, 421].  An important trend would be to unify further searching with browsing. An important issue to be settled in the future is a standard protocol to query search engines. One proposal for such a protocol is STARTS [316], which could allow us to choose the best sources for querying, evaluate the query at these sources, and merge the query results. This protocol would make it easier to build metasearchers, but at the same time that is one of the reasons for not having a standard. In that way, metasearchers cannot profit from the work done by search engines and Web directories. This is a particular case of the federated searching problem from heterogeneous sources as it is called in the database community [656]. This is a problem already studied in the case of the Web, including discovery and ranking of sources [161, 845, 319]. These issues are also very important for digital libraries [649] (see also Chapter 15) and visualization issues [15]. A related topic is metadata standards for the Web (see Chapter 6) BIBLIOGRAPHIC DISCUSSION        395 and their limitations [544]. XML helps [436, 213, 306], but semantic integration is still needed. Hyperlinks can also be used to infer information about the Web. Although this is not exactly searching the Web, this is an important trend called Web mining. Traditionally, Web mining had been focused on text mining, that is, extracting information from Web pages. However, the hyperlink structure can be exploited to obtain useful information. For example, the ParaSite system [736] uses hyperlink information to find pages that have moved, related pages, and personal Web pages. HITS, already mentioned in Section 13.4.4, has also been used to find communities and similar pages [444, 298]. Other results on exploiting hyperlink structure can be found in [639, 543, 154]. Farther improvements in this problem include Web document clustering [810, 120, 162] (already mentioned), connectivity services (for example, asking which Web pages point to a given page [92]), automatic link generation [320], extracting information [100, 115], etc. Another trend is intranet applications. Many companies do not want their private networks to be public. However, for business reasons they want to allow Web users to search inside their intranets obtaining partial information. This idea leads to the concept of portals for which there are already several commercial products. New models to see Web sites as databases and/or information systems are also important.
mir-0273	13.10    Bibliographic Discussion There are hundreds of books about the Web. Many of them include some information about searching the Web and tips for users. A recent book edited by Abrams includes a chapter on searching the Web [3]. Other sources are [682]. the special numbers of Scientific American on the Internet (March 1997) and IEEE's Internet Computing on Search Technologies (July/August 1998). For details about crawlers and other software agents see [166, 817]. In addition, the best source for references to the Web is the Web itself. To start with, there are many Web sites devoted to Inform and rate search engines and Web directories. Among them we can distinguish Search Engine Watch [749] and Search Engine Showdown [609]. A survey about Web characterizations is given by Pitkow [641] and a good directory to Web characteristics is [217]. Other Web pages provide pointers and references related to searching the Web, in particular the World Wide Web Consortium (www.w3.org), the World Wide Web journal (w3j . com) and WWW conferences. These and other pointers are available in the Web page of this book (see Chapter 1). Acknowledgements We would like to thank the following for their helpful comments: Omar Alonso, Eric Brown, Pablo de la Fuente, Monika Henzinger and Gonzalo Navarre
mir-0275	14.1    Introduction Despite the image sometimes presented of libraries as archaic collections of dusty books accessed through a card catalog, libraries were among the earliest institutions to make use of information retrieval systems. This early adoption took two main forms: searching remote electronic databases provided by commercial vendors in order to provide reference services to patrons, and the creation and searching of catalog records for materials held within the library. Each of these applications followed different developmental paths resulting in different products and functionality. According to Hildreth [372], Proceeding along different paths, the developmental histories of online public access catalogs (OPACs) and conventional information retrieval (IR) systems differed in three respects: origins of systems development, file and database content, and intended users, (p.10) Initial development of information retrieval systems was carried out by government laboratories in support of research in science and technology, based on bibliographic databases containing largely textual information, with trained search intermediaries as the intended users, OPACs were developed initially inhouse by large, usually academic, library systems, and later by commercial vendors of turnkey systems, f They used standardized record formats, generally the MARC record with minimal subject information (title, a few subject headings, and a classification number); and unlike commercial IR systems, they were intended from the outset for end users (library patrons). These factors led to significant differences between commercial IR systems and OPACs. f "Turnkey systems1 include software (and often hardware) and are usually developed with a specific library type and size in mind; within the constraints of the system, some customizing to suit the particular library is often possible. 397 398        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS Developed independently of each other, information retrieval systems and OPACs are quite different in character and use, and will be treated separately in this chapter. For these applications, a brief history, overview of current trends, some sample records and search examples will be given, and profiles of well-known systems will be presented. (Topics related to the use of IR systems in libraries, through Reference and Technical Services departments, and the techniques by which reference librarians perform the reference function, are beyond the scope of this chapter.) An important recent phenomenon, the digital library (see Chapter 15), has the potential to integrate information retrieval functions in the library under a common interface, eliminating the distinction between locally held and remote resources. Some examples of libraries which have attempted this integration will be discussed.
mir-0276	14.2    Online IR Systems and Document Databases A synergistic relationship exists between the producers and vendors of document databases^ (see Figure 14.1). In general, database producers create a product which they license to the database vendors. These vendors or search services provide search software and access to their customers, who benefit from the ability to search multiple databases from a single source. It is common to speak of the online database industry, since production of databases has usually been undertaken by corporations, organizations, or government on a for-profit or cost-recovery basis. These database producers have seen databases as products for sale or lease, often to libraries, and usually by a third party or database vendor. The role of database vendor is to license databases from their producers and add value by making them available to users. Database vendors provide some degree of standardization to the record formats, create indexes (usually in the form of inverted files), and provide a common interface for searching multiple databases. Examples of well known database vendors are DIALOG, LEXIS-NEXIS, OCLC, and H.W. Wilson; profiles are given in Figure 14.2. Some database producers choose to serve as their own search service providers, leading to a degree of vertical integration within the database industry; examples are the National Library of Medicine (NLM), which provides free access to its Medline database through the Web, and the H.W. Wilson Company, which markets its own series of databases. A significant aspect of these major commercial services is the very large size of their databases and the need for rapid, reliable service for many simultaneous users. In a description of their computing complex, LEXIS-NEXIS [510] give their database size as 1.3 billion documents, with 1.3 million subscribers, and 120 million annual searches. They return an answer set within six to ten seconds. | 'Database' is commonly used by producers and vendors of document databases when referring to their product. These databases lack the tabular structure of relational databases and contain bibliographic information and/or the full-text of documents. This usage will be followed in this chapter. ONLINE IR SYSTEMS AND DOCUMENT DATABASES        399 Database Producers: design database structure collect in-scope literature enter bibliographic information in standard form abstract (or edit authors' abstracts) index with (usually) controlled vocabulary generate file updates at regular intervals market backfile and updates to vendors Database Vendors: create search software license databases from producers standardize (as possible) record structure mount databases, creating inverted indexes update databases as appropriate (daily, weekly, monthly) provide documentation for searchers market to clients provide service and training to client base Figure 14.1    Role of database producers and vendors. with a claimed availability above 99.99% and reliability of 99.83%.   Similarly, DIALOG claims to be over 50 times the size of the Web.
mir-0277	14.2.1    Databases The history of commercial online retrieval systems begins with the creation of databases of bibliographic information in electronic form. In fact, Neufeld and Cornog claim 'databases can almost be said to have created the information industry as we now know it' [600]. Abstracting and indexing tools in printed form were available in the nineteenth century and became increasingly available in the twentieth century. Professional organizations, commercial firms, and government bodies served as publishers, selecting relevant materials from the world's literature, creating bibliographic records for them, and providing abstracts and indexing information. These databases were concentrated in the sciences, with titles such as Chemical Abstracts, Biological Abstracts, and Engineering Index, but humanities (Historical Abstracts) and social sciences (PsycINFO) products soon became available. As publishers of abstracts and indexes turned to computer-assisted typesetting and printing for their products, the resulting magnetic tapes of information began to be used for information retrieval purposes. Today virtually all print abstracting and indexing products are also available in electronic form, and many new products are available solely in electronic form, without a print equivalent. As storage costs have dropped dramatically, many of these electronic databases 400        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS ï  The DIALOG Corporation DIALOG, 'the world's largest online information company,' contains about 500 databases covering a full range of subjects, including science, technology and medicine, humanities, business, and electronic newspapers. Bibliographic and full-text databases are included. Some databases are also available in CD-ROM versions for onsite searching. URL: http://www.dialog.com ï  LEXIS-NEXIS LEXIS-NEXIS markets full-text databases to the legal and business community. LEXIS provides access to 4800 legal research products including state and federal case law, statutes, and regulations. NEXIS covers over 18,000 news and business sources. URL: http://www.lexis-nexis.com ï  OCLC OCLC (the Online Computer Library Center, Inc.), which began as a bibliographic utility for cooperative cataloging of library materials, now offers access to over 70 databases and 1.5 million full-text articles. Features include an interface oriented to end-users and links to documents as well as to an inter-library loan module. URL: http://www.oclc. org/oclc/memi/f s.html ï  H.W. Wilson H.W. Wilson began producing print indexes in 1898, and now offers 40 databases to the public, school, and college library market. Wilson has electronic, CD-ROM, magnetic tape, and Web-based versions of its databases. URL: http: //www. hwwilson. com/default .htm Figure 14.2    Profiles of database vendors. have expanded to include not only bibliographic information about documents, but the text of the documents themselves. These are referred to as full-text databases, and include databases of journal articles and newspapers as well as reference materials such as encyclopedias and directories. Characteristics of some common databases (as available on DIALOG) are given in Figure 14.3. Databases and Indexing In general, bibliographic databases are expensive to produce, because they require rigorous selection and analysis of the documents that they cover. Some databases cover materials in a specific group of journals, others attempt to be comprehensive, collecting the world's literature within the defined subject scope. Every item must be examined for relevance to the database's goals, indexed, ONLINE IR SYSTEMS AND DOCUMENT DATABASES        401 Æ CA SEARCH: Chemical Abstracts Coverage:   bibliographical records for worldwide literature of chemistry and its applications File size: 14 million records; weekly updates of 11,000 records ï  MEdigital libraryINE Coverage: the broad field of biomedicine, including clinical and experimental medicine, dentistry, nursing, pharmacology, psychiatry, etc. It indexes articles from 3,700 journals worldwide File size:   about 9.5 million records;  weekly updates of 7700 records ï  New York Times - Fulltext Coverage: full-text of New York Times from 1981 to the present File size: 1.8 million records; daily updates ï  PsycINFO: Psychological Abstracts Coverage: bibliographic records for materials in psychology and related behavioral and social sciences, including psychiatry, sociology, anthropology, education, pharmacology, and linguistics; 1887 to the present File size: 1.5 million records; monthly updates of 5000 records Figure 14.3    Characteristics of some well known databases on DIALOG. abstracted, and entered in the system. Despite the promise of SGML tagging of materials by primary producers, most of this work is still done by the database producer, with a clerical staff to handle data input and subject specialists to abstract (more commonly, edit the author's abstract) and index the material. Each bibliographic database is a unique product designed to meet the information needs of a particular user group. Therefore, there is no single standard for the content of a database record. Typically, it contains tagged information that includes a record key, bibliographic data such as author, title, and source of the document, an abstract, and subject indicators such as indexing terms or category codes. In full-text databases (see Chapters 2 and 4), the text of the document is also included. Sample database records from BIOSIS PREVIEWS (Biological Abstracts) and Historical Abstracts are shown in Figures 14.4 and 14.5. Note that the vocabulary (descriptors and codes) used for subject description is very dependent on the field of study (in this case, biology and history). As these database records show, the subject information they contain is of two types: so-called "natural language' or 4free text1 information found in the title or abstract field, and terms from an indexing or controlled vocabulary which are assigned by human indexers. Most databases include indexing terms in a descriptor field, usually taken from a database-specific thesaurus (e.g., for PsycINFO, the Thesaurus of Psychological Index Terms). Other types of codes or indexing may be applied as relevant to the database (for instance, biosystematic 402        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS DIALOG(R)File 5:BIOSIS PREVIEWS(R) (c) 1998 BIOSIS. All rts. reserv. 13165209 BIOSIS Number: 99165209 Population genetics of the Komodo dragon Varanus komodoensis Ciofi C; Bruford M; Swingland I R D.I.C.E., Univ. Kent, Kent, UK Bulletin of the Ecological Society of America 77 (3 SUPPL. PART 2). 1996. 81. Full Journal Title: 1996 Annual Combined Meeting of the Ecological Society of America on Ecologists/Biologists as Problem Solvers, Providence, Rhode Island, USA, August 10-14, 1996. Bulletin of the Ecological Society of America ISSN: 0012-9623 Language: ENGLISH Document Type: CONFERENCE PAPER Print Number: Biological Abstracts/RRM Vol. 048 Iss. 010 Ref. 171812 Descriptors/Keywords: MEETING ABSTRACT; VARANUS KOMODOENSIS; KOMODO DRAGON; MONITOR LIZARD; GENETIC DIVERGENCE; GENE FLOW; EVOLUTION; GENETIC DIVERSITY; SPECIES RANGE; POPULATION SIZE; POPULATION GENETICS; LESSER SUNDA REGION; INDONESIAN ISLANDS; ORIENTAL REGION; KOMODO; RINCA; FLORES; GILI MOTANG; INDONESIA Concept Codes: 03506 Genetics and Cytogenetics-Animal 03509 Genetics and Cytogenetics-Population Genetics (1972- ) 07508 Ecology; Environmental Biology-Animal 62800 Animal Distribution (1971- ) 00520 General Biology-Symposia, Transactions and Proceedings of Conferences, Congresses, Review Annuals Biosystematic Codes: 85408 Sauria Super Taxa: Animals; Chordates; Vertebrates; Nonhuman Vertebrates; Reptiles Figure 14.4    Sample record: BIOSIS PREVIEWS.^ codes in BIOSIS PREVIEWS, historical time periods in Historical Abstracts). The assignment of these subject terms contributes significantly to the cost of database production.   Obviously an automated indexing system would be of interest to database producers, though production systems currently in use are best described as performing 'machine-assisted1 rather than automatic indexing. ß With permission of BIOSIS UK. The format of this record has now changed as BIOSIS now use New Relational Indexing ONLINE IR SYSTEMS AND DOCUMENT DATABASES        403 DIALOG(R)File 39: Historical Abstracts (c) 1998 ABC-CLIO. All rts. reserv. 1488625 47A-9910 THE  U.S.S.  KEARSARGE,   SIXTEEN  IRISHMEN,  AND  A  DARK  AND STORMY NIGHT. Sloan, Edward W American Neptune 1994 54(4): 259-264. NOTE: Based on primary sources, including the Official Records of the Union and Confederate Navies in the War of the Rebellion, Series I and II (1894-1927); 28 notes. DOCUMENT TYPE: ARTICLE ABSTRACT: Tells the story of the Union navy's Kearsarge, a sloop-of-war that patrolled English seas looking for Confederate commerce raiders.   Upon docking at the Irish port of Cobh (Queenstown) in November 1863, 16 locals stowed away. They were subsequently returned to Cobh, but in the meantime Captain John Winslow temporarily enlisted the men in order, he said, that they be justifiably clothed and fed, although other ship diaries indicate that the ship was short-handed and Winslow intended a real enlistment. Whatever the reality, the captain inadvertently created an international crisis since his action technically violated the British Foreign Enlistments Act. It is unclear whether Confederates plotted the incident to embarrass the Union in Britain because there are disparities between official accounts and the diaries of individual crewmen. (S ) DESCRIPTORS: USA ; Civil War ; Ireland -(Cobh) ; Kearsarge -(vessel) ; Political Crisis ; Military Service ; Stowaways ; 1862-1864 HISTORICAL PERIOD: 1860D 1800H HISTORICAL PERIOD (Starting): 1862 HISTORICAL PERIOD (Ending): 1864 Figure 14.5    Sample record: Historical Abstracts. Prom ABC-CLIO,CA,USA. A subject of early (and ongoing) research has been the relative value of 'free text' and controlled vocabulary terms in contributing to retrieval performance. This subject was addressed in the Cranfield studies in the 1960s [415], and has continued to be examined by researchers up to the present time; good reviews of this research have been presented by Svenonius [752], Lancaster [479], and Rowley [688]. No definitive answer has been found, though later studies seem to suggest a complementarity between the two types of indexing in promoting good retrieval.
mir-0278	14.2.2    Online Retrieval Systems The use of the computer for bibliographic information retrieval was first demonstrated in the 1950s, and initiated by the National Library of Medicine in 1964 using batch processing [107]. Also in the 1960s, federally funded projects were carried out to develop prototype online systems which were then implemented in government research laboratories.   The first production service, Lockheed's 404        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS DIALOG system, was implemented for NASA and subsequently made available to other government locations before becoming a commercial activity in the early 1970s and undergoing several changes in ownership. Today DIALOG operates worldwide with databases offered via the Internet to libraries and other organizations as well as individuals. With a few exceptions, database vendors do not produce information but rather make it available to searchers via a common search interface. Database vendors license databases from the producers, process the databases to introduce as much standardization as is feasible (e.g., standard field names), mount the database through the creation of inverted indexes, create database descriptions and aids to searchers in a standard format, and conduct training sessions for clients (see Figure 14.1). These organizations offer a value-added service by providing a common gateway to multiple databases. A database vendor may offer cross-database searches; for example, DIALOG allows the searcher to search simultaneously a predetermined or searcher-selected grouping of databases to create a merged set of references, then process the set to remove duplicates.
mir-0279	14.2.3    IR in Online Retrieval Systems Since the inception of these online retrieval services, their retrieval functionality has been based primarily on the Boolean model for retrieval, in contrast to research in the IR field which has focused on improving retrieval performance through non-Boolean models, such as the vector space model (see Chapter 2). A number of factors guided the choice of the Boolean model as the basis for these services. Research in indexing and retrieval at the time, particularly the Cran-field studies, a series of experiments comparing natural and controlled vocabulary indexing, suggested that 'natural language' retrieval provided a level of retrieval performance comparable to manual indexing. Boolean logic was already being used in some libraries for manual retrieval systems, such as edge-notched cards and optical coincidence cards, and seemed to offer a natural mechanism for implementing retrieval based on combinations of words in documents. Research on alternate retrieval models was in its infancy, and the effectiveness of these models had not been proven for large databases. Most significantly, perhaps, the limited processing and storage capability of the computers of the time, while enough to support the inverted file structures and logical operations required for Boolean retrieval in an online environment, could not provide real time retrieval performance for other retrieval models which were more computationally intensive. Despite developments in IR research which suggested that alternative models might provide improved retrieval performance, Boolean retrieval has remained the commonest access method offered by database vendors, although in recent years some systems have added a form of natural language input with ranked output processing as an alternative access method. Reasons that have been suggested for the predominance of Boolean searching include financial considerations (cost of major changes in search software and database structures), service issues (a client community trained on existing systems), and lack of evidence in ONLINE IR SYSTEMS AND DOCUMENT DATABASES        405 support of viable alternatives in operational environments [662]. In general, database vendors use proprietary search software which is specific to their system, so that information professionals who search multiple systems are required to learn a different command vocabulary for each. A standard has been developed for a Common Command Language, NISO Z39.58 or ISO 8777, as described in Chapter 4, but it does not substitute for the advanced search features which are unique to individual search systems. The basic functionality for an IR search system is the ability to search for single terms or phrases, or Boolean combinations of them, to create sets of documents that can be further manipulated, then printed or displayed. Typically the system will also offer the option of using proximity operators to specify term relationships (A adjacent to B, A within n words of B, etc.) as discussed in Chapter 5, and to specify the location of the search term within the record (A occurring in title field, B occurring in the descriptor field, etc.). Of course, these capabilities require the storage of a significant amount of positional information within the inverted index. Other functions that may be available are the ability to browse the database index to select search terms (see Chapter 10) or to follow the term relationships within a database thesaurus to find candidate search terms (see Chapter 7). Other, more sophisticated functions, perhaps associated with a specific category of database, are also available, such as the ability to conduct structural searches for compounds in a chemistry database. As a term is entered by a searcher, the system creates a 'set' corresponding to all documents containing that term, and assigns a set number for the searcher's use. Multiple sets of retrieved documents are maintained in temporary storage. These set numbers serve as surrogates for the document set when issuing search commands, and Boolean logic can be used to manipulate existing sets. A display command allows the searcher to review the search history and return to previous sets. Based on data about the size of a set retrieved with search term or expression, and a review of the associated documents and their indexing, searchers continually revise a search until they feel they have achieved the best possible outcome. This iterative process is as much art as science, and its success is highly dependent on the skill and subject knowledge of the searcher. A typical Boolean search on DIALOG is shown in Figure 14.6. In this search, the user requests a specific database (file 61, Library and Information Science Abstracts) and then uses the 'Select Steps' or ss command to create sets of records. The '(w)1 represents a proximity operator, so set 5 (S5) will contain all records containing the phrases 'document retrieval' or 'text retrieval*1 or 'information retrieval.1 Set 13 (S13) will contain all records containing the term 'OPAC or the phrase 'online public access catalog.' The k?' is a truncation operator, and l? ?' limits truncation to one letter, so alternate spellings and plural of "catalog' and the singular or plural of 'OPAC will be retrieved. The two sets are combined with a Boolean AND operator, and finally the set is further limited to records that contain the terms in the title (ti) or descriptor (de) field, resulting in 100 records for review. 406        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS begin 61 File 61:LISA(LIBRARYINF0SCI)  1969-1998/May (c)	1998 Reed Reference Publishing Set	Items	Description ? ss (document or		information or text)(w)retrieval SI	7363	DOCUMENT S2	92299	INFORMATION S3	6219	TEXT S4	29302	RETRIEVAL S5	15338	(DOCUMENT OR INFORMATION OR TEXT)(W)RETRIEVAL ? ss opac?	? or online(w)public(w)access(w)catalog? S6	1111	OPAC? ? S7	20922	ONLINE S8	32238	PUBLIC S9	16388	ACCESS S10	18798	CATALOG? Sll	424	ONLINE(W)PUBLIC(W)ACCESS(W)CATALOG? S12	1246	OPAC? ? OR ONLINE(W)PUBLIC(W)ACCESS(W)CATALOG? ? s s5 and	sl2 15338	S5 1246	S12 S13	146	S5 AND S12 ? s sl3/ti	de S14	100	S13/TI,DE Figure 14.6    A DIALOG search.
mir-0280	14.2.4    'Natural Language' Searching To ensure their place in the market, database vendors continually develop new features that they feel will be of value to their client group, as well as add new database products. In general, these new features are augmentations to the existing Boolean search engine ó removal of duplicates, sophisticated ranking or sorting within the retrieved set. However, about five years ago several of the major database vendors announced they were adding 'natural language' search functionality to their systems. WESTLAW (a legal resources vendor) introduced its WIN system, DIALOG offered TARGET, and LEXIS-NEXIS announced a system called FREESTYLE [758, 653]. WIN and FREESTYLE accept a natural language query; TARGET requires the searcher to eliminate terms that are not useful for searching. All three systems provide ranked lists of retrieved documents. The 'natural language' systems are offered as auxiliary modules to standard Boolean searching, and are not intended to replace it. A sample TARGET search is shown in Figure 14.7. In this search in BIOSIS, the searcher is first provided with a series of instructions on dealing with phrases, synonyms, etc. The searcher enters a series of search terms {up to 25) at the k?* prompt, in this case 'komodo dragon food ONLINE PUBLIC ACCESS CATALOGS (OPACS)        407 ? target Input search, terms separated by spaces (e.g., DOG CAT FOOD). You can enhance your TARGET search with the following options: - PHRASES are enclosed in single quotes (e.g., 'DOG FOOD') - SYNONYMS are enclosed in parentheses (e.g., (DOG CANINE)) - SPELLING variations are indicated with a ? (e.g., DOG? to search DOG, DOGS) - Terms that MUST be present are flagged with an asterisk (e.g., DOG *F0OD) Q = QUIT  H = HELP ? komodo dragon food diet nutrition Your TARGET search request will retrieve up to 50 of the statistically most relevant records. Searching 1997-1998 records only . . .Processing Complete Your search retrieved 50 records. Press ENTER to browse results C = Customize display Q = QUIT H = HELP Figure 14.7    A TARGET search on DIALOG. diet nutrition'. By default the search is limited to the most recent two years of the file, and the 50 highest scoring records are available for display in ranked order. In this example no restrictions are made on the search terms but as the on-screen instructions indicate, Boolean logic can be imposed on the search terms, resulting in a Boolean search with ranked output.
mir-0281	14.3    Online Public Access Catalogs (OPACs) Library catalogs serve as lists of the library's holdings, organized as finding tools for the collection. For many years the card catalog served this function, and later computer-produced catalogs in book, microfilm, and microfiche form. Online catalogs were implemented in libraries during the 1970s, although these first catalogs were usually modules linked to the automated circulation system and had brief catalog records and very limited functionality. (The circulation system was the first component of what are now called library management systems (LMSs) or integrated library systems (ILSs) to be introduced). By the 1980s, true online public access catalogs had been implemented. Hildreth [372] has described the history of online catalogs by classifying them according to three generations. In the first generation, OPACs were largely known-item finding tools, typically searchable by author, title, and control number, and contained relatively short, non-standard bibliographic records. As is typical of technologies in their infancy, they were basically an old technology (the card catalog) in an automated form. In the second generation, increased search functionality included access by subject headings and, latterly, keyword, 408        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS some basic Boolean search capability, and ability to browse subject headings. Second generation catalogs also offered a choice of display formats (e.g., short, medium, long) and improved usability (for instance, different dialogs for novices and experts, more informative error messages, etc.). According to Hildreth, problems with second generation systems included failed searches, navigational confusion, problems with the subject indexing vocabulary and excessively large, badly organized retrieval sets. Needed enhancements for third generation systems, as delineated by Hildreth, included search strategy assistance, integrated free text/controlled vocabulary approaches, augmented cataloging records, cross-database access, natural language input, individualized displays and context-sensitive error correction. For many years library catalogs remained on what Hildreth referred to as the 'second generation plateau.' One of the barriers to innovation in OPAC development has been the cost of developing new systems and the need for a reliable customer base. Prom the perspective of the library, selecting and migrating to a new system is a costly process, and with library budgets traditionally squeezed, libraries have been cautious in selecting new and untried systems. They have learned to be wary of the 'it's in the next release; syndrome, while system developers have required a stable customer base to fund new systems. Third generation systems are now appearing, and with features not envisioned by Hildreth, who was speaking in a pre-Web environment. The availability of electronic resources on the Web has blurred the distinction between local and global resources, and between cataloging information and other electronic databases. According to a recent vendor survey [632], Automated system vendors have a vested interest in the transition of libraries to a mixed digital/print environment. Many see their own survival dependent upon their ability to help libraries thrive in this mixed arena, (p.47) Therefore, much of the emphasis in recent library systems development has been on the deployment of functionality for library management systems within new open systems architectures [351]. Features appearing in these new systems include improved graphical user interfaces (GUIs), support for Z39.50, electronic forms, hypertext links and Dublin Core (a developing metadata standard for multimedia materials), and incorporation of Java programming. Systems are also beginning to move beyond the basic Boolean search functionality, and some, like EGSFs Q series (described in section 14.3.3) have advanced search features.
mir-0282	14.3.1    OPACs and Their Content Libraries use standardized systems for cataloging and classifying the materials (texts and other media) they hold. Typically, they follow the Anglo-American Cataloging Rules to describe these materials, an organizational scheme (such as Library of Congress or the Dewey Decimal Classification) to assign subject codes, and use a subject heading list (such as the Library of Congress Subject ONLINE PUBLIC ACCESS CATALOGS (OPACS)        409 00723cam    22002418a 4500001001300000008004100013005001700054 010001800071020003300089040001300122050002600135082001700161 100002000178245007400198250001200272260005200284300003400336 504006400370650004100434 97002718 970417sl997        ilua         b        001 0 eng 19971128134653.1 $a      97002718 $a0838907075  (acid-free paper) $adigital libraryC$cdigital libraryC 00$aZ699.35.M28$bH34  1997 00$a025.3/16$221 1 $aHagler,  Ronald. 14$aThe bibliographic record and information technology / $cRonald Hagler. $a3rd ed. $aChicago   :$bAmerican Library Association,$cl997. $axvi,   394 p.   :$bill.   ;$c24 cm. $alncludes bibliographical references   (p.375-380) and index. 650    0041        0$aMachine-readable bibliographic data.# 001	0013 008	0041 005	0017 010	0018 020	0033 040	0013 050	0026 082	0017 100	0020 245	0074 250	0012 260	0052 300	0034 504	0064 Figure 14.8    Sample MARC record. Headings) to assign a series of subject descriptors. Given this standardization, cooperative cataloging ventures by library consortia have the potential to lower the cost per unit to catalog library materials, broaden access through shared databases, and facilitate the sharing of materials. Thus library cataloging relies on centralized and shared information through bibliographic utilities such as the Online Computer Library Center (OCLC). (OCLC is also a database vendor with characteristics shown in Figure 14.2.) The structure that underlies this cooperation among many libraries supporting distinct online catalogs is the MARC Record. MARC (Machine Readable Cataloging Record) is a data format that implements national and international standards, such as the Information Interchange Format (ANSI Z39.2) and the Format for Information Exchange (ISO 2709). With some variations (USMARC, UKMARC, etc.) it is used worldwide. A sample MARC record is shown in Figure 14.8. The MARC record has three parts: a fixed length (24 character) leader; a record directory showing the 3-digit tag for each field contained in the record with the length of that field in characters; and the data-containing fields and subfields themselves. Subfields are indicated by codes (e.g., ($a') within the field and are specific to each field. For instance, field 260 contains publication information and may have subfields for place, publisher, and date. (To improve readability the record here has been reformatted slightly, so that the field tag (e.g., 001) and field length (e.g., 0013) from the directory are repeated with the data for each field). A recent innovation is the adoption of the 856 field for holdings information to include URLs, allowing the specification of Web hyperlinks. 410        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS
mir-0283	14.3.2    OPACs and End Users Probably the greatest challenge for designers of OPACs is to create usable systems. OPACs are found in every type of library, and while users of research libraries might be expected to be knowledgeable about library practices in organizing and accessing information, elsewhere the end user could as easily be a schoolchild, college undergraduate, or patron of a local public library with little or no formal training in library use (what Borgman calls 'perpetual novices' [105]). The underlying record structure (the MARC record) is detailed and complex, and the organizational structures (LCSH, LC classification scheme) are far from intuitive. The most common type of searching in OPACs is subject searching, and failures by users in topical searching are well documented [484]. Common failures are null sets ('zero results'), or at the other extreme, information overload in which more references are retrieved than can easily be examined [484]. According to one study of transaction logs for the MELVYL catalog [252], 82% of in-library users had a zero retrieval for one or more searches. Interestingly, over 25% of users continued their search through ten or more tries, and another 25% did not appear to retrieve any useful information. Writing in 1986, Borgman [104] raised the question, 'Why are online catalogs hard to use?/ and in 1996, revisited the problem with kWhy are online catalogs still hard to use?' [105]. She argues the reason is that they do not incorporate knowledge about user behavior, and place too heavy a burden on the searcher for query specification. Greater contextual assistance for searchers has been suggested by a number of researchers [105, 252, 371].
mir-0284	14.3.3    OPACs: Vendors and Products The OPAC market is a specialized one, and products are developed and marketed by a limited number of vendors who compete for market position. While it is rare to find a library of any size that does not have a library management system, libraries are constantly in a state of flux, upgrading their systems as old ones become obsolete or unsupported, and introducing new systems. For example, many academic libraries had OPACs based on the venerable mainframe-based NOTIS software, and have undertaken to identify a suitable replacement. Most of the vendors target niche markets: academic libraries, public libraries, and school and special libraries. Profiles of three such vendors are found in Figure 14.9. Fuller details of these and other systems can be found in [351] and [61].
mir-0285	14.3.4    Alternatives to Vendor OPACs While early OPACs were developed inhouse, sometimes by enthusiastic amateurs at considerable expenditure of time and money, and a significant risk of failure, ONLINE PUBLIC ACCESS CATALOGS (OPACS)        411 Æ Endeavor Information Systems, Inc. With a significant academic library clientele, Endeavor has replaced a number of NOTIS systems. Its system, Voyager, is based on a multi-tier architecture with Oracle as the DBMS. The public access client and server are Z39.50 compliant. The search engine supports natural language queries and relevance ranking to display results. URL: http://www.endinfosys.com ï  Innovative Interfaces, Inc.  (Ill) A large company for this industry, III has an academic library customer base, and also a public library presence. Its newest system, Millennium, is based on its INNOPAC library management system but adds a thin client architecture with modules developed in Java. In addition to its own search engine, INNOPAC uses one licensed from Fulcrum Technologies. In Millennium, relevance ranking is available for full-text searching. URL: http://www.iii.com ï  EOS International (EOSi) EOSi markets to smaller libraries; it has a large special library clientele plus a significant academic, public, and school library customer base. Its Q series of library management system tools uses a three-tier, client/server architecture. The search engine is Excalibur Retrieval Ware, on license from Excalibur Technologies. Standard Boolean searching is available but greater functionality is supplied by natural language entry, dictionary-based query expansion, fuzzy search for bad data, and relevance ranked output. URL: http://www.eosintl.com Figure 14.9    Library management system vendors. today's environment supports turnkey systems developed by a third party. However, there are some instances of systems developed with a research focus for implementation in academic libraries. Notable examples are the Okapi system [416] at City University, London, MARIAN [264] at Virginia Tech, the MELVYL system at the University of California [526], and the Cheshire II system [486] for a UC Berkeley branch library. The Cheshire II system was designed for the UC Berkeley Mathematics, Statistics and Astronomy library using standards such as Z39.50 and SGML. It provides integrated access to bibliographic, full-text and multimedia resources. The search engine offers both probabilistic ranking and Boolean searches, which can be combined in a single search. Cheshire II was designed as a research as well as an operational environment, and issues such as combining probabilistic and 412        LIBRARIES AND BIBLIOGRAPHICAL SYSTEMS Boolean models, and design of the client interface to support searching with a variety of Z39.50 servers while minimizing cognitive overload on searchers [486].
mir-0286	14.4    Libraries and Digital Library Projects Libraries are concerned with enhanced, seamless access to electronic information from all sources. These libraries [351] see the library's Web pages, not the OPAC, as the entry point for library users. Through the web pages the user gains access to the library catalog, networked information resources, and locally created information, (p.5) Through the Web, a single interface can provide access to the local OPAC and reference materials, as well as to remotely accessible databases in the sciences, humanities, and business, including full-text journals, newspapers, and directories. Special collections, in multimedia as well as text formats, become available to the user through the same gateway. Many libraries, particularly academic and large public libraries, have undertaken digital library projects to achieve interoperability, ease of use, and equity of access (see Chapter 15). Two such projects, the Los Angeles Public Library's Virtual Electronic Library project (http://www.lapl.org), and University of Pennsylvania's Digital Library (http://www.library.upenn.edu) are described in [351]. The Web not only provides integration in terms of resources and collections, but the accompanying standards which support interoperability lead to a uniform search architecture. With this approach, the traditional distinction between information retrieval from OPACs and from remote electronic databases is beginning to disappear.
mir-0287	14.5    Trends arid Research Issues With a few exceptions, librarians are consumers of information systems, whether information retrieval systems provided by database vendors, or turnkey OPACs. Even in the digital library environment, their emphasis is on providing integrated access to a diversity of modules for information retrieval. Their interest therefore is in obtaining and using systems which offer ease of integration in their automated environment, and ease of use for themselves and their patrons. The former goal is approached through standards such as SGML and Z39.50, and the development and application of these standards is an important trend in the design of IR systems for libraries. For the latter goal, ease of use. the trend toward user-centered research and design is significant because it offers the potential to answer Bergman's query, 'Why are online catalogs still hard to use?' [105]. Much of the recent research interest is in cognitive and behavioral BIBLIOGRAPHIC DISCUSSION        413 issues (as reviewed in [482]). Developing an understanding of information need, either in general or for a specific client group, has been an important component of this work. Researchers are also interested in the searching behavior of users. Obviously, there is no single 'user' group, and studies have focused on groups such as trained intermediaries, children, and subject specialists, in both the search service and OPAC environment. One such project conducted over two years is the Getty Online Search Project which studied the end user search behavior of humanities scholars [67]. The interest in end user behavior also extends to an examination of relevance, since an understanding of the criteria by which users determine if retrieved information meets their information need is critical to achieving user-centered design.
mir-0288	14.6    Bibliographic Discussion The early history of online databases and systems makes interesting reading, and Hahn's 'Pioneers of the Online Age' is a good place to start [331]. The early history of online systems is also described by Bourne [107], and the history of electronic databases by Neufeld and Cornog [600]. The current status of the online industry is profiled annually in the May 1 issue of Library Journal (see, for example, [757]). An overview of research issues in OPACs is provided by Large and Beheshti [482]. A 1996 issue of the Journal of the American Society for Information Science was a special topic issue on 'Current Research in Online Public Access Systems' [68]. Comparative information on OPACs (and other library management system software) is readily available. A Council on Library and Information Resources report profiles 12 major vendors and their products [351]. The April 1 issue of Library Journal each year includes an 'Automated System Marketplace" update which discusses trends in library management systems, provides company and product information, and tabulates sales. Library Technology Reports frequently publishes 'consumer reports' of online systems; for instance, one issue was devoted to a survey of Z39.50 clients [813]. Recent monographs by Allen [13] and Marchionini [542] address the issues of user-centered design and electronic information seeking behavior.
mir-0290	15.1    Introduction Information retrieval is essential for the success of digital libraries (digital librarys), so they can achieve high levels of effectiveness while at the same time affording ease of use to a diverse community. Accordingly, a significant portion of the research and development efforts related to digital librarys has been in the IR area. This chapter reviews some of these efforts, organizes them into a simple framework, and highlights needs for the future. Those interested in a broader overview of the field are encouraged to refer to the excellent book by Lesk [501] and the high quality papers in proceedings of the ACM Digital Libraries Conferences. Those more comfortable with online information should refer to D-Lib Magazine [280]; the publications of the National Science Foundation (NSF), Defense Advanced Research Projects Agency (DARPA), and National Aeronautics and Space Administration (NASA) 'Research on Digital Libraries Initiative7 (digital libraryI) [349]; or online courseware [268]. There also have been special issues of journals devoted to the topic [265, 267, 710]. Recently, it has become clear that a global focus is needed [270] to extend beyond publications that have a regional [55] or national emphasis [221]. Many people's views of digital librarys are built from the foundation of current libraries [683]. Capture and conversion (digitization) are key concerns [160], but digital librarys are more than digital collections [634]. It is very important to understand the assumptions adopted in this movement towards digital librarys [509] and, in some cases, to relax them [29]. Futuristic perspectives of libraries have been a key part of the science fiction literature [811] as well as rooted in visionary statements that led to much of the work in IR and hypertext [135]. digital librarys have been envisaged since the earliest days 415 416        DIGITAL LIBRARIES of the IR field. Thus, in Libraries of the Future, Licklider lays out many of the challenges, suggests a number of solutions, and clearly calls for IR-related efforts [516]. He describes and predicts a vast expansion of the world of publishing, indicating the critical need to manage the record of knowledge, including search, retrieval, and all the related supporting activities. He notes that to handle this problem we have no underlying theory, no coherent representation scheme, no unification of the varied approaches of different computing specialties ó and so must tackle it from a number of directions. After more than 30 years of progress in computing, we still face these challenges and work in this field as a segmented community, viewing digital librarys from one or another perspective: database management, human-computer interaction (HCI), information science, library science, multimedia information and systems, natural language processing, or networking and communications. As can be seen in the discussion that follows, this practice has led to progress in a large number of separate projects, but has also made interoperability one of the most important problems to solve [624]. Since one of the threads leading to the current interest in digital librarys came out of discussions of the future of IR [264], since people's needs still leave a rich research agenda for the IR community [197], and since the important role of Web search systems demonstrates the potential value of IR in digital librarys [711], it is appropriate to see how IR may expand its horizons to deal with the key problems of digital librarys and how it can provide a unifying and integrating framework for the digital library field. Unfortunately, there is little agreement even regarding attempts at integrating database management and text processing approaches [325]. Sometimes, though, it is easier to solve a hard problem if one takes a broader perspective and solves a larger problem. Accordingly we briefly and informally introduce the k5S' model as a candidate solution and a way to provide some theoretical and practical unification for digital librarys. We argue that digital librarys in particular, as well as many other types of information systems, can be described, modeled, designed, implemented, used, and evaluated if we move to the foreground five key abstractions: streams, structures, spaces, scenarios, and societies. 'Streams' have often been used to describe texts, multimedia content, and other sequences of abstract items, including protocols, interactive dialogs, server logs, and human discussions. 'Structures' cover data structures, databases, hypertext networks, and all of the IR constructs such as inverted files, signature files, MARC records (see Chapter 8 for more details), and thesauri. "Spaces" cover not only ID, 2D, 3D, virtual reality, and other multidimensional forms, some including time, but also vector spaces, probability spaces, concept spaces, and results of multidimensional scaling or latent-semantic indexing. 'Scenarios' not only cover stories, HCI designs and specifications, and requirements statements, but also describe processes, procedures, functions, services, and transformations ó the active and time-spanning aspects of digital librarys. Scenarios have been essential to our understanding of different digital library user communities" needs [525], and are particularly important in connection with social issues [48]. 'Societies' cover these concerns especially regarding authors, librarians, annotators, and other stakeholders. For the sake of brevity we omit further DEFINITIONS        417 direct discussion of this abstraction, especially since anthropologists, communication researchers, psychologists, sociologists, and others are now engaging in digital library research. Since the 5S model can be used to describe work on databases, HCI, hyper-bases, multimedia systems, and networks, as well as other fields related to library and information science, we refer to it below to help unify our coverage and make sure that it encompasses all aspects of digital librarys. For example, the 5S model in general, and scenarios in particular, may help us move from a paper-centered framework for publishing and communicating knowledge [195] to a hybrid paper/electronic one with a variety of streams and spaces. The 5S model is a simple way to organize our thinking and understand some of the changes that digital librarys will facilitate: The boundaries between authors, publishers, libraries, and readers evolved partly in response to technology, particularly the difficulty and expense of creating and storing paper documents. New technologies can shift the balance and blur the boundaries. [525] To ground these and other subsequent discussions, then, we explore a number of definitions of digital librarys, using 5S to help us see what is missing or emphasized in each.
mir-0291	15.2    Definitions Since digital library is a relatively new field, many workshops and conferences continue to have sessions and discussions to define a 'digital library' [266, 347]. Yet, defining digital librarys truly should occur in the context of other related entities and practices [315]. Thus, a 'digital archive' is like a digital library, but often suggests a particular combination of space and structure, and emphasizes the scenario of preservation, as in 'digital preservation' that is based upon digitization of artifacts. Similarly, 'electronic preservation' calls for media migration and format conversions to make digital librarys immune to degradation and technological obsolescence. Maintaining integrity' in a digital library requires ensuring authenticity, handled by most regular libraries, as well as consistency, which is a concern whenever one must address replication and versioning, as occurs in database systems and in distributed information systems. While these concerns are important, we argue that 'digital library1 is a broader concept. Because it is true that the 'social, economic, and legal questions are too important to be ignored in the research agenda in digital libraries' [525], we really prefer definitions that have communities of users (societies)  as part of a digital library: digital librarys are constructed ó collected and organized ó by a community of users. Their functional capabilities support the information needs and uses of that community. digital library is an extension, enhancement, and integration of a variety of information institutions as physical places where resources are selected, collected, organized, preserved, and accessed in support of a user community. [48] 418        DIGITAL LIBRARIES This definition has many aspects relating to 5S, but largely omits streams, and only indirectly deals with spaces by calling for extensions beyond physical places. Its coverage of scenarios is weak, too, only giving vague allusion to user support. In contrast, definitions that emphasize functions and services are of particular importance to the development community [299], as are definitions concerned with distributed multimedia information systems: The generic name for federated structures that provide humans both intellectual and physical access to the huge and growing worldwide networks of information encoded in multimedia digital formats. [97] While brief, this definition does tie closely with 5S, though it is weak on scenarios, only mentioning the vague and limited concept of 'access.' To the IR community a digital library can be viewed as an extended IR system, in the context of federation and media variations [48]. Also, digital librarys must support (large) collections of documents, searching, and cataloging/indexing. They bring together in one place all aspects of 5S, and many of the concerns now faced by IR researchers: multilingual processing, search on multimedia content, information visualization, handling large distributed collections of complex documents, usability, standards, and architectures, all of which are explored in the following sections.
mir-0292	15.3    Architectural Issues Since digital librarys are part of the global information infrastructure, many discussions of them focus on high level architectural issues [611]. On the one hand, digital librarys can be just part of the 'middleware' of the Internet, providing various services that can be embedded in other task-support systems. In this regard they can be treated separately from their content, allowing development to proceed without entanglement in problems of economics, censorship, or other social concerns. On the other hand, digital librarys can be independent systems and so must have an architecture of their own in order to be built. Thus, many current digital librarys are cobbled together from pre-existing pieces, such as search engines, Web browsers, database management systems, and tools for handling multimedia documents. From either perspective, it is helpful to extend definitions into more operational forms that can lead to specification of protocols when various components are involved. Such has been one of the goals of efforts at the Corporation for National Research Initiatives (CNRI), as illustrated in Figure 15.1. Thus, Kahn and Wilensky proposed one important framework [426]. Anns et at have extended this work into digital library architectures [28, 31]. One element is a digital object, which has content (bits) and a handle (a type of name or identifier) [189], and also may have properties, a signature, and a log of transactions that involve it. Digital objects have associated metadata, that can be managed in sets [472]. Repositories of digital objects can provide security and can respond to a repository access protocol [30]. Significant progress has been niade toward adopting a scheme of digital object identifiers, first illustrated by the Online Handles are used to access Digital Objects ARCHITECTURAL ISSUES        419 Handle system Repository Handle__ (metadata) Signature -(optional) Signature ó (optional) 1 Transaction log Digital Object Digital Object Handle(metadata)	X Properties j----- Content ógt; (bits)		222		wm	¶i Signature-----* (optional)	ó	_	ó 1 Transaction log Handle Digital Object Content -(bits) Signature ó (optional) J Transacti ion log I Handle Digital Object Signature ó (optional) 1 Transaction log Digital Object Security provided by Repository Figure 15.1    Digital objects, handles, and repositories (adapted from [428, 28, 31, 30]). 420        DIGITAL LIBRARIES Computer Library Center, Inc. (OCLCs) Persistent URLs (PURLs) [654], and agreement seems likely on a standard for Digital Object Identifiers (DOIs) [396]. Other implementation efforts have focused more on services [473] and security [475]. A useful testbed for this work has been computer science reports [210], most recently through the Networked Computer Science Technical Reference Library, NCSTRL [471]. Two large Digital Libraries Initiative (digital libraryI) projects have devoted a good deal of attention to architecture, taking radically different approaches. At Stanford, the key concern has been interoperability [624]. Their 'InfoBus' [625] allows a variety of information resources to be connected through suitable mediators and then used via the shared bus through diverse interfaces. At the University of Michigan, the emphasis has been on agent technologies [97]. This approach can have a number of classes of entities involved in far-flung distributed processing. It is still unknown how efficiently an agent-based digital library can operate or even be built. Ultimately, software to use in digital librarys will be selected as a result of comparisons. One basis for such comparisons is the underlying conceptual model [820]. Another basis is the use of metrics, which is the subject of recent efforts towards definition and consensus building [499]. In addition to metrics traditionally used in IR, dealing with efficiency, effectiveness, and usability, a variety of others must be selected, according to agreed-upon scenarios. Also important to understand is the ability of digital librarys to handle a variety of document types (combinations of streams and structures), to accurately and economically represent their content and relationships (structures), and to support a range of access approaches and constraints (scenarios).
mir-0293	15.4    Document Models, Representations, and Access Without documents there would be no IR or digital librarys. Hence, it is appropriate to consider definitions of 'document' [709], and to develop suitable formalizations [508], as well as to articulate research concerns [505]. For efficiency purposes, especially when handling millions of documents and gigabytes, terabytes, or petabytes of space, compression is crucial [825]. While that is becoming more manageable, converting very large numbers of documents using high quality representations [151] can be prohibitively expensive, especially relative to the costs of retrieval, unless items are popular. All of these matters relate to the view of a document as a stream (along with one or more organizing structures); alternatively one can use scenarios to provide focus on the usage of documents. These problems shift, and sometimes partially disappear, when one considers the entire life cycle and social context of a document [124, 353] or when digital librarys become an integral part of automation efforts that deal with workflow and task support for one or more document collections.
mir-0294	15.4.1    Multilingual Documents One social issue with documents relates to culture and language [633]. Whereas there are many causes of the movement toward English as a basis for global DOCUMENT MODELS, REPRESENTATIONS, AND ACCESS        421 scientific and technical interchange, digital librarys may actually lead to an increase in availability of non-English content. Because digital librarys can be constructed for a particular institution or nation, it is likely that the expansion of digital librarys will increase access to documents in a variety of languages. Some of that may occur since many users of information desire it from all appropriate sources, regardless of origin, and so will wish to carry out a parallel (federated) search across a (distributed) multilingual collection. The key aspects of this matter are surveyed in [613]. At the foundation, there are issues of character encoding. Unicode provides a single 16-bit coding scheme suitable for all natural languages [783]. However, a less costly implementation may result from downloading fonts as needed from a special server or gateway, or from a collection of such gateways, one for each special collection [208]. The next crucial problem is searching multilingual collections. The simplest approach is to locate words or phrases in dictionaries and to use the translated terms to search in collections in other languages [387]. However, properly serving many users in many languages calls for more sophisticated processing [612]. It is likely that research in this area will continue to be of great importance to both the IR and digital library communities.
mir-0295	15.4.2    Multimedia Documents Prom the 5S perspective, we see that documents are made up of one or more streams, often with a structure imposed (e.g., a raster organization of a pixel stream represents a color image). Multimedia documents' streams usually must be synchronized in some way, and so it is promising that a new standard for handling this over the Web has been adopted [379]. At the same time, as discussed in Chapters 11 and 12, IR has been applied to various types of multimedia content. Thus, at Columbia University, a large image collection from the Web can be searched on content using visual queries [158]. IBM developed the Query By Image Content (QBIC) system for images and video [257] and has generously helped build a number of important image collections to preserve and increase access to key antiquities [300]. Similarly, the Carnegie Mellon University digital libraryI project, Informedia [146], has focused on video content analysis, word spotting, summarization, search, and in-context results presentation [146]. Better handling of multimedia is at the heart of future research on many types of documents in digital librarys [354]. Indeed, to properly handle the complexity of multimedia collections, very powerful representation, description, query and retrieval systems, such as those built upon logical inference [283], may be required.
mir-0296	15.4.3    Structured Documents While multimedia depends on the stream abstraction, structured documents require both the abstractions of streams and structures. Indeed, structured documents in their essence are streams with one or more structures imposed. 422        DIGITAL LIBRARIES often by the insertion of markup in the stream, but sometimes through a separate external structure, like pointers in hypertext. Since Chapter 6 of this book covers many of the key issues of document structure, we focus in this section on issues of particular relevance to digital librarys [288]. For example, since digital librarys typically include both documents and metadata describing them, it is important to realize that metadata as in MARC records can be represented as an SGML document (see Chapter 6 for more details) and that SGML content can be included in the base document and/or be kept separately [293]. Structure is often important in documents when one wants to add value or make texts 'smart' [167]. It can help identify important concepts [626]. SGML is often used to describe structure since most documents fall into one or more common logical structures [750], that can be formally described using a Document Type Definition (DTD). Another type of structure that is important in digital librarys, as well as earlier paper forms, results from annotation [548]. In this case stream and structure are supplemented by scenarios since annotations result from users interacting with a document collection, as well as collaborating with each other through these shared artifacts [680]. Structure is also important in retrieval. Macleod was one of the first to describe special concerns related to IR involving structured documents [533]. Searching on structure as well as content remains one of the distinguishing advantages of IR systems like OpenText (formerly 'PAT' [38]). Ongoing work considers retrieval with structured documents, such as with patterns and hierarchical texts [439]. An alternative approach, at the heart of much of the work in the Berkeley digital libraryI project [775], shifts the burden of handling structure in documents to the user, by allowing multiple layers of filters and tools to operate on so-called 'multivalent documents' [774]. Thus, a page image including a table can be analyzed with a table tool that understands the table structure and sorts it by considering the values in a user-selected column. Structure at the level above documents, that is, of collections of documents, is what makes searching necessary and possible. It also is a defining characteristic of digital librarys, especially when the collections are distributed.
mir-0297	15.4.4    Distributed Collections Though our view of digital librarys encompasses even those that are small, self-contained, and constrained to a personal collection with a suitable system and services, most digital librarys are spread across computers, that is spanning physical and/or logical spaces. Dealing with collections of information that are distributed in nature is one of the common requirements for digital library technology. Yet, proper handling of such collections is a challenging problem, possibly since many computer scientists are poorly equipped to think about situations involving spaces as well as the other aspects of 5S. Of particular concern is working with a number of digital librarys, each separately constructed, so the information systems are truly heterogeneous. Integration requires support for at least some popular scenarios (often a simple search that DOCUMENT MODELS, REPRESENTATIONS, AND ACCESS        423 External Servers Figure 15.2    Architecture of the BioKleisli system (adapted from [829, 128]). is a type of least common denominator) by systems that expect differing types of communication streams (e.g., respond to different protocols and query languages), have varying types of streams and structures, and combine these two differently in terms of representations of data and metadata. To tackle this problem, one approach has been to develop a description language for each digital library and to build federated search systems that can interpret that description language [161]. However, when digital library content is highly complex (e.g., when there are 'unstructured' collections, meaning that the structure is complex and not well described), there is need for richer description languages and more powerful systems to interpret and support highly expressive queries/operations [828, 209, 128]. An architecture of this type is illustrated in Figure 15.2 for the BioKleisli system [829]. In addition to these two approaches - namely reducing functionality for end users in order to give digital library developers more freedom and increasing functionality by making the federated system smarter and able to use more computational resources on both servers and clients - there is the third approach of making each digital library support a powerful protocol aimed at effective retrieval. This third course is supported by the Computer Interchange of Museum Information (CIMI) effort [570], wherein a Z39.50 interface exists on a number of museum information servers and clients [570]. While Z39.50 was aimed at the needs of libraries desiring interoperability among library catalogs, it does support many of the needs for digital librarys. Thus, the CIMI interoperability demonstration, with its support for multimedia content, is of great import, but does leave open further improvement 424        DIGITAL LIBRARIES in supporting richer digital library interaction scenarios, involving more powerful federated searchers.
mir-0298	15.4.5    Federated Search Federated search work has often been prompted by challenging application requirements. For example, to allow computer science technical reports from around the world to become accessible with minimal investment and maximal local control, the NSF-funded Wide Area TEchnical Report Service (WATERS) initiative was launched [279]. This was then integrated with an effort begun earlier with DARPA funding, the Computer Science Technical Report (CSTR) project [260], leading to a hybrid effort, the Networked CS Technical Reference (previously, Report) Library (NCSTRL) [471]. At the heart of NC-STRL is a simple search system, a well-thought-out open federated digital library protocol and the Dienst reference implementation, developed at Cornell University [210]. While this system was custom-built with little dependence on other software, its type of operation could be constructed more rapidly atop various supports like CORBA [788]. Federated search has had an interesting history, with workers adopting a variety of approaches. First, there are those interested in collecting the required information, often through Web crawling of various sorts [715]. Second, there are those focusing on intelligent search [27]. One example is work emphasizing picking the best sites to search [126]. These efforts often assume some integrated information organization across the distributed Internet information space [393]. Third, there is work on fusion of results. This can be viewed in the abstract, regardless of whether the various collections are nearby or distributed, with the target of improving retrieval by culling from a number of good sources [76]. One approach adopts a probabilistic inference network model [139]. Another views the problem as database merging [791]. Alternatively, one can assume that there are a number of search engines distributed to cover the collection, that must be used intelligently [292]. Fourth, there are commercial solutions, including through special Web services [223]. Probably the most visible is the patented, powerful yet elegant, approach by Infoseek Corporation [394]. Finally, there is a new line of work to develop comprehensive and realistic architectures for federated search [219, 218]. The long-term challenge is to segment the collection and/or its indexes so that most searches only look at a small number of the most useful sources of information, yet recall is kept high. Ultimately, however, there are rich types of use of digital library content, once one of these approaches to search is carried out.
mir-0299	15.4.6    Access When priceless objects are described by digital library image collections [300], when collections are large and/or well organized so as to appear of value to communities of users, or when there are valuable services in information manipulation (searching, ordering, reporting, summarizing, etc.)   afforded by a digital library, some method PROTOTYPES, PROJECTS, AND INTERFACES        425 of payment is often required [194, 191, 49, 251]. Though previously access to scientific literature was not viewed as a commodity as it is today [328], digital librarys clearly must manage intellectual property [559]. These services must support agreed-upon principles [586], copyright practices [705], as well as contracts and other agreements and laws [346]. Though technology is only part of the picture [822], a key to the implementation of policies for access management [30] is having trusted systems [746]. Security is one topic often ignored by the IR community. However, many aspects of security can be of fundamental importance in digital librarys [302, 301]. Just as encryption is essential to support electronic commerce, watermarking and stronger mechanisms are crucial in digital librarys to protect intellectual property rights and to control the types of access afforded to different user groups. Scenarios are important here, to ensure that suitable constraints are imposed on processing, all the way from input to output. For example, secret documents may not even be made visible in searches through metadata. On the other hand, advertising full documents as well as allowing locating and viewing metadata records is appropriate when the purpose of security is to enforce payment in ;pay by the drink' document downloading systems. Inference systems can be used for complicated rights management situations [16]. A deeper understanding of these requirements and services can be obtained by considering representative digital library projects, such as those mentioned in the next section.
mir-0300	15.5    Prototypes, Projects, and Interfaces Though numerous efforts in the IR, hypertext, multimedia, and library automation areas have been underway for years as precursors of today's digital library systems, one of the first new efforts aimed at understanding the requirements for digital librarys and constructing a prototype from scratch was the ENVISION project, launched in 1991 [269]. Based on discussions with experts in the field and a careful study of prospective users of the computer science collection to be built with the assistance of ACM, the ENVISION system was designed to extend the MARIAN search system [264] with novel visualization techniques [273, 360]. Careful analysis has shown its 2D approach to management of search results is easy to use and effective for a number of digital library activities [610]. The CORE project, another early effort, is an electronic library prototype on chemical journal articles. Its collection included, for each article, both scanned images and an SGML marked-up version, as well as indexes for full-text Boolean searching. It was undertaken by the American Chemical Society, Chemical Abstracts Service, OCLC, Bellcore, and Cornell University, along with other partners [237]. This project also was concerned with collection building as well as testing of a variety of interfaces that were designed based on user studies. One of the most visible project efforts is the Digital Libraries Initiative, initially supported by NSF, DARPA, and NASA [349]. Phase 1 provided funding for six large projects over the period 1994-1998. These projects spanned a wide 426        DIGITAL LIBRARIES range of major topics in developing the National Information Infrastructure (Nil) and addressed future technological problems. The Illinois project [777] focused on manually structured text documents in full systems with many users; the Berkeley project [775] emphasized automatically recognized image documents, also with large systems. The Santa Barbara [776] and Carnegie Mellon [146] projects investigated the ability to manipulate new media; Carnegie Mellon focused on segmenting and indexing video using speech recognition and program structure, and Santa Barbara concentrated on indexing maps using image processing and region metadata. Stanford [745] and Michigan [784] investigated the intermediaries to perform operations on large digital libraries; Stanford investigated interoperability of different search services, and Michigan concentrated on interacting software agents to provide services to users [710]. Since these projects have been described elsewhere in depth, it should suffice here to highlight some of the connections of those projects with the IR community. First, each project has included a component dealing with document collections. The Illinois project produced SGML versions of a number of journals while the Berkeley project concentrated on page images and other image classes. Santa Barbara adopted a spatial perspective, including satellite imagery, while Carnegie Mellon University (CMU) focused on video. Stanford built no collections, but rather afforded access to a number of information sources to demonstrate interoperability. At the University of Michigan, some of the emphasis was on having agents dynamically select documents from a distributed set of resources. Second, the digital libraryI projects all worked on search. Text retrieval, and using automatically constructed cross-vocabulary thesauri to help find search terms, was emphasized in Illinois. Image searching was studied at Berkeley and Santa Barbara while video searching was investigated at CMU. Michigan worked with agents for distributed search while Stanford explored the coupling of a variety of architectures and interfaces for retrieval. Finally, it is important to note that the digital libraryI efforts all spent time on interface issues. Stanford used animation and data flows to provide flexible manipulation and integration of services [192]. At Michigan, there were studies of the PAD+4- approach to 2D visualization [70]. Further discussion of interfaces can be found below in subsection 15.5.2. It should be noted that these projects only partially covered the 5S issues. Structures were not well studied, except slightly in connection with the Illinois work on SGML and the Berkeley work on databases. Scenarios were largely ignored, except in some of the interface investigations. Similarly, spaces were not investigated much, except in connection with the vocabulary transfer work at Illinois and the spatial collection and browsing work at Santa Barbara. Other projects in the broader international scene, some of which are discussed in the next section, may afford more thorough 5S coverage. Since the announcement of digital libraryL activities and interest related to digital libraries have increased dramatically. The six digital libraryI projects were highly visible and grew in scope; however, it was quickly realized that digital libraryI still needed additional direction and coherence.  During the initial funding period of the digital libraryI PROTOTYPES, PROJECTS, AND INTERFACES        427 program, additional workshops were created to develop consensus on the directions and boundaries with discussions from various communities. An important aspect that many people realized from the workshops is the importance of efforts in domains outside computer and information science to the advances in digital libraries research [324]. A follow-on program, Digital Libraries Initiative - Phase 2 (digital libraryI-2), jointly supported by NSF, DARPA, NASA, the National Library of Medicine (NLM), the Library of Congress (LoC), the National Endowment for the Humanities (NEH), and others, was announced in the spring of 1998 focus less on technology research than digital libraryI, but, more importantly, supporting research across the information life cycle, from content creation, access, and use to preservation and archiving, moving towards the concept of digital libraries as human-centered systems. digital libraryI-2 will emphasize the study of interactions between digital libraries and humans, fuller understanding of and improving access to digital content and collections, and interoperability and integration toward flexible information environments at the level of individual, group, and institution [324, 216]. The program will involve people not only from science and engineering but also from arts and humanities.
mir-0301	15.5.1    International Range of Efforts digital library efforts, accessible over the Internet, can now lead to worldwide access. Since each nation wishes to share the highlights of its history, culture, and accomplishments with the rest of the world, developing a digital library can be very helpful [86]. Indeed, we see many nations with active digital library programs [270], and there are many others underway or emerging. One of the largest efforts is the European ERCIM program [239], This is enhanced by the large eLib initiative in the UK [778]. There are good results from activities in New Zealand [601] and Australia [389]. In Singapore, billions are being invested in developing networked connectivity and digital libraries as part of educational innovation programs [729]. For information on other nations, see the online table pointing to various national projects associated with a recent special issue on this topic [270], As mentioned briefly above, many nations around the world have priceless antiquities that can be more widely appreciated through digital librarys [300]. Whether in pilot mode or as a commercial product, IBM Digital Library [390], with its emphasis on rights management, has been designed and used to help in this regard. These projects all require multimedia and multilingual support, as discussed earlier. Different scenarios of use are appropriate in different cultures, and different structures and spaces are needed for various types of collections. Indeed, many international collections aim for global coverage, but with other criteria defining their focus. Thus, the Networked Digital Library of Theses and Dissertations (Ndigital libraryTD) [594] is open to all universities, as well as other supporting organizations, with the aim of providing increased access to scholarly 428        DIGITAL LIBRARIES resources as a direct result of improving the skills and education of graduate students, who directly submit their works to the digital library.
mir-0302	15.5.2    Usability Key to the success of digital library projects is having usable systems. This is a serious challenge! Simple library catalog systems were observed in 1986 to be difficult to use [104], and still remain so after a further decade of research and development [105]. The above mentioned ENVISION project's title began with the expression 'User-Centered' and concentrated most of its resources on work with the interface [360]. A 1997 study at Virginia Tech of four digital library systems concluded that many have serious usability problems [434], though the design of the Illinois digital libraryI system seemed promising. The Virginia Tech study uncovered an important aspect of the situation, and suggested that it will be years before digital library systems are properly understood and used. A pre-test asked about user expectations for a digital library, and found that very few had worked with a digital library. The post-test showed that user expectations and priorities for various features changed dramatically over the short test period. Thus, it is likely that in general, as digital library usage spreads, there will be an increase in understanding, a shift in what capabilities users expect, and a variety of extensions to the interfaces now considered. Early in the digital libraryI work, digital library use was perceived as a research focus [98], and understanding and assessing user needs became a key concern [382]. For two years, a workshop was held at the Allerton conference center of the University of Illinois on this topic. Since the 1995 event [313] had a diverse group of researchers, it was necessary to understand the various perspectives and terminologies. There were discussions of fundamental issues, such as information, from a human factors perspective [214], as well as specific explorations of tasks like document browsing [528]. The 1996 event was more focused due to greater progress in building and studying usability of digital librarys [314]. Thus, there was discussion of Stanford's Sense-Maker system which supports rapid shifting between contexts that reflect stages of user exploration [51]. Social concerns that broaden the traditional IR perspective were highlighted [367]. In addition, there was movement towards metrics (see discussion earlier about digital library metrics) and factors for adopting digital librarys [429]. digital library interfaces and usability concerns have been central to many efforts at Xerox PARC. Some of the research considers social issues related to documents [354] while other research bridges the gap between paper and digital documents [353]. There are many issues about documents, especially their stability and how multimedia components as well as active elements affect retrieval, preservation, and other digital library activities [506]. Some insight into digital library use may result from actual user observation as well as other measures of what (parts of) documents are read [507]. There also has been collaboration between PARC and the UCB digital libraryI team, which has extended the Xerox magic filter work into multivalent documents (discussed earlier) as well as having developed results visualization methods like STANDARDS        429 TileBars where it is easy to spot the location of term matches in long documents [355]. Further work is clearly needed in digital library projects to improve the systems and their usability. But for these systems to work together, there also must be some emphasis on standards.
mir-0303	15.6    Standards Since there are many digital library projects worldwide, involving diverse research, development, and commercial approaches, it is imperative that standards are employed so as to make interoperability and data exchange possible. Since by tradition any library can buy any book, and any library patron can read anything in the library, digital librarys must make differences in representation transparent to their users. In online searching as well, data that can be understood by clients as well as other digital librarys should be what is transferred from each information source. At the heart of supporting federated digital librarys, especially, is agreement on protocols for computer-computer communication.
mir-0304	15.6.1    Protocols and Federation In the 1980s it became clear that as library catalog systems proliferated, and library patrons sought support for finding items not locally available through inter-library loan or remote cataloging search, some protocol was needed for searching remote bibliographic collections. The national standard Z39.50, which later became an international standard as well, led to intensive development of implementations and subsequent extensive utilization [515]. One example of widespread utilization was the WAIS system (based on Z39.50), very popular before the World Wide Web emerged. Ongoing development of Z39.50 has continued, including its application to digital librarys, as demonstrated in the CIMI project described earlier, where a number of different clients and server implementations all worked together. Also mentioned earlier is the NCSTRL effort, starting with CS technical reports, in which the Dienst protocol was developed [210]. This is a 'lighter' protocol than Z39.50, designed to support federated searching of digital librarys, but also connected to the centralized preprint service (CoRR) at Los Alamos National Laboratory. Dienst seems suitable for electronic theses and dissertations as well as technical reports, and so it has been considered in regard to Ndigital libraryTD. These protocols assume that each server and client will be changed to use the protocol. A less intrusive approach, but one harder to implement and enforce, is to have some mechanism to translate from a special server or gateway system to/from each of the information sources of interest. The STARTS protocol [316] was proposed to move in this direction, but competition among search services on the Internet is so severe that acceptance seems unlikely.   Though 430        DIGITAL LIBRARIES this is unfortunate, simple federated schemes have been implemented in the digital libraryI projects at Stanford and Illinois, and a simple one is in use in Ndigital libraryTD. Yet, even more important than new protocols for digital library federated search is agreement on metadata schemes, which does seem feasible.
mir-0305	15.6.2    Metadata In the broadest sense, metadata can describe not only documents but also collections and whole digital librarys along with their services [50]. In a sense, this reflects movement toward holistic treatment like 5S. Yet in most digital library discussions, metadata just refers to a description of a digital object. This is precisely the role played by library catalog records. Hence, cataloging schemes like MARC are a starting point for many metadata descriptions [514]. While MARC has been widely used, it usually involves working with binary records which must be converted for interchange. One alternative is to encode MARC records using some readable coding scheme, like SGML [293]. Another concern with MARC is that there are a number of national versions with slight differences, as well as differences in cataloging practices that yield the MARC records. USMARC is one such version. It is very important in the digital library field, and can be encoded using SGML, or easily converted to simpler metadata schemes like the 'Dublin Core' [513]. Other 'crosswalks' exist between Dublin Core (DC), MARC, and schemes like GILS, proposed for a Government Information Locator Service [598]. A mapping also exists between DC and the Z39.50 protocol discussed in the previous section [503]. DC is a simple scheme, with 15 core elements that can be used to describe any digital object. What is of real import is that it has been widely accepted. That is because there have been years of discussion and development, focused around international workshops [806, 620, 560, 833, 333]. The core elements include seven that describe content (Title, Subject, Description, Source, Language, Relation, and Coverage). There are four elements that deal with intellectual property issues (Creator, Publisher, Contributor, and Rights). Finally, to deal with instances of abstract digital objects, there are four other types (Data, Type, Format, and Identifier). Since digital objects and their metadata often have to be interchanged across systems, the problem of packaging arises. The Warwick Framework, which evolved out of the same type of discussions leading to DC, deals with packages and connections between packages [472]. In general, such discussion about metadata is crucial to allow the move from traditional libraries (with their complex and expensive cataloging), past the Web (with its general lack of cataloging and metadata), to a reasonable environment wherein metadata is available for all sorts of digital objects (suitable to allow the organization of vast collections in digital librarys [734]). Because the Web has need of such organization, this has become an interest of its coordinating body, the WWW Consortium [84]. In 1996, as concern increased about protecting children from exposure to objectionable materials, TRENDS AND RESEARCH ISSUES        431 metadata schemes became connected with censoring and filtering requirements. The problem was renamed for the more general case, in keeping with Harvest's treatment of 'resource discovery,' to 'resource description.' The Resource Description Framework (RDF) thus became an area of study for the Consortium [753]. It should be noted that RDF can lead to header information inside digital objects, including those coded in SGML or HTML, as well as XML (see Chapter 6 for more details). In the more general case, however, RDF is essentially a scheme for annotating digital objects, so alternatively the descriptions can be stored separately from those objects. These options bring us back to the Warwick Framework where there may be multiple containers, sometimes connected through indirection, of packages of metadata, like MARC or DC. We see that digital librarys can be complex collections with various structuring mechanisms for managing data and descriptions of that data, the so-called metadata. However, coding may combine data with metadata, as is specified in the guidelines of the Text Encoding Initiative (TEI) [670]. This reminds us of the complexities that arise when combining streams and structures, where there are many equivalent representations. We also see that for digital library standards to be useful, such as appears to be the case for DC, the structures involved must be relatively simple, and have well understood related scenarios of use. While this now appears to work for data interchange, further work is required for interoperability, i.e., interchange through the streams involved in protocols.
mir-0306	15.7    Trends and Research Issues There are many remaining challenges in the digital library field. While TEI provides guidance in complex encoding situations, and has been advocated by the University of Michigan for electronic theses and dissertations, it is unclear how far the rest of the scholarly community will move towards the thorough markup and description of digital objects that characterize humanistic study [670]. Though such markup is valuable to support context-dependent queries as well as electronic document preservation, it will only be generally feasible when there are less expensive tools and more efficient methods for adding in such markup and description, which may occur as XML usage expands. Then, too, the IR community must provide guidance regarding automatic indexing of marked up documents, metadata, full-text, multimedia streams, and complex hypermedia networks so that the rich and varied content of digital librarys can be searched. On a grander scale are the problems of handling worldwide digital librarys, in the context of varying collection principles, enormous difference in response time between local and remote servers, and the needs of users for different views [474]. Thus, one type of scenario might deal with searching all dissertations worldwide, another might be concerned with finding recent results from a particular research group, a third might consider only freely available works in a particular specialty area, a fourth might deal with seeking the new works recently highly rated by a distributed group of close friends, and yet another might involve the most 432        DIGITAL LIBRARIES readable overviews in an unknown area. Other key research challenges have been highlighted in various workshops aimed at establishing an agenda for investigation [525]. Of central concern is covering the range from personal to global digital librarys, the so-called 'scaling' problem. At the same time, the problem of interoperability must be faced [624]. As argued earlier, we view the solution to these problems to be the acknowledgement of the role of 5S in the digital library arena and the focus of research and development on treating streams, structures, spaces, scenarios, and societies as first class objects and building blocks for digital librarys. We will continue to explore this approach in future work, and believe that, to the extent that integrated support for 5S is developed, real progress will be made towards the next generation of digital libraries.
mir-0307	15.8    Bibliographical Discussion As explained in section 15.1, there are many good sources of information about digital libraries. The best pair are the book by Lesk [501] and the online D-Lib Magazine [280]. Pointers to the latest information and sources can be found through online courseware [268]. New books will appear from MIT Press and other publishers. Large funding initiatives, programs, and projects (e.g., [216, 778, 349]) involving the US National Science Foundation (see e.g., the call for Digital Libraries Initiative - Phase 2, NSF 98-63, http://www.dli2.nsf.gov) and other sponsors, and becoming more and more international in nature (e.g., International Digital Libraries Collaborative, NSF 99-6, will lead to a continuing stream of reports on workshops (e.g., [266, 313, 314, 333, 833, 525]) and high quality research presentations at premiere events like the ACM Digital Libraries conferences (e.g. [50, 192, 382, 507, 548, 705, 791]). Acknowledgements The preparation of this chapter and work described therein was supported in part by US Department of Education grant P116B61190 and by NSF grants CDA-9303152, CDA-9308259, CDA-9312611, DUE-975219G, DUE-975240, and IRI-9116991.
mir-0308	Appendix Porter's Algorithm The rules in the Porter algorithm are separated into five distinct phases numbered from 1 to 5. They are applied to the words in the text starting from phase 1 and moving on to phase 5. Further, they are applied sequentially one after the other as commands in a program. Thus, in what follows, we specify the Porter algorithm in a pseudo programming language whose commands take the form of rules for suffix substitution (as above). This pseudo language adopts the following (semi-formal) conventions: ï  A consonant variable is represented by the symbol C which is used to refer to any letter other than a,e,i,o,u and other than the letter y preceded by a consonant. ï  A vowel variable is represented by the symbol V which is used to refer to any letter which is not a consonant. ï  A generic letter (consonant or vowel) is represented by the symbol L. ï  The symbol lt;jgt; is used to refer to an empty string (i.e., one with no letters). ï  Combinations of C, V", and L are used to define patterns. ï  The symbol * is used to refer to zero or more repetitions of a given pattern. ï  The symbol -f is used to refer to one or more repetitions of a given pattern. ï  Matched parentheses are used to subordinate a sequence of variables to the operators * and -f. ï  A generic pattern is a combination of symbols, matched parentheses, and the operators * and ~f. 433 434       PORTER'S ALGORITHM ï  The substitution rules are treated as commands which are separated by a semicolon punctuation mark. ï  The substitution rules are applied to the suffixes in the current word. ï  A conditional if statement is expressed as 'if (pattern) rule' and the rule is executed only if the pattern in the condition matches the current word. ï  A line which starts with a % is treated as a comment. ï  Curly brackets (braces) are used to form compound commands. ï  A 'select rule with longest suffix' statement selects a single rule for execution among all the rules in a compound command. The rule selected is the one with the largest matching suffix. Thus, the expression (C)* refers to a sequence of zero or more consonants while the expression ((V)*(C)*)* refers to a sequence of zero or more vowels followed by zero or more consonants which can appear zero or more times. It is important to distinguish the above from the sequence (V * C) which states that a sequence must be present and that this sequence necessarily starts with a vowel, followed by a subsequence of zero or more letters, and finished by a consonant. Finally, the command if (*y * L) then ed ógt; lt;/gt; states that the substitution of the suffix ed by nil (i.e., the removal of the suffix ed) only occurs if the current word contains a vowel and at least one additional letter. The Porter algorithm is applied to each word in the text (simple formulation) and is given by the following procedure. % Phase 1: Plurals and past participles. select rule with longest suffix { sses ógt; ss; ies ó? i; ss ó* ss; s ógt; lt;/gt;; } select rule with longest suffix { if ((Cn(V)+(C) + )+(VTeed) then eed ógt; ee; if (*V*ed or *V*ing) then { PORTER'S ALGORITHM        435 select rule with longest suffix { ed ógt;  ing ógt; 0; } select rule with longest suffix { at ógt; ate; bl ó? ble; iz ógt; ize; if ((*CiC2) and  (Ci   =   C2)  and (Ci 0 {l,s,z})) then CXC2 ógt; Ci; if   {({CY{{V)+(C)+)ClV1C2)   and (C2 £ {w,x,y})) then e; } if (*V*y) then y ógt; i; if((C)*((I/)+(C)+)+(I/)* select rule with longest suffix { ational ó? ate; tional ógt; tion; enci ógt; ence; anci ó? ance; izer ógt; ize; abli ó^ able; alii ógt; al; entli ógt; ent; eli ó? e; ousli ógt;ï ous; ization ógt; ize; ation ó? ate; at or ógt; ate; alism ó? al; iveness ó? ive; fulness óy fill: ousness ó-*- ous; aiiti ógt;- al; iviti ógt; ive; biliti ó? ble; } select rule with longest suffix { icate ó* ic; at ive -ó? dgt;; 436        PORTER'S ALGORITHM alize ó? al; iciti ógt; ic; ical ó? ic; ful ó* 4gt;ness ó? cp; } if ((C)*((V)+(C)+)((V)+(C)+)+(V)*) then select rule with longest suffix { al ógt; 0; ance ó? 0; ence ógt; 0; er ógt; 0; ic ógt; 4gt;; able ój- cp; ible ógt;¶ 0; ant ó? (p; ement ógt; 4gt;\ ment ógt; lt;p\ ent ó? 0; ou ógt; 0; ism ó? 0; ate ógt; 0; iti ó? 0; ous ógt;gt; 0; ive ó? 0; ize óª¶ 0; if (*s or *t) then ion ógt; 0; } select rule with longest suffix { if ((Cy((V)+(C)+)((V)+(C)+)+(Vy) then e ó c6; if (((Cn(V0+(C)+)(VT) and not ((*CiViC2) and (C2 {w,x,y}))) then e ó^ nil; } if {(Cy((V)+{C) + )((V)+(C)+) + V*ll) then 11 ó* 1;

160722	Retrieval from hierarchical texts by partial patterns Structured texts (for example dictionaries and user manuals) typically have a heirarchical (tree-like) structure. We describe a query language for retrieving information from collections of hierarchical text. The language is based on a tree pattern matching notion called tree inclusion. Tree inclusion allows easy expression of queries that use the structure and the content of the document. In using it a user need not be aware of the whole structure of the database. Thus a language based on tree inclusion is data independent, a property made necessary because of the great variance in the structure of the texts.
160724	Explorations of NLP for information management (panel): observations from practice in mono- and multi-lingual applications An abstract is not available.
160726	Simple word strings as compound keywords: an indexing and ranking method for Japanese texts This paper describes a new indexing method for Japanese text databases using the simple keyword string , in which a compound word is treated as a string of simple words, which are the smallest units in Japanese grammar which still maintain their meanings. This method allows retrieved texts to be ranked, according to the similarity of their meaning to the query, without using a control vocabulary or thesaurus. This paper also introduces the keyword feature , which describes the syntactic and semantic characteristics of a word, and results in more precise keyword extraction and text retrieval as well as simple dictionary maintenance.
160728	A comparison of indexing techniques for Japanese text retrieval A series of Japanese full-text retrieval experiments were conducted using an inference network document retrieval model. The retrieval performance of two major indexing methods, character-based and word-based, were evaluated. Using structured queries, the character-based indexing performed retrieval as well as, or slightly better, than the word-based system. This result has practical significance since the character-based indexing speed is considerably faster than the traditional word-based indexing. All the queries in this experiment were automatically formulated from natural language input.
160730	Development of a modern OPAC: from REVTOLC to MARIAN Since 1986 we have investigated the problems and possibilities of applying modern information retrieval methods to large online public access library catalogs (OPACs). In the Retrieval Experiment—Virginia Tech OnLine Catalog (REVTOLC) study we carried out a large pilot test in 1987 and a larger, controlled investigation in 1990, with 216 users and roughly 500,000 MARC records. Results indicated that a forms-based interface coupled with vector and relevance feedback retrieval methods would be well received. Recent efforts developing the Multiple Access and Retrieval of Information with Annotations (MARIAN) system have involved used of a specially developed object-oriented DBMS, construction of a client running under NeXTSTEP, programming of a distributed server with a thread assigned to each user session to increase concurrency on a small network of NeXTs, refinement of algorithms to use objects and stopping rules for greater efficiency, usability testing and iterative interface refinement.
160735	Content awareness in a file system interface: implementing the “pile” metaphor for organizing information The pile is a new element of the desktop user interface metaphor, designed to support the casual organization of documents. An interface design based on the pile concept suggested uses of content awareness for describing, organizing, and filing textual documents. We describe a prototype implementation of these capabilities, and give a detailed example of how they might appear to the user. We believe the system demonstrates how content awareness can be not only used in a computer filing system, but made an integral part of the user's experience.
160737	A browser for bibliographic information retrieval, based on an application of lattice theory An application of mathematical lattice theory, called relationship lattices, is utilized to attack problems of operational bibliographic information retrieval. The proposed solution offers an interface to the information searcher enabling operation in a world of concepts, authors, and document records and their relationships. This hides the complexities of query language and database structures, and it allows to use a personally preferred terminology and to browse, query and download document records in a convenient way. The main component of the proposed solution is a personal thesaurus built up as a relationship lattice.
160738	An application of least squares fit mapping to text information retrieval This paper describes a unique example-based mapping method for document retrieval. We discovered that the knowledge about relevance among queries and documents can be used to obtain empirical connections between query terms and the canonical concepts which are used for indexing the content of documents. These connections do not depend on whether there are shared terms among the queries and documents; therefore, they are especially effective for a mapping from queries to the documents where the concepts are relevant but the terms used by article authors happen to be different from the terms of database users. We employ a Linear Least Squares Fit (LLSF) technique to compute such connections from a collection of queries and documents where the relevance is assigned by humans, and then use these connections in the retrieval of documents where the relevance is unknown. We tested this method on both retrieval and indexing with a set of MEDLINE documents which has been used by other information retrieval systems for evaluations. The effectiveness of the LLSF mapping and the significant improvement over alternative approaches was evident in the tests.
160741	On the evaluation of Boolean operators in the extended Boolean retrieval framework The retrieval models based on the extended boolean retrieval framework, e.g., the fuzzy set model and the extended boolean model have been proposed in the past to provide the conventional boolean retrieval system with the document ranking facility. However, due to undesirable properties of evaluation formulas for the AND and OR operations, the former generates incorrect ranked output in certain cases and the latter suffers from the complexity of computation. There have been a variety of fuzzy operators to replace the evaluation formulas. In this paper we first investigate the behavioral aspects of the fuzzy operators and address important issues to affect retrieval effectiveness. We then define an operator class called positively compensatory operators giving high retrieval effectiveness, and present a pair of positively compensatory operators providing high retrieval efficiency as well as high retrieval effectiveness. All the claims are justified through experiments.
160753	A model of information retrieval based on a terminological logic An abstract is not available.
160754	A probabilistic relational model for the integration of IR and databases In this paper, a probabilistic relational model is presented which combines relational algebra with probabilistic retrieval. Based on certain independence assumptions, the operators of the relational algebra are redefined such that the probabilistic algebra is a generalization of the standard relational algebra. Furthermore, a special join operator implementing probabilistic retrieval is proposed. When applied to typical document databases, queries can not only ask for documents, but for any kind of object in the database. In addition, an implicit ranking of these objects is provided in case the query relates to probabilistic indexing or uses the probabilistic join operator. The proposed algebra is intended as a standard interface to combined database and IR systems, as a basis for implementing user-friendly interfaces.
160756	SPIDER: a multiuser information retrieval system for semistructured and dynamic data The access structure, the retrieval model, and the system architecture of the SPIDER information retrieval system are described. The access structure provides efficient weighted retrieval on dynamic data collections. It is based on signatures and non-inverted item descriptions. The signatures provide upper bounds for the exact retrieval status values such that only a small number of exact retrieval status values have to be computed. SPIDER's retrieval model is a probabilistic retrieval model that is capable to exploit the database scheme of semistructured data collections. This model can be considred as a further development of the Binary Independence Indexing (BII) model. The system architecture was derived systematically from a given set of requirements such as effective and efficient retrieval on dynamic data collections, exploitation of the database scheme, computed views, and the integration of information retrieval functionality and database functionality.
160758	Using statistical testing in the evaluation of retrieval experiments The standard strategies for evaluation based on precision and recall are examined and their relative advantages and disadvantages are discussed. In particular, it is suggested that relevance feedback be evaluated from the perspective of the user. A number of different statistical tests are described for determining if differences in performance between retrieval methods are significant. These tests have often been ignored in the past because most are based on an assumption of normality which is not strictly valid for the standard performance measures. However, one can test this assumption using simple diagnostic plots, and if it is a poor approximation, there are a number of non-parametric alternatives.
160760	The effect multiple query representations on information retrieval system performance An abstract is not available.
160761	An evaluation of query processing strategies using the TIPSTER collection The TIPSTER collection is unusual because of both its size and detail. In particular, it describes a set of information needs, as opposed to traditional queries. These detailed representations of information need are an opportunity for research on different methods of formulating queries. This paper describes several methods of constructing queries for the INQUERY information retrieval system, and then evaluates those methods on the TIPSTER document collection. Both AdHoc and Routing query processing methods are evaluated.
160762	Multiple access and retrieval information with ANnotations (abstract) An abstract is not available.
160763	Project envision (abstract) An abstract is not available.
160764	Incremental interface design (abstract): a prototype graphical user interface for grateful med An abstract is not available.
160765	BRAQUE (abstract): an interface to support browsing and interactive query formulation in information retrieval systems An abstract is not available.
160766	ELSA (abstract): an electronic library search assistant An abstract is not available.
160767	Queries-R-Links: browsing and retrieval via interactive querying An abstract is not available.
160768	A common query interface for multilingual document retrieval from databases of the European Community Institutions (abstract) An abstract is not available.
160689	Relevance feedback and inference networks Relevance feedback, which modifies queries using judgements of the relevance of a few, highly-ranked documents, has historically been an important method for increasing the performance of information retrieval systems. In this paper, we extend the inference network model introduced by Turtle and Croft to include relevance feedback techniques. The difference between relevance feedback on text abstracts and full text collections is studied. Preliminary results for relevance feedback on the structured queries supported by the inference net model are also reported.
160690	Efficient context-sensitive plausible inference for information disclosure Plausible inference is an essential aspect of logic-based information disclosure. This paper proposes a context-sensitive plausible inference mechanism based on a so-called index expression belief network. Plausible inference is cloaked as probabilistic evidence propagation within this network. Preliminary experiments show general evidence propagation algorithms to be too inefficient for real-life information disclosure applications. The paper sketches two optimizations whereby efficient, special-purpose evidence propagation may be realized.
160691	Automatic indexing based on Bayesian inference networks In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.
160692	Overview of the first TREC conference The first Text REtrieval Conference (TREC-1) was held in early November 1992 and was attended by about 100 people working in the 25 participating groups. The goal of the conference was to bring research groups together to discuss their work on a new large test collection. There was a large variety of retrieval techniques reported on, including methods using automatic thesaurii, sophisticated term weighting, natural language techniques, relevance feedback, and advanced pattern matching. As results had been run through a common evaluation package, groups were able to compare the effectiveness of different techniques, and discuss how differences among the sytems affected performance.
160693	Approaches to passage retrieval in full text information systems Large collections of full-text documents are now commonly used in automated information retrieval. When the stored document texts are long, the retrieval of complete documents may not be in the users' best interest. In such circumstance, efficient and effective retrieval results may be obtained by using passage retrieval strategies designed to retrieve text excerpts of varying size in response to statements of user interest. New approaches are described in this study for implementing selective passage retrieval systems, and identifying text passages responsive to particular user needs. An automated encyclopedia search system is used to evaluate the usefulness of the proposed methods.
160695	Subtopic structuring for full-length document access We argue that the advent of large volumes of full-length text, as opposed to short texts like abstracts and newswire, should be accompanied by corresponding new approaches to information access. Toward this end, we discuss the merits of imposing structure on full-length text documents; that is, a partition of the text into coherent multi-paragraph units that represent the pattern of subtopics that comprise the text. Using this structure, we can make a distinction between the main topics, which occur throughout the length of the text, and the subtopics, which are of only limited extent. We discuss why recognition of subtopic structure is important and how, to some degree of accuracy, it can be found. We describe a new way of specifying queries on full-length documents and then describe an experiment in which making use of the recognition of local structure achieves better results on a typical information retrieval task than does a standard IR measure.
160696	The identification of important concepts in highly structured technical papers Automatic abstracting, typically based on extraction of important sentences from a text, has been treated as a largely separate task from automatic indexing. This paper describes an approach in which the indexing and abstracting tasks are effectively combined. It is applicable to highly structured empirical research papers, whose content can be organised using a semantic frame. During a scan of a source text, stylistic clues and constructs are used for extracting candidate fillers for the various slots in the frame. Subsequently, an actual concept name is chosen for each slot by comparing the various candidates and their weights.
160697	Is Huffman coding dead? (extended abstract) An abstract is not available.
160699	Compression of indexes with full positional information in very large text databases This paper describes a combination of compression methods which may be used to reduce the size of inverted indexes for very large text databases. These methods are Prefix Omission, Run-Length Encoding, and a novel family of numeric representations called n-s coding . Using these compression methods on two different text sources (the King James Version of the Bible and a sample of Wall Street Journal Stories), the compressed index occupies less than 40% of the size of the original text, even when both stopwords and numbers are included in the index. The decreased time required for I/O can almost fully compensate for the time needed to uncompress the postings. This research is part of an effort to handle very large text databases on the CM-5, a massively parallel MIMD supercomputer.
160700	Analysis of multiterm queries in a dynamic signature file organization Our analysis combines the concerns of signature extraction and signature file organization which have usually been treated as separate issues. We also relax the uniform frequency and single term query assumptions and provide a comprehensive analysis for multiterm query environments where terms can be classified based on their query and database occurrence frequencies. The performance of three superimposed signature generation schemes is explored as they are applied to one dynamic signature file organization based on linear hashing: Linear Hashing with Superimposed Signatures (LHSS). First scheme (SM) allows all terms set the same number of bits regardless of their discriminatory power whereas the second and third methods (MMS and MMM) emphasize the terms with high query and low database occurrence frequencies. Of these three schemes, only MMM takes the probability distribution of the number of query terms into account in finding the optimal mapping strategy. Derivation of performance evaluation formulas is provided together with the results of various experimental settings. Suggestions as to how to implement the given techniques in real life cases are also provided. Results indicate that MMM outperforms the other methods as the gap between the discriminatory power of the terms gets larger. The absolute value of the savings provided by MMM reach a maximum for the high query weight case. However, the extra savings decline sharply for high weight and moderately for the low weight queries with the increase in database size.
160703	Computation of term associations by a neural network An abstract is not available.
160704	Cluster analysis for hypertext systems Identifying nodes of information that are highly related has many applications in any information systems, and in particular in hypertext systems. In this paper we present a technique to identify “natural” clusters in a hypertext. A natural cluster is a cluster that is not arbitrary, but depends only on intrinsic properties of the hypertext. In our case, the property we will use to identify the clusters is the number of independent paths between nodes. Using the graph theoretic definition of k -edge-components we present an aggregation technique to cluster the nodes. We then use this techniques to cluster three medium sized hypertexts that were developed by different authors for different users, using different methodologies. We also show how to use clustering to improve data display, browsing and retrieval.
160706	Constant interaction-time scatter/gather browsing of very large document collections The Scatter/Gather document browsing method uses fast document clustering to produce table-of-contents-like outlines of large document collections. Previous work [1] developed linear-time document clustering algorithms to establish the feasibility of this method over moderately large collections. However, even linear-time algorithms are too slow to support interactive browsing of very large collections such as Tipster, the DARPA standard text retrieval evaluation collection. We present a scheme that supports constant interaction-time Scatter/Gather of arbitrarily large collections after near-linear time preprocessing. This involves the construction of a cluster hierarchy . A modification of Scatter/Gather employing this scheme, and an example of its use over the Tipster collection are presented.
160708	Integrating a dynamic lexicon with a dynamic full-text retrieval system There has been a great deal of interest within the Information Retrieval community in evaluating the use of linguistic knowledge to improve the indexing and searching of textual databases. Such systems must often employ a lexicon to store information about the words and phrases comprising the application's domain. Unlike a static lexicon, a dynamic lexicon raises practical concerns about the coordination between the state of the lexicon and IR indexing schemes based on lexical knowledge. Additionally, it introduces a host of database management issues, many of which are similar to those found in the text databases as well. In this paper, we explore a range of system design and performance issues that arise when integrating a dynamic lexicon with a dynamic full-text information retrieval system. We observe that the principle of functional isolation argues against the use of language-dependent information in article indexes and favors the use of query-time strategies for applying lexical knowledge. We propose and evaluate a system architecture which embodies this principle. We also show how a storage and retrieval infrastructure based on Burkowski's [BURKOWSKI92] “containment model” abstraction can be employed to implement both the text retrieval and lexicon facilities required in an integrated system.
160710	A user-centred evaluation of ranking algorithms for interactive query expansion The evaluation of 6 ranking algorithms for the ranking of terms for query expansion is discussed within the context of an investigation of interactive query expansion and relevance feedback in a real operational environment. The yardstick for the evaluation was provided by the user relevance judgements on the lists of the candidate terms for query expansion. The evaluation focuses on the similarities in the performance of the different algorithms and how the algorithms with similar performance treat terms.
160713	Concept based query expansion Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We address the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness.
160715	Using WordNet to disambiguate word senses for text retrieval This paper describes an automatic indexing procedure that uses the “IS-A” relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches.
160717	MURAX: a robust linguistic approach for question answering using an on-line encyclopedia Robust linguistic methods are applied to the task of answering closed-class questions using a corpus of natural language. The methods are illustrated in a broad domain: answering general-knowledge questions using an on-line encyclopedia. A closed-class question is a question stated in natural language, which assumes some definite answer typified by a noun phrase rather than a procedural answer. The methods hypothesize noun phrases that are likely to be the answer, and present the user with relevant text in which they are marked, focussing the user's attention appropriately. Furthermore, the sentences of matching text that are shown to the user are selected to confirm phrase relations implied by the question, rather than being selected solely on the basis of word frequency. The corpus is accessed via an information retrieval (IR) system that supports boolean search with proximity constraints. Queries are automatically constructed from the phrasal content of the question, and passed to the IR system to find relevant text. Then the relevant text is itself analyzed; noun phrase hypotheses are extracted and new queries are independently made to confirm phrase relations for the various hypotheses. The methods are currently being implemented in a system called MURAX and although this process is not complete, it is sufficiently advanced for an interim evaluation to be presented.
160718	Viewing morphology as an inference process Morphology is the area of linguistics concerned with the internal structure of words. Information Retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. This paper will describe our experiments to determine the importance of morphology, and the effect that it has on performance. We will also describe the role of morphological analysis in word sense disambiguation, and in identifying lexical semantic relationships in a machine-readable dictionary. We will first provide a brief overview of morphological phenomena, and then describe the experiments themselves.
160720	Structured answers for a large structured document collection There is a simple method for integrating information retrieval and hypertext. This consists of treating nodes as isolated documents and retrieving them in order of similarity. If the nodes are structured, in particular, if sets of nodes collectively constitute documents, we can do better. This paper shows how the formation of the hypertext, the retrieval of nodes in response to content based queries, and the presentation of the nodes can be achieved in a way that exploits the knowledge encoded as the structure of the documents. The ideas are then exemplified in an SGML based hypertext information retrieval system.
188495	A sequential algorithm for training text classifiers An abstract is not available.
188496	Expert network: effective and efficient learning from human decisions in text categorization and retrieval An abstract is not available.
188497	Towards language independent automated learning of text categorization models An abstract is not available.
188498	Using IR techniques for text classification in document analysis An abstract is not available.
188499	An evaluation method for stemming algorithms An abstract is not available.
188500	On the measurement of inter-linker consistency and retrieval effectiveness in hypertext databases An abstract is not available.
188508	Query expansion using lexical-semantic relations An abstract is not available.
188509	Perceptual speed, learning and information retrieval performance An abstract is not available.
188511	Term relevance feedback and query expansion: relation to design An abstract is not available.
188517	Modelling information retrieval agents with belief revision An abstract is not available.
188519	Polyrepresentation of information needs and semantic entities: elements of a cognitive theory for information retrieval interaction An abstract is not available.
188521	Investigating aboutness axioms using information fields An abstract is not available.
188544	A probabilistic terminological logic for modelling information retrieval An abstract is not available.
188546	Retrieving terms and their variants in a lexicalized unification-based framework An abstract is not available.
188548	Word sense disambiguation and information retrieval An abstract is not available.
188550	A full-text retrieval system with a dynamic abstract generation function An abstract is not available.
188552	A document retrieval model based on term frequency ranks An abstract is not available.
188554	Automatic combination of multiple ranked retrieval systems An abstract is not available.
188556	Properties of extended Boolean models in information retrieval An abstract is not available.
188557	OHSUMED: an interactive retrieval evaluation and new large test collection for research An abstract is not available.
188558	Results of applying probabilistic IR to OCR text An abstract is not available.
188559	Natural language vs. Boolean query evaluation: a comparison of retrieval performance An abstract is not available.
188560	Inferring probability of relevance using the method of logistic regression An abstract is not available.
188561	Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval An abstract is not available.
188562	The formalism of probability theory in IR: a foundation or an encumbrance? An abstract is not available.
188563	LyberWorld—a visualization user interface supporting fulltext retrieval An abstract is not available.
188564	A system for discovering relationships by feature extraction from text databases An abstract is not available.
188583	Information filtering based on user behavior analysis and best match text retrieval An abstract is not available.
188585	Improving text retrieval for the routing problem using latent semantic indexing An abstract is not available.
188586	The effect of adding relevance information in a relevance feedback environment An abstract is not available.
188589	Passage-level evidence in document retrieval An abstract is not available.
188591	Effective retrieval of structured documents An abstract is not available.
188593	Document and passage retrieval based on hidden Markov models An abstract is not available.
188594	Synthetic workload performance analysis of incremental updates An abstract is not available.
188597	Document filtering for fast ranking An abstract is not available.
188599	Adapting a full-text information retrieval system to the computer troubleshooting domain An abstract is not available.
188601	Evaluating interactive retrieval systems An abstract is not available.
215326	NetSerf: using semantic knowledge to find Internet information archives An abstract is not available.
215327	Dissemination of collection wide information in a distributed information retrieval system An abstract is not available.
215328	Searching distributed collections with inference networks An abstract is not available.
215329	Fast evaluation of structured queries for information retrieval Information retrieval systems are being challenged to manage larger and larger document collections. In an effort to provide better retrieval performance on large collections, more sophisticated retrieval techniques have been developed that support rich, structured queries. Structured queries are not amenable to previously proposed optimization techniques. Optimizing execution, however, is even more important in the context of large document collections. We present a new structured query optimization technique which we have implemented in an inference network-based information retrieval system. Experimental results show that query evaluation time can be reduced by more than half with little impact on retrieval effectiveness.
215330	Efficient recompression techniques for dynamic full-text retrieval systems An abstract is not available.
215331	Design of reusable IR framework An abstract is not available.
215332	Parallel text retrieval on a high performance supercomputer using the Vector Space Model An abstract is not available.
215333	A trainable document summarizer An abstract is not available.
215334	Generating summaries of multiple news articles An abstract is not available.
215335	Integrating IR and RDBMS using cooperative indexing An abstract is not available.
215336	A language for queries on structure and contents of textual databases An abstract is not available.
215343	An NF 2 relational interface for document retrieval, restructuring and aggregation An abstract is not available.
215345	Fast and quasi-natural language search for gigabytes of Chinese texts An abstract is not available.
215347	A new character-based indexing method using frequency data for Japanese documents An abstract is not available.
215349	Little words can make a big difference for text classification An abstract is not available.
215351	Evaluation of evaluation in information retrieval An abstract is not available.
215352	Searchers and searchers: differences between the most and least consistent searches An abstract is not available.
215353	Information processing in the context of medical care An abstract is not available.
215355	Towards new measures of information retrieval evaluation An abstract is not available.
215357	Learning collection fusion strategies An abstract is not available.
215358	Combining multiple evidence from different properties of weighting schemes An abstract is not available.
215359	Efficient processing of vague queries using a data stream approach An abstract is not available.
215360	Document analysis for visualization An abstract is not available.
215361	Users' models of the information space: the case for two search models An abstract is not available.
215362	The newspaper image database: empirical supported analysis of users' typology and word association clusters An abstract is not available.
215363	Human memory models and term association An abstract is not available.
215364	A case-based approach to intelligent information retrieval An abstract is not available.
215365	A comparison of classifiers and document representations for the routing problem An abstract is not available.
215366	Evaluating and optimizing autonomous text classification systems An abstract is not available.
215367	Noise reduction in a statistical approach to text categorization An abstract is not available.
215369	Partial orders for document representation: a new methodology for combining document features An abstract is not available.
215371	Cluster-based text categorization: a comparison of category search strategies An abstract is not available.
215372	Probabilistic Datalog—a logic for powerful retrieval methods An abstract is not available.
215373	Probability kinematics in information retrieval An abstract is not available.
215375	An image retrieval model based on classical logic An abstract is not available.
215376	One term or two? An abstract is not available.
215377	Detecting content-bearing words by serial clustering—extended abstract An abstract is not available.
215379	Applying probabilistic term weighting to OCR text in the case of a large alphabetic library catalogue An abstract is not available.
215380	Relevance feedback with too much data An abstract is not available.
215381	On the reuse of past optimal queries An abstract is not available.
215383	Optimization of relevance feedback weights An abstract is not available.
243200	The network computer (abstract) An abstract is not available.
243202	Query expansion using local and global document analysis An abstract is not available.
243203	The design of a high performance information filtering system A high performance information filtering system has three main requirements: it must be effective in supplying users with useful information, it must do so in a timely fashion, and it must be able to handle a large throughput of information and a large number of user profiles efficiently. These three requirements pose a difficult problem, and to our knowledge no existing system is capable of meeting all three. In this paper we describe a system which combines a number of techniques from other information retrieval and filtering systems, and is capable of providing high performance on a typical workstation platform. We provide estimates of computing resource usage, and show that our system is also scalable.
243206	Pivoted document length normalization An abstract is not available.
243208	Retrieving spoken documents by combining multiple index sources An abstract is not available.
243209	Viewing stemming as recall enhancement An abstract is not available.
243212	Querying across languages: a dictionary-based approach to multilingual information retrieval An abstract is not available.
243213	Experiments in multilingual information retrieval using the SPIDER system An abstract is not available.
243214	Visualizing search results: some alternatives to query-document similarity An abstract is not available.
243216	Reexamining the cluster hypothesis: scatter/gather on retrieval results An abstract is not available.
243218	Evaluation of a tool for visualization of information retrieval results An abstract is not available.
243236	An architecture for implementing extensible information-seeking environments An abstract is not available.
243237	Document retrieval facilities for repository-based system development environments An abstract is not available.
243238	Performance evaluation of a distributed architecture for information retrieval An abstract is not available.
243239	Elicitations during information retrieval: implications for IR system design An abstract is not available.
243249	Evaluating user interfaces to information retrieval systems: a case study on user support An abstract is not available.
243250	Efficient processing of one and two dimensional proximity queries in associative memory An abstract is not available.
243253	Efficient transaction support for dynamic information retrieval systems An abstract is not available.
243256	Image organization and retrieval with automatically constructed feature vectors An abstract is not available.
243258	Phonetic string matching: lessons from information retrieval An abstract is not available.
243261	Experiments on using semantic distances between words in image caption retrieval An abstract is not available.
243265	Automatic linking of thesauri An abstract is not available.
243266	A new method of weighting query terms for ad-hoc retrieval An abstract is not available.
243267	A relevance terminological logic for information retrieval An abstract is not available.
243268	Retrieval of complex objects using a four-valued logic An abstract is not available.
243269	Using n -grams for Korean text retrieval An abstract is not available.
243270	On Chinese text retrieval An abstract is not available.
243271	An application of plausible reasoning to information retrieval An abstract is not available.
243272	A belief network model for IR An abstract is not available.
243273	Document filtering with inference networks An abstract is not available.
243274	Incremental relevance feedback for information filtering An abstract is not available.
243275	Method combination for document filtering An abstract is not available.
243276	Combining classifiers in text categorization An abstract is not available.
243277	Training algorithms for linear text classifiers An abstract is not available.
243278	Context-sensitive learning methods for text categorization An abstract is not available.
243279	Detection of shifts in user interests for personalized information filtering An abstract is not available.
243280	Interactive information retrieval systems: from user centered interface design to software design An abstract is not available.
243281	Panel: building and using test collections An abstract is not available.
243282	System demonstrations: abstracts An abstract is not available.
243283	IR application development with FireWorks An abstract is not available.
243284	A novel client-server protocol for the demanding Opac user An abstract is not available.
243286	WING: a multiple-view smooth information retrieval system An abstract is not available.
243287	Visualizing search results with Envision An abstract is not available.
243288	Ariadne: electronic information for computer scientists An abstract is not available.
243289	WebCompass: an agent-based metasearch and metadata discovery tool for the Web An abstract is not available.
243292	Querying hierarchically structured texts with generalized context-free grammars An abstract is not available.
243293	The CD-ROM of Crete: a multimedia tourism application, based on geographic interaction and information retrieval techniques An abstract is not available.
243294	An efficient retrieval algorithm of two-trie structures An abstract is not available.
243295	An efficient retrieval algorithm of binary digital search-trees using hierarchical structures An abstract is not available.
243296	Assessed relevance and stylistic variation An abstract is not available.
243297	A spatial feature based photograph retrieval system An abstract is not available.
243298	On the potential utility of negative relevance feedback in interactive information retrieval An abstract is not available.
243299	Retrieval of paintings by specifying impression words An abstract is not available.
243300	Extraction of a word list from an existing dictionary to be used in a communication-aid software An abstract is not available.
243301	Merging hypertext and information retrieval in the interface An abstract is not available.
243302	Fast full text search with free word using TS-file An abstract is not available.
243303	OLISTICO: an evaluation environment for interactive IR applications An abstract is not available.
243305	Foundations of advanced information visualization for information retrieval systems An abstract is not available.
243321	Courseware, training and curriculum in information retrieval An abstract is not available.
243322	Research in information retrieval and the practical needs of research and cultural libraries An abstract is not available.
243323	Cross-linguistic information retrieval workshop An abstract is not available.
243325	Networked information retrieval An abstract is not available.
248146	A deductive data model for query expansion An abstract is not available.
250974	Posters: abstracts An abstract is not available.
253171	Recent trends in automatic information retrieval Substantial successes were achieved in the early years in automatic indexing and retrieval using single term indexing theories with term weight assignments based on frequency considerations. The development of more refined indexing systems using thesaurus aids and automatically constructed term association maps changed the retrieval effectiveness only slightly. The recent introduction of the relevance concept in the form of probabilistic retrieval models provided a firm basis for term weighting and document ranking practices. However, the probabilistic methods were not helpful in substantially enhancing the retrieval effectiveness. At the present time, attempts are made to add artificial intelligence concepts to the document retrieval environment in the form of fancy graphics interfaces, learning systems for query and document indexing and for collection searching, extended logic models relating documents and information requests, and analysis methods based on the use of semantic maps and other kinds of knowledge structures. Using the earlier developments and evaluation results as guidelines, an attempt is made to outline the information retrieval environment of the future and to assess the usefulness of some of the currently proposed search and retrieval methods.
253175	Using structural representation of anomalous states of knowledge for choosing document retrieval strategies We report on a project which attempts to classify representations of the anomalous states of knowledge (ASKs) of users of document retrieval systems on the basis of structural characteristics of the representations, and which specifies different retrieval strategies and ranking mechanisms for each ASK class. The classification and retrieval strategy specification is based on 53 real problem statements, 35 of which have a total of 250 evaluated documents. Four facets of the ASK structures have been tentatively identified, whose combinations determine the method and order of application of five basic ranking strategies. This work is still in progress, so results presented here are incomplete.
253178	Document presentation and query formulation in Muse Several problems of document presentation and query, formulation arising in systems dealing with multimedia documents are discussed. Examples from a prototype distributed multimedia document filing system are described.
253180	An approach to multimedia information management The integrated management of multimedia information, that is of complex information consisting of conventional data, text, graphics, images and voice, is of great interest not only in fields like information Retrieval, Office Automation, Computer Aided Instruction, Computer Aided Design, but also in other emerging fields such as Tourist Applications, Computer generated films, Newspapers and magazines production, and so on. In this paper the definition of multimedia object, partially derived from the ECMA standard “Office Document Architecture”, is given and an approach to multimedia information management is proposed. A multilevel environment, where multimedia information can be handled, stored and retrieved and the inner level of which consists of a general purpose Multimedia Data Base Management System (MDBMS), is described. They are outlined the main functionalities that languages which describe and manipulate multimedia objects should provide.
253183	Methodological issues for the design of an office information server: focal topics for the analysis from an office system perspective This paper deal with the necessity of consideration of organizational and user requirements to create the basis for the successful design of future office information servers. Today volumes of the order of 10.000 to 50.000 multimode documents and 1 to 10 million documents at a company level (companies with over 1000 employees) per system have to be archived. The average amount of filing is 12 to 16 running metres of paper per year, with an increasing tendency (Ben84). However the shortcoming of systems of today is not the incapability of storage of big amounts of information but the fact they only support particular (and well structured) office tasks in an operative way. Future systems have to support all kind of work (procedures) people do in an office, consider the strategic and operative goals of the particular office and take the user behaviours especially their knowledge to solve problems and the individual kind of doing their jobs (e.g. search strategies, reminder functions) into account. Analysis methods for the collection of data for the design of office systems have mostly been developed in the context of office automation to develop systems to support particular tasks, to restructure offices to improve the profitability of the company. Because of that fact, these methods must be quite extended to supply data which are suited for the successful design and development of future office information servers. Focal topics of the analysis from an office system perspective are: information generation and location, consistency and permanence, work support, information handling and manipulation, access right and confidentiality, accountability, information flow and use of abstractions. Detailed dimensions concerning these areas are posted in this paper.
253185	IR, NLP, AI and UFOS: or IR-relevance, natural language problems, artful intelligence and user-friendly online systems User Friendly Online Searching is examined in the context of Natural Language Processing in Information Retrieval and Artificial Intelligence. Opportunities for synergetic R & D are identified as the basis for Intelligent Information Retrieval and Artificial Retrieval Intelligence.
253186	The visual display of information in an information retrieval environment This paper gives an overview of the graphical techniques which have been used in the representation of information in a document collection environment. An assessment of the applicability of existing multivariate data graphical techniques to the vector space model is presented.
253188	Improved subject access, browsing and scanning mechanisms in modern on-line IR Focusing on communication, the paper analyses and proposes practical solutions to key problems in online IR, in particular concerned with ill defined and “muddled” information requirements, concept interpretation in searching and text representation. The need for development of new additional browsing and scanning feedback devices, based on existing methods to support searchers is emphasised. The paper points to economically feasible indexing methods fitting the potentials of current information technology and adaptable to in-house information environments: the SAP (Subject Access Project) principles. Focusing display mechanisms and the term frequency analysis feature Zoom are discussed and suggested combined into a flexible database front-end. The design principles are outlined and demonstrated in a worked example.
253189	S-tree: a dynamic balanced signature index for office retrieval The signature approach is an access method for partial-match retrieval which meets many requirements of an office environment. Signatures are hash coded binary words derived from objects stored in the data base. They serve as a filter for retrieval in order to discard a large number of nonqualifying objects. In an indexed signature method the signatures of objects stored on a single page are used to form a signature for that page. In this paper we describe a new technique of indexed signatures which combines the dynamic balancing of B-trees with the signature approach. The main problem of appropriate splitting is solved in a heuristic way. Operations are described and a simple performance analysis is given. The analysis and some experimental results indicate a considerable performance gain. Moreover, the new S-tree approach supports a clustering on a signature basis. Further remarks on adaptability complete this work.
253190	Improved hierarchical bit-vector compression in document retrieval systems The “concordance” of an information retrieval system can often be stored in form of bit-maps, which are usually very sparse and should be compressed. Hierarchical bit-vector compression consists of partitioning a vector v i into equi-sized blocks, constructing a new bit-vector v i +1 which points to the non-zero blocks in v i , dropping the zero-blocks of v i , and repeating the process for v i +1. We refine the method by pruning some of the tree branches if they ultimately point to very few documents; these document numbers are then added to an appended list which is compressed by the prefix-omission technique. The new method was thoroughly tested on the bit-maps of the Responsa Retrieval Project, and gave a relative improvement of about 40% over the conventional hierarchical compression method.
253192	Text compression using prediction In the compression of the text files, the dependencies between the successive characters should be exploited to as great an extent as possible. There are two obvious possibilities: either to detect and encode often occurring character strings, or to encode successors of character blocks. This paper presents two methods based on the latter approach. In the first method we encode only the most probable successors of blocks, whereas in the second we encode them all, using the knowledge of their distribution. The second method uses recursion to store effectively the dependencies between the characters and this results in good compression gains in practical cases.
253193	Incorporating syntactic information into a document retrieval strategy: an investigation This paper deals with mechanisms for performing text retrieval which incorporate a degree of linguistic processing into the overall strategy. We have performed some experiments using parsing of text an a test collection of documents and queries to try and find out exactly if and how parsing could contribute to an overall improvement in retrieval effectiveness. Investigating this topic has led us to the definition of a retrieval strategy which incorporates parsing of query text and a more “shallow” parsing of document texts, whose retrieval effectiveness is investigated and described. Our results indicate that significant improvements in retrieval effectiveness can be obtained by incorporating such linguistic processing into an overall retrieval strategy.
253194	CALIN: a user interface based on a simple natural language In the framework of an application dealing with classified advertisement matching, a dedicated user interface has been designed and implemented. Its major originality relies on the user's language which is neither an artificial one, nor the usual natural language, but in fact the ad language. Beyond the language itself, the interface provides some facilities such as paraphrasing or explanations when needed. An expert system approach has been adopted and the interface is built up from the knowledge given by experts. They are in charge of describing what are acceptable ads, from both syntax and semantics points of view … Although designed in the context of ad matching, that interface may interestingly be adapted to other retrieving systems. We especially think that an adlike language is well-suited to ask questions since it is based on natural simple expressions. A given sentence involves terms that stand for elementary conditions applying to instances of a logical object contained inside the information system. This approach defines a complete interface, involving both a language and aiding capabilities. Moreover, the query language, although less powerful, represents a compromise between artificial languages and the usual natural language, with respect to ergonomics and analysis complexity.
253196	Solving grammatical ambiguities within a surface syntactical parser for automatic indexing This paper describes linguistic tools specifically designed for performant automatic indexing of natural language texts. By performant indexing, we mean the ability of the system to extract noun phrases (considered as main conceptual frames regarding text content) without processing full syntactic analysis of sentences (surface analyzer) both with its ability in learning unknown words. The paper describes the overall principles of this parser, emphasizing the use of syntactic networks and precedence matrix to fulfil the above goals of reducing the analysis cost and infering new vocabularies without interrupting the indexing process.
253197	A design of a distributed full text retrieval system This paper describes the design of a distributed information system for full text retrieval. The system is similar in functionality to STAIRS and is being developed on a network of PC's interconnected by PC Network. The implementation is built on a generalisation of the remote procedure call concept. Communications are based upon the recent CCITT X.400 standard. Examples are given of the design strategy for a subset of the STAIRS system.
253198	A common architecture for different text processing techniques in an information retrieval environment The following paper gives an overview on a text processing software called REALIST (Retrieval Aids by Linguistics and Statistics) which integrates different text processing techniques into a common surface. It supports the user by offering the environment of a given term, using morphological, syntactic and statistic means. The user can call up the processing results, use it for indexing, classification or retrieval purposes and combine them as he wishes e.g. to set up a search logic. The text processing is done on a main frame computer, the results are transferred to a minicomputer where the evaluation is performed. REALIST is a stand alone package, fitting any existing search systems. In the retrieval context, this technique reduces connecting time and improves the search results. REALIST is able to run on English and German texts. Each REALIST component has been separately tested with good success. An integrated version is currently under test at the US Patent ad Trademark Office using 150000 English patent abstracts, and a German version is being tested with 12000 legal texts of the European Community.
253199	COREL: a conceptual retrieval system An abstract is not available.
253200	Hierarchic document classification using Ward's clustering method In this paper, we discuss the application of a recent hierarchic clustering algorithm to the automatic classification of files of documents. Whereas most hierarchic clustering algorithms involve the generation and updating of an inter-object dissimilarity matrix, this new algorithm is based upon a series of nearest neighbor searches. Such an approach is appropriate to several clustering methods, including Ward's method which has been shown to perform well in experimental studies of hierarchic document clustering. A description is given of heuristics which can increase the efficiency of the new algorithm when it is used to cluster three document collections by Ward's method.
253202	User-oriented document clustering: a framework for learning in information retrieval In information retrieval, cluster analysis is an important tool employed to enhance both efficiency and effectiveness of the retrieval process. Most clustering algorithms have difficulty in reflecting the closeness of documents as perceived by the user. A two phase scheme for document clustering, whose results reflect the “conceptual” clusters that are perceived by the user of the retrieval system, is proposed. Since the clusters obtained by this scheme are not characterized in terms of the document representations, a strategy for cluster searching is also developed. Both the proposed document clustering scheme and document searching strategy are experimentally evaluated using a test collection from the SMART system. The preliminary experimental results obtained are very encouraging.
253203	The efficiency of inverted index and cluster searches The processing time and disk space requirements of an inverted index and top-down cluster search are compared. The cluster search is shown to use both more time and more disk space, mostly due to the large number of cluster centroids needed by the search. When shorter centroids are used, the efficiency of the cluster search improves, but the inverted index search remains more efficient.
253205	On extending the vector space model for Boolean query processing An information retrieval model, named the Generalized Vector Space Model (GVSM), is extended to handle situations where queries are specified as (extended) Boolean expressions. It is shown that this unified model, unlike currently available alternatives, has the advantage of incorporating term correlations into the retrieval process. The query language extension is attractive in the sense that most of the algebraic properties of the strict Boolean language are still preserved. Although the experimental results for extended Boolean retrieval are not always better than the vector processing method, the developments here are significant in facilitating commercially available retrieval systems to benefit from the vector based methods. The proposed scheme is compared to the p-norm model advanced by Salton and coworkers. An important conclusion is that it is desirable to investigate further extensions that can offer the benefits of both proposals.
253206	An experimental study of factors important in document ranking The ability to effectively rank retrieved documents in order of their probable relevance to a query is a critical factor in statistically-based keyword retrieval systems. This paper summarizes a set of experiments with different methods of term weighting for documents, using measures of term importance within an entire document collection, term importance within a given document, and document length. It is shown that significant improvements over no term weighting can be made using a combination of weighting measures and normalizing for document length.
253208	(invited paper) A new theoretical framework for information retrieval A new framework based on a non-classical logic is proposed for investigating IR. The paper motivates the use of a particular conditional logic as the 'right' logic for IR. A new principle, the logical uncertainty principle, is proposed, to deal with the inherent uncertainty associated with applicable inferences.
253211	User-specified domain knowledge for document retrieval The introduction of domain knowledge into a document retrieval system has two important consequences; an increase in the effectiveness of retrieval and a decrease in the efficiency of text processing. In this paper, a method is presented of combining user-specified domain knowledge with efficient retrieval techniques based on probabilistic models. The domain knowledge is represented as a collection of frames that contain rules specifying recognition conditions for domain concepts and relationships between concepts. The inference network represented in these frames is used to infer the concepts that are related to a user's query. This approach is being implemented as part of the I 3 R expert intermediary system.
253212	IOTA: a full text information retrieval system IOTA is a prototype of an Information Retrieval System which can manage a corpus made of highly structured, full text documents. The first version presented here has intelligent capabilities related to heuristic pattern matching procedures for processing natural language queries, which involve an automatically built thesaurus. The paper emphazises the overall principles of query processing and gives hints about the underlying techniques used while constructing the thesaurus and automatically indexing highly structured documents.
253215	An information retrieval system based on artificial intelligence techniques This paper describes a possible use of Artificial Intelligence models and techniques in the design of a small Information Retrieval system. In particular, some knowledge representation models, such as semantic networks and frame-like structures, are viewed as interesting tools for the implementation of a thesaurus, and also for a description of the stored documents' contents. In addition, a parser based on the ATN (Augmented Transition Network) model which can analyze Italian sentences concerning a legal domain is described. We are including it in an user/system interface whose goal is to provide the user with the possibility of expressing search topics by using noun phrases or other linguistic expressions, rather than single words or Boolean combinations of them. Finally, some tasks requiring automated reasoning facilities are outlined. The Kernel of the system, i.e. the component which both performs traditional information retrieval and allows the insertion of new documents, is described in an appendix. It was first applied to a bibliographic database containing about a thousand references (with abstracts) of both papers and books concerned with Artificial Intelligence; now we are working on its application to a legal domain, with a database of laws, decrees and sentences concerning pollution and environmental protection.
253216	The using of inference mechanisms to improve the retrieval facilities from large relational databases This paper describes the development of “intelligent” tools aimed at improving the retrieval facilities from large relational databases. When a natural language query does not correspond directly to the data contained in the base, a class of inferential processes called “transformations” is applied. The original query is thus automatically converted into one or more “semantically close” ones. “Semantically close” means that the data possibly obtained with the new query will give useful information about the data originally searched for.
253217	A machine learning approach in information retrieval An abstract is not available.
253218	An automatic and tunable document indexing system In this article we present an interactive automatic document indexing software together with various index tuning/optimization strategies. After stems are generated from the raw text, the initial index vocabulary is narrowed down and tuned with the use of indexing versus clustering theory relationships. The narrowed down vocabulary is further optimized with the inclusion of term phrases and virtual terms corresponding to high and low frequency terms respectively. The results of performance experimentation which proved significant improvements of index vocabulary optimization are presented. The exploitation of the term discrimination value concept in index and retrieval system tuning and optimization is discussed.
253220	Performance of self-taught documents: exploiting co-relevance structure in a document collection In this paper we study the behavior of an information retrieval system in which index terms are assigned at random to both documents and requests. The random indexing is then modified by means of a feedback mechanism derived from a normal probability model and applied to both the request and document representations. Of interest is the convergence properties of the representation vectors. After few feedback iterations, it is found that well defined clusters form that accurately represent the corelevance structure among the documents—in effect the feedback mechanism has permitted the documents to index themselves. This approach offers an interesting way to extend the dimensionality of the indexing vocabulary. Both this application and a theoretical analysis of the impact of extending the indexing vocabulary are discussed.
253221	Two models of retrieval with probabilistic indexing We describe two retrieval models for probabilistic indexing. The binary independence indexing (BII) model is a generalized version of the Maron & Kuhns indexing model. In this model, the indexing weight of a descriptor in a document is an estimate of the probability of relevance of this document with respect to queries using this descriptor. The retrieval-with-probabilistic-indexing (RPI) model is suited to different kinds of probabilistic indexing. Therefore we assume that each indexing model has its own concept of 'correctness' to which the probabilities relate. The concept of correctness is not necessarily identical with the concept of relevance, it is only required to depend on relevance. In addition to the probabilistic indexing weights, the RPI model provides the possibility of relevance weighting of search terms. Both retrieval models are compared in experiments, showing equally good results.
253222	Probabilistic models for document retrieval: a comparison of perfromance on exterimental and synthetic data bases Probabilistic document retrieval systems consistent with the two Poisson independence model outperforms the binary independence model if the terms are distributed as described by the model's assumptions. The Two Poisson Effectiveness Hypothesis suggests that retrieval models based upon the two Poisson model will outperform binary independent models when used on a “real-world” database, where independence and two Poisson term occurrence distributions fail to hold, because the added information obtained from incorporating term frequency information will more than compensate for the non-Poisson distributions of terms. Searches of the MED1033 database suggest that if terms are not independent and frequencies of term occurrence are not distributed in a two Poisson manner, the binary independence sequential retrieval model outperforms the two Poisson independence retrieval model.
253224	Non-binary independence model An abstract is not available.
253225	The maximum entropy principle in information retrieval Applications, assumptions and properties of the maximum entropy principle are discussed. The maximum entropy principle integrates prior estimates of relevance with the observed distribution of term combinations. The result may be a reordering of the segments of a database, compared to a naive estimate. Numerical examples obtained by solution of the non-linear equations for the dual variables are presented and discussed. * Supported in part by the National Science Foundation under grant IST-8318630.
253226	An interpretation of index term weighting schemes based on document components A theory of indexing is presented and is based on viewing a document as constituted of components. A component may be chosen as any run of text unit that can be: (a) judged as to its relevancy property; and (b) considered as independent within the document. By looking at the constituent components of a document in relation to the universe of all components from the collection, we have been able to apply Bayes' decision theory to derive the index term representation for the document, as well as attaching an initial probabilistic weight for each term based on a Principle of Document Self-Recovery. It turns out that different choices of document components, such as a word or a whole abstract, can lead to different term weighting schemes that have been introduced before and are based on probability considerations; specifically, Edmundson and Wyllys' term significance formula, Sparck Jones' inverse document frequency, and later modified by Croft and Harper into the 'combination match' formula. Thus, a unified interpretation of various probabilistic term weighting schemes appears possible.
253504	Graphical information resources: maps and beyond The rise of computer graphics offers a new challenge for information retrieval: how to search and retrieve information which is partly or wholly graphical. As an example, procedures for handling geographical information, such as street maps and directories are explained. With this data, it is possible to find routes on maps, retrieve locations and names of people or businesses, and draw maps. But a comparison of these programs with programs for face processing or computer typesetting makes clear how far we are from general purpose routines. Today successful graphics routines contain a great deal of local domain knowledge. There is no analog of the simple keyword systems that handle textual documents in any subject area. Just as computational linguists have found that subject matter expertise is necessary to do really sophisticated processing of English, it seems also necessary to sophisticated processing of pictures; the difference is that we don't know how to do unsophisticated processing of graphics.
253505	Implications of Boolean structure for probabilistic retrieval An abstract is not available.
253506	Generalized vector spaces model in information retrieval In information retrieval, it is common to model index terms and documents as vectors in a suitably defined vector space. The main difficulty with this approach is that the explicit representation of term vectors is not known a priori. For this reason, the vector space model adopted by Salton for the SMART system treats the terms as a set of orthogonal vectors. In such a model it is often necessary to adopt a separate, corrective procedure to take into account the correlations between terms. In this paper, we propose a systematic method (the generalized vector space model) to compute term correlations directly from automatic indexing scheme. We also demonstrate how such correlations can be included with minimal modification in the existing vector based information retrieval systems. The preliminary experimental results obtained from the new model are very encouraging.
253507	Handling multiple data bases in document retrieval There is no such thing as a standard document. Bibliographic information comes in a wide variety of formats. Existing retrieval systems handle different document styles either by creating an artificial document type or by providing different and independent data bases. Neither approach seems satisfactory. In this paper we describe a data model which we feel is more appropriate for document representation and show it can handle the multiple document type problem quite naturally.
253508	Theoretical measure in P/Q document spaces Early work on the application of user interest profiles to aid in the interpretation of queries in an information retrieval system developed the concepts of retrieval subspaces that are either ellipsoids or Cassini ovals. This report extends the theoretical basis of these models, exploring other models and the effect of different metrics on the conceptual retrieval subspace.
253509	Composite document extended retrieval: an overview Experimental information retrieval (IR) systems, some dating back to the sixties, have demonstrated the viability of fully automatic document storage and retrieval methodologies with small to medium size bibliographic collections [72]. Many of these experimental systems utilize the vector space model in which each important term (such as a word stem) identifies a different dimension in a space, so that matrix methods and vector operations can be defined on queries and documents. Statistical techniques have been very effective, and probabilistic enhancements have given additional improvements [84]. However, the basic vector space model is oriented towards recording the essential information in the text of a title/abstract combination rather than describing more complex document structures. It is necessary to extend the model in order to handle composite documents. On the other hand, commonly available retrieval systems that employ Boolean logic queries and utilize inverted file storage schemes can without modification accommodate such documents, albeit with somewhat less effectiveness than is possible with more sophisticated systems. Hence, it is also of interest to consider how Boolean logic systems can be extended to give better performance, especially with composite documents, and to integrate those approaches with vector methods.
253510	Automatic assignment of soft Boolean operators The conventional bibliographic retrieval systems are based on Boolean query formulations and inverted file implementations. Such systems provide rapid responses in answer to search queries but they are not easy to use by uninitiated patrons. An extended Boolean retrieval strategy has been devised in which the Boolean operators are treated more or less strictly, depending on the setting of a special parameter, known as the p-value. The extended system is much more forgiving than the conventional system, and provides better retrieval effectiveness. In this study various problems associated with the determination of appropriate p-values are discussed, and suggestions are made for an automatic assignment of p-values. Evaluation output is included to illustrate the operations of the suggested procedures.
253511	Output ranking methodology for document-clustering-based Boolean retrieval systems An abstract is not available.
253512	The New Oxford English Dictionary and its potential users: some preliminary comments An abstract is not available.
253513	Processing free-text input to obtain a database of medical information The Linguistic String Project of New York University has developed computer programs that convert the information in free-text documents of a technical specialty into a structured form suitable for mapping into a relational database. The processing is based upon the restrictions on the use of language that are characteristic of the subject matter and the document type. These restrictions are summarized in a “sublanguage grammar” that provides a set of word classes and formulas corresponding to the objects and relations of interest in the domain. The programs are independent of the particular sublanguage grammar employed. The application to narrative patient records will be described and the applicability of the methods to other domains discussed.
253514	An approach to multikey sequencing in an equiprobable keyterm retrieval situation We present a simple generalised technique, for sequencing a multi-attribute file, which can be used in a situation where the query pattern is unknown and the term content equiprobable. The method is based on constructing a short spanning path through the records thereby minimising the sum of the Hamming distances between them. Retrieval performance is simulated over a range of query expressions and results suggest a significant reduction in the number of block accesses, using this technique, as compared with records randomly distributed over the file space.
253515	Optimization of inverted vector searches A simple algorithm is presented for increasing the efficiency of information retrieval searches which are implemented using inverted files. This optimization algorithm employs knowledge about the methods used for weighting document and query terms in order to examine as few inverted lists as possible. An extension to the basic algorithm allows greatly increased performance optimization at a modest cost in retrieval effectiveness. Experimental runs are made examining several different term weighting models and showing the optimization possible with each.
253516	P-trees: storage efficient multiway trees A new variation of high order multiway tree structures, the P-tree is presented. P-trees have average access costs that are significantly better than those of B-trees and are no worse (and often better) in storage utilization. Unlike compact B-trees, they can be maintained dynamically, and unlike dense multiway trees and B-trees, their associated insertion algorithm, which is also presented, is cheap and involves (at most) a very localized rearrangement of keys.
253518	Optimization of a hierarchical file organization for spelling correction A spelling program using a hierarchically organized file seems to be promising, since it can correct more than common typing mistakes. However, its speed of detecting spelling errors in the inputs is rather slow. Here some techniques of modifying the program to improve the speed are presented.
253519	Designing an information retrieval interface based on user characteristics With the increasing number of information retrieval systems and databases available and the increasing demand of end users to utilize the systems, a need exists for improved interfaces and improved training mechanisms. This paper reports on a project to develop an integrated online instruction and assistance system to be used as a “front end” to the U.S. Department of Energy's RECON retrieval system. The conceptual framework for the interface design is based on individual characteristics of its current and prospective users, predominantly scientists conducting energy research. We are building a prototype based on information gained from interviews with scientists using the system (either directly or through a search intermediary) and interviews with search intermediaries. The paper reports on research in progress, including the results of the interviews and the preliminary design of the interface. The conference presentation will include a fuller description of the interface than can be specified here.
253520	Different levels of expertise for an expert system in information retrieval An abstract is not available.
253521	One-time complete indexing of text: theory and practice Indexing according to occurrences of selected word fragments, called “n-grams”, offers a significant alternative to keyword indexing and full text scanning methods in the design of information systems based on documents. Finite sets of n-grams can be selected to allow effective fixed indexing of all words, numbers, and special terms in text. The characteristics of such indexing can be modeled statistically and validated over a wide range of text. The model provides a descriptive and predictive tool for controlling precision and recall in searching and for scaling estimates of relevance to an adaptive reference noise distribution for a target collection. Special techniques such as partial inversion of index terms, probabilistic ordering of index terms, and various types of data compression allow n-gram indexing to be competitive in performance with other approaches.
253522	Experiments with cited titles for automatic document indexing and similarity measure in a probabilistic context An abstract is not available.
253523	A learning algorithm applied to document redescription An abstract is not available.
253524	The cluster hypothesis revisited A new means of evaluating the cluster hypothesis is introduced and the results of such an evaluation are presented for four collections. The results of retrieval experiments comparing a sequential search, a cluster-based search, and a search of the clustered collection in which individual documents are scored against the query are also presented. These results indicate that while the absolute performance of a search on a particular collection is dependent on the pairwise similarity of the relevant documents, the relative effectiveness of clustered retrieval versus sequential retrieval is independent of this factor. However, retrieval of entire clusters in response to a query usually results in a poorer performance than retrieval of individual documents from clusters.
253525	Adaptive document clustering An abstract is not available.
253526	Concepts of the cover coefficient-based clustering methodology Document clustering has several unresolved problems. Among them are high time and space complexity, difficulty of determining similarity thresholds, order dependence, nonuniform document distribution in clusters, and arbitrariness in determination of various cluster intiators. To overcome these problems to some degree, the cover coefficient based clustering methodology has been introduced. The concepts used in this methodology have created certain new concepts, relationships, and measures such as the effect of indexing on clustering, an optimal vocabulary generation for indexing, and a new matching function. These new concepts are discussed. The result of performance experiments that show the effectiveness of the clustering methodology and the matching function are also included. In these experiments, it has been also observed that the majority of the documents obtained in a search are concentrated in a few clusters containing a low percentage of documents of the database.
253527	The LIVE-project: retrieval experiments based on evaluation viewpoints Besides the operators 'and', 'or' and 'not' the GRIPS retrieval language contains thesaurus — operators to extend the query and truncation — and context-operators for freetext and Boolean searching. In a similar way several other viewpoints and evaluation measures were defined and applied in the retrieval experiment. Under the assumption that the evaluation measure is an interval scale averaging is done by calculating the arithmetic mean. As levels of the experimental factor /8/ the following similarity measures were used: inner product measure, cosine measure, overlap measure, coefficient of Jaccard and Euclidean distance. As situative factors /8/ the number of documents retrieved by GRIPS, the number of descriptiors of the queries, generality and topic of documents were used. The retrieval experiment is not yet finished completely but several results have already been obtained. For example in the average (over 81 queries) for the above defined viewpoint the ranking with the inner product measure does not indicate a significant improvement compared with the GRIPS-output. In the case of the Euclidean distance measure it seems that in the average the user has to inspect less nonrelevant documents. This means an improvement compared with the unordered retrieved set from the GRIPS-output. For more details of the so called 'two-level retrieval process' and further experimental results we refer to the long version of this paper.
253528	On retrieval tests with an inhomogeneous query collection Retrieval tests are often performed with an inhomogeneous query collection, that means certain properties of queries vary within the collection. This paper investigates under which conditions this inhomogeneity may lead to a loss of measurement precision and validity and should be controlled by an experimental design. In particular, interrelationships between the design of a retrieval test, inhomogeneity of the query collection and measurement theoretical properties of the evaluation measure will be discussed. Based on these results, some recommendations for experimental design of retrieval tests are presented.
253529	A testbed for information retrieval research: the Utah retrieval system architecture The Utah Retrieval System Architecture provides an excellent testbed for the development and testing of new algorithms or techniques for information retrieval. URSA™ is a message-based structure capable of running on a variety of system configurations, ranging from a single mainframe processor to a system distributed across a number of dissimilar processors. It can readily support a variety of specialized backend processors, such as high-speed search engines. The architecture divides the components of a text retrieval system into two classes: servers and clients. A triple of servers (index, search, and document access) for each database provide the capabilities normally associated with a retrieval system. Possible clients for these servers include a window-based user interface, whose query language can be easily modified, a connection to a mainframe host processor, or Al-based query modification programs that wish to use the database. Any module in the system can be replaced by a new module using a different algorithm as long as the new module complies with the message formats for that function. In fact, with some care this module switch can occur while the system is running, without affecting the users. A monitor program collects statistics on all system messages, giving information regarding query complexity, processing time for each module, queueing times, and bandwidths between every module. This paper discusses the background of URSA and its structure, with particular emphasis on the features that make it a good testbed for information retrieval techniques.
253530	An integrated hierarchical file organization for data selection and retrieval This paper presents an approach, the IHF approach, of organising data of a hierarchical structure into one single file, maintaining adequate representation of the relationships among the data. The IHF approach requires each data item be stored in the basic form of name-value pair and consists in assigning each data item with identifier which provides not only the data item identification but also an implicit identification of the hierarchical structural relationships among groups of data items. Data selection and retrieval operations are effectively implemented with the concept of masking of relevant identifier bits, without resorting to any linking devices. Application of the IHF technique on CAFS is examined.
253531	RUBRIC: an environment for full text information retrieval An abstract is not available.
253532	ANNOD: a navigator of natural-language organized (textual) data ANNOD is the name of a system developed at the National Library of Medicine (NLM), which implements a set of linguistic and empirical techniques that permit retrieval of natural language information in response to natural language queries. The system is based on Dr. Gerard Salton's SMART [1] document retrieval system and is presently implemented on a mini-computer as part of an Interactive TExt Management System, ITEMS.[2] Actual experience with retrieval of information from NLM's Hepatitis Knowledge Base (HKB), an encyclopedic hierarchical, full-text file, is presented. The techniques used in ANNOD include: automatic stemming of words, common word deletion, thesaurus expansion, a complex empirical matching (ranking) algorithm (similarity measure), and techniques expressly designed to permit rapid response in a mini-computer environment. Preliminary testing demonstrates high efficiency in identifying portions of a text which are relevant to users.
253533	The user's mental model of an information retrieval system An empirical study was performed to train naive subjects in the use of a prototype Boolean logic-based information retrieval system on a bibliographic database. Subjects were undergraduates with little or no prior computing experience. Subjects trained with a conceptual model of the system performed better than subjects trained with procedural instructions, but only on complex, problem-solving tasks. Performance was equal on simple tasks. Differences in patterns of interaction with the system (based on a stochastic process model) showed parallel results. Most subjects were able to articulate some description of the system's operation, but few articulated a model similar to the card catalog analogy provided in training. Eleven of 43 subjects were unable to achieve minimal competency in system use. The failure rate was equal between training conditions and genders; the only differences found between those passing and failing the benchmark test were academic major and in frequency of library use.
253534	A study of the relationship between user profiles and user queries An abstract is not available.
253535	A conceptual model and experiments on how people classify and retrieve documents An abstract is not available.
258526	Users lost (summary): reflections on the past, future, and limits of information science An abstract is not available.
258528	Fast and effective query refinement An abstract is not available.
258529	On relevance weights with little relevance information An abstract is not available.
258530	Learning routing queries in a query zone An abstract is not available.
258531	Comparing representations in Chinese information retrieval An abstract is not available.
258532	Chinese text retrieval without using a dictionary An abstract is not available.
258534	PAT-tree-based keyword extraction for Chinese information retrieval An abstract is not available.
258535	Almost-constant-time clustering of arbitrary corpus subsets4 An abstract is not available.
258537	Feature selection, perceptron learning, and a usability case study for text categorization An abstract is not available.
258539	Projections for efficient document clustering An abstract is not available.
258540	Phrasal translation and query expansion techniques for cross-language information retrieval An abstract is not available.
258542	QUILT: implementing a large-scale cross-language text retrieval system An abstract is not available.
258544	Cross-language speech retrieval: establishing a baseline performance An abstract is not available.
258546	Dempster-Shafer's theory of evidence applied to structured documents: modelling uncertainty An abstract is not available.
258547	Computationally tractable probabilistic modeling of Boolean operators An abstract is not available.
258551	A method for monolingual thesauri merging An abstract is not available.
258552	Textual context analysis for information retrieval An abstract is not available.
258554	Effective use of natural language processing techniques for automatic conflation of multi-word terms: the role of derivational morphology, part of speech tagging, and shallow parsing An abstract is not available.
258557	Guessing morphology from terms and corpora An abstract is not available.
258560	Optimal demand-oriented topology for hypertext systems An abstract is not available.
258561	Passage retrieval revisited Ranking based on passages addresses some of the shortcomings of whole-document ranking. It provides convenient units of text to return to the user, avoids the difficulties of comparing documents of different length, and enables identification of short blocks of relevant material amongst otherwise irrelevant text. In this paper we explore the potential of passage retrieval, based on an experimental evaluation of the ability of passages to identify relevant documents. We compare our scheme of arbitrary passage retrieval to several other document retrieval and passage retrieval methods; we show experimentally that, compared to these methods, ranking via fixed-length passages is robust and effective. Our experiments also show that, compared to whole-document ranking, ranking via fixed-length arbitrary passages significantly improves retrieval effectiveness, by 8% for TREC disks 2 and 4 and by 18%-37% for the Federal Register collection.
258564	Exploration of text collections with hierarchical feature maps An abstract is not available.
258568	Users' perception of the performance of a filtering system An abstract is not available.
258569	Time, relevance and interaction modelling for information retrieval An abstract is not available.
258572	How to read less and know more: approximate OCR for Thai An abstract is not available.
258576	Overlapping statistical word indexing: a new indexing method for Japanese text An abstract is not available.
258580	Effectiveness of a graphical display of retrieval results An abstract is not available.
258582	Cat-a-Cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy An abstract is not available.
258585	A probabilistic model for distributed information retrieval An abstract is not available.
258587	Analyses of multiple evidence combination An abstract is not available.
258589	Image retrieval by appearance An abstract is not available.
258591	Using semantic contents and WordNet in image retrieval An abstract is not available.
258594	Image retrieval by hypertext links An abstract is not available.
258597	Automatic feedback using past queries: social searching? An abstract is not available.
258601	Exploiting clustering and phrases for context-based information retrieval An abstract is not available.
258603	The potential and actual effectiveness of interactive query expansion An abstract is not available.
122861	The significance of the Cranfield tests on index languages An abstract is not available.
122862	Complete formal model for information retrieval systems An abstract is not available.
122863	Automatic text structuring and retrieval-experiments in automatic encyclopedia searching An abstract is not available.
122864	The use of phrases and structured queries in information retrieval An abstract is not available.
122865	Combining model-oriented and description-oriented approaches for probabilistic indexing An abstract is not available.
122866	Some inconsistencies and misnomers in probabilistic information retrieval An abstract is not available.
122867	Generative models for bitmap sets with compression applications: (extended abstract) An abstract is not available.
122868	Posting compression in dynamic retrieval environments An abstract is not available.
122869	A hybrid bilevel image decode algorithm for group 4 FAX An abstract is not available.
122870	The CORE electronic chemistry library An abstract is not available.
122871	Retrieval algorithm effectiveness in a wide area network information filter An abstract is not available.
122872	Distributed representations in a text based information retrieval system: a new way of using the vector space model An abstract is not available.
122873	To see, or not to see— is That the query? An abstract is not available.
122874	Integrating query thesaurus, and documents through a common visual representation An abstract is not available.
122875	A case-based architecture for a dialogue manager for information-seeking processes An abstract is not available.
122876	Addressing the requirements of a dynamic corporate textual information base An abstract is not available.
122877	Data conversion, aggregation and deduction for advanced retrieval from the heterogeneous fact databases An abstract is not available.
122878	Querying office systems about document roles An abstract is not available.
122879	Query modification and expansion in a network with adaptive architecture An abstract is not available.
122880	Using the cosine measure in a neural network for document retrieval An abstract is not available.
122881	Preference structure, inference and set-oriented retrieval An abstract is not available.
122883	Distributed indexing: a scalable mechanism for distributed information retrieval An abstract is not available.
122884	On the allocation of documents in multiprocessor information retrieval systems An abstract is not available.
122885	An object-oriented modeling of the history of optimal retrievals An abstract is not available.
122886	Retrieving software objects in an example-based programming environment An abstract is not available.
122887	A self-organizing semantic map for information retrieval An abstract is not available.
122888	Incorporating a semantic analysis into a document retrieval strategy An abstract is not available.
122889	Complementary structures in disjoint science literatures An abstract is not available.
122890	An efficient directory system for document retrieval An abstract is not available.
122891	Image query processing based on multi-level signatures An abstract is not available.
122892	A two-level hypertext retrieval model for legal data An abstract is not available.
122893	Automatic generation of “hyper-paths” in information retrieval systems: a stochastic and an incremental algorithms An abstract is not available.
122894	Creating segmented databases from free text for text retrieval An abstract is not available.
122896	Retrieval performance in Ferret a conceptual information retrieval system An abstract is not available.
122897	The smart document retrieval project An abstract is not available.
133167	Relevance feedback revisited Researchers have found relevance feedback to be effective in interactive information retrieval, although few formal user experiments have been made. In order to run a user experiment on a large document collection, experiments were performed at NIST to complete some of the missing links found in using the probabilistic retrieval model. These experiments, using the Cranfield 1400 collection, showed the importance of query expansion in addition to query reweighting, and showed that adding as few as 20 well-selected terms could result in performance improvements of over 100%. Additionally it was shown that performing multiple iterations of feedback is highly effective.
133169	Incremental relevance feedback Although relevance feedback techniques have been investigated for more than 20 years, hardly any of these techniques has been implemented in a commercial full-text document retrieval system. In addition to pure performance problems, this is due to the fact that the application of relevance feedback techniques increases the complexity of the user interface and thus also the use of a document retrieval system. In this paper we concentrate on a relevance feedback technique that allows easily understandable and manageable user interfaces, and at the same time provides high-quality retrieval results. Moreover, the relevance feedback technique introduced unifies as well as improves other well-known relevance feedback techniques.
133171	Measuring the informativeness of a retrieval process Evaluation of information retrieval systems should be based on measures of the information provided by the retrieval process, “informativeness” measures which take into account the interactive and full-text nature of present-day systems and the different types of questions which are asked of them. Desirable properties for an informativeness measure are developed, including context sensitivity, user centrality, and logarithmic response. A hypergraph-based framework for measuring the informativeness of a retrieval process is presented and a measure developed which satisfies the desired properties. The measure is compared to previously developed information measures and illustrated via an application.
133172	An evaluation of phrasal and clustered representations on a text categorization task Syntactic phrase indexing and term clustering have been widely explored as text representation techniques for text retrieval. In this paper we study the properties of phrasal and clustered indexing languages on a text categorization task, enabling us to study their properties in isolation from query interpretation issues. We show that optimal effectiveness occurs when using only a small proportion of the indexing terms available, and that effectiveness peaks at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word-based indexing. We also present results suggesting that traditional term clustering method are unlikely to provide significantly improved text representations. An improved probabilistic text categorization method is also presented.
133175	Automatic document classification: natural language processing, statistical analysis, and expert system techniques used together In this paper we describe an automated method of classifying research project descriptions: a human expert classifies a sample set of projects into a set of disjoint and pre-defined classes, and then the computer learns from this sample how to classify new projects into these classes. Both textual and non-textual information associated with the projects are used in the learning and classification phases. Textual information is processed by two methods of analysis: a natural language analysis followed by a statistical analysis. Non-textual information is processed by a symbolic learning technique. We present the results of some experiments done on real data: two different classifications of our research projects.
133177	Classifying news stories using memory based reasoning We describe a method for classifying news stories using Memory Based Reasoning (MBR) a k -nearest neighbor method), that does not require manual topic definitions. Using an already coded training database of about 50,000 stories from the Dow Jones Press Release News Wire, and SEEKER [Stanfill] (a text retrieval system that supports relevance feedback) as the underlying match engine, codes are assigned to new, unseen stories with a recall of about 80% and precision of about 70%. There are about 350 different codes to be assigned. Using a massively parallel supercomputer, we leverage the information already contained in the thousands of coded stories and are able to code a story in about 2 seconds. Given SEEKER, the text retrieval system, we achieved these results in about two person-months. We believe this approach is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc.
133179	Term position ranking: some new test results Presents seven sets of laboratory results testing variables in term position ranking which produce a phrase effect by weighting the distance between proximate terms. Results of the 73 tests conducted by this project are included, covering variant term position algorithms, sentence boundaries, stopword counting, every pairs testing, field selection, and combinations of algorithm including collection frequency, record frequency and searcher weighted. The discussion includes the results of tests by Fagan and by Croft, the need for term stemming, proximity as a precision device, comparisons with Boolean, and the quality of test collections.
133180	Experiments in automatic statistical thesaurus construction A well constructed thesaurus has long been recognized as a valuable tool in the effective operation of an information retrieval system. This paper reports the results of experiments designed to determine the validity of an approach to the automatic construction of global thesauri (described originally by Crouch in [1] and [2] based on a clustering of the document collection. The authors validate the approach by showing that the use of thesauri generated by this method results in substantial improvements in retrieval effectiveness in four test collections. The term discrimination value theory, used in the thesaurus generation algorithm to determine a term's membership in a particular thesaurus class, is found not to be useful in distinguishing a “good” from an “indifferent” or “poor” thesaurus class). In conclusion, the authors suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus classes. Experimental results show that the alternate approach described herein in some cases produces thesauri which are comparable in retrieval effectiveness to those produced by the first method at much lower cost.
133181	Use of syntactic context to produce term association lists for text retrieval One aspect of world knowledge essential to information retrieval is knowing when two words are related. Knowing word relatedness allows a system given a user's query terms to retrieve relevant documents not containing those exact terms. Two words can be said to be related if they appear in the same contexts Document co-occurrence gives a measure of word relatedness that has proved to be too rough to be useful. The relatively recent apparition of on-line dictionaries and robust and rapid parsers permits the extraction of finer word contexts from large corpora. In this paper, we will describe such an extraction technique that uses only coarse syntactic analysis and no domain knowledge. This technique produces lists of words related to any work appearing in a corpus. When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique.
133183	Versioning a full-text information retrieval system In this paper, we present an approach to the incorporation of object versioning into a distributed full-text information retrieval system. We propose an implementation based on “partially versioned” index sets, arguing that its space overhead and query-time performance make it suitable for full-text IR, with its heavy dependence on inverted indexing. We develop algorithms for computing both historical queries and time range queries and show how these algorithms can be applied to a number of problems in distributed information management, such as data replication, caching, transactional consistency, and hybrid media repositories.
133185	Retrieval activities in a database consisting of heterogeneous collections of structured text The first part of this paper briefly describes a mathematical framework (called the containment model) that provides the operations and data structures for a text dominated database with a hierarchical structure. The database is considered to be a hierarchical collection of continuous extents each extent being a word, word phrase, text element or non-text element. The filter operations making up a search command are expressed in terms of containment criteria that specify whether a contiguous extent will be selected or rejected during a search. This formalism, comprised of the mathematical framework and its associated language, defines a conceptual layer upon which we can construct a well-defined higher level layer, specifically the user interface that serves to provide a level of functionality that is closer to the needs of the user and the application domain. With the conceptual layer established, we go on to describe the design and implementation of a versatile interface which handles queries that search and navigate a heterogeneous collection of structured documents. Interface functionality is provided by a set of “worker” modules supported by an “environment” that is the same for all interfaces. The interface environment allows a worker to communicate with the underlying text retrieval engine using a well-defined command protocol that is based on a small set of filter operators. The overall design emphasizes: a) interface flexibility for a variety of search and browsing capabilities, b) the modular independence of the interface with respect to its underlying retrieval engine, and c) the advantages to be accrued by defining retrieval commands using operators that are part of a text algebra that provides a sound theoretical foundation for the database.
133186	A textual object management system Computer programs that access significant amounts of text usually include code that manipulates the textual objects that comprise it. Such programs include electronic mail readers, typesetters and, in particular, full-text information retrieval systems. Such code is often unsatisfying in that access to textual objects is either efficient, or flexible, but not both. A programming language like Awk or Perl provides very general facilities for describing textual objects, but at the cost of rescanning the text for every textual object. At the other extreme, full-text information retrieval systems usually offer access to a very limited number of kinds of textual objects, but this access is very efficient. The system described in this paper is a programming tool for managing textual objects. It provides a great deal of flexibility, giving access to very complex document structure, with a large number of constituent kinds of textual objects. Further, it provides access to these objects very efficiently, both in terms of time and auxiliary space, by being very careful to access secondary storage only when absolutely necessary.
133188	Towards a probabilistic modal logic for semantic-based information retrieval Semantic-based approaches to Information Retrieval make a query evaluation similar to an inference process based on semantic relations. Semantic-based approaches find out hidden semantic relationships between a document and a query, but quantitative estimation of the correspondence between them is often empiric. On the other hand, probabilistic approaches usually consider only statistical relationships between terms. It is expected that improvement may be brought by integrating these two approaches. This paper demonstrates, using some particular probabilistic models which are strongly related to modal logic, that such an integration is feasible and natural. A new model is developed on the basis of an extended modal logic. It has the advantages of : (1) augmenting a semantic-based approach with a probabilistic measurement, and (2) augmenting a probabilistic approach with finer semantic relations than just statistical ones. It is shown that this model verifies most of the conditions for an absolute probability function .
133190	An analysis of vector space models based on computational geometry This paper analyzes the properties, structures and limitations of vector-based models for information retrieval from the computational geometry point of view. It is shown that both the pseudo-cosine and the standard vector space models can be viewed as special cases of a generalized linear model. More importantly, both the necessary and sufficient conditions have been identified, under which ranking functions such as the inner-product, cosine, pseudo-cosine, Dice, covariance and product-moment correlation measures can be used to rank the documents. The structure of the solution region for acceptable ranking is analyzed and an algorithm for finding all the solution vectors is suggested.
133191	Latent semantic indexing is an optimal special case of multidimensional scaling Latent Semantic Indexing (LSI) is a technique for representing documents, queries, and terms as vectors in a multidimensional real-valued space. The representtions are approximations to the original term space encoding, and are found using the matrix technique of Singular Value Decomposition. In comparison Multidimensional Scaling (MDS) is a class of data analysis techniques for representing data points as points in a multidimensional real-valued space. The objects are represented so that inter-point similarities in the space match inter-object similarity information provided by the researcher. We illustrate how the document representations given by LSI are equivalent to the optimal representations found when solving a particular MDS problem in which the given inter-object similarity information is provided by the inner product similarities between the documents themselves. We further analyze a more general MDS problem in which the interdocument similarity information, although still in inner product form is arbitrary with respect to the vector space encoding of the documents.
133194	A system for retrieving speech documents An information retrieval model is presented for the retrieval of speech documents, i.e. audio recordings containing speech. The indexing vocabulary consists of indexing features that have the following characteristics. First, they are easy to recognize by speech recognition methods. Second, the number of different indexing features is small such that a reasonable amount of training data is sufficent to train the hidden Markov models that are used by the speech recognition process. Third, the retrieval method based on such indexing features achieves an acceptable retrieval effectiveness as shown by experiments on text collections. Fourth, these indexing features cannot only be identified in speech documents but also in text documents. From the last characteristic follows that speech documents and text documents can be retrieved simultaneously. Analogously, the queries may contain either speech or text. Thus, we have a simple multimedia retrieval model where two different medias are indexed coherently. We also describe a prototype retrieval system under development.
133195	N-Poisson document modelling This paper is a report of a study investigating the validity of the Multiple Poisson ( nP ) model of word distribution in document collections. An nP distribution is a mixture of n Poisson distributions with different means. We describe a practical algorithm for determining if a certain word is distributed acording to an nP distribution and computing the distribution parameters. The algorithm was applied to every word in four different document collections. It was found that over 70% of frequently occurring words and terms indeed behave according to the nP distributions. The results indicate that the proportion of nP words depends on the collection size, document length and the frequency of the individual words. Most of the nP words recognised are distributed according to the mixture of relatively few single Poisson distributions (two, three or four). There is an indication that the number of single Poisson components in the mixture of relatively few single Poisson distributions (two, three or four). There is an indication that the number of single Poisson components in the mixture depends on the collection frequency of words.
133197	An incrementally extensible document retrieval system based on linguistic and logical principles Most natural language based document retrieval systems use the syntax structures of constituent phrases of documents as index terms. Many of these systems also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structures. However, the retrieval performance of such systems remains fairly disappointing. Some systems therefore use a meaning representation language to index and retrieve documents. In this paper, a system is presented that uses Horn Clause Logic as meaning representation language, employs advanced techniques from natural Language Processing to achieve incremental extensibility, and uses methods from Logic Programming to achieve robustness in the face of insufficient data.
133199	Probabilistic retrieval based on staged logistic regression The goal of a probabilistic retrieval system design is to rank the elements of the search universe in descending order of their estimated probability of usefulness to the user. Previously explored methods for computing such a ranking have involved the use of statistical independence assumptions and multiple regression analysis on a learning sample. In this paper these techniques are recombined in a new way to achieve greater accuracy of probabilistic estimate without undue additional computational complexity. The novel element of the proposed design is that the regression analysis be carried out in two or more levels or stages. Such an approach allows composite or grouped retrieval clues to be analyzed in an orderly manner -- first within groups, and then between. It compensates automatically for systematic biases introduced by the statistical simplifying assumptions, and gives rise to search algorithms of reasonable computational efficiency.
133202	Integration of probabilistic fact and text retrieval In this paper, a model for combining text and fact retrieval is described. A query is a set of conditions, where a single condition is either a text or fact condition. Fact conditions can be interpreted as being vague, thus leading to nonbinary weights for fact conditions with respect to database objects. For text conditions, we use descriptions of the occurence of terms in documents instead of precomputed indexing weights, thus treating terms similar to attributes. Probabilistic indexing weights for conditions are computed by introducing the notion of correctness (or acceptability) of a condition w.r.t. an object. These indexing weights are used in retrieval for a probabilistic ranking of objects based on the retrieval for a probabilistic ranking of objects based on the retrieval-with-probabilistic-indexing (RPI) model, for which a new derivation is given here.
133203	A loosely-coupled integration of a text retrieval system and an object-oriented database system Document management systems are needed for many business applications. This type of system would combine the functionality of a database system, (for describing, storing and maintaining documents with complex structure and relationships) with a text retrieval system (for effective retrieval based on full text). The retrieval model for a document management system is complicated by the variety and complexity of the objects that are represented. In this paper, we describe an approach to complex object retrieval using a probabilistic inference net model, and an implementation of this approach using a loose coupling of an object-oriented database system (IRIS) and a text retrieval system based on inference nets (INQUERY). The resulting system is used to store long, structured documents and can retrieve document components (sections, figures, etc.) based on their contents or the contents of related components. The lessons learnt from the implementation are discussed.
133205	Automating the assignment of submitted manuscripts to reviewers The 117 manuscripts submitted for the Hypertext '91 conference were assigned to members of the review committee, using a variety of automated methods based on information retrieval principles and Latent Semantic Indexing. Fifteen reviewers provided exhaustive ratings for the submitted abstracts, indicating how well each abstract matched their interests. The automated methods do a fairly good job of assigning relevant papers for review, but they are still somewhat poorer than assignments made manually by human experts and substantially poorer than an assignment perfectly matching the reviewers' own ranking of the papers. A new automated assignment method called “ n of 2 n ” achieves better performance than human experts by sending reviewers more papers than they actually have to review and then allowing them to choose part of their review load themselves.
133207	Design of an OPAC database to permit different subject searching accesses in a multi-disciplines universities library catalogue database This paper presents searching approaches and user interface capabilities of DUO, an Online Public Access Catalogue (OPAC) designed to permit the users of three Universities of the Northeast of Italy different subject searching accesses to the co-operative multi-disciplines library catalogue database. The co-operative catalogue database is managed by one of the software systems developed under the italian national project for library automation: the SBN project. Since the SBN database has not been designed to be efficiently accessed for end-user searches, the DUO database has been designed to avoid duplication of the SBN database data and to be usable for making efficient subjects accesses to the catalogue documents. The DUO design choices are presented, in particular the main choice of designing a “virtual” document that corresponds to each SBN document and that has unstructured data usable for subject search purposes. The paper presents a new kind of user-OPAC dialogue that makes available to the user different search approaches and on-line dictionaries. In particular the user during the interaction with the search tool can represent his information needs with the support of interface capabilities that are based on retrieval path history, and words and codes on-line dictionaries. DUO is the first Italian OPAC that has been made openly available to users of universities and research institutions. For this reason, it is also the first time that OPAC log data is going to be collected in Italy. This work mainly intends to make a modern OPAC available to the users of a SBN catalogue database, but it is going to permit also to build up a knowledge on OPAC usage in Italy.
133208	Searching for historical word-forms in a database of 17th-century English text using spelling-correction methods This paper discusses the application of algorithmic spelling-correction techniques to the identification of those words in a database of 17 th century English text that are most similar to a query word in modern English. The experiments have used n -gram matching, non-phonetic coding and dynamic programming methods for spelling correction, and have demonstrated that high-recall searches can be carried out, although some of the searches are very demanding of computational resources. The methods are, in principle, applicable to historical texts in many languages and from many diffeent periods.
133209	A faster algorithm for constructing minimal perfect hash functions Our previous research on one-probe access to large collections of data indexed by alphanumeric keys has produced the first practical minimal perfect hash functions for this problem. Here, a new algorithm is described for quickly finding minimal perfect hash functions whose specification space is very close to the theoretical lower bound, i.e., around 2 bits per key. The various stages of processing are detailed, along with analytical and empirical results, including timing for a set of over 3.8 million keys that was processed on a NeXTstation in about 6 hours.
133210	Parameterised compression for sparse bitmaps Full-text retrieval systems often use either a bitmap or an inverted file to identify which documents contain which terms, so that the documents containing any combination of query terms can be quickly located. Bitmaps of term occurrences are large, but are usually sparse, and thus are amenable to a variety of compression techniques. Here we consider techniques in which the encoding of each bitvector within the bitmap is parameterised, so that a different code can be used for each bitvector. Our experimental results show that the new methods yield better compression than previous techniques.
133211	Frame-sliced partitioned parallel signature files The retrieval capabilities of the signature file access method have become very attractive for many data processing applications dealing with both formatted and unformatted data. However, performance is still a problem, mainly when large files are used and fast response required. In this paper, a high performance signature file organization is proposed, integrating the latest developments both in storage structure and parallel computing architectures. It combines horizontal and vertical approaches to the signature file fragmentation. In this way, a new, mixed decomposition scheme, particularly suitable for parallel implementation, is achieved. The organization, based on this fragmentation scheme, is called Fragmented Signature File . Performance analysis shows that this organization provides very good and relatively stable performance, covering the full range of possible queries. For the same degree of parallelism, it outperforms any other parallel signature file organization that has been defined so far. The proposed method also has other important advantages concerning processing of dynamic files, adaptability to the number of available processors, load balancing, and, to some extent, fault-tolerant query processing.
133212	Cognitive differences in end user searching of a CD-ROM index Cognitive abilities of fifty university students were tested using eight tests from the Kit of Factor-Referenced Cognitive Tests. All students searched for references on the same topic using a standard computerized index, and performance in the searches was analyzed using a variety of measures. Effects for cognitive differences, as well as for differences in demographic characteristics and knowledge, were identified using multiple regression. Perceptual speed had an effect on the quality of searches, and logical reasoning, verbal comprehension, and spatial scanning abilities influenced search tactics. It is suggested that information retrieval systems can be made more accessible to users with different levels of cognitive abilities through improvements that will assist users to scan lists of terms, choose appropriate vocabulary for searching, and select useful references.
133213	Developing a theory to guide the process of designing information retrieval systems The dominant approaches to information retrieval system design are based on rational theory and cognitive engineering. However, these theories as well as approaches in other disciplines, reviewed in this paper, do not account for communication, or interaction, among design participants which is critical to design outcomes. This research attempts to develop a descriptive design model that accounts for communication among users, designers, and developers throughout the design process. A pilot study has been completed and a preliminary model that represents a first step in understanding participants' evolving perceptions and expectations of the design process and its outcomes is described in this paper.
133214	Scatter/Gather: a cluster-based approach to browsing large document collections Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval. We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm.
133215	Bead: explorations in information visualization We describe work on the visualization of bibliographic data and, to aid in this task, the application of numerical techniques for multidimensional scaling. Many areas of scientific research involve complex multivariate data. One example of this is Information Retrieval. Document comparisons may be done using a large number of variables. Such conditions do not favour the more well-known methods of visualization and graphical analysis, as it is rarely feasible to map each variable onto one aspect of even a three-dimensional, coloured and textured space. Bead is a prototype system for the graphically-based exploration of information. In this system, articles in a bibliography are represented by particles in 3-space. By using physically-based modelling techniques to take advantage of fast methods for the approximation of potential fields, we represent the relationships between articles by their relative spatial positions. Inter-particle forces tend to make similar articles move closer to one another and dissimilar ones move apart. The result is a 3D scene which can be used to visualize patterns in the high-D information space.
133216	The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynamic queries . Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria.
133217	Panel on Corpus Linguistics and information retrieval An abstract is not available.
290947	Advantages of query biased summaries in information retrieval An abstract is not available.
290948	A theory of term weighting based on exploratory data analysis An abstract is not available.
290950	New techniques for open-vocabulary spoken document retrieval An abstract is not available.
290953	A study of retrospective and on-line event detection An abstract is not available.
290954	On-line new event detection and tracking An abstract is not available.
290956	Web document clustering: a feasibility demonstration An abstract is not available.
290957	The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval An abstract is not available.
290958	Resolving ambiguity for cross-language retrieval An abstract is not available.
290959	Cross-language information retrieval with the UMLS metathesaurus An abstract is not available.
290961	Using a generalized instance set for automatic text categorization An abstract is not available.
290965	Automatic essay grading using text categorization techniques An abstract is not available.
290967	The future of Internet search (keynote address) An abstract is not available.
290970	Distributional clustering of words for text classification An abstract is not available.
290972	Improved algorithms for topic distillation in a hyperlinked environment An abstract is not available.
290974	Effective retrieval with distributed collections An abstract is not available.
290976	Evaluating database selection techniques: a testbed and experiment An abstract is not available.
290978	The impact of query structure and query expansion on retrieval performance An abstract is not available.
290980	A flexible model for retrieval of SGML documents An abstract is not available.
290982	Discovering typical structures of documents: a road map approach An abstract is not available.
290984	A cognitive model for searching for III-defined targets on the Web: the relationship between search strategies and user satisfaction An abstract is not available.
290986	Comparing interactive information retrieval systems across sites: the TREC-6 interactive track matrix experiment An abstract is not available.
290987	Aspect windows, 3-D visualizations, and indirect comparisons of information retrieval systems An abstract is not available.
290989	Modeling and combining evidence provided by document relationships using probabilistic argumentation systems An abstract is not available.
290991	Predicting the performance of linearly combined IR systems An abstract is not available.
290992	Experiments in Japanese text retrieval and routing using the NEAT system An abstract is not available.
290995	Improving automatic query expansion An abstract is not available.
290996	Boosting and Rocchio applied to text filtering An abstract is not available.
290998	Learning while filtering documents An abstract is not available.
291000	Spatial querying for image retrieval: a user-oriented evaluation An abstract is not available.
291001	Extracting classification knowledge of Internet documents with mining term associations: a semantic approach An abstract is not available.
291003	Improving two-stage ad-hoc retrieval for short queries An abstract is not available.
291005	DOLORES: a system for logic-based retrieval of multimedia objects An abstract is not available.
291007	RELIEF: combining expressiveness and rapidity into a single system An abstract is not available.
291008	A language modeling approach to information retrieval An abstract is not available.
291009	Efficient construction of large test collections An abstract is not available.
291011	Compressed inverted files with reduced decoding overheads An abstract is not available.
291013	Fast searching on compressed text allowing errors An abstract is not available.
291014	How reliable are the results of large-scale information retrieval experiments? Two stages in measurement of techniques for information retrieval are gathering of documents for relevance assessment and use of the assessments to numerically evaluate effectiveness. We consider both of these stages in the context of the TREC experiments, to determine whether they lead to measurements that are trustworthy and fair. Our detailed empirical investigation of the TREC results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found. We propose a new pooling strategy that can significantly in- crease the number of relevant documents found for given effort, without compromising fairness.
291017	Variations in relevance judgments and the measurement of retrieval effectiveness An abstract is not available.
291019	Measures of relative relevance and ranked half-life: performance indicators for interactive IR An abstract is not available.
291021	Tools for searching the Web (panel) An abstract is not available.
291024	Modern classical document indexing: a linguistic contribution to knowledge-based IR An abstract is not available.
291025	The use of MMR, diversity-based reranking for reordering documents and producing summaries An abstract is not available.
291027	A method for scoring correlated features in query expansion An abstract is not available.
291028	Using maps as a user interface to a digital library An abstract is not available.
291030	Comparison between proximity operation and dependency operation in Japanese full-text retrieval An abstract is not available.
291031	Term-ordered query evaluation versus document-ordered query evaluation for large document databases An abstract is not available.
291033	Lessons from BMIR-J2: a test collection for Japanese IR systems An abstract is not available.
291037	Automatically locating, extracting and analyzing tabular data An abstract is not available.
291038	Using global colour features for general photographic image indexing and retrieval An abstract is not available.
291041	Automatic acquisition of phrasal knowledge for English-Chinese bilingual information retrieval An abstract is not available.
291043	Visual interactions with a multidimensional ranked list An abstract is not available.
291045	Predicting query times An abstract is not available.
291047	The WebCluster project. Using clustering for mediating access to the World Wide Web An abstract is not available.
291049	Automatic abstracting of magazine articles: the creation of 'Highlight' abstracts An abstract is not available.
291052	Optimizing recall/precision scores in IR over the WWW An abstract is not available.
291054	Interactive multidimensional document visualization An abstract is not available.
291055	Speech retrieval using phonemes with error correction An abstract is not available.
291057	Optimizing query evaluation in n-gram indexing An abstract is not available.
291059	Four text classification algorithms compared on a Dutch corpus An abstract is not available.
291060	Automatic acquisition of terminological relations from a corpus for query expansion An abstract is not available.
291062	Keyword extraction of radio news using term weighting with an encyclopedia and newspaper articles An abstract is not available.
291064	Efficient search server assignment in a disproportionate system environment An abstract is not available.
291066	Multilingual keyword extraction for term suggestion An abstract is not available.
291068	Experiments of collecting WWW information using distributed WWW robots An abstract is not available.
291070	Presenting Web site search results in context: a demonstration An abstract is not available.
291071	Towards a fast precision-oriented image retrieval system An abstract is not available.
291072	Cheshire II: combining probabilistic and Boolean retrieval An abstract is not available.
291073	Teraphim : an engine for distributed information retrieval An abstract is not available.
291074	Personal browser An abstract is not available.
291075	A research prototype image retrieval system An abstract is not available.
291076	The structured information manager (SIM) An abstract is not available.
291077	PWA: an extended probabilistic Web algebra An abstract is not available.
291078	Cafe: an indexed approach to searching genomic databases An abstract is not available.
291079	Fast speculative search engine on the highly parallel computer EM-X An abstract is not available.
345559	INSYDER — an information assistant for business intelligence The WWW is the most important resource for external business information. This paper presents a tool called INSYDER, an information assistant for finding and analysis business information from the WWW. INSYDER is a system using different agents for crawling the Web, evaluating and visualising the results. These agents, the used visualisations, and a first summary of user studies are presented.
345562	Structured translation for cross-language information retrieval The paper introduces a query translation model that reflects the structure of the cross-language information retrieval task. The model is based on a structured bilingual dictionary in which the translations of each term are clustered into groups with distinct meanings. Query translation is modeled as a two-stage process, with the system first determining the intended meaning of a query term and then selecting translations appropriate to that meaning that might appear in the document collection. An implementation of structured translation based on automatic dictionary clustering is described and evaluated by using Chinese queries to retrieve English documents. Structured translation achieved an average precision that was statistically indistinguishable from Pirkola's technique for very short queries, but Pirkola's technique outperformed structured translation on long queries. The paper concludes with some observations on future work to improve retrieval effectiveness and on other potential uses of structured translation in interactive cross-language retrieval applications.
345563	Automatic adaptation of proper noun dictionaries through cooperation of machine learning and probabilistic methods The recognition of Proper Nouns (PNs) is considered an important task in the area of Information Retrieval and Extraction. However the high performance of most existing PN classifiers heavily depends upon the availability of large dictionaries of domain-specific Proper Nouns, and a certain amount of manual work for rule writing or manual tagging. Though it is not a heavy requirement to rely on some existing PN dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating. In this paper we propose a technique for the automatic updating of an PN Dictionary through the cooperation of an inductive and a probabilistic classifier. In our experiments we show that, whenever an existing PN Dictionary allows the identification of 50% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90% of the remaining 50%.
345564	Document centered approach to text normalization In this paper we present an approach to tackle three important problems of text normalization: sentence boundary disambiguation, disambiguation of capitalized words when they are used in positions where capitalization is expected, and identification of abbreviations. The main feature of our approach is that it uses a minimum of pre-built resources, instead dynamically inferring disambiguation clues from the entire document itself. This makes it domain independent, closely targeted to each individual document and portable to other languages. We thoroughly evaluated this approach on several corpora and it showed high accuracy.
345565	OCELOT: a system for summarizing Web pages We introduce OCELOT, a prototype system for automatically generating the “gist” of a web page by summarizing it. Although most text summarization research to date has focused on the task of news articles, web pages are quite different in both structure and content. Instead of coherent text with a well-defined discourse structure, they are more often likely to be a chaotic jumble of phrases, links, graphics and formatting commands. Such text provides little foothold for extractive summarization techniques, which attempt to generate a summary of a document by excerpting a contiguous, coherent span of text from it. This paper builds upon recent work in non-extractive summarization, producing the gist of a web page by “translating” it into a more concise representation rather than attempting to extract a text span verbatim. OCELOT uses probabilistic models to guide it in selecting and ordering words into a gist. This paper describes a technique for learning these models automatically from a collection of human-summarized web pages.
345566	Extracting sentence segments for text summarization: a machine learning approach With the proliferation of the Internet and the huge amount of data it transfers, text summarization is becoming more important. We present an approach to the design of an automatic text summarizer that generates a summary by extracting sentence segments. First, sentences are broken into segments by special cue markers. Each segment is represented by a set of predefined features (e.g. location of the segment, average term frequencies of the words occurring in the segment, number of title words in the segment, and the like). Then a supervised learning algorithm is used to train the summarizer to extract important sentence segments, based on the feature vector. Results of experiments on U.S. patents indicate that the performance of the proposed approach compares very favorably with other approaches (including Microsoft Word summarizer) in terms of precision, recall, and classification accuracy.
345569	An experimental comparison of naive Bayesian and keyword-based anti-spam filtering with personal e-mail messages The growing problem of unsolicited bulk e-mail, also known as “spam”, has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in “encrypted” form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.
345572	Text filtering by boosting naive Bayes classifiers Several machine learning algorithms have recently been used for text categorization and filtering. In particular, boosting methods such as AdaBoost have shown good performance applied to real text data. However, most of existing boosting algorithms are based on classifiers that use binary-valued features. Thus, they do not fully make use of the weight information provided by standard term weighting methods. In this paper, we present a boosting-based learning method for text filtering that uses naive Bayes classifiers as a weak learner. The use of naive Bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio. Applied to TREC-7 and TREC-8 filtering track documents, the proposed method obtained a significant improvement in LF1, LF2, F1 and F3 measures compared to the best results submitted by other TREC entries.
345573	Document filtering method using non-relevant information profile Document filtering is a task to retrieve documents relevant to a user's profile from a flow of documents. Generally, filtering systems calculate the similarity between the profile and each incoming document, and retrieve documents with similarity higher than a threshold. However, many systems set a relatively high threshold to reduce retrieval of non-relevant documents, which results in the ignorance of many relevant documents. In this paper, we propose the use of a non-relevant information profile to reduce the mistaken retrieval of non-relevant documents. Results from experiments show that this filter has successfully rejected a sufficient number of non-relevant documents, resulting in an improvement of filtering performance.
345574	Question-answering by predictive annotation We present a new technique for question answering called Predictive Annotation. Predictive Annotation identifies potential answers to questions in text, annotates them accordingly and indexes them. This technique, along with a complementary analysis of questions, passage-level ranking and answer selection, produces a system effective at answering natural-language fact-seeking questions posed against large document collections. Experimental results show the effects of different parameter settings and lead to a number of general observations about the question-answering problem.
345576	Bridging the lexical chasm: statistical approaches to answer-finding This paper investigates whether a machine can automatically learn the task of finding, within a large collection of candidate responses, the answers to questions. The learning process consists of inspecting a collection of answered questions and characterizing the relation between question and answer with a statistical model. For the purpose of learning this relation, we propose two sources of data: Usenet FAQ documents and customer service call-center dialogues from a large retail company. We will show that the task of “answer-finding” differs from both document retrieval and tradition question-answering, presenting challenges different from those found in these problems. The central aim of this work is to discover, through theoretical and empirical investigation, those statistical techniques best suited to the answer-finding problem.
345577	Building a question answering test collection The TREC-8 Question Answering (QA) Track was the first large-scale evaluation of domain-independent question answering systems. In addition to fostering research on the QA task, the track was used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task. As with document relevance judging, assessors had legitimate differences of opinions as to whether a response actually answers a question, but comparative evaluation of QA systems was stable despite these differences. Creating a reusable QA test collection is fundamentally more difficult than creating a document retrieval test collection since the QA task has no equivalent to document identifiers.
345578	Document clustering using word clusters via the information bottleneck method We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering. Given a joint empirical distribution of words and documents, p ( x , y ), we first cluster the words, Y , so that the obtained word clusters, Ytilde;, maximally preserve the information on the documents. The resulting joint distribution. p ( X , Ytilde; ), contains most of the original information about the documents, I ( X ; Ytilde; ) &ap; I ( X ; Y ), but it is much less sparse and noisy. Using the same procedure we then cluster the documents, X , so that the information about the word-clusters is preserved. Thus, we first find word-clusters that capture most of the mutual information about to set of documents, and then find document clusters , that preserve the information about the word clusters. We tested this procedure over several document collections based on subsets taken from the standard 20 Newsgroups corpus. The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents. Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms. Moreover, the double clustering procedure improves all the distributional clustering methods examined here.
345579	Latent semantic space: iterative scaling improves precision of inter-document similarity measurement We present a novel algorithm that creates document vectors with reduced dimensionality. This work was motivated by an application characterizing relationships among documents in a collection. Our algorithm yielded inter-document similarities with an average precision up to 17.8% higher than that of singular value decomposition (SVD) used for Latent Semantic Indexing. The best performance was achieved with dimensional reduction rates that were 43% higher than SVD on average. Our algorithm creates basis vectors for a reduced space by iteratively “scaling” vectors and computing eigenvectors. Unlike SVD, it breaks the symmetry of documents and terms to capture information more evenly across documents. We also discuss correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log-likelihood estimation.
345582	An investigation of linguistic features and clustering algorithms for topical document clustering We investigate four hierarchical clustering methods (single-link, complete-link, groupwise-average, and single-pass) and two linguistically motivated text features (noun phrase heads and proper names) in the context of document clustering. A statistical model for combining similarity information from multiple sources is described and applied to DARPA's Topic Detection and Tracking phase 2 (TDT2) data. This model, based on log-linear regression, alleviates the need for extensive search in order to determine optimal weights for combining input features. Through an extensive series of experiments with more than 40,000 documents from multiple news sources and modalities, we establish that both the choice of clustering algorithm and the introduction of the additional features have an impact on clustering performance. We apply our optimal combination of features to the TDT2 test data, obtaining partitions of the documents that compare favorably with the results obtained by participants in the official TDT2 competition.
345584	The impact of database selection on distributed searching The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts — database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step.
345587	Hill climbing algorithms for content-based retrieval of similar configurations The retrieval of stored images matching an input configuration is an important form of content-based retrieval. Exhaustive processing (i.e., retrieval of the best solutions) of configuration similarity queries is, in general, exponential and fast search for sub-optimal solutions is the only way to deal with the vast (and ever increasing) amounts of multimedia information in several real-time applications. In this paper we discuss the utilization of hill climbing heuristics that can provide very good results within limited processing time. We propose several heuristics, which differ on the way that they search through the solution space, and identify the best ones depending on the query and image characteristics. Finally we develop new algorithms that take advantage of the specific structure of the problem to improve performance.
345591	Partial collection replication versus caching for information retrieval systems The explosion of content in distributed information retrieval (IR) systems requires new mechanisms to attain timely and accurate retrieval of unstructured text. In this paper, we compare two mechanisms to improve IR system performance: partial collection replication and caching. When queries have locality, both mechanisms return results more quickly than sending queries to the original collection(s). Caches return results when queries exactly match a previous one. Partial replicas are a form of caching that return results when the IR technology determines the query is a good match. Caches are simpler and faster, but replicas can increase locality by detecting similarity between queries that are not exactly the same. We use real traces from THOMAS and Excite to measure query locality and similarity. With a very restrictive definition of query similarity, similarity improves query locality up to 15% over exact match. We use a validated simulator to compare their performance, and find that even if the partial replica hit rate increases only 3 to 6%, it will outperform simple caching under a variety of configurations. A combined approach will probably yield the best performance.
345593	Hierarchical classification of Web content This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level. We use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.
345594	A practical hypertext catergorization method using links and incrementally available class information As WWW grows at an increasing speed, a classifier targeted at hypertext has become in high demand. While document categorization is quite a mature, the issue of utilizing hypertext structure and hyperlinks has been relatively unexplored. In this paper, we propose a practical method for enhancing both the speed and the quality of hypertext categorization using hyperlinks. In comparison against a recently proposed technique that appears to be the only one of the kind, we obtained up to 18.5% of improvement in effectiveness while reducing the processing time dramatically. We attempt to explain through experiments what factors contribute to the improvement.
345597	Topical locality in the Web Most web pages are linked to others with related content . This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point , is the foundation for a usable World-Wide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.
345598	Interactive Internet search: keyword, directory and query reformulation mechanisms compared This article compares search effectiveness when using query-based Internet search (via the Google search engine), directory-based search (via Yahoo) and phrase-based query reformulation assisted search (via the Hyperindex browser) by means of a controlled, user-based experimental study. The focus was to evaluate aspects of the search process. Cognitive load was measured using a secondary digit-monitoring task to quantify the effort of the user in various search states; independent relevance judgements were employed to gauge the quality of the documents accessed during the search process. Time was monitored in various search states. Results indicated the directory-based search does not offer increased relevance over the query-based search (with or without query formulation assistance), and also takes longer. Query reformulation does significantly improve the relevance of the documents through which the user must trawl versus standard query-based internet search. However, the improvement in document relevance comes at the cost of increased search time and increased cognitive load.
345602	Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web Most information retrieval systems on the Internet rely primarily on similarity ranking algorithms based solely on term frequency statistics. Information quality is usually ignored. This leads to the problem that documents are retrieved without regard to their quality. We present an approach that combines similarity-based similarity ranking with quality ranking in centralized and distributed search environments. Six quality metrics, including the currency , availability , information-to-noise ratio , authority , popularity , and cohesiveness , were investigated. Search effectiveness was significantly improved when the currency, availability, information-to-noise ratio and page cohesiveness metrics were incorporated in centralized search. The improvement seen when the availability, information-to- noise ratio, popularity, and cohesiveness metrics were incorporated in site selection was also significant. Finally, incorporating the popularity metric in information fusion resulted in a significant improvement. In summary, the results show that incorporating quality metrics can generally improve search effectiveness in both centralized and distributed search environments.
345603	Does “authority” mean quality? predicting expert quality ratings of Web documents For many topics, the World Wide Web contains hundreds or thousands of relevant documents of widely varying quality. Users face a daunting challenge in identifying a small subset of documents worthy of their attention. Link analysis algorithms have received much interest recently, in large part for their potential to identify high quality items. We report here on an experimental evaluation of this potential. We evaluated a number of link and content-based algorithms using a dataset of web documents rated for quality by human topic experts. Link-based metrics did a good job of picking out high-quality items. Precision at 5 is about 0.75, and precision at 10 is about 0.55; this is in a dataset where 0.32 of all documents were of high quality. Surprisingly, a simple content-based metric performed nearly as well; ranking documents by the total number of pages on their containing site.
345608	Document classification on neural networks using only positive examples (poster session) In this paper, we show how a simple feed-forward neural network can be trained to filter documents when only positive information is available, and that this method seems to be superior to more standard methods, such as tf-idf retrieval based on an “average vector”. A novel experimental finding that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation (“Hadamard”) of the information prior to the training of the network.
345610	New paradigms in information visualization (poster session) We present three new visualization front-ends that aid navigation through the set of documents returned by a search engine (hit documents). We cluster the hit documents to visually group these documents and label the groups with related words. The different front-ends cater for different user needs, but all can browse cluster information as well as drilling up or down in one or more clusters and refining the search using one or more of the suggested related keywords.
345612	Latent semantic indexing model for Boolean query formulation (poster session) A new model named Boolean Latent Semantic Indexing model based on the Singular Value Decomposition and Boolean query formulation is introduced. While the Singular Value Decomposition alleviates the problems of lexical matching in the traditional information retrieval model, Boolean query formulation can help users to make precise representation of their information search needs. Retrieval experiments on a number of test collections seem to show that the proposed model achieves substantial performance gains over the Latent Semantic Indexing model.
345615	Generation of user profiles for information filtering — research agenda (poster session) In information filtering (IF) systems, user long-term needs we expressed as user profiles. The quality of a user profile has a major impact on the performance of IF systems. The focus of the proposed research is on the study of user profile generation and update. The paper introduces methods for user profile generation, and proposes a research agenda for their comparison and evaluation.
345618	Variance based classifier comparison in text catergorization (poster session) Text categorization is one of the key functions for utilizing vast amount of documents. It can be seen as a classification problem, which has been studied in pattern recognition and machine learning fields for a long time and several classification methods have been developed such as statistical classification, decision tree, support vector machines and so on. Many researchers applied those classification methods to text categorization and reported their performance (e.g., decision tree[3], Bayes classifier[2], support vector machine[l]). Yang conducted comprehensive study of comparison or text categorization and reported that k nearest neighbor and support vector machines works well for text categorization[4]. In the previous studies, classification methods were usually compared using single pair of training and test data However, classification method with more complex family of classifiers requires more training data and small training data may result in deriving unreliable classifier, that is, the performance of the derived classifier varies much depending on training data. Therefore, we need to take the size of training data into account when comparing and selecting a classification method. In this paper, we discuss how to select a classifier from those derived by various classification methods and how the size of training data affects the performance of the derived classifier. In order to evaluate the reliability of classification method, we consider the variance of accuracy of derived classifier. We first construct a statistical model. In the text categorization, each document is usually represented with a feature vector that consists of weighted frequencies of terms. In the vector space model, document is a point in high dimensional feature space and a classifier separates the feature space into subspaces each of which is labeled with a category.
345621	The use of phrases from query texts in information retrieval (poster session) An abstract is not available.
345622	Pseudo-frequency method (poster session): an efficient document ranking retrieval method for n-gram indexing Although n-gram ( n successive characters) indexing is widely used in retrieval systems for documents in Japanese and other Asian languages, it is difficult to process ranking retrieval efficiently using n-gram indexing. This is because frequency information for query words needs to be computed using indexed data since this information is not directly available from the n-gram index. To reduce processing costs, this paper proposes a pseudo-frequency method, which uses a word's estimated frequencies instead of precise ones. The results of experiments on NTCIR, a Japanese IR test collection, showed that the proposed method speeded up retrieval without degrading retrieval effectiveness.
345623	Lexical semantic relatedness and online new event detection (poster session) An abstract is not available.
345624	Modeling question-response patterns by scaling and visualization (poster session) The evaluation of question difficulty is usually considered the domain of Latent Trait Theory. However, these methods require standardized question sets normalized by large populations, rendering them inefficient for use in the numerous areas where questions must be evaluated. A new technique is illustrated that models the question-response cycle well, but without the procedural difficulty of the traditional methods.
345625	The effect of query type on subject searching behavior of image databases (poster session): an exploratory study An abstract is not available.
345628	The role of a judge in a user based retrieval experiment (poster session) An abstract is not available.
345630	Auto-construction of a live thesaurus from search term logs for interactive Web search (poster session) The purpose of this paper is to present an on-going research that is intended to construct a live thesaurus directly from search term logs of real-world search engines. Such a thesaurus designed can contain representative search terms, their frequency in use, the corresponding subject categories, the associated and relevant terms, and the hot visiting Web sites/pages the search terms may reach.
345632	Cognitive approach for building user model in an information retrieval context (poster session) The recent development of communication networks and multimedia system provides users with the availability of a huge amount of information making worse the problem of information overload [9]. The evolution of system design is necessary becoming more user centred, and more personally involving. A review of survey studies on Internet users since 1993 confirms that a greater percentage of people are becoming online citizens, and professionals are integrating more online components into their work process. A review of the experimental literature on Internet user's reveals that there is intense interest in humanising the online environment by integrating affective and cognitive components [8]. We are specially in concern with the effects on the evolution on information retrieval. We can notice significant changes in the information retrieval world over the past five or so years due to the emergence of Internet and one of its most important and widely used services, the world wide Web (WWW) or simply the Web. While reviewing the progress of research in information retrieval and user modelling, we can observe that many systems and prtotypes are created [5], [10], [11], but all of them, share some basic limitations: the techniques used to represent knowledge in the user model is based on simple list of keyword, the type of the considered knowledge is very limited, usually restricted to single word, or to (some) structural characteristic: the learning capability are very poor. We aim to propose a cognitive approach for building user model in an information retrieval context. In fact cognitive approach is based on identifying how users process information and what constitutes an appropriate model to represent this process, and because IR, under the cognitive paradigm takes the user into account in a high-priority way [1]. However, within the cognitive paradigm, there is no general model valid for our documentary approach that satisfactorily how user knowledge is represented for the purpose of processing information. The lack of such a model does not allow one to identify a user's cognitive state with regard to his or her information needs and requirements. Methodology adopting the cognitive viewpoint in IR are Synthesised by Daniel [4] in three groups, which comprise the representation of: users and their problem, which stems from the hypothesis proposed by Belkin on the `anomalous states of knowledge' (ASK), according to which the user searches for information search strategy, which compile the different ways search strategies and processes are carried out, depending on the variable involved - user, intermediary, IR systems [6],[7] document and information, which is considered a major goal of current IR research, since it embraces the whole corpus of studies about user models intended to eliminate the intermediary's role in retrieval system. The aim of this approach is to allow users direct access to the system by means of the representation of documents and intelligent interfaces. User-centered paradigm now dominates in studies of information needs and information retrieval. We have the goal of developing new approaches to information retrieval which are based on user modelling techniques for building and managing the representation of the user preferances. In this paper, we describe two complementary approaches which are necessary for building user model and its integration in an information retrieval system: a conceptual one based on the description of knowledge needed by the user in an information retrieval context, a functional approach which deals with dynamic aspects of the model. Within this approach we aim to determine the role played by the model in an information retrieval context. In many studies of IR interesting in user modelling we find different kind of knowledge trying to describe user's need. So our conceptual approach has consisted in enumerating these knowledge and integrating them in their adequate components in information retrieval architecture. Almost of these studies that identified cognitive characteristics have used quantitative methods to measure them. What is needed is a qualitative study and appropriate method to ascertain these cognitive characteristic [12]. Our main objective is the development of techniques for modelling the user as an interactive part of IR, so we propose our functional approach which deals with identifying cognitive characteristic within the role played by the user model in an information retrieval architecture. So we began by presenting the conceptual approach and so the functional one.
345636	Multimedia information retrieval from recorded presentations (poster session) In presentation recording special effort is usually put into the automation of the production process, that is in automatically creating high quality data files without much or any need for manual recording and post-editing [5]. With the advent of such systems and their usage in classroom teaching, at conferences, etc., there is an increasing need for techniques and abilities which enable users to search in those documents and to localize some specific information. In this paper we describe how we integrated information retrieval techniques into the Authoring on the Fly (AOF) system, an approach for automatic presentation recording. We have chosen the AOF system for two reasons. On the one hand, it is a well-established way for presentation recording, used by various universities and institutions 1 . On the other hand it is general enough to illustrate typical problems and challenges a developer is facing when designing a system for information retrieval from multimedia data streams which occur in the presentation recording scenario.
345638	Influence of speech recognition errors on topic detection (poster session) We investigate the effect of speech-recognition errors on a system for the unsupervised, nearly synchronous clustering of broadcast news stories, using the TDT (Topic Detection and Tracking) Corpora. Two questions are addressed: (1) Are speech recognition errors detrimental to the performance of the system? (2) Can a background collection of contemporaneous clean text improve performance? We investigate both the large-cluster and small-cluster limits.
345641	Word document density and relevance scoring (poster session) Previous work addressing the issue of word distribution in documents has shown the importance of Word repetitiveness as an indicator of the word content-bearing characteristics. In this paper we propose a simple method using a measure of the tendency of words to repeat within a document to separate the words with similar document frequencies, but different topic discriminating characteristics. We describe the application of the new measure in query-document relevance scoring. Experiments on the TREC Ad Hoc and Spoken Document Retrieval tasks [7] show useful performance improvements.
345643	Ranking digital images using combination of evidences (poster session) An abstract is not available.
345646	Collaborative filtering and the generalized vector space model (poster session) Collaborative filtering is a technique for recommending documents to users based on how similar their tastes are to other users. If two users tend to agree on what they like, the system will recommend the same documents to them. The generalized vector space model of information retrieval represents a document by a vector of its similarities to all other documents. The process of collaborative filtering is nearly identical to the process of retrieval using GVSM in a matrix of user ratings. Using this observation, a model for filtering collaboratively using document content is possible.
345648	Theme-based retrieval of Web news (poster session) We present our framework for classification of Web news, based on support vector machines, and some of the initial measurements of its accuracy.
345650	Stemming and its effects on TFIDF ranking (poster session) An abstract is not available.
345652	Exploration of a heuristic approach to threshold learning in adaptive filtering (poster session) In this paper we examine the learning behavior of a heuristic threshold setting approach to information filtering. In particular, we study how different initial threshold settings and different updating parameter settings affect threshold learning. The results on one of the TREC news databases indicate that (1) learning allows recovery from the inevitable non-optimality of the initial conditions, and (2) a greater “willingness to learn” (expressed by a deliberate lowering of the score threshold in the learning stage) does eventually lead to a higher performance in spite of the expected initial performance penalty.
345656	On the design and evaluation of a multi-dimensional approach to information retrieval (poster session) We present a method of searching text collections that takes advantage of hierarchrical information within documents and integrates searches of structured and unstructured data. We show that Multidimensional databases (MDB), designed for accessing data along hierarchical dimensions, are effective for information retrieval. We demonstrate a method of using On-Line Analytic Processing (OLAP) techniques on a text collection. This combines traditional information retrieval and the slicing, dicing, drill-down, and roll-up of OLAP. We demonstrate use of a prototype for searching documents from the TREC collection.
345658	SWAMI (poster session): a framework for collaborative filtering algorithm development and evaluation We present a Java-based framework, SWAMI (Shared Wisdom through the Amalgamation of Many Interpretations) for building and studying collaborative filtering systems. SWAMI consists of three components: a prediction engine, an evaluation system, and a visualization component. The prediction engine provides a common interface for implementing different prediction algorithms. The evaluation system provides a standardized testing methodology and metrics for analyzing the accuracy and run-time performance of prediction algorithms. The visualization component suggests how graphical representations can inform the development and analysis of prediction algorithms. We demonstrate SWAMI on the Each Movie data set by comparing three prediction algorithms: a traditional Pearson correlation-based method, support vector machines, and a new accurate and scalable correlation-based method based on clustering techniques.
345660	Learning probabilistic models of the Web (poster session) In the World Wide Web, myriads of hyperlinks connect documents and pages to create an unprecedented, highly complex graph structure - the Web graph. This paper presents a novel approach to learning probabilistic models of the Web, which can be used to make reliable predictions about connectivity and information content of Web documents. The proposed method is a probabilistic dimension reduction technique which recasts and unites Latent Semantic Analysis and Kleinberg's Hubs-and-Authorities algorithm in a statistical setting. This meant to be a first step towards the development of a statistical foundation for Web—related information technologies. Although this paper does not focus on a particular application, a variety of algorithms operating in the Web/Internet environment can take advantage of the presented techniques, including search engines, Web crawlers, and information agent systems.
345661	Effects of out of vocabulary words in spoken document retrieval (poster session) The effects of out-of-vocabulary (OOV) items in spoken document retrieval (SDR) are investigated. Several sets of transcriptions were created for the TREC-8 SDR task using a speech recognition system varying the vocabulary sizes and OOV rates, and the relative retrieval performance measured. The effects of OOV terms on a simple baseline IR system and on more sophisticated retrieval systems are described. The use of a parallel corpus for query and document expansion is found to be especially beneficial, and with this data set, good retrieval performance can be achieved even for fairly high OOV rates.
345663	Towards an adaptive and task-specific ranking mechanism in Web searching (poster session) An abstract is not available.
345664	Beyond the traditional query operators (poster session) An abstract is not available.
345665	Bayes optimal metasearch: a probabilistic model for combining the results of multiple retrieval systems (poster session) We introduce a new, probabilistic model for combining the outputs of an arbitrary number of query retrieval systems. By gathering simple statistics on the average performance of a given set of query retrieval systems, we construct a Bayes optimal mechanism for combining the outputs of these systems. Our construction yields a metasearch strategy whose empirical performance nearly always exceeds the performance of any of the constituent systems. Our construction is also robust in the sense that if “good” and “bad” systems are combined, the Performance of the composite is still on par with, or exceeds, that of the best constituent system. Finally, our model and theory provide theoretical and empirical avenues for the improvement of this metasearch strategy.
345666	Information access for context-aware appliances (poster session) The emergence of networked context-aware mobile computing appliances potentially offers opportunities for remote access to huge online information resources. Information access in context-aware information appliances can utilize existing techniques developed for effective information retrieval and information filtering; however, practical physical and operational features of these devices and the availability of context information itself suggest that the document selection process should make use of this contextual data.
345667	Finding relevant passages using noun-noun compounds (poster session): coherence vs. proximity Intuitively, words forming phrases are a more precise description of content than words as a sequence of keywords. Yet, evidence that phrases would be more effective for information retrieval is inconclusive. This paper isolates a neglected class of phrases, that is abundant in communication, has an established theoretical foundation, and shows promise for an effective expression of the user's information need: the noun-noun compound ( NNC ). In an experiment, a variety of meaningful NNC s were used to isolate relevant passages in a large and varied corpus. In a first pass, passages were retrieved based on textual proximity of the words or their semantic peers. A second pass retained only passages containing a syntactically coherent structure equivalent to the original NNC . This second pass showed a dramatic increase in precision. Preliminary results show the validity of our intuition about phrases in the special but very productive case of NNC s.
345668	Semantic Explorer — navigation in documents collections; Proxima Daily — learning personal newspaper (demonstration session) An abstract is not available.
345670	Integrated search tools for newspaper digital libraries (demonstration session) An abstract is not available.
345671	Managing photos with AT&T Shoebox (demonstration session) An abstract is not available.
345672	ClusterBook, a tool for dual information access (demonstration session) An abstract is not available.
345673	Uexküll (demonstration session): an interactive visual user interface for document retrieval in vector space An abstract is not available.
345674	TimeMine (demonstration session): visualizing automatically constructed timelines An abstract is not available.
345675	The Cambridge University Multimedia Document Retrieval demo system (demonstration session) An abstract is not available.
344658	Salton Award lecture: on theoretical argument in information retrieval (summary only): on theoretical argument in information retrieval The last winner of the Salton Award, Tefko Saracevic, gave an acceptance address at SIGIR in Philadelphia in 1997. Previous winners were William Cooper (1994), Cyril Cleverdon (1991), Karen Sparck Jones (1988) and Gerard Salton himself (1985). In this talk, I plan to follow the tradition of acceptance addresses, and present a personal view of and retrospective on some of the areas in which I work. However, I will not be saying much about what are perhaps the two most obvious parts of my work: the probabilistic approach to retrieval and evaluation of retrieval systems. Rather I will attempt to get under the skin of my take on IR, by discussing the nature of theoretical argument in the field, partly through examples. This talk is about the place of theory in the study of information retrieval (in some sense following Bill Cooper's 1994 topic), but not so much Theory with a capital T — rather what might be described as small-t theory. The field has a very strong pragmatic orientation, reflected both in the attitudes of the commercial participants and in the emphasis on formal evaluation in the academic environment. Nevertheless, there are many theoretical ideas buried in, or implied by, the ways we talk about the field — the language we use to discuss it. I will be discussing two areas to illustrate these low-level theoretical ideas: precision devices, and the apparent symmetry between retrieval and filtering. The phrase `precision device' used to have a rather clear meaning in IR, in the days of set-based retrieval systems. In that context, a precision device was a device to enable the restriction of the retrieved set to those most likely (out of the documents originally included) to be useful. These days, with the ubiquitous scoring and ranking methods largely replacing set-based retrieval, the idea has lost its meaning It is worth exploring the formal relationships involved to understand the change a little better. My second area is to do with the relation between filtering and the more traditional type of adhoc information retrieval. There is a tendency and a temptation to see these as the same kind of thing, sometimes with a more specific assumption of duality, based on the inversion of the roles of documents and queries. It is important to see how far this parallel extends, and where it breaks down. I explore the nature of the duality and the kinds of reasons why it does break down. These examples reflect my interest in the basic logical structure of information retrieval systems and the situations in which such systems may be found. I argue for a certain level of logical argument in information retrieval, which might be taken as small-t theory, though not as capital-T Theory. I believe there are reasons to think a Grand Theory of IR to be an unattainable goal — such a theory would have to encompass so many different aspects of retrieval, having to do for example with human cognition and behaviour and the structure of knowledge, as well as with the statistical concepts that inform the probabilistic approach. However, accepting the unattainability of a Grand Theory does not preclude the development of further and more useful models based on particular aspects and lower-level logic The low-level logic is important not only in its own right, but as the basis for linking together more sophisticated theories concerned with more restricted domains. The most elaborate and complete theory of (say) user behaviour is of no use at all without a strong linkage between the parts of that theory and the entities relevant to IR that fall outside its scope. The glue that provides that linkage has to be low-level logic.
345512	Relevance and contributing information types of searched documents in task performance End-users base the relevance judgements of the searched documents on the expected contribution to their task of the information contained in the documents. There is a shortage of studies analyzing the relationships between the experienced contribution, relevance assessments and type of information initially sought. This study categorizes the types of information in documents being used in writing a research proposal for a master's thesis by eleven students throughout the various stages of the proposal writing process. The role of the specificity of the searched information in influencing its contribution is analyzed. The results demonstrate that different types of information are sought at different stages of the writing process and thus the contribution of the information also differs at the different stages. The categories of the contributing information can be understood of topicality.
345538	Relevance feedback with a small number of relevance judgements: incremental relevance feedback vs. document clustering The use of incremental relevance feedback and document clustering were investigated in an relevance feedback environment in which the number of relevance judgements was quite small. Through experiments on the TREC collection, the incremental relevance feedback approach was found not to improve the overall search effectiveness. The clustering approach was found to be promising, although it sometimes over-focuses on a particular topic in a query and ignores the others. To overcome this problem, a query-biased clustering algorithm was developed and shown to be effective.
345539	Do batch and user evaluations give the same results? Do improvements in system performance demonstrated by batch evaluations confer the same benefit for real users? We carried out experiments designed to investigate this question. After identifying a weighting scheme that gave maximum improvement over the baseline in a non-interactive evaluation, we used it with real users searching on an instance recall task. Our results showed the weighting scheme giving beneficial results in batch studies did not do so with real users. Further analysis did identify other factors predictive of instance recall, including number of documents saved by the user, document recall, and number of documents seen by the user.
345541	A novel method for the evaluation of Boolean query effectiveness across a wide operational range Traditional methods for the system-oriented evaluation of Boolean IR system suffer from validity and reliability problems. Laboratory-based research neglects the searcher and studies suboptimal queries. Research on operational systems fails to make a distinction between searcher performance and system performance. This approach is neither capable of measuring performance at standard points of operation (e.g. across R0.0-R1.0). A new laboratory-based evaluation method for Boolean IR systems is proposed. It is based on a controlled formulation of inclusive query plans, on an automatic conversion of query plans into elementary queries, and on combining elementary queries into optimal queries at standard points of operation. Major results of a large case experiment are reported. The validity, reliability, and efficiency of the method are considered in the light of empirical and analytical test data.
345543	Evaluating evaluation measure stability This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.
345545	IR evaluation methods for retrieving highly relevant documents This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query 1 ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.
345546	Automatic generation of overview timelines We present a statistical model of feature occurrence over time, and develop tests based on classical hypothesis testing for significance of term appearance on a given date. Using additional classical hypothesis testing we are able to combine these terms to generate “topics” as defined by the Topic Detection and Tracking study. The groupings of terms obtained can be used to automatically generate an interactive timeline displaying the major events and topics covered by the corpus. To test the validity of our technique we extracted a large number of these topics from a test corpus and had human evaluators judge how well the selected features captured the gist of the topics, and how they overlapped with a set of known topics from the corpus. The resulting topics were highly rated by evaluators who compared them to known topics.
345548	Event tracking based on domain dependency This paper proposes a method for event tracking on broadcast news stories based on distinction between a topic and an event. A topic and an event are identified using a simple criterion called domain dependency of words: how greatly a word features a given set of data. The method was tested on the TDT corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the usefulness of the method.
345550	Improving text categorization methods for event tracking Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.
345551	Evaluation of a simple and effective music information retrieval method We developed, and then evaluated, a music information retrieval (MIR) system based upon the intervals found within the melodies of a collection of 9354 folksongs. The songs were converted to an interval-only representation of monophonic melodies and then fragmented t into length-n subsections called n-grams. The length of these n-grams and the degree to which we precisely represent the intervals are variables analyzed in this paper. We constructed a collection of “musical word” databases using the text-based, SMART information retrieval system. A group of simulated queries, some of which contained simulated errors, was run against these databases. The results were evaluated using the normalized precision and normalized recall measures. Our concept of “musical words” shows great merit thus implying that useful MIR systems can be constructed simply and efficiently using pre-existing text-based information retrieval software. Second, this study is a formal and comprehensive evaluation of a MIR system using rigorous statistical analyses to determine retrieval effectiveness.
345552	Phonetic confusion matrix based spoken document retrieval Combined word-based index and phonetic indexes have been used to improve the performance of spoken document retrieval systems primarily by addressing the out-of-vocabulary retrieval problem. However, a known problem with phonetic recognition is its limited accuracy in comparison with word level recognition. We propose a novel method for phonetic retrieval in the CueVideo system based on the probabilistic formulation of term weighting using phone confusion data in a Bayesian framework. We evaluate this method of spoken document retrieval against word-based retrieval for the search levels identified in a realistic video-based distributed learning setting. Using our test data, we achieved an average recall of 0.88 with an average precision of 0.69 for retrieval of out-of-vocabulary words on phonetic transcripts with 35% word error rate. For in-vocabulary words, we achieved a 17% improvement in recall over word-based retrieval with a 17% loss in precision for word error rites ranging from 35 to 65%.
345553	Multiple evidence combination in image retrieval: Diogenes searches for people on the Web In this work, we examine evidence combination mechanisms for classifying multimedia information. In particular, we examine linear and Dempster-Shafer methods of evidence combination in the context of identifying personal images on the World Wide Web. An automatic web search engine named Diogenes 1 searches the web for personal images and combines different pieces of evidence for identification. The sources of evidence consist of input from face detection/recognition and text/HTML analysis modules. A degree of uncertainty is involved with both of these sources. Diogenes automatically determines the uncertainty locally for each retrieval and uses this information to set a relative significance for each evidence. To our knowledge, Diogenes is the first image search engine using Dempster-Shafer evidence combination based on automatic object recognition and dynamic local uncertainty assessment. In our experiments Diogenes comfortably outperformed some well known commercial and research prototype image search engines for celebrity image queries.
345554	Link-based and content-based evidential information in a belief network model This work presents an information retrieval model developed to deal with hyperlinked environments. The model is based on belief networks and provides a framework for combining information extracted from the content of the documents with information derived from cross-references among the documents. The information extracted from the content of the documents is based on statistics regarding the keywords in the collection and is one of the basis for traditional information retrieval (IR) ranking algorithms. The information derived from cross-references among the documents is based on link references in a hyperlinked environment and has received increased attention lately due to the success of the Web. We discuss a set of strategies for combining these two types of sources of evidential information and experiment with them using a reference collection extracted from the Web. The results show that this type of combination can improve the retrieval performance without requiring any extra information from the users at query time. In our experiments, the improvements reach up to 59% in terms of average precision figures.
345556	The feature quantity: an information theoretic perspective of Tfidf-like measures The feature quantity , a quantitative representation of specificity introduced in this paper, is based on an information theoretic perspective of co-occurrence events between terms and documents. Mathematically, the feature quantity is defined as a product of probability and information, and maintains a good correspondence with the tfidf -like measures popularly used in today's IR systems. In this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.
342777	Efficient variants of Huffman codes in high level languages Although it is well-known that Huffman Codes are optimal for text compression in a character-per-character encoding scheme, they are seldom used in practical situations since they require a bit-per-bit decoding algorithm, which has to be written in some assembly language, and will perform rather slowly. A number of methods are presented that avoid these difficulties. The decoding algorithms efficiently process the encoded string on a byte-per-byte basis, are faster than the original algorithm, and can be programmed in any high level language. This is achieved at the cost of storing some tables in the internal memory, but with no loss in the compression savings of the optimal Huffman codes. The internal memory space needed can be reduced either at the cost of increased processing time, or by using non-binary Huffman codes, which give sub-optimal compression. Experimental results for English and Hebrew text are also presented.
312666	The decomposition of human-written summary sentences An abstract is not available.
312668	The automatic construction of large-scale corpora for summarization research An abstract is not available.
312669	Phrase recognition and expansion for short, precision-biased queries based on a query log An abstract is not available.
312670	The paraphrase search assistant: terminological feedback for iterative information seeking An abstract is not available.
312671	Phrasier: a system for interactive document retrieval using keyphrases An abstract is not available.
312673	Content-based retrieval using heuristic search An abstract is not available.
312675	Content-based retrieval for music collections An abstract is not available.
312676	Relevance feedback retrieval of time series data An abstract is not available.
312677	Combining multiple evidence from different types of thesaurus for query expansion An abstract is not available.
312678	Context-sensitive vocabulary mapping with a spreading activation network An abstract is not available.
312679	Deriving concept hierarchies from text An abstract is not available.
312680	A hidden Markov model information retrieval system An abstract is not available.
312681	Information retrieval as statistical translation An abstract is not available.
312682	An algorithmic framework for performing collaborative filtering An abstract is not available.
312684	Comparing the performance of database selection algorithms An abstract is not available.
312685	A probabilistic solution to the selection and fusion problem in distributed information retrieval An abstract is not available.
312687	Cluster-based language models for distributed retrieval An abstract is not available.
312688	Adaptive cluster-based browsing using incrementally expanded queries and its effects (poster abstract) An abstract is not available.
312690	A comparison of query translation methods for English-Japanese cross-language information retrieval (poster abstract) An abstract is not available.
312692	Estimating precision by random sampling (poster abstract) An abstract is not available.
312693	Evaluating a visualisation of image similarity (poster abstract) An abstract is not available.
312696	Fundamental properties of aboutness (poster abstract) An abstract is not available.
312698	A general language model for information retrieval (poster abstract) An abstract is not available.
312700	Hierarchical neural networks for text categorization (poster abstract) An abstract is not available.
312701	Improving retrieval on imperfect speech transcriptions (poster abstract) An abstract is not available.
312703	Information seeking at different stages of the R&D research process (poster abstract) An abstract is not available.
312711	An intelligent adaptive filtering agent based on an on-line learning model (poster abstract) An abstract is not available.
312714	Interactive Internet search through automatic clustering (poster abstract): an empirical study An abstract is not available.
312718	Jester 2.0 (poster abstract): evaluation of an new linear time collaborative filtering algorithm An abstract is not available.
312721	A knowledge management tool for speech interfaces (poster abstract) An abstract is not available.
312725	Machine translation and monolingual information retrieval (poster abstract) An abstract is not available.
312727	Music retrieval as text retrieval (poster abstract): simple yet effective An abstract is not available.
312730	NACSIS test collection workshop (NTCIR-1) (poster abstract) An abstract is not available.
312732	A new approach for image classification and retrieval (poster abstract) An abstract is not available.
312733	Query expansion method based on word contribution (poster abstract) An abstract is not available.
312737	Searching on the Web (poster abstract): two types of expertise An abstract is not available.
312739	Searching program source code with a structured text retrieval system (poster abstract) An abstract is not available.
312741	Statistical phrases for vector-space information retrieval (poster abstract) An abstract is not available.
312743	Supporting content retrieval from WWW via “basic level categories” (poster abstract) An abstract is not available.
312745	30,000 hits may be better than 300 (poster abstract): precision anomalies in Internet searches An abstract is not available.
312748	Ultra-summarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries An abstract is not available.
312749	Visual MeSH An abstract is not available.
312751	Web searching behavior of aerospace engineers (poster abstract) An abstract is not available.
312752	Advanced search technologies for unfamiliar metadata (demonstration abstract) An abstract is not available.
312755	Ant World (demonstration abstract) An abstract is not available.
312756	CHROMA (demonstration abstract): a content-based image retrieval system An abstract is not available.
312758	Crystal (demonstration abstract): a content-based music retrieval system An abstract is not available.
312759	CueVideo (demonstration abstract): automated video/audio indexing and browsing An abstract is not available.
312763	A demonstration of WHIRL (demonstration abstract) An abstract is not available.
312764	“Drag-and-drop” technique for MEDLINE searching (demonstration abstract) An abstract is not available.
312765	Extraction/gathering with the Taylor system (demonstration abstract) An abstract is not available.
312767	Information access across the language barrier (demonstration abstract): the MuST system An abstract is not available.
312768	Information retrieval library (IRLIB) (demonstration abstract) An abstract is not available.
312769	An Internet-based newspaper filtering and personalization system (demonstration abstract) An abstract is not available.
312770	Jester 2.0 (demonstration abstract): collaborative filtering to retrieve jokes An abstract is not available.
312771	The MultiText retrieval system (demonstration abstract) An abstract is not available.
312772	Text and image retrieval in Cheshire II (demonstration abstract) An abstract is not available.
312773	Visualizing Internet search results with adaptive self-organizing maps (demonstration abstract) An abstract is not available.
312774	WebCluster, a tool for mediated information access (demonstration abstract) An abstract is not available.
313469	ATTICS (poster abstract): a software platform for online text classification An abstract is not available.
313472	Discovering Chinese words from unsegmented text (poster abstract) An abstract is not available.
312618	“User revealment”—a comparison of initial queries and ensuing question development in online searching and in human reference interactions An abstract is not available.
312634	Visualization of search results: a comparative evaluation of text, 2D, and 3D interfaces An abstract is not available.
312637	From reading to retrieval: freeform ink annotations as queries An abstract is not available.
312639	SCAN: designing and evaluating user interfaces to support retrieval from speech archives An abstract is not available.
312645	Document expansion for speech retrieval An abstract is not available.
312647	A re-examination of text categorization methods An abstract is not available.
312649	Probabilistic latent semantic indexing An abstract is not available.
312652	A similarity-based probability model for latent semantic indexing An abstract is not available.
312654	Using a belief revision operator for document ranking in extended Boolean models An abstract is not available.
312656	Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web An abstract is not available.
312659	A new statistical formula for Chinese text segmentation incorporating contextual information An abstract is not available.
312661	Information retrieval based on context distance and morphology An abstract is not available.
312662	Partial replica selection based on relevance for information retrieval An abstract is not available.
312663	Efficient distributed algorithms to build inverted files An abstract is not available.
312664	Effective document presentation with a locality-based similarity heuristic An abstract is not available.
312665	Summarizing text documents: sentence selection and evaluation metrics An abstract is not available.
277434	Real life information retrieval (panel): commercial search engines An abstract is not available.
277435	ACHIRA (abstracts): automatic construction of hypertexts for information retrieval applications An abstract is not available.
277436	Semantic search and semantic categorization (abstracts) An abstract is not available.
277437	Visual SOM (abstract) An abstract is not available.
511707	Progress report on automatic information retrieval An abstract is not available.
511708	Automated monitoring to support the analysis and evaluation of information systems This paper is based on, and extracted in part from, a much more expanded and detailed manuscript entitled "Monitoring and Evaluation of On-Line Information System Usage", accepted for publication in Information Processing and Management.
511709	A clustering strategy based on a formalism of the reproductive process in natural systems Given a set of objects each of which is represented by a finite number of attributes or features and a clustering criterion that associates a value of utility to any classification, the objective of a clustering method is to identify that classification of the objects which optimizes the criterion. A new strategy to solve this problem is developed. The approach is, in essence, a modification of the reproductive plan, a type of adaptive procedure devised by Holland [2], which embodies many principles found in the adaptation of natural systems through evolution. The proposed approach differs from conventional methods in the sense that the search through the space of possible solutions proceeds in a parallel fashion.The adaptive clustering strategy requires the specification of methods for the generation of an initial population of classifications, the parent selection, the modifications and the replacement of current classifications with new ones. The effects of changing several of these features are investigated. Experimental results show that it is possible to devise clustering strategies based on the principles of adaptation in natural systems that are both effective and efficient.
511710	Use of dynamic discrimination values in a document retrieval system The use of discrimination values as a term weighting function in document retrieval systems is examined. It is shown that regular discrimination values are too costly to compute after every update to the data base. Dynamic discrimination values that are easy to update are defined for use as approximations to regular values. Experiments are performed comparing regular vs. dynamic discrimination values. Actual user queries from an operational data base are used to evaluate dynamic discrimination values in a production environment. Generalized forms of normalized recall and precision are used as evaluation measures. Retrieval results indicate statistically significant improvements using dynamic discrimination weighting.
511711	An empirical comparison: tree and lattice structures for symbolic data bases Unidirectional trees and lattices may both be used to hold a symbolic data base consisting of lexes, lexemes or other symbol strings. This paper empirically compares placing symbolic information into both trees and lattices (a lattice may be thought of as a unidirectional network with single and initiating and terminating nodes).
511712	Document representation models for retrieval systems Document retrieval system models are presented. Measures to rank the closeness of documents to a query are given. Algorithms to calculate the measures for graph and partition models are provided.
511713	The economic implementation of experimental retrieval techniques on a very large scale using an intelligent terminal The results and achievements of research in information retrieval have had little influence on the types of retrieval mechanism implemented in the large commercial on-line retrieval systems. Commercial systems still use simple Boolean techniques while experiments have shown that other techniques, such as those making use of relevance information, perform better. Reasons for this are suggested. A strategy is described for the implementation of high-powered techniques which overcomes this problem - an intelligent terminal is used in conjunction with an existing commercial I. R. system. The added processing capability permits the implementation of more sophisticated retrieval techniques very cheaply. Retrieval methods that can be implemented on such a terminal used in this way are described.Particular emphasis is placed upon the practical implementation of a term weighting scheme based on relevance feedback information which generates a ranked list of documents in answer to a query.
511714	The role of information retrieval in the second computer revolution Information retrieval is conceptually fundamental in human communication as well as in man-computer communication. Computing and Information Retrieval professionals have the opportunity to apply information retrieval techniques within the second computer revolution to foster a new potential revolution in education, brought about by the advent of the personal computer.
511715	Productivity, information technology and the office It is well known that American productivity has advanced very little in the last ten years in contrast to many other countries' rapidly rising productivity. It is becoming evident that major productivity gains can be made, particularly in the office workforce, which constitutes the majority of the American workforce today. Rapidly developing information technologies are making it possible to achieve radically increased productivity in the office. This paper will discuss the specific technologies; the specific major office functions; how they interrelate; and how they are making such radical productivity increases possible. "The Paperless Office", created by Micronet, Inc. in Washington, D.C., will be described. The current project to automate the office activities of the American Productivity Center in Houston, Texas, will be described. Some projections of the significance of these projects will be given.
511716	Message extraction through estimated relevance METER is a text analysis and retrieval system for non-expert computer users to exploit statistical associations between index terms of documents. It will run on a DEC PDP-11/45 minicomputer with continually changing collections of up to 20,000 documents at a time. A scaled version of METER with all major features of the full system has been implemented on a DEC PDP-11/70 as an experimental test bed for evaluation and comparison of associative retrieval algorithms. Although the basic structure of METER is similar to earlier statistical systems for retrospective document searches, the severe requirements of frequent updates of a document collection, of running on a small processor, and of meeting needs of users with little technical training have led to some novel developments. Among these are an update procedure that draws as much as possible on intermediate results from previous updates and a user interface that provides for control over the process of retrieval without calling for knowledge of how that process works.
511717	On the implementation of some models of document retrieval Recently several models of the search process in a document retrieval system have been proposed and retrieval experiments have shown that they will improve system performance. These include models which use relevance judgements to rank documents in order of probability of relevance and models of retrieval from clusters of documents. In this paper various models are compared in terms of the ease with which they could be implemented. An important consideration is how this implementation would be affected by the introduction of new hardware such as content-addressable memories. The main conclusion is that models which concentrate on improving the effectiveness of the search process are not rendered redundant by the availability of new hardware. However, the efficiency of their implementation would be improved.
511718	Document storage and retrieval An abstract is not available.
511719	On-Line Personal Bibliographic Retrieval System The Norris Cotton Cancer Center (NCCC) On-Line Personal Bibliographic Retrieval System was developed to assist researchers at the Center in managing personal or project-related collections of reference materials. The system supports on-line entry, storage, and retrieval of bibliographic citations for collections of books, journal articles, reprints, reports, manuscripts, other documents, and audio-visual materials.The NCCC system is intended for relatively small collections of materials (under 10,000 items) and is not intended to duplicate or compete in any way with MEDLARS, CANCERLINE, or any of the other on-line bibliographic retrieval services. It is similar in concept to Mitre's SHOEBOX (1), but is much less complex and requires far less computer resources. The major advantage of the NCCC system is that it is simple, small, and easy to use.None of the techniques used in developing the NCCC system is particularly new or innovative. Rather, several well-known approaches to bibliographic retrieval were combined to produce a system that was easy and inexpensive to develop and is easy and inexpensive to use. The system is now running on the Dartmouth College Computing system's equivalent of a Honeywell 66/DPS-3. The programs are all written in BASIC.
511720	Information implications into the eighties: panel discussion An abstract is not available.
511755	The basis for a theory of information retrieval An abstract is not available.
511756	The generalized retrieval problem An abstract is not available.
511757	Restricted evaluation in information retrieval An abstract is not available.
511758	Term frequency and term value An abstract is not available.
511759	A comparison of search term weighting: term relevance vs. inverse document frequency The term relevance weighting method has been shown to produce optimal information retrieval queries under well-defined conditions. The parameters needed to generate the term relevance factors cannot unfortunately be estimated accurately in practice; futhermore, in realistic test situations, it appears difficult to obtain improved retrieval results using the term relevance weights over much simpler term weighting systems such as, for example, the inverse document frequency weights.It is shown in this study that the inverse document frequency weights and the term relevance weights are closely related over a wide range of the frequency spectrum. Methods are introduced for estimating the term relevance weights, and experimental results are given comparing the inverse document frequency with the estimated term relevance weights.
511760	Incorporating different search models into one document retrieval system Many effective search strategies derived from different models are available for document retrieval systems. However, it does not appear that there is a single most effective strategy. Instead, different strategies perform optimally under different conditions. This paper outlines the design of an adaptive document retrieval system that chooses the best search strategy for a particular situation and user. In order to be able to support a variety of search strategies, a general network representation of the documents and terms in the database is proposed. This network representation leads to efficient methods of generating and using document and term classifications.One of the most desirable features of an adaptive system would be the ability to learn from experience. A method of incorporating this learning ability into the system is described. The adaptive control strategy for choosing search strategies enables the system to base its actions on a number of factors, including a model of the current user.Finally, some ideas for a flexible interface for casual users are suggested. Part of this interface is the heuristic search, which is used when searches based on formal models have failed. The heuristic search provides a browsing capability for the user.
511761	An approach to probabilistic retrieval The objective is to relate the effectiveness of retrieval, the fuzzy set concept and the processing of Boolean query. The use of a probabilistic retrieval scheme is motivated. It is found that there is a correspondence between probabilistic retrieval schmes and fuzzy sets. A fuzzy set corresponding to a potentially optimal probabilistic retrieval scheme is obtained. Then the retrieval scheme for the fuzzy set is constructed.
511762	Performance measurement in a fuzzy retrieval environment We shall consider retrieval performance measures for generalized (non-Boolean) queries and indexing functions. The meanings of recall and precision in such a generalized system will be discussed. Finally, we shall explore the meaning and difficulty of using such measures to compare Boolean and non-Boolean retrieval systems.
511763	On the limitations of document ranking algorithms in information retrieval A document retrieval system should rank documents in order of their usefulness or satisfaction to the users. This principle was first explicated in the classic paper by Maron and Kuhns (1). Additional considerations concerning document ranking have been suggested by other researchers (2,3). Particular attention will be given here to the ranking algorithm appropriate for those presenting the same request, but having different information needs. The research on which this report is based identifies limitations associated with sequencing rules that use a probability ranking technique (4). Three basic and somewhat interdependent limitations will be discussed.
511764	Simulation of user judgments in bibliographic retrieval systems The general model and simulation algorithms for bibliographic retrieval systems presented in an earlier paper are expanded. The new model integrates the physical as well as the logical and semantic elements of these systems. A modified algorithm is developed for the simulation of user relevance judgments, and is validated, by means of recall-precision curves and a Kolmogorov-Smirnov test of recall, for two test collections. Other approaches to goodness-of-fit testing are suggested.
511765	Statistical models for unformatted text In this note, we will describe some of the outstanding problems concerning statistical information retrieval models, and the underlying stochastic language production models they assume. The problems can be separated into classes according to the underlying language model, which can be either a sequence model or a grammar model. Both kinds of model are based on a stochastic process, but there is a different filter for the realization. The grammar models use a stochastic context sensitive grammar, and the sequence models use a high order Markov chain.Most of these problems cannot be solved without experimentation with information retrieval concepts and systems. Most information retrieval systems that currently exist have had to make operational assumptions about the answers to these questions. It is expected that more precise knowledge of solutions for these problems will simplify the design and improve the effectiveness of statistical information retrieval systems.
511766	Document classification, indexing and abstracting may be inherently difficult problems The main features of document indexing are abstracted. It is shown that the easy part of indexing, namely the question whether there is a bounded number of descriptors for indexing a document is NP-complete. Thus even the most efficient algorithm for exact indexing is not, at least at the present time, bounded by a polynomial-time function.
511767	The nearest neighbour problem in information retrieval: an algorithm using upperbounds An abstract is not available.
511768	Experiments on the application of clustering techniques to data validation (abstract) An abstract is not available.
511769	Dynamic clustering procedures for bibliographic data Clustering is an important tool for efficient retrieval of documents in bibliographic database systems. It can be also used to find research trend from a set of research papers. This paper discusses new clustering procedures called dynamic ones which seem to be suitable for bibliographic data handling. These procedures are developed to solve the following problems.(1) Depending on the characteristics of data, several different clustering procedures are required to obtain good results.(2) Large clusters are tend to be generated.Dynamic clustering procedures are difined to be procedures which change parameter values according to the characteristics of data. Similarity values and threshold values are dynamically modified to handle the above problems. Furthermore, to treat the latter problem data duplication is considered.
511770	Representation issues in information retrieval system design The representation problem confronting information retrieval system designers is outlined in terms of three issues: what to represent, forms of representation, and functions of representation. Questions raised by each of these issues are identified and selected research projects which have begun to explore these questions are described.
511771	Document Contents Representation model of sentence retrieval system SCAT-IR A "Document Contents Representation" (DCR) model is introduced from a formal viewpoint to deal with the entire contents of a document such as individual sentences of a text, bibliography, references, etc. in a scientific information system. A "Mapping Definition Language" (MDL) is proposed to map directly and naturally the document contents into the DCR model. An application of the DCR model and MDL to scientific documents is shown. Some examples of advanced retrieval by SCAT-IR system implemented on the basis of the DCR model and MDL are illustrated.
511772	Spatial representations of knowledge: validity and applications to information science This research examined the relationship between subjects' knowledge of an area and their similarity ratings of pairs of concepts. Knowledge was operationalized as (1) test score performance, and (2) correspondence with an expert's similarity judgments. Subjects (n=55) were exposed to a three-week module on social psychology as part of their introductory psychology course. At the end of this course segment they were administered an examination and, later, a term similarity questionnaire. Students' similarity ratings related to both exam score and similarity ratings of the expert. Furthermore, as expected, test performance was strongly related (r=+.55) to the correlation with the expert's similarity judgments. A model based on term similarity judgments was proposed as a theoretical explanation of how term dependence models influence information retrieval system performance.
511773	Content analysis as a word-processing option A simple content-analysis program incorporated in a word-processing system can display the most significant sentence of a page of text and give a short list of the more important words. This could help authors write titles, summaries, and descriptor lists. The content-analysis program relies on word frequency, precedence, and co-occurrence as indicators of content significance. Test show it performs at least as well as some trained indexers.
511774	Outline of a dynamic self-tuning and adaptive information retrieval system A self-tuning adaptive information retrieval system as an extension of the concept of a "classical" document retrieval system, is outlined. This system accepts documents and search requests in natural language, as well as the system-proposals previously produced by the system itself or prepared by the system operator. It produces a system-proposal that consists of a list of documents ranked according to their relevance to the query.Incorporated into the system is a system valuation subsystem that uses weighted relevance judgements. This subsystem gives as output an effectiveness value and an efficiency value: both together measure the quality of an information retrieval system.The computation of the quality values and the values themselves are independent of a specific implementation. The retrieval process in this system consists of two parts, namely a query-document match and a query-query match.
511775	Expert/consultation system for a retrieval data-base with semantic network of concepts This paper describes a development and implementation of an expert/consultation system for a retrieval data-base, that interfaces between the user and a retrieval system. The system's objective is to perform the information consultant's job in assisting a user to select the right vocabulary terms for his query. It is particularly useful for a novice user of a controlled-vocabulary, index-based retrieval system, who is not familiar with the vocabulary and the system Thesaurus. The user will enter his terms/keywords, that represent his information need, and the system will apply search procedures on its knowledge-base, and will find relevant concepts to be used as query-terms. The system is interactive; it can explain to the user why/how a concept was discovered/suggested, and it can back-track and try to find alternatives in case the user rejects a suggested concept. Two versions of the system were developed, utilizing two search and interaction strategies. Experiments will be conducted with the two alternatives in order to find out user preference and to compare performance. Performance will also be compard with an alternative "conventional" approach, which is an On-Line-Thesarus - developed as part of this study.
511795	Hardware systems for text information retrieval As databases become very large, conventional digital computers cannot provide satisfactory response time. This is particularly true for text databases, which must often be several orders of magnitude larger than formatted databases to store a useful amount of information. Even the standard techniques for improving system performance (such as inverted files) may not be sufficient to give the desired performance, and the use of an unconventional hardware organization may become necessary.A variety of different organizations has been proposed to enhance processing of text retrieval operations. Most of these have concentrated on the design of fast, efficient search engines. These can be divided into three classes: associative memories, cellular pattern matchers, and finite state automata. The advantages and disadvantages inherent in each of these approaches are discussed, along with a number of proposed implementations. Finally, the text retrieval system under development at the University of Utah is discussed in more detail.
511796	Artificial intelligence implications for information retrieval Overall, the field of information retrieval is already more aware than many other fields of the relevance of artificial intelligence (AI) [1-6]. Nonetheless there remain exciting applications of artificial intelligence that have been so far overlooked. In this paper we will point out some of the ways artificial intelligence might influence the field of information retrieval. We will then examine one application in more detail to discover the kind of technical problems involved in its fruitful exploitation.Before proceeding, it is important to interject a note of caution. While the promise of artificial intelligence is indeed bright, the time of complete fulfillment of its promise is a long way off. Of course, some of the expected contributions are shorter term than others. However, the more difficult problems will fall only after a good deal of basic research is accomplished. Artificial intelligence researchers have, in the past, been culpable of what can most charitably be described as over-optimism [7,8]. This naivete on the part of even the most respected of researchers stemmed from the profound subtleties underlying intelligent behavior. The problem is compounded by the fact that some of the most difficult of intelligent behavior (i.e. common sense) seems intuitively easy.
511797	Applications for information retrieval techniques in the office An abstract is not available.
511799	Intelligent information systems Natural language processing techniques developed for Artificial Intelligence programs can aid in constructing powerful information retrieval systems in at least two areas. Automatic construction of new concepts allows a large body of information to be organized compactly and in a manner that allows a wide range of queries to be answered. Also, using natural language processing techniques to conceptually analyze the documents being stored in a system greatly expands the effectiveness of queries about given pieces of text. However, only robust conceptual analysis methods are adequate for such systems. This paper will discuss approaches to both concept learning, in the form of Generalization-Based Memory, and powerful, robust text processing achieved by Memory-Based Understanding. These techniques have been implemented in the computer systems IPP, a program that reads, remembers and generalizes from news stories about terrorism, and RESEARCHER, currently in the prototype stage, that operates in a very different domain (technical texts, patent abstracts in particular).
511800	Using discourse analysis for the design of information retrieval interaction mechanisms An abstract is not available.
511802	Progress report on project information bridge Project Information Bridge is a West German Federal Ministry for Research and Technology supported project. It has been conducted in cooperation with the Gesellschaft für Information und Dokumentation (Society for Information and Documentation) (GID) and the various data base hosts in West Germany. Its goal was to develop and test a working prototype of an add-on package for existing IR systems.This report covers the time period from October 1981 through March 1983 during which the prototype was successfully developed and tested. The principal emphasis was to provide users with search formulation aids applicable to specific document files searchable using differing IR systems. Aids for thesaurus construction for natural language words and for file construction and indexing have not at this date been tested. Should tests be completed prior to the conference date, information concerning these topics will be presented at the conference.
511803	Natural language grammars for an information system The User Specialty Languages (USL) System is an applications independent natural language interface to a Relational Database System. It provides non DP-trained people with a tool to introduce, query, manipulate and analyse the data stored in a Relational Database via natural language. USL interfaces with different languages; in the present paper the grammar developed for Spanish is presented, and compared with the German grammar which was previously implemented and upon which it is based. Their main differences are pointed out, and the generality of the system to deal with other natural languages shown.
511804	Information retrieval using a transportable natural language interface This paper describes work in progress to develop a facility for natural language access to a variety of computer databases and database systems. This facility, called IRUS for Information Retrieval using the RUS parsing system, allows users who are unfamiliar with the technical characteristics of the underlying database system to query databases using typed English input. This system can be thought of as a stand-alone query system or as part of a management information system (MIS) or a decision support system (DSS).Many systems boast of having a "user-friendly" or "English-like" or even "English" interface so that users require a minimum of special training to use the system, but most such systems use shallow, relatively ad hoc techniques that are not robust or linguistically sound. We are using a large, well-tested, theoretically-based, general parser of English that has been developed and extended in a variety of research projects for over a decade.One of the primary emphases of IRUS is transportability, which includes three types of changes: (1) changing the domain, (2) changing data bases within the same domain, and (3) changing data base systems. The use of a general parser for English is an important part of the solution to the transportability problem, but there are other parts as well, since portions of the system beyond the parser must know the conceptual content of the domain, the way in which this is reflected in a collection of datasets, and the operating characteristics of the dbms being used to access these datasets.Other researchers have investigated similar issues [8, 5, 6, 12]. We have attacked this problem by building a knowledge-based system, with procedural components independent of domain and data base structure, directed by domain and database dependent knowledge structures. We are also building tools for conveniently creating and maintaining these knowledge structures, with an eventual goal of allowing end-users to extend and modify these knowledge structures to suit their own needs. Given this set of goals, and these tools, we consider the current implementation, which uses the System 1022 dbms on the DEC KL-2060, to be only one of a set of possible implementations, and are not constraining IRUS on the basis of 1022's strengths and weaknesses.This paper presents an overview of the IRUS system, emphasizing those aspects of the design that are critical to transportability. We describe the parsing system, which is a completely independent module that has been interfaced to a variety of different applications, and then discuss the other modules which bridge the gap between the parser and the dbms.
511806	Evaluation of the 2-Poisson model as a basis for using term frequency data in searching The early work on the probabilistic models of retrieval assumed that the document representation is binary, indicating only the presence or absence of index terms. The 2-Poisson (TP) model which was proposed as a model of how the occurrence frequency of specialty words in a collection is distributed, has since been used to develop retrieval strategies that incorporate term frequency information. This work investigates the use of the TP model, in this context, further. It is shown that the search effectiveness, when no relevance information is assumed, can be further enhanced by using this model. Furthermore, when the term weights proposed in this work are used in conjunction with weights known as term significance weights, the results are very encouraging.
511807	A discrimination gain hypothesis Underlying many of the probabilistic models for information retrieval are assumptions of stochastic dependence or independence of varying degrees of severity for the index terms describing the documents. These models generally specify a matching function, that is a function which compares a query with each document. The form of that function is to a large extent determined by the particular dependence/independence assumption. For example, if the index terms are assumed to be independently distributed over both the set of relevant and non-relevant documents then the matching function will in general be linear, whereas an assumption of dependence will lead to a non-linear function.Irrespective of the form that the matching function may take it is always assumed that the search terms in the query are known. In this paper I wish to address the problem of the choice of search terms and how this choice may be affected by an independence assumption.
511809	A study of the overlap among document representations Most previous investigations comparing the performance of different representations have used recall and precision as performance measures. However, there is evidence to show that these measures are insensitive to an important difference between representations. To explain, two representations may perform similarly on these measures, while retrieving very different sets of documents. Equivalence of representations should be decided on the basis of similarity in performance and similarity in the documents retrieved. This study compared the performance of four representations in the PsycAbs database. In addition, overlap between retrieved sets was also computed where overlap is the proportion of retrieved documents that are the same for pairs of document representations. Results indicate that for any two representations considered, performance values differed slightly while overlap scores were also low, thus supporting the evidence that recall and precision as performance measures mask differences between the sets of retrieved documents. Results are interpreted to propose an optimal ordering of the representations and to examine the contribution of each representation given this combination.
511810	A clustering scheme In this paper, a new clustering algorithm has been described. The algorithm proposed determines both the number of clusters in a collection, and the number of elements in each cluster before beginning the final clustering process. The complexity assessment of the algorithm and the implementation issues are also emphasized.
511811	The normalized recall and related measures The normalized recall is one of the most popular evaluation measures for information retrieval systems. In this paper an overview of its development is given. It is then shown that the normalized recall is closely related to other measures such as the CRE-measure and the expected search length. Some implications are analysed.
511813	User interfaces to information systems: choices vs. commands Do users prefer selection from a menu or specification of keywords to retrieve documents? We tried two experiments, one using an on-line library catalog and the other an on-line news wire. In the first, library users could either issue keyword commands to see book catalog entries, or choose categories from a menu following the Dewey Decimal classification of the books. In the second, news wire users could read Associated Press news stories either by posting a keyword profile against which all stories were matched, or by selecting them from a menu of current news items. For the library users, keyword searches were clearly preferred, by votes of 3 and 4 to 1; for the news stories, retrieval by keyword search is 50% less common than menu choice.We suggest that the difference is based on the degree of user foreknowledge of the data base and its organization. Menu-type interfaces tell the user what is available. If the user already knows, as in the library where a majority of the users have a particular book in mind, then the menu is merely time-consuming. But when the user does not know what is available (almost the definition of "news" is that it is new, and unpredictable), the menu is valuable because it displays the choice.
511814	End user touch searching for cancer therapy literature: a rule based approach This paper reviews work towards building an expert system for searching the cancer therapy literature on MEDLINE. A modified subset of the Medical Subject Headings (MeSH) has been stored on a micro-computer and accessed via a touch terminal. Searches, previously requested of the Oncology Information Service at the University of Leeds, have been used to test out the principle of end user searching and the results compared with the searching expertise of a MEDLARS indexer. Original program development was in PASCAL, but a rule-based approach, which is independent of a particular programming language, has been developed for search term and frame selection adopting a 'blackboard' philosophy in tracing the process of selection. Work is progressing on an implementation using the expert systems programming language PROLOG, which has been found a very suitable language for representing rules and provides a ready made rule interpreter. It is suggested that this approach is superior in terms of retrieval performance compared with alternative approaches to end-user searching which fail to exhibit detailed knowledge regarding the subject matter of the search.
511816	Transaction log analysis online catalogs From November 1981 to April 1983, OCLC's Office of Research has been conducting research into online public access catalogs (OPACs). This project has been funded in part by the Council on Library Resources, Inc. as an attempt to provide new insight into the use of online catalogs by obtaining information which may serve as input for better system design of OPACs, utilizing not only desired user features but also more effective searching.The overall study is concerned with the patron and the system and consists of three major parts. The first is the study of current use of online catalogs, i.e., the actual use - what is really happening. The second element is concerned with the perceived patron use of the catalogs and involves the use of questionnaires and focus group interviews at the participating institutions. The third part is an application of the findings from the first two parts.This paper focuses on the current utilization of OPACs. The methodology chosen to employ is to obtain machine-readable transaction logs, via tapes, from the online catalogs and subsequently analyzing these transactions by stochastic search pattern development and mathematical models utilizing Markov chain analysis and the development of transition probability matrices.
511817	The user view of file management: recommendations for a user interface based on analysis of UNIX file system use The structures in which users store their files facilitate retrieval by enabling users to deduce a file's contents from its place in the organization. This study examined structures created by UNIX users to organize their files within a hierarchical directory scheme, and examined the relation between structure and command usage. Users' difficulties in managing the complexity of a hierarchical structure limited the amount of information about files that these structures contained. Tree complexity increased in a negatively accelerating function with the number of files. Users who grouped their files into few directories arranged in shallow trees could navigate through the tree easily, but they sacrificed information: directory names were less specific, and users made more command errors. More sophisticated users created deeper trees. They were able to manage more files but also made extensive use of navigation aids.
511818	End user behavior on an online information retrieval system: a computer monitoring study We report on a computer monitoring study of users of the Ohio State University Libraries' online catalog, an established and heavily used information retrieval system. To our knowledge, this is the first monitoring study of an online catalog performed without system-defined user sessions. Online catalogs represent a class of retrieval systems which are designed for end users, require little or no formal training, and replace an existing manual system. The study characterizes user behavior in terms of types of searches done, patterns of use, time spent on searching, errors, and system problems. Preliminary results suggest that users have much shorter sessions than on other types of retrieval systems. Patterns of use vary between campus libraries, academic quarters, and between short and long sessions. Results of the study will be applied to improving the user interface and other system features.
511820	A network organization used for document retrieval A network organization for implementing a document retrieval system is proposed. This organization has significant advantages in terms of the range of searches that can be used when compared to either inverted or clustered file organizations. Algorithms for generating and maintaining the network are described together with experiments designed to test their efficiency and effectiveness.
511821	RESEDA, an Information Retrieval system using artificial intelligence and knowledge representation techniques The RESEDA project is concerned with the construction of AI Information Retrieval systems working on databases containing biographical data. There exist in RESEDA two fundamental ways of retrieving information requested by a user. In the first case, the information we wish to obtain is data which already exists in the base. This data can be obtained by direct match with the "search model" corresponding to the user's question. If this is not possible, we can still try to get an answer by using the inference procedures of the "transformation" type. The second method retrieves information which, in contrast, is created ex nihilo by the search procedure itself. It expresses, in fact, the possibility of a new causal relationship, within the base, between an "episode" provided explicitly by the user and one or more "episodes" that the system retrieves by applying an inference procedure of the type "hypothesis".
511823	Current practice in the evaluation of multikey search algorithms File structures and algorithms for multikey searching allow more than a single key to be used in locating a record for use in retrieval or update. Such algorithms are of use in many different kinds of information systems, including database systems, information retrieval systems, and pattern recognition and image processing systems. Such algorithms have received increased attention in recent years. However, they are not as well understood as those which handle single keys.Multikey algorithms are more difficult to evaluate than those based on the use of single keys. There are simply more factors to be considered. The evaluations performed for such algorithms should allow comparisons in order to be useful to a community of researchers and users. Theoretical analyses should be based on reasonable and clearly stated assumptions. Experiments should be repeatable and statistically valid whether they are based on "real" data or on randomly generated data.
511824	Combinatorial compression and partitioning of large dictionaries: theory and experiments A method for compressing large dictionaries is proposed, based on transforming words into lexicographically ordered strings of distinct letters, together with permutation indexes. Algorithms to generate such strings are described. Results of applying the method to the dictionaries of two databases, in Hebrew and English, are presented in detail. The main message is a method of partitioning the dictionary such that the "information bearing fraction" is stored in fast memory, and the bulk in auxiliary memory.
511825	Derived search keys for bibliographic retrieval A principle of information science states that the entropy of a set of symbols is maximised when the probability of occurrence of each becomes the same. This paper presents the results of a number of experiments which utilise this principle to construct fixed length keys from pertinent fields in order to locate and retrieve unique records as well as clusters with lexically homogeneous information. Each key incorporates codes derived by various positional selection methods and their discriminating strength proves to be well over 95%.
511827	An approach to enhancement of statistical survey databases This paper deals with statistical databases that are generated from statistical surveys and that reside in organizations which perform a large number of surveys--some of which are repetitive. Examples of such organizations are Federal statistical agencies such as the Energy Information Administration, Bureau of Labor Statistics, National Center for Educational Statistics, National Center for Health Statistics, etc; state governments that have bureaus or departments that collect such data; and marketing research departments of most large consumer-oriented companies. Computer processing has provided a powerful tool for storing, manipulating, and analyzing statistical survey data. However, in addition to these advantages, computing has created a major problem in that most data analysts and users have lost touch with the data and their generation. They no longer have the feel and sense for the data that once was possible. In this paper we present an approach to database design that will directly attack this problem and enhance the usefulness of such databases as well.
511828	Knowledge-Based Report Generation: a technique for automatically generating natural language reports from databases Knowledge-Based Report Generation is a technique for automatically generating natural language summaries from databases. It is so named because it applies the tools of knowledge-based expert systems design to the problem of text generation. The technique is currently being applied to the design of an automatic natural language stock report generator. Examples drawn from the implementation of the stock report generator are used to describe the components of a knowledge-based report generator.
511830	Some research problems in automatic information retrieval Information retrieval components are currently incorporated in several types of information systems, including bibliographic retrieval systems, data base management systems and question-answering systems. Some of the problems arising in the real-time environment in which these systems operate are briefly discussed. Certain recent advances in information retrieval research are then mentioned, including the formulation of new probabilistic retrieval models, and the development of automatic document analysis and Boolean query processing techniques.
511831	Information retrieval: new directions: old solutions An abstract is not available.
511832	Open problems in information retrieval An abstract is not available.
383953	Applying summarization techniques for term selection in relevance feedback Query-expansion is an effective Relevance Feedback technique for improving performance in Information Retrieval. In general query-expansion methods select terms from the complete contents of relevant documents. One problem with this approach is that expansion terms unrelated to document relevance can be introduced into the modified query due to their presence in the relevant documents and distribution in the document collection. Motivated by the hypothesis that query-expansion terms should only be sought from the most relevant areas of a document, this investigation explores the use of document summaries in query-expansion. The investigation explores the use of both context-independent standard summaries and query-biased summaries. Experimental results using the Okapi BM25 probabilistic retrieval model with the TREC-8 ad hoc retrieval task show that query-expansion using document summaries can be considerably more effective than using full-document expansion. The paper also presents a novel approach to term-selection that separates the choice of relevant documents from the selection of a pool of potential expansion terms. Again, this technique is shown to be more effective that standard methods.
383954	Temporal summaries of new topics We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.
383955	Generic text summarization using relevance measure and latent semantic analysis In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents. The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations. Both methods strive to select sentences that are highly ranked and different from each other. This is an attempt to create a summary with a wider coverage of the document's main content and less redundancy. Performance evaluations on the two summarization methods are conducted by comparing their summarization outputs with the manual summaries generated by three independent human evaluators. The evaluations also study the influence of different VSM weighting schemes on the text summarization performances. Finally, the causes of the large disparities in the evaluators' manual summarization results are investigated, and discussions on human text summarization patterns are presented.
383956	A new approach to unsupervised text summarization The paper presents a novel approach to unsupervised text summarization. The novelty lies in exploiting the diversity of concepts in text for summarization, which has not received much attention in the summarization literature. A diversity-based approach here is a principled generalization of Maximal Marginal Relevance criterion by Carbonell and Goldstein cite{carbonell-goldstein98}. We propose, in addition, an information-centric approach to evaluation, where the quality of summaries is judged not in terms of how well they match human-created summaries but in terms of how well they represent their source documents in IR tasks such document retrieval and text categorization. To find the effectiveness of our approach under the proposed evaluation scheme, we set out to examine how a system with the diversity functionality performs against one without, using the BMIR-J2 corpus, a test data developed by a Japanese research consortium. The results demonstrate a clear superiority of a diversity based approach to a non-diversity based approach.
383957	Vector-space ranking with effective early termination Considerable research effort has been invested in improving the effectiveness of information retrieval systems. Techniques such as relevance feedback, thesaural expansion, and pivoting all provide better quality responses to queries when tested in standard evaluation frameworks. But such enhancements can add to the cost of evaluating queries. In this paper we consider the pragmatic issue of how to improve the cost-effectiveness of searching. We describe a new inverted file structure using quantized weights that provides superior retrieval effectiveness compared to conventional inverted file structures when early termination heuristics are employed. That is, we are able to reach similar effectiveness levels with less computational cost, and so provide a better cost/performance compromise than previous inverted file organisations.
383958	Static index pruning for information retrieval systems We introduce static index pruning methods that significantly reduce the index size in information retrieval systems.We investigate uniform and term-based methods that each remove selected entries from the index and yet have only a minor effect on retrieval results. In uniform pruning, there is a fixed cutoff threshold, and all index entries whose contribution to relevance scores is bounded above by a given threshold are removed from the index. In term-based pruning, the cutoff threshold is determined for each term, and thus may vary from term to term. We give experimental evidence that for each level of compression, term-based pruning outperforms uniform pruning, under various measures of precision. We present theoretical and experimental evidence that under our term-based pruning scheme, it is possible to prune the index greatly and still get retrieval results that are almost as good as those based on the full index.
383959	Rank-preserving two-level caching for scalable search engines An abstract is not available.
383960	Using event segmentation to improve indexing of consumer photographs Automatic albuming --- the automatic organization of photographs, either as an end in itself or for use in other applications -- is an application that promises to be of great assistance to photographers. Relatively sophisticated image content analysis techniques have been used for image indexing, organization and retrieval. In this paper, we describe a method of organizing photographs into events using spoken photograph captions. The results of this process can be used to improve image indexing and retrieval.
383961	Ranking retrieval systems without relevance judgments The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics. It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text REtrieval Conference (TREC) has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems. Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to as pseudo-relevance judgments .Rankings of systems with our methodology correlate positively with official TREC rankings, although the performance of the top systems is not predicted well. The correlations are stable over a variety of pool depths and sampling techniques. With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.
383963	Evaluation by highly relevant documents Given the size of the web, the search engine industry has argued that engines should be evaluated by their ability to retrieve highly relevant pages rather than all possible relevant pages. To explore the role highly relevant documents play in retrieval system evaluation, assessors for the mbox{TREC-9} web track used a three-point relevance scale and also selected best pages for each topic. The relative effectiveness of runs evaluated by different relevant document sets differed, confirming the hypothesis that different retrieval techniques work better for retrieving highly relevant documents. Yet evaluating by highly relevant documents can be unstable since there are relatively few highly relevant documents. TREC assessors frequently disagreed in their selection of the best page, and subsequent evaluation by best page across different assessors varied widely. The discounted cumulative gain measure introduced by J"{a}rvelin and Kek"{a}l"{a}inen increases evaluation stability by incorporating all relevance judgments while still giving precedence to highly relevant documents.
383964	Meta-scoring: automatically evaluating term weighting schemes in IR without precision-recall In this paper, we present a method that can automatically evaluate performance of different term weighting schemes in information retrieval without resorting to precision-recall based on human relevance judgments. Specifically, the problem is: given two document-term matrixes generated from two different term weighting schemes, can we tell which term weighting scheme will performance better than the other? We propose a meta-scoring function, which takes as input the document-term matrix generated by some term weighting scheme and computes a goodness score from the document-term matrix. In our experiments, we found out that this score is highly correlated with the precision-recall measurement for all the collections and term weighting schema we tried. Thus, we conclude that our meta-scoring function can be a substitute for the precision-recall measurement that needs relevance judgments of human subject. Furthermore, this meta-scoring function is not limited only to text information retrieval can be applied to fields such as image and DNA retrieval.
383965	Improving cross language retrieval with triangulated translation Most approaches to cross language information retrieval assume that resources providing a direct translation between the query and document languages exist. This paper presents research examining the situation where such an assumption is false. Here, an intermediate (or pivot) language provides a means of transitive translation of the query language to that of the document via the pivot, at the cost, however, of introducing much error. The paper reports the novel approach of translating in parallel across multiple intermediate languages and fusing the results. Such a technique removes the error, raising the effectiveness of the tested retrieval system, up to and possibly above the level expected, had a direct translation route existed. Across a number of retrieval situations and combinations of languages, the approach proves to be highly effective.
383966	Improving query translation for cross-language information retrieval using statistical models Dictionaries have often been used for query translation in cross-language information retrieval (CLIR). However, we are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word. In addition, a word-by-word query translation is not precise enough. In this paper, we explore several methods to improve the previous dictionary-based query translation. First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns. Second, the best word translations are selected based on the cohesion of the translation words. Our experimental results on TREC English-Chinese CLIR collection show that these techniques result in significant improvements over the simple dictionary approaches, and achieve even better performance than a high-quality machine translation system.
383968	Evaluating a probabilistic model for cross-lingual information retrieval This work proposes and evaluates a probabilistic cross-lingual retrieval system. The system uses a generative model to estimate the probability that a document in one language is relevant, given a query in another language. An important component of the model is translation probabilities from terms in documents to terms in a query. Our approach is evaluated when 1) the only resource is a manually generated bilingual word list, 2) the only resource is a parallel corpus, and 3) both resources are combined in a mixture model. The combined resources produce about 90% of monolingual performance in retrieving Chinese documents. For Spanish the system achieves 85% of monolingual performance using only a pseudo-parallel Spanish-English corpus. Retrieval results are comparable with those of the structural query translation technique (Pirkola, 1998) when bilingual lexicons are used for query translation. When parallel texts in addition to conventional lexicons are used, it achieves better retrieval results but requires more computation than the structural query translation technique. It also produces slightly better results than using a machine translation system for CLIR, but the improvement over the MT system is not significant.
383970	Document language models, query models, and risk minimization for information retrieval We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.
383972	Relevance based language models We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model : probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.
383974	A statistical learning learning model of text classification for support vector machines This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?
383975	A study of thresholding strategies for text categorization Thresholding strategies in automated text categorization are an underexplored area of research. This paper presents an examination of the effect of thresholding strategies on the performance of a classifier under various conditions. Using k-Nearest Neighbor (kNN) as the classifier and five evaluation benchmark collections as the testbets, three common thresholding methods were investigated, including rank-based thresholding (RCut), proportion-based assignments (PCut) and score-based local optimization (SCut); in addition, new variants of these methods are proposed to overcome significant problems in the existing approaches. Experimental results show that the choice of thresholding strategy can significantly influence the performance of kNN, and that the ``optimal'' strategy may vary by application. SCut is potentially better for fine-tuning but risks overfitting. PCut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making. RCut is most natural for online response but is too coarse-grained for global or local optimization. RTCut, a new method combining the strength of category ranking and scoring, outperforms both PCut and RCut significantly.
383976	On feature distributional clustering for text categorization We describe a text categorization approach that is based on a combination of feature distributional clusters with a support vector machine (SVM) classifier. Our feature selection approach employs distributional clustering of words via the recently introduced information bottleneck method , which generates a more efficient word-cluster representation of documents. Combined with the classification power of an SVM, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency. Comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set. We discuss the potential reasons for this dependency.
383981	Iterative residual rescaling We consider the problem of creating document representations in which inter-document similarity measurements correspond to semantic similarity. We first present a novel subspace-based framework for formalizing this task. Using this framework, we derive a new analysis of Latent Semantic Indexing (LSI), showing a precise relationship between its performance and the uniformity of the underlying distribution of documents over topics. This analysis helps explain the improvements gained by Ando's (2000) Iterative Residual Rescaling (ours) algorithm: ours can compensate for distributional non-uniformity. A further benefit of our framework is that it provides a well-motivated, effective method for automatically determining the rescaling factor ours depends on, leading to further improvements. A series of experiments over various settings and with several evaluation metrics validates our claims.
383982	Expressive retrieval from XML documents The emergence of XML as a standard interchange format for structured documents/data has given rise to many XML query language proposals. However, some of these languages do not support information retrieval-style ranked queries based on textual similarity. There have been several extensions to these query languages to support keyword search, but the resulting query languages cannot express queries such as ``find books and CDs with similar titles'' . Either these extensions use keywords as mere boolean filters, or similarities can be calculated only between data values and constants rather than two data values. We propose ELIXIR, an textbf{underline{e}}xpressive and textbf{underline{e}}fficienttextbf{underline{l}}anguage for textbf{underline{X}}ML textbf{underline{i}}nformation textbf{underline{r}}etrieval that extends the query language XML-QL cite{deutsch-www8,deutsch-deb99} with a textual similarity operator. ELIXIR is a general-purpose XML information retrieval language, sufficiently expressive to handle the above query. Our algorithm for answering ELIXIR queries rewrites the original ELIXIR query into a series of XML-QL queries that generate intermediate relational data, and uses relational database techniques to efficiently evaluate the similarity operators on this intermediate data, yielding an XML document with nodes ranked by similarity. Our experiments demonstrate that our prototype scales well with the size of the XML data and complexity of the query.
383985	XIRQL: a query language for information retrieval in XML documents Based on the document-centric view of XML, we present the query language XIRQL. Current proposals for XML query languages lack most IR-related features, which are weighting and ranking, relevance-oriented search, datatypes with vague predicates, and semantic relativism. XIRQL integrates these features by using ideas from logic-based probabilistic IR models, in combination with concepts from the database area. For processing XIRQL queries, a path algebra is presented, that also serves as a starting point for query optimization.
383986	Empirical investigations on query modification using abductive explanations In this paper we report on a series of experiments designed to investigate query modification techniques motivated by the area of abductive reasoning. In particular we use the notion of abductive explanation, explanations being a description of data that highlight important features of the data. We describe several methods of creating abductive explanations, exploring term reweighting and query reformulation techniques and demonstrate their suitability for relevance feedback.
383987	Generic summaries for indexing in information retrieval This paper examines the use of generic summaries for indexing in information retrieval. Our main observations are that: (1) With or without pseudo-relevance feedback, a summary index may be as effective as the corresponding fulltext index for precision-oriented search of highly relevant documents . %43 But a reasonably sophisticated summarizer, using a compression ratio of 10-30%, is desirable for this purpose. (2) In pseudo-relevance feedback, using a summary index at initial search and a fulltext index at final search is possibly effective for precision-oriented search, regardless of relevance levels . This strategy is significantly more effective than the one using the summary index only and probably more effective than using summaries as mere term selection filters. %the use of summaries as mere term selection filters. %The summary quality is probably not a critical factor for this strategy, For this strategy, the summary quality is probably not a critical factor, and a compression ratio of 5-10% appears best.
383989	Automatic generation of concise summaries of spoken dialogues in unrestricted domains Automatic summarization of open domain spoken dialogues is a new research area. This paper introduces the task, the challenges involved, and presents an approach to obtain automatic extract summaries for multi-party dialogues of four different genres, without any restriction on domain. We address the following issues which are intrinsic to spoken dialogue summarization and typically can be ignored when summarizing written text such as newswire data: (i) detection and removal of speech disfluencies; (ii) detection and insertion of sentence boundaries; (iii) detection and linking of cross-speaker information units (question-answer pairs). A global system evaluation using a corpus of 23 relevance annotated dialogues containing 80 topical segments shows that for the two more informal genres, our summarization system using dialogue specific components significantly outperforms a baseline using TFIDF term weighting with maximum marginal relevance ranking (MMR).
383990	Enhanced topic distillation using text, markup tags, and hyperlinks Topic distillation is the analysis of hyperlink graph structure to identify mutually reinforcing authorities (popular pages) and hubs (comprehensive lists of links to authorities). Topic distillation is becoming common in Web search engines, but the best-known algorithms model the Web graph at a coarse grain, with whole pages as single nodes. Such models may lose vital details in the markup tag structure of the pages, and thus lead to a tightly linked irrelevant subgraph winning over a relatively sparse relevant subgraph, a phenomenon called topic drift or contamination . The problem gets especially severe in the face of increasingly complex pages with navigation panels and advertisement links. We present an enhanced topic distillation algorithm which analyzes text, the markup tag trees that constitute HTML pages, and hyperlinks between pages. It thereby identifies subtrees which have high text- and hyperlink-based coherence w.r.t. the query. These subtrees get preferential treatment in the mutual reinforcement process. Using over 50 queries, 28 from earlier topic distillation work, we analyzed over 700,000 pages and obtained quantitative and anecdotal evidence that the new algorithm reduces topic drift.
383991	Transparent Queries: investigation users' mental models of search engines Typically, commercial Web search engines provide very little feedback to the user concerning how a particular query is processed and interpreted. Specifically, they apply key query transformations without the users knowledge. Although these transformations have a pronounced effect on query results, users have very few resources for recognizing their existence and understanding their practical importance. We conducted a user study to gain a better understanding of users knowledge of and reactions to the operation of several query transformations that web search engines automatically employ. Additionally, we developed and evaluated Transparent Queries, a software system designed to provide users with lightweight feedback about opaque query transformations. The results of the study suggest that users do indeed have difficulties understanding the operation of query transformations without additional assistance. Finally, although transparency is helpful and valuable, interfaces that allow direct control of query transformations might ultimately be more helpful for end-users.
383992	Why batch and user evaluations do not give the same results Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the TREC Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.
383993	Evaluating a content based image retrieval system Content Based Image Retrieval (CBIR) presents special challenges in terms of how image data is indexed, accessed, and how end systems are evaluated. This paper discusses the design of a CBIR system that uses global colour as the primary indexing key, and a user centered evaluation of the systems visual search tools. The results indicate that users are able to make use of a range of visual search tools, and that different tools are used at different points in the search process. The results also show that the provision of a structured navigation and browsing tool can support image retrieval, particularly in situations in which the user does not have a target image in mind. The results are discussed in terms of their implications for the design of visual search tools, and their implications for the use of user-centered evaluation for CBIR systems.
383995	Evaluating topic-driven web crawlers Due to limited bandwidth, storage, and computational resources, and to the dynamic nature of the Web, search engines cannot index every Web page, and even the covered portion of the Web cannot be monitored continuously for changes. Therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed. The issue is even more important for topic-specific search engines, where crawlers must make additional decisions based on the relevance of visited pages. However, it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing. We propose three different methods to evaluate crawling strategies. We apply the proposed metrics to compare three topic-driven crawling algorithms based on similarity ranking, link analysis, and adaptive agents.
383999	Effective site finding using link anchor information Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
384003	Stable algorithms for link analysis The Kleinberg HITS and the Google PageRank algorithms are eigenvector methods for identifying ``authoritative'' or ``influential'' articles, given hyperlink or citation information. That such algorithms should give reliable or consistent answers is surely a desideratum, and in~cite{ijcaiPaper}, we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns. In this paper, we extend the analysis and show how it gives insight into ways of designing stable link analysis methods. This in turn motivates two new algorithms, whose performance we study empirically using citation data and web hyperlink data.
384005	Modeling score distributions for combining the outputs of search engines In this paper the score distributions of a number of text search engines are modeled. It is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. Experiments show that this model fits TREC-3 and TREC-4 data for not only probabilistic search engines like INQUERY but also vector space search engines like SMART for English. We have also used this model to fit the output of other search engines like LSI search engines and search engines indexing other languages like Chinese. It is then shown that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. We also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents. We hypothesize that all 'good' text search engines operating on any language have similar characteristics. This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques.
384007	Models for metasearch Given the ranked lists of documents returned by multiple search engines in response to a given query, the problem of metasearch is to combine these lists in a way which optimizes the performance of the combination. This paper makes three contributions to the problem of metasearch: (1) We describe and investigate a metasearch model based on an optimal democratic voting procedure, the Borda Count; (2) we describe and investigate a metasearch model based on Bayesian inference; and (3) we describe and investigate a model for obtaining upper bounds on the performance of metasearch algorithms. Our experimental results show that metasearch algorithms based on the Borda and Bayesian models usually outperform the best input system and are competitive with, and often outperform, existing metasearch strategies. Finally, our initial upper bounds demonstrate that there is much to learn about the limits of the performance of metasearch.
384009	The score-distributional threshold optimization for adaptive binary classification tasks The thresholding of document scores has proved critical for the effectiveness of classification tasks. We review the most important approaches to thresholding, and introduce the score-distributional (S-D) threshold optimization method. The method is based on score distributions and is capable of optimizing any effectiveness measure defined in terms of the traditional contingency table. As a byproduct, we provide a model for score distributions , and demonstrate its high accuracy in describing empirical data. The estimation method can be performed incrementally, a highly desirable feature for adaptive environments. Our work in modeling score distributions is useful beyond threshold optimization problems. It directly applies to other retrieval environments that make use of score distributions,e.g., distributed retrieval, or topic detection and tracking. The most accurate version of S-D thresholding --- although incremental --- can be computationally heavy. Therefore, we also investigate more practical solutions. We suggest practical approximations and discuss adaptivity, threshold initialization, and incrementality issues. The practical version of S-D thresholding has been tested in the context of the TREC-9 Filtering Track and found to be very effective [2].
384011	A meta-learning approach for text categorization We investigate a meta-model approach, called Meta-learning Using Document Feature characteristics (MUDOF), for the task of automatic textual document categorization. It employs a meta-learning phase using document feature characteristics. Document feature characteristics, derived from the training document set, capture some inherent category-specific properties of a particular category. Different from existing categorization methods, MUDOF can automatically recommend a suitable algorithm for each category based on the category-specific statistical characteristics. Hence, different algorithms may be employed for different categories. Experiments have been conducted on a real-world document collection demonstrating the effectiveness of our approach. The results confirm that our meta-model approach can exploit the advantage of its component algorithms, and demonstrate a better performance than existing algorithms.
384012	Maximum likelihood estimation for filtering thresholds Information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile. Documents with scores above profile-specific dissemination thresholds are delivered. An optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non-relevant documents. The parameters of the distribution can be estimated using relevance information, but relevance information obtained while filtering is biased . This paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias. The new algorithm, which is based on the Maximum Likelihood principle, jointly estimates the parameters of the density distributions for relevant and non-relevant documents and the ratio of the relevant document in the corpus. Experiments with TREC-8 and TREC-9 Filtering Track data demonstrate the effectiveness of the algorithm.
384013	Unsupervised and supervised clustering for topic tracking We investigate important differences between two styles of document clustering in the context of Topic Detection and Tracking. Converting a Topic Detection system into a Topic Tracking system exposes fundamental differences between these two tasks that are important to consider in both the design and the evaluation of TDT systems. We also identify features that can be used in systems for both tasks.
384015	Intelligent information triage In many applications, large volumes of time-sensitive textual information require triage: rapid, approximate prioritization for subsequent action. In this paper, we explore the use of prospective indications of the importance of a time-sensitive document, for the purpose of producing better document filtering or ranking. By prospective, we mean importance that could be assessed by actions that occur in the future. For example, a news story may be assessed (retrospectively) as being important, based on events that occurred after the story appeared, such as a stock price plummeting or the issuance of many follow-up stories. If a system could anticipate (prospectively) such occurrences, it could provide a timely indication of importance. Clearly, perfect prescience is impossible. However, sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. We describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. Unlike many information-retrieval applications for which document labeling is a laborious, manual process, for many prospective criteria it is possible to build very large, labeled, training corpora automatically. Such corpora can be used to train text classification procedures that will predict the (prospective) importance of each document. This paper illustrates the process with two case studies, demonstrating the ability to predict whether a news story will be followed by many, very similar news stories, and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story. We conclude by discussing how the comprehensibility of the learned classifiers can be critical to success.}
384017	Discovering information flow suing high dimensional conceptual space This paper presents an informational inference mechanism realized via the use of a high dimensional conceptual space. More specifically, we claim to have operationalized important aspects of GÂ?rdenforss recent three-level cognitive model. The connectionist level is primed with the Hyperspace Analogue to Language (HAL) algorithm which produces vector representations for use at the conceptual level. We show how inference at the symbolic level can be implemented by employing Barwise and Seligmans theory of information flow. This article also features heuristics for enhancing HAL-based representations via the use of quality properties, determining concept inclusion and computing concept composition. The worth of these heuristics in underpinning informational inference are demonstrated via a series of experiments. These experiments, though small in scale, show that informational inference proposed in this article has a very different character to the semantic associations produced by the Minkowski distance metric and concept similarity computed via the cosine coefficient. In short, informational inference generally uncovers concepts that are carried, or, in some cases, implied by another concept, (or combination of concepts).
384019	A study of smoothing methods for language models applied to Ad Hoc information retrieval Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing , which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.
384021	Topic segmentation with an aspect hidden Markov model We present a novel probabilistic method for topic segmentation on unstructured text. One previous approach to this problem utilizes the hidden Markov model (HMM) method for probabilistically modeling sequence data [7]. The HMM treats a document as mutually independent sets of words generated by a latent topic variable in a time series. We extend this idea by embedding Hofmann's aspect model for text [5] into the segmenting HMM to form an aspect HMM (AHMM). In doing so, we provide an intuitive topical dependency between words and a cohesive segmentation model. We apply this method to segment unbroken streams of New York Times articles as well as noisy transcripts of radio programs on SpeechBot , an online audio archive indexed by an automatic speech recognition engine. We provide experimental comparisons which show that the AHMM outperforms the HMM for this task.
384022	Finding topic words for hierarchical summarization Hierarchies have long been used for organization, summarization, and access to information. In this paper we define summarization in terms of a probabilistic language model and use the definition to explore a new technique for automatically generating topic hierarchies by applying a graph-theoretic algorithm, which is an approximation of the Dominating Set Problem. The algorithm efficiently chooses terms according to a language model. We compare the new technique to previous methods proposed for constructing topic hierarchies including subsumption and lexical hierarchies, as well as the top TF.IDF terms. Our results show that the new technique consistently performs as well as or better than these other techniques. They also show the usefulness of hierarchies compared with a list of terms.
384024	Exploiting redundancy in question answering Our goal is to automatically answer brief factual questions of the form ``When was the Battle of Hastings?'' or ``Who wrote The Wind in the Willows ?''. Since the answer to nearly any such question can now be found somewhere on the Web, the problem reduces to finding potential answers in large volumes of data and validating their accuracy. We apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half. The success of our approach depends on the idea that the volume of available Web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts. A query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of Web data. These passages are analyzed to identify candidate answers. The frequency of these candidates within the passages is used to ``vote'' for the most likely answer. The approach is experimentally tested on questions taken from the TREC-9 question-answering test collection. As an additional demonstration, the approach is extended to answer multiple choice trivia questions of the form typically asked in trivia quizzes and television game shows.
384025	High performance question/answering In this paper we present the features of a Question/Answering (Q/A) system that had unparalleled performance in the TREC-9 evaluations. We explain the accuracy of our system through the unique characteristics of its architecture: (1) usage of a wide-coverage answer type taxonomy; (2) repeated passage retrieval; (3) lexico-semantic feedback loops; (4) extraction of the answers based on machine learning techniques; and (5) answer caching. Experimental results show the effects of each feature on the overall performance of the Q/A system and lead to general conclusions about Q/A from large text collections.
384028	Searcher performance in question answering There are many tasks that require information finding. Some can be largely automated, and others greatly benefit from successful interaction between system and searcher. We are interested in the task of answering questions where some synthesis of information is required-the answer would not generally be given from a single passage of a single document. We investigate whether variation in the way a list of documents is delivered affected searcher performance in the question answering task. We will show that there is a significant difference in performance using a list customized to the task type, compared with a standard web-engine list. This indicates that paying attention to the task and the searcher interaction may provide substantial improvement in task performance.
384029	Toward an improved concept-based information retrieval system This paper presents a novel information retrieval system that includes 1) the addition of concepts to facilitate the identification of the correct word sense, 2) a natural language query interface, 3) the inclusion of weights and penalties for proper nouns that build upon the Okapi weighting scheme, and 4) a term clustering technique that exploits the spatial proximity of search terms in a document to further improve the performance. The effectiveness of the system is validated by experimental results.
384030	Metasearch consistency We investigate the performance of metasearch algorithms in terms of how much they improve consistency . We find that three different metasearch algorithms, each over three datasets, usually improve the consistency of search results; sometimes the improvement is dramatic. Furthermore, consistency tends to improve when performance improves.
384031	Anchor text mining for translation extraction of query terms This paper presents an approach to automatically extracting the bilingual translations of many Web query terms through mining the Web anchor texts. Some preliminary experiments are conducted on using 109,416 Web pages containing both Chinese and English anchor texts in their in-links to extract Chinese translations of 200 English queries selected from popular query terms in Taiwan. It is found that the effective translations of 75% of the popular query terms can be extracted, in which 87.2% cannot be obtained in common translation dictionaries.
384032	Selecting expansion terms in automatic query expansion An abstract is not available.
384033	An experimental framework for email categorization and management Many problems are difficult to adequately explore until a prototype exists in order to elicit user feedback. One such problem is a system that automatically categorizes and manages email. Due to a myriad of user interface issues, a prototype is necessary to determine what techniques and technologies are effective in the email domain. This paper describes the implementation of an add-in for Microsoft Outlook 2000 TM that intends to address two problems with email: 1) help manage the inbox by automatically classifying email based on user folders, and 2) to aid in search and retrieval by providing a list of email relevant to the selected item. This add-in represents a first step in an experimental system for the study of other issues related to information management. The system has been set up to allow experimentation with other classification algorithms and the source code is available online in an effort to promote further experimentation.
384034	Analyses of multiple-evidence combinations for retrieval strategies An abstract is not available.
384035	Flexible pseudo-relevance feedback using optimization tables An abstract is not available.
384037	Quantifying the utility of parallel corpora Our English-Chinese cross-language IR system is trained from parallel corpora; we investigate its performance as a function of training corpus size for three different training corpora. We find that the performance of the system as trained on the three parallel corpora can be related by a simple measure, namely the out-of-vocabulary rate of query words.
384038	Unitary operators for fast latent semantic indexing (FLSI) Latent Semantic Indexing (LSI) dramatically reduces the dimension of the document space by mapping it into a space spanned by conceptual indices. Empirically, the number of concepts that can represent the documents are far fewer than the great variety of words in the textual representation. Although this almost obviates the problem of lexical matching, the mapping incurs a high computational cost compared to document parsing, indexing, query matching, and updating. This paper shows how LSI is based on a unitary transformation, for which there are computationally more attractive alternatives. This is exemplified by the Haar transform, which is memory efficient, and can be computed in linear to sublinear time. The principle advantages of LSI are thus preserved while the computational costs are drastically reduced.
384039	Probabilistic combination of content and links Previous research has shown that citations and hypertext links can be usefully combined with document content to improve retrieval. Links can be used in many ways, e.g., link topology can be used to identify important pages, anchor text can be used to augment the text of cited pages, and activation can be spread to linked pages. This paper introduces a probabilistic model that integrates content matching and these three uses of link information in a single unified framework. Experiments with a web collection show benefits for link information especially for general queries.
384041	Structure and content-based segmentation of speech transcripts algorithm for the segmentation of an audio/video source into topically cohesive segments based on automatic speech recognition (ASR) transcriptions is presented. A novel two-pass algorithm is described that combines a boundary-based method with a content-based method. In the first pass, the temporal proximity and the rate of arrival of ngram features is analyzed in order to compute an initial segmentation. In the content- based second pass, changes in content-bearing words are detected by using the ngram features as queries in an information-retrieval system. The second pass validates the initial segments and merges them as needed. Feasibility of the segmentation task can vary enormously depending on the structure of the audio content, and the accuracy of ASR. For real-world corporate training data our method identifies, at worst, a single salient segment of the audio and, at best, a high-level table-of-contents. We illustrate the algorithm in detail with some examples and validate the results with segmentation boundaries generated manually.
384042	Text summarization via hidden Markov models A sentence extract summary of a document is a subset of the document's sentences that contains the main ideas in the document. We present an approach to generating such summaries, a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of this method with summaries generated by humans, showing that we obtain significantly higher agreement than do earlier methods.
384045	Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback An abstract is not available.
384061	Interactive phrase browsing within compressed text An abstract is not available.
384062	Query-biased web page summarisation: a task-oriented evaluation We present a system that offers a new way of assessing web document relevance and new approach to the web-based evaluation of such a system. Provisionally named WebDocSum, the system is a query-biased web page summariser that aims to provide an alternative to the short, irrelevant abstracts typical of many web search result lists. Based on an initial evaluation the system appears to be more useful in helping users gauge document relevance than the traditional ranked titles/abstracts approach.
384063	Query expansion based on predictive algorithms for collaborative filtering An abstract is not available.
384064	Query optimization for vector space problems We present performance measurement results for a parallel SQL based information retrieval system implemented on a PC cluster system. We used the Web-TREC dataset under a left-deep query execution plan. We achieved satisfactory speed up.
384065	Generic topic segmentation of document texts Topic segmentation is an important initial step in many text-based tasks. A hierarchical representation of a texts topics is useful in retrieval and allows judging relevancy at different levels of detail. This short paper describes research on generic algorithms for topic detection and segmentation that are applicable on texts of heterogeneous types and domains.
384066	Towards the use of prosodic information for spoken document retrieval An abstract is not available.
384067	A homogeneous framework to model relevance feedback Relevance feedback is an appreciated process to produce increasingly better retrieval. Usually, positive feedback plays a fundamental role in the feedback process whereas the role of negative feedback is limited. We think that negative feedback is a promising precision oriented mechanism and we propose a logical framework in which positive and negative feedback are homogeneously modeled. Evaluation results against small test collections are provided.
384068	Combining semantic and syntactic document classifiers to improve first story detection In this paper we describe a type of data fusion involving the combination of evidence derived from multiple document representations. Our aim is to investigate if a composite representation can improve the online detection of novel events in a stream of broadcast news stories. This classification process otherwise known as first story detection FSD (or in the Topic Detection and Tracking pilot study as online new event detection [1]), is one of three main classification tasks defined by the TDT initiative. Our composite document representation consists of a semantic representation (based on the lexical chains derived from a text) and a syntactic representation (using proper nouns). Using the TDT1 evaluation methodology, we evaluate a number of document representation combinations using these document classifiers.
384069	Browsing in a digital library collecting linearly arranged documents A method of assisting a user in finding the required documents effectively is proposed. A user being informed which documents are worth examining can browse in a digital library (DL) in a linear fashion. Computational evaluations were carried out, and a DL and its navigator are designed and constructed.
384070	Feature selection for polyphonic music retrieval An abstract is not available.
384071	Automatic information extraction from web pages Many web pages have implicit structure. In this paper, we show the feasibility of automatically extracting data from web pages by using approximate matching techniques. This can be applied to generate automatic wrappers or to notify/display web page differences, web page change monitoring, etc.
384072	Automatic web search query generation to create minority language corpora The Web is a valuable source of language specific resources but collecting, organizing and utilizing this information is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries to collect documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant and a subset of documents is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful for finding documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Applying the same approach to multiple languages show that our system generalizes to a variety of languages.
384073	Perpetual consistency improves image retrieval performance An ideal retrieval system should retrieve images that satisfy the user's need, and should, therefore, measure image similarity in a manner consistent with human's perception. However, existing computational similarity measures are not perceptually consistent. This paper proposes an approach of improving retrieval performance by improving the perceptual consistency of computational similarity measures for textures based on relevance feedback judgments.
384074	Intelligent object-based image retrieval suing cluster-driven personal preference learning This paper introduces a personalization method for image retrieval based on the learning of personal preferences. The proposed system indexes objects based on shape and groups them into a set of clusters, or prototypes. Our personalization method refines corresponding prototypes from objects provided by the user in the foreground, and simultaneously adapts the database index in the background.
384075	Construction of a hierarchical classifier schema using a combination of text-based and image-based approaches Web document hierarchical classification approaches often rely on textual features alone even though web pages include multimedia data. We propose a new hierarchical integrated web classification approach that combines image-based and text-based approaches. Instead of using a flat classifier to combine text and image classification, we perform classification on a hierarchy differently on different levels of the tree, using text for branches and images only at leaves. The results of our experiments show that the use of the hierarchical structure improved web document classification performance significantly.
384080	A method based on the chi-square test for document classification We introduce a method for document classification based on using the chi-square test to identify characteristic vocabulary of document classes.
384083	Query clustering using content words and user feedback Query clustering is crucial for automatically discovering frequently asked queries (FAQs) or most popular topics on a question-answering search engine. Due to the short length of queries, the traditional approaches based on keywords are not suitable for query clustering. This paper describes our attempt to cluster similar queries according to their contents as well as the document click information in the user logs.
384086	Modifications of Kleinberg's HITS algorithm using matrix exponentiation and web log records An abstract is not available.
384090	Cite me, cite my references?: (Scholarly use of the ACM SIGIR proceedings based on two citation indexes) A three-part study was designed to document Internet use in scholarly research, using the Annual SIGIR Conference Proceedings from 1997 through 1999. The results suggest an increasing trend toward electronic self-publishing. Furthermore, while electronic availability did not insure that one would be cited, the most highly cited articles were available on the "free" web. The study also found that electronic availability has not, in most cases, decreased the length of time between publication and citation.
384091	iFind: a web image search engine An abstract is not available.
384092	Building interoperable digital library services: MARIAN, open archives, and the NDLTD In this demonstration, we present interoperable and personalized search services for the Networked Digital Library of Theses and Dissertations (NDLTD). Using standard protocols and software, including those specified by the Open Archives Initiative (OAI), distributed sites can share metadata easily. On top of these harvesting protocols, we implement a union collection of theses managed by the MARIAN digital library system. Our demonstration covers aspects of NDLTD, OAI, and MARIAN.
384093	AUTINDEX: an automatic multilingual indexing system An abstract is not available.
384094	Does visualization improve our ability to find and learn from internet based information? An abstract is not available.
384095	The HySpirit retrieval platform An abstract is not available.
384096	Distributed resource discovery and structured data searching with Chesire II This demonstration will show describe the construction and application of Cross-Domain Information Servers using features of the standard Z39.50 information retrieval protocol[Z39.50]. The system is currently being used to build and search distributed indexes for databases with disparate structured data (SGML and XML). We use the Z39.50 Explain Database to determine the databases and indexes of a given server, then use the Z39.50 SCAN facility to extract the contents of the indexes. This information is used to build collection documents that can be retrieved using probabilistic retrieval algorithms.
384097	Searching the deep web: distributed explorit directed query applications In 1999 a directed query distributed search engine was integrated into a new Department of Energy Virtual Library of Energy Science and Technology. Millions of pages of government information across multiple agencies were made immediately searchable via one query, setting the stage for the development of a variety of interagency initiatives and applications.
384098	CROWSE: a system for organizing repositories and web search results An abstract is not available.
384099	MS read: user modeling in the web environment MS Read is a prototype application implemented as an extension of the Web Browser that creates an evolving model of the users topic of interest. It uses that model to analyze documents that are accessed while searching and browsing the Web. In the presented version of MS Read the model is used to highlight topic related terminology in the documents. MS Read model of the user need is created by applying natural language processing to search queries captured within the Browser and to topic descriptions explicitly provided by the user while browsing and reading documents. It is semantically enhanced using linguistic and custom knowledge resources.
636814	The Semantic Binary Relationship Model of information The Semantic Binary Relationship Model (SBRM) is a first-order formalism which combines an organisationally simple basis (i.e. binary relationships) with the capabilities of semantic networks and logical integrity and deduction rules. The aim is to permit the efficient modelling of practical enterprises In a DBMS context, whilst accommodating the requirements of knowledge-based systems. The theoretical foundations of the SBRM are described, with particular attention to inheritance hierarchies and rule representation. The low-level unit of SBRM information is the triple. A 4Mbyte associatively-accessed triple store is being constructed, and will form the heart of a smart information machine based on the SBRM.
636815	Shared processing with an advanced intelligent terminal We have built a prototype distributed information retrieval system known as TBIRD, based on an inverted file and shared between a personal computer, acting as an advanced intelligent terminal, and a timeshared mainframe. It was developed to study the response and cost in comparison with a conventional system based on an unintelligent terminal. It is shown, by the transfer of most of the processing to the personal computer, that the computing costs can be reduced by a substantial factor and that the response time need not be degraded except when the mainframe is lightly used or when the communications channel between the processors is slow (< 2,400 bps).
636816	Vector space model of information retrieval: a reevaluation In this paper we, in essence, point out that the methods used in the current vector based systems are in conflict with the premises of the vector space model. The considerations, naturally, lead to how things might have been done differently. More importantly, it is felt that this investigation will lead to a clearer understanding of the issues and problems in using the vector space model in information retrieval.
636817	Development of the BDS online information retrieval system Beijing Document Service(BDS) is a new-type information service providing online information retrieval service mainly in Beijing and other cities besides. This paper presents the background, goals, design considerations of the BDS system and what have been done in achieving the goal of loading the NTIS Bibliographic Data Base into the system and providing its online service to the public. System appraisal is covered and in light of problems experienced by BDS, possible approaches of how modernized information retrieval systems might be developed in China are also discussed.
636818	A global approach to record clustering and file reorganization We present an integrated method for record clustering and reorganization which can be applied to any set of queries whose frequencies of request are known. The clustering algorithm works by splitting and merging current clusters and, furthermore, produces a new assignment of these clusters to pages in secondary storage. The reorganization algorithm is an on-line, incremental procedure for allocating the records to their new physical locations such that the number of pages swapped in and out of the memory buffer is as small as possible.
636819	A document-document similarity measure based on cited titles and probability theory, and its application to relevance feedback retrieval The use of cited title terms of a scientific document for automatic indexing is explored. It offers a means of index term selection as well as term relevance weighting, based on author-provided relevance information and Bayes Theorem as in probabilistic retrieval. The latter quantitative consideration leads to a new measure of document-document similarity measure which is shown to have importance both for initial search and in relevance feedback retrieval, by offering a choice of iterative strategies.Extension of the concept of cited title terms to citing title terms shows that these two approaches are compatible with the current two competing models of probability of relevance for document retrieval (Robertson <u>et al</u>. 1982), if a document can also be regarded as a query. Their term usage may therefore provide the necessary statistics for parameter estimation to test both theories.
636820	Two axioms for evaluation measures in information retrieval In this paper evaluation measures for information retrieval system outputs are investigated from a measurement theoretic point of view. Two axioms are introduced: the axiom of monoto-nicity and the Archimedian axiom. It is shown that the measures fullfilling these axioms are exactly the measures equivalent to some measure of the form ?a + ?d where a is the number of relevant retrieved documents and d is the number of nonrelevant not retrieved documents. Some consequences for retrieval tests are discussed.
636821	Monitoring and evaluation of information systems via transaction log analysis Transaction log analysis represents a powerful methodology which allows examination of both user commands and system responses when conducting an online information search. Machine-readable transaction log tapes from online catalogs are obtained and subsequently analyzed using stochastic pattern developments within parsed user sessions, mathematical models utilizing Markov chain analysis and the development of state transition probability matrices, which illustrate the probability of proceeding from one user or system state to another state. The objective of monitoring information systems and using transaction log analysis is to discover the extent to which systems are used and to determine the actual user patterns when conducting an information search. This in turn can aid in the evaluation of such systems and assist in the improvement of existing and future systems. Such analysis can assist in system design, while the predictive power of the methodology may allow real-time aids to be developed.As examples of the use of the methodology, patron use and system response patterns from several online public catalogs have been obtained by transaction log tapes. This paper presents an overview of the methodology, results obtained, and efforts that are being conducted within OCLC's Office of Research.
636822	Bridging the gap between AI and IR Information retrieval, in the broadest sense of the term, includes a concern with 'expert' or 'knowledge-based' systems and their potential future successors. It is unlikely that sophisticated systems of this sort can be developed in such a way as to use an entire natural language without the assistance of an advanced, unified theory of language and logic. The need for and probable character of such a theory are discussed.
636823	Knowledge based systems versus thesaurus: an architecture problem about expert systems design The use of expert systems (ES) within information retrieval systems (IRS) seems to be an interesting way, particularly for the query process. Nevertheless we must examine what knowledge we need. We think that the thesaurus may be the kernel of which knowledge : for this, we must define it larger than in classical IRS.After some recalls about what may be the principal features of a query ES, we discuss about the relationship between thesaurus and a query expert system. The problem is to determine if the thesaurus must be integrated within the knowledge base.In fact this choice is an architecture problem of the ES. We analyze, in parallel, the effects of this choice about thesaurus representation, ES functionnalities, ES architecture.The choice of an architecture depends on the goal searched: ie a general IR expert system able to handle a set of thesauri (independent thesaurus) or a specialized IR expert system which can be very performant but strongly tied to a specific area (integrated thesaurus).
636824	Some remarks about the inference techniques of RESEDA, an intelligent information retrieval system The aim of this paper is to provide some details about the inference procedures of RESEDA, an "intelligent" Information Retrieval system using techniques borrowed from Knowledge Engineering. A RESEDA prototype has been operational for over a year : amongst its characteristics are a "case grammar" like knowledge representation language, intensive use of temporal data, use of the notion of "type", automatic generation of logical links, etc. First, the paper makes it clear what is meant in RESEDA by "level zero inference". Subsequently, it provides an informal description of the two kinds of high level inference operation, relying on information in the rule base, that are implemented in the system : these are known as "transformations" and "hypotheses". Finally, the article describes in some detail the computational structure of the "machines" which enable RESEDA's inference engine to execute this type of high level operation.
636825	Situational nearness in intellectual data bases The notions of a situation and a distance between situations in a narrow subject domain are introduced. An approach to the construction of an intellectual data base is suggested. The main task of this data base is to find situations which are near to the one fixed by user. The theory of finite groups is considered as an example.
636826	Dependency parsing for information retrieval This paper describes the development of a parser based on the Moulton and Robinson (1981) dependency theory of syntax, and several strategies by which we are attempting to apply the outputs of this parser to the processes of Information Retrieval. We first discuss the limits of present Information Retrieval theory and the potential benefits of linguistic analysis for Information Retrieval. Next we briefly present the Moulton and Robinson theory, contrast it to rewrite rule based theories, and outline its general advantages as an approach to natural language processing. Next we describe the parser we have implemented based on the Moulton and Robinson theory, and some of the implementation issues we have addressed. Finally, we discuss several strategies by which this parser could be applied to Information Retrieval, and the problems involved in this application.
636827	Computerised information retrieval systems for open learning The paper starts with a theoretical consideration of the requirements for a computerised information retrieval system to aid open learning within an educational establishment. The requirements for such a system include consideration of the need to fulfill information retrieval objectives and also educational objectives. These requirements are then considered in the context of the theoretical information retrieval work which has been carried out by Belkin and others and takes into account the representation of the user's anomalous state of knowledge. The paper then considers the practical problems of trying to implement such a system. Attention is focussed on the use of current and developing information technology to fulfill both information retrieval and educational objectives. It is shown that current systems as exemplified by PRESTEL, DIALOG, BROWSE and STAF individually will not fulfill the requirements of this system. However, in combination these types of systems should be quite suitable. Another solution is the use of an expert system. The paper also considers the use of an expert system to "replace" the traditional teacher.
636828	Computing text constituency: an algorithmic approach to the generation of text graphs An algorithm for text summarization (automatic abstracting) is presented which constitutes the text condensation component of TOPIC, a knowledge-based text information system. Based on the results of text parsing knowledge representation structures of text segments are evaluated in order to determine dominant concepts. By means of an interpretation schema dominant concepts are related in terms of thematic units indicative of the topic(s) of the text segment under consideration. The mutual compatibility of topics of adjacent text segments is determined and corresponding text constituents are constructed. Finally, a text graph is generated linking appropriate text constituents on various levels of text constituency. Accordingly, facilities for text-oriented information retrieval will be based on the manipulation of these text graphs.
636829	MARS: a retrieval tool on the basis of morphological analysis In this paper we present the system MARS which has been designed for potential database users working with retrieval systems. It is an instrument that is to assist the retriever in specifying the search queries. MARS is an aid for increasing termprecision as well as for finding previously unknown terms which are morphologically related to user search terms. This is achieved by way of linguistic procedures as opposed to conventional mechanical operations such as truncation etc. The morpheme lexicon and the morphological decomposition procedures are described briefly. MARS was put to the test in a real-world situation the results of which will be presented.
636830	Term conflation for information retrieval This paper describes two experiments concerned with term conflation for information retrieval, and the CATALOG retrieval system designed utilizing the results of the experiments. The experiments performed here has as their aim 1) finding a theoretical basis and method for maximizing the effect of conflation, and 2) determining if conflation can be automated with no loss of system performance.Experimental results indicate that,1. Experienced searchers generally truncate terms at root morpheme boundaries. When searchers do not truncate at root boundaries, the deviations are small.2. Small deviations from root boundaries do not significantly affect retrieval performance.3. There is no significant performance difference between automatic conflation and manual conflation carried out by experienced searchers.4. Based on 3, term conflation can be automated in a retrieval system with no average loss of performance, thus allowing easier and user access to the system.A retrieval system incorporating the information in 4 is described, and shown to be feasible.
636831	Retrieval test evaluation of a rule based automatic indexing (AIR/PHYS) The automatic indexing system AIR/PHYS and its evaluation by means of a retrieval test with 309 requests and 15,000 documents is described. First, the underlying conception of a rule based approach is given which is suited to the task of a controlled-vocabulary indexing of even large subject fields. Preconditions, performance and results of the retrieval test are described, including first results of retrieval runs with weighted automatic indexing.
636832	The automatic extraction of words from texts especially for input into information retrieval systems based on inverted files The automatic extraction of words from texts to form the input for information retrieval systems based on inverted files is partly considered on a theoretical basis, and partly in relation to experience gained from developing what has become an operational system. This system was developed to operate on abstracted texts, but is being modified to handle more extended texts either for input into an inverted file or as a stage in creating pre-coordinate indexes. The system is capable of handling compound words, homographs, and synonyms and identifying particular forms of text (such as authors) on the basis of what are termed semantic markers.
636833	Advances in a Bayesian decision model of user stopping behavior for scanning the output of an information retrieval system The formal modeling of information storage and retrieval systems has been an important element in the analysis and design of these systems. The retrieval mechanism has been viewed as a probablistic decision problem, often involving utilities. One key element is the evaluation of such retrieval systems. In this paper, we focus on the impact of the stopping rule, which determines when the user chooses to stop scanning the list of records retrieved in response to a given query. We shall first trace the evolution of the modelling and use of the stopping rule approach. Then, we shall briefly report on some recent results in our attempt to better model the generation of stopping rules.
636670	Information technology and the science of information An abstract is not available.
636671	A term weighting model based on utility theory An abstract is not available.
636672	A comparison of two weighting schemes for Boolean retrieval An abstract is not available.
636673	Probabilistic models of indexing and searching An abstract is not available.
636674	A performance evaluation of similarity measures, document term weighting schemes and representations in a Boolean environment An abstract is not available.
636675	Information retrieval theory and design based on a model of the user's concept relations An abstract is not available.
636676	Conceptual information retrieval An abstract is not available.
636677	Message extraction through estimation of relevance An abstract is not available.
636678	Representation of knowledge in a legal information retrieval system An abstract is not available.
636679	Retrieving time information from natural-language texts An abstract is not available.
636680	The automatic generation of literature abstracts: an approach based on the identification of self-indicating phrases An abstract is not available.
636681	Establishing a basis for mapping natural-language statements onto a database query language An abstract is not available.
636682	The fact database: a system based on inferential methods An abstract is not available.
636683	Methods for the administration of textual data in database systems An abstract is not available.
636684	Problems in the simulation of bibliographic retrieval systems An abstract is not available.
636685	Measurement-theoretical investigation of the MZ-metric An abstract is not available.
636686	Comparative analysis of hardware versus software text search An abstract is not available.
636687	An associative file store using fragments for run-time indexing and compression An abstract is not available.
636688	A backend machine architecture for information retrieval An abstract is not available.
636689	Browsing through databases An abstract is not available.
636690	A probabilistic algorithm for nearest neighbour searching An abstract is not available.
636691	A model of a document-clustering-based information retrieval system with a Boolean search request formulation An abstract is not available.
636692	'Memex' as an image of potentiality in information retrieval research and development An abstract is not available.
636693	Where do we go from here? An abstract is not available.
636714	The growing crisis of traditional information retrieval systems: what is to follow? An abstract is not available.
636715	FAKYR: a method base system for education and research in information retrieval An information retrieval system FAKYR is described which incorporates a variety of methods for organizing and retrieving information and for evaluating retrieval effectiveness. The system has been developed in order to support education and research in the area of information retrieval. With respect to this purpose FAKYR is a comfortable and large method data base.
636716	LIARS: a software environment for testing query processing strategies This paper describes the Louisiana Information Access and Retrieval System, LIARS, a software system which provides an environment within which various strategies for query processing (and, to a certain extent, document indexing) can be empirically tested.
636717	The implementation of a document retrieval system The significant advances made in theoretical and experimental research in information retrieval have many implications for system design. One possible design for a document retrieval system based on these advances is presented. A major part of this system design has been implemented as a bibliography filing and retrieval system for the Computer Science department at the University of Massachusetts. The implementation issues considered here are functionality, user interface and file organization. The main point of this implementation was to demonstrate that an efficient, effective and flexible system can be constructed using modern techniques.
636718	An intelligent terminal for implementing relevance feedback on large operational retrieval systems Research has shown that document retrieval based on weighting functions and incorporating relevance feedback may be more effective than retrieval based on Boolean combinations. These novel methods have not been adopted by the large operational systems. In this paper, a software implementation of an 'intelligent terminal' is described. It overcomes many of the objections to non-Boolean retrieval. It is linked to an operational database via EURONET, and includes such features as the weighting of search terms, the construction and submission of search statements in the Common Command Language of EURONET, and the use of relevance feedback information to improve retrieval.
636719	Messidor: a distributed information retrieval system MESSIDOR is an interactive retrieval system. It differs from current systems in that it allows the <u>simultaneous search of several bibliographic databases</u>. The databases may be on different sites such as ESA in Frascati, TELESYSTEMES in Sophia Antipolis... The databases may use different query languages such as QUEST, MISTRAL... However a user converses with the system MESSIDOR with <u>a single language</u>, the MESSIDOR language.The system's prototype is implemented on a MICRAL 80--30 micro-computer.
636720	Adapting a data organization to the structure of stored information A data organization for information retrieval (IR) systems is described which uses the structures imposed on the stored information. Trees are used as the main structure of data as information contents are often hierarchically structured (e.g. classifications, thesauri). However, these trees have been expanded to pseudo networks by so-called cross connecting paths. So-called data connecting paths link the information structures and the main data file. Terms occurring in the query formulation may be weighted. These weights are interpreted and then used by both the retrieval and ranking algorithm. One of the paramount problems is how to combine weighted query terms. Since the well-known IR schemes (Boolean retrieval, fuzzy retrieval etc.) do not work in our environment, a specific IR model was developed which allows to deduct suitable query and ranking evaluation algorithms.
636721	On the architecture of a system integrating data base management and information retrieval The data model, i.e. data structures and operations needed for a system integrating the management of formated textual data (DBMIRS) are discussed. It is investigated how this data model fits into the ANSI-SPARC three schema architecture for data base management systems. The conclusion is that the DBMIRS should be regarded to be a new external data model. This would require only small changes to the concepts discussed so far for the conceptual and internal level. The advantages of this approach are:- The dichotomy between IRS and DBMS does not further exist.- The main part of the software can be used for IR systems as well as DB systems.- IR research can concentrate on the problems inherent in the retrieval of texts, e.g. automatic indexing, relevance feedback techniques.
636722	Probabilistic approaches to the document retrieval problem An abstract is not available.
636723	The unified probabilistic model for IR An abstract is not available.
636724	Explanation and generalization of vector models in information retrieval An abstract is not available.
636725	Incorporation of relevance feedback into Boolean retrieval systems An abstract is not available.
636726	An evaluation of term dependence models in information retrieval In Practical retrieval environments the assumption is normally made that the terms assigned to the documents of a collection occur independently of each other. The term independence assumption is unrealistic in many cases, but its use leads to a simple retrieval algorithm. More realistic retrieval systems take into account dependencies between certain term pairs and possibly between term triples. In this study, methods are outlined for generating dependency factors for term pairs and term triples and for using them in retrieval. Evaluation output is included to demonstrate the effectiveness of the suggested methodologies.
636727	A decision theory approach to optimal automatic indexing A decision theory approach to the development of retrieval systems is presented. Within this framework, optimalindexing is defined. Both the searching and the indexing problem turn out to have a common structure which is described using the concept of a 'recognition problem'. A knowledge based approach to an approximately optimalindexing, strictly related to the information need of user is outlined. The theory and the used approximation methods are illustrated by a brief description of WAI/AIR projects and some of their results.
636728	Simulation of bibliographic retrieval databases using hyperterms An abstract is not available.
636729	Techniques for measuring the stability of clustering: a comparative study Among the significant factors in assessing the suitability of a clustering technique to a given application is its stability; that is, how sensitive the algorithm is to perturbations in the input data. A number of techniques that appear to be suitable for measuring the stability of clustering have been published in the literature. The details about each of these measures, such as a description of the steps involved in their computation and an identification of precisely what they measure, are presented. These measures are considered in the context of analysing the stability characteristics of clustering techniques and are compared using a framework developed for this purpose. The question of generalizing some of these measures is addressed and the measures are also analyzed to identify conditions under which they can be reduced to one another.
636730	Retrieval of abstracts by analogy An abstract is not available.
636731	From research to application: the cite natural language information retrieval system Large operational information retrieval systems typically employ inverted file structures and Boolean logic operators for efficient text retrieval. These systems require considerable user training for effective use. As a consequence, searching is commonly performed by professional intermediaries on behalf of end users.By contrast, many small scale experimental retrieval systems incorporate desirable user interface features, such as natural (English) language querying, ranked output and relevance feedback.The author describes the design and implementation of a natural language search interface to MEDLINE, the National library of Medicines largest and most heavily used data base. The CITE (Current Information Transfer in English) prototype system is a large-scale, weighted logic information retrieval system with natural language query input, ranked search output, dynamic user feedback and automatic associative vocabulary mapping capahilities.
636732	Machine intelligence vs. machine-aided intelligence in information retrieval: a historical perspective An abstract is not available.
636733	Information retrieval by voice input and output Voice recognition and synthesis will become increasinglyimportant in the 1980's for both data input and output to many computer systems. However, they are still only successful for special applications where the syntax of the language is precisely defined and the number of words in the language is limited. Such a case is a modified form of the Query Language for an Information Retrieval System. Some early experience with a micro-based Information Retrieval system developed at Belfast, called Micro-BIRD, shows that it is not difficult to build a simple voice interface at low cost, but its usefulness is still limited with current technology.
636734	Is text compression by prefixes and suffixes practical? One approach to text compression is to replace high-frequency variable-length fragments of words by fixed-length codes pointing to a <u>compression table</u> containing these high-frequency fragments. It is shown that the problem of optimal fragment compression is NP-hard even if the fragments are restricted to prefixes and suffixes. This seems to be a simplest fragment compression problem which is NP-hard, since a polynomial algorithm for compressing by prefixes only (or suffixes only) has been found recently. Various compression heuristics based on using both prefixes and suffixes have been tested on large Hebrew and English texts. The best of these heuristics produce a net compression of some 37% for Hebrew and 45% for English using a prefix/suffix compression table of size 256.
636806	Framework for the development of an experimental mixed-mode message system We describe a framework for the development of a mixed-mode message system for an office environment. Messages may be composed of attributes, text, images, and voice. Message retrieval is based on content. We discuss several issues related to the development of such systems. Text retrieval techniques are important for content retrieval in this environment.
636807	Evaluation of access methods to text documents in office systems This paper compares two different approaches for indexing archived text documents. The first approach is based on inversion of words in the text, the second on the generation of a signature file representing the text content. A system reflecting the word inversion approach is compared against two systems reflecting the signature scanning approach and using, alternatively, superimposed coding and the concatenation of word signatures. Performances are estimated using analytical models of these systems. Characteristics are evaluated in function of office environment requirements. The evaluations derive from a model for estimating the statistical parameters of text archives.This work has been partially developed as part of the EEC ESPRIT project on "Mixed-Mode Message Filing System" in the Office Systems area.
636808	An interactive database end user facility for the definition and manipulation of forms A system is presented which makes possible the definition and manipulation of views by users who are not familiar with the overall organization of the database. The method used is interactive "conversation" and the user's view of data is an "abstract form" represented by a set of hierarchically related dataitems. A view is specified only in terms of a set of dataitems and the mappings required to materialize the data are automatically generated as the result of this conversation. The form becomes then the basis for accessing the database. Queries are submmited by specifying selection conditions on the fields of a form. Selected occurrences are retrieved from the database and may be scanned as conventional paper forms.
636809	Nested transactions in a combined IRS-DBMS architecture The possibility to put an Information Retrieval System (IRS)on top of a Data Base Management System (DBMS) is investigated with respect to concurrency control and recovery. The simple mapping of one IRS transaction to one DBMS transaction is analyzed and found to be unsatisfactory. Therefore, the idea of generating a sequence of DBMS transactions for a single IRS transaction is discussed. This notion of "nested" transactions assumes standard concurrency control technology but is applied twice: in the underlaying DBMS layer and in the IRS layer. The consequence is that multi-user control and recovery is also necessary in the IRS layer but it utilizes the concurrency control and recovery function of the DBMS.
636810	A semantic model and schema notation for bibliographic retrieval systems Logical models or schema are used to represent the entities, relationships, and transformations of an information system. The relational model and the relational algebra have been developed to perform this function for database systems, but until recently only a few writers (e.g., Crawford, MacLeod, Schek) have studied logical models for bibliographic retrieval systems. This paper examines an extension of the relational model, called a semantic model, as a schema for bibliographic systems. An alternative notation is suggested for the semantic model, based on the Warnier/Orr diagram. An experimental semantic model user interface, which has been developed for an experimental microcomputer information retrieval system, is briefly described.
636811	The use of adaptive mechanisms for selection of search strategies in document retrival systems A document retrieval system can incorporate many types of flexibility. One example of this is the ability to choose a search strategy that is appropriate for a particular user and query. This paper investigates the use of adaptive mechanisms to control the selection of search strategies. The experimental results indicate that, although an adaptive mechanism is capable of learning the appropriate response in simple situations, there are serious problems with using them to make complex decisions in a document retrieval system.
636812	Query enhancement by user profiles We describe a theoretical model and an on-going series of experiments aimed at a priori query enhancement. The model presents a synthesis of concepts from retrospective and current awareness retrieval systems, employing the user profile as a factor in interpreting a query. It is expected that this will provide a more personalized response to queries.
636813	The Utah Text Retrieval Project: a status report The Utah Text Retrieval Project addresses a number of areas in information retrieval, including basic system structure, user interfaces integrating information retrieval with word processing, indexing techniques, and the use of specialized backend processors. Although the work on the development of a high-speed text search engine is generally the best known, probably the most exciting aspect of the project is the message-based architecture, which provides an adaptable testbed for information retrieval techniques. It can support a variety of index and search strategies, while instrumenting their performance so that they can be accurately compared in an identical environment.This paper describes the goals and design decisions for the Utah Retrieval System Architecture (URSA). It discusses the prototype system's features and limitations, and the changes that will be made to produce the production version.
62459	Optimum probability estimation based on expectations Probability estimation is important for the application of probabilistic models as well as for any evaluation in IR. We discuss the interdependencies between parameter estimation and other properties of probabilistic models. Then we define an optimum estimate which can be applied to various typical estimation problems in IR. A method for the computation of this estimate is described which uses expectations from empirical distributions. Some experiments show the applicability of our method, whereas comparable approaches are partially based on false assumptions or yield estimates with systematic errors.
62461	Concept based retrieval in classical IR systems This paper describes some aspects of a project with the aim of developing a user-friendly interface to a classical Information Retrieval (IR) System in order to improve the effectiveness of retrieval. The character by character approach to IR has been abandoned in favor of an approach based on the meaning of both the queries and the texts containing the information to be sought. The concept space, locally derived from a thesaurus, is used to represent a query as well as documents retrieved in atomic concept units. Dependencies between the search terms are taken into account. The meanings of the query and the retrieved documents (results of Elementary Logical Conjuncts (ELCs)) are compared. The ranking method on the semantical level is used in connection with existing data of a classical IR system. The user enters queries without using complex Boolean expressions.
62465	Coefficients of combining concept classes in a collection This report considers combining information to improve retrieval. The vector space model has been extended so different classes of data are associated with distinct concept types and their respective subvectors. Two collections with multiple concept types are described, ISI-1460 and CACM-3204. Experiments indicate that regression methods can help predict relevance, given query-document similarity values for each concept type. After sampling and transformation of data, the coefficient of determination for the best model was .48 (.66) for ISI (CACM). Average precision for the two collections was 11% (31%) better for probabilistic feedback with all types versus with terms only. These findings may be of particular interest to designers of document retrieval or hypertext systems since the role of links is shown to be especially beneficial.
62467	A cluster-based approach to thesaurus construction The importance of a thesaurus in the successful operation of an information retrieval system is well recognized. Yet techniques which support the automatic generation of thesauri remain largely undiscovered. This paper describes one approach to the automatic generation of global thesauri, based on the discrimination value model of Salton, Yang, and Yu and on an appropriate clustering algorithm. This method has been implemented and applied to two document collections. Preliminary results indicate that this method, which produces improvements in retrieval performance in excess of 10 and 15 percent in the test collections, is viable and worthy of continued investigation.
62469	Towards interactive query expansion In an era of online retrieval, it is appropriate to offer guidance to users wishing to improve their initial queries. One form of such guidance could be short lists of suggested terms gathered from feedback, nearest neighbors, and term variants of original query terms. To verify this approach, a series of experiments were run using the Cranfield test collection to discover techniques to select terms for these lists that would be effective for further retrieval. The results show that significant improvement can be expected from this approach to query expansion.
62470	The automatic indexing system AIR/PHYS - from research to applications Since October 1985, the automatic indexing system AIR/PHYS has been used in the input production of the physics data base of the Fachinformationsentrum Karlsruhe/West Germany. The texts to be indexed are abstracts written in English. The system of descriptors is prescribed. For the application of the AIR/PHYS system a large-scale dictionary containing more than 600 000 word-descriptor relations reap. phrase-descriptor relations has been developed. Most of these relations have been obtained by means of statistical and heuristical methods. In consequence, the relation system is rather imperfect. Therefore, the indexing system needs some fault- tolerating features. An appropriate indexing approach and the corresponding structure of the AIR/PHYS system are described. Finally, the conditions of the application as well as problems of further development are discussed.
62473	Retrieval based on user behaviour This paper gives an overview of the ongoing research in the Active Data Bases project at the Vrije Universiteit, Amsterdam. In this project we are specifying and building a system that helps a user in his search for useful and interesting information in large, complex information systems. The system is able to do this, because it learns from the interaction about the users and the data it contains. The indications of the users are expressed in terms of interests in the data, which serve as building blocks for user and data models. These models are then used to improve the search for interesting data.
62475	Query processing in a heterogeneous retrieval network The concept of a large-scale information retrieval network incorporating heterogeneous retrieval systems and users is introduced, and the necessary components for enabling term-based searching of any database by untrained end-users are outlined. We define a normal form for expression of queries, show that such queries can be automatically produced, if necessary, from a natural-language request for information, and give algorithms for translating such queries, with little or no loss of expressiveness, into equivalent queries on both Boolean and term-vector type retrieval systems. We conclude with a proposal for extending this approach to arbitrary database models.
62477	Some measures and procedures for evaluation of the user interface in an information retrieval system Planning the evaluation of an information retrieval system involves two steps: first, a determination of performance descriptors and measures appropriate to the system objectives and, secondly, a development of an evaluation design which ensures the effect of variation in components of interest will be isolated and assessed in an unbiased fashion. This paper examines the question of retrieval system evaluation from the perspective of the user. It presents evaluation procedures which are appropriate to this perspective and which can be used to isolate the effect of variation in the user interface to the system. The general procedure is exemplified by an application to evaluation of an experimental OPAC interface.
62479	IR-NLI II: applying man-machine interaction and artificial intelligence conceptsto information retrieval This paper addresses the problem of building expert interfaces to information retrieval systems. In particular, the problem of augmenting the capabilities of such interfaces with user modeling features is discussed and the main benefits of this approach are outlined. The paper presents a prototype system called IR-NLI II, devoted to model by means of artificial intelligence techniques the human intermediary to information retrieval systems. The overall organization of the IR-NLI II system is presented, together with a short description of the two main modules implemented so far, namely the Information Retrieval Expert Subsystem and the User Modeling Subsystem. An example of interaction with IR-NLI II is described. Perspectives and future research directions are finally outlined.
62481	Intelligent support for interface systems This paper describes how a language for building interfaces to information systems, that is being developed by the Office of Research at OCLC, can be linked to an artificial intelligence environment, Poplog. A demonstration system, showing how Poplog could provide some intelligent support for a D interface, has been developed. It is suggested that this could form the basis for intelligent support for interface systems.
62482	A parallel multiprocessor machine dedicated to relational and deductive data bases Efficiency in databases is a major requirement. This paper presents some solutions to cope with this problem. One solution is to execute operations in parallel: this is done in the “Delta Driven Computer” DDC, which is a multiprocessor machine with distributed memory dedicated to relational and deductive databases. In DDC, relations are distributed among the nodes of the machine, and the data are processed asynchronously in each node. To do that in an efficient way, a coprocessor, specialized for relational operations, is also proposed. It is called &mgr;SyC, for “microprogrammable Symbolic Coprocessor”. This paper is divided into two parts. The first part describes DDC, presenting the architecture, the languages, and an original computational model. The second part describes &mgr;SyC, its architecture, instruction set and the data structures used at the &mgr;SyC level.
62483	Flexible selection among objects: a framework based on fuzzy sets Up to now, most of the retrieving systems are founded on a Boolean selection mechanism. It appears that this way of doing is not powerful enough to deal with some applications, especially when the size (number) of the results must be controlled. In that case, some kind of flexibility is needed in query expression. In this paper, we suggest the use of a fuzzy sets based approach. The basic principles of this approach are presented and compared to more conventional solutions providing only limited extensions. Moreover, the implementation aspects related to our approach are discussed to show that reasonable performances can be expected.
62485	The document management component of a multimedia data model We describe ESTRELLA a multimedia object oriented data model developed by MATRA. This model is based upon objects, classes (organized in a lattice) and functions (allow to dynamically implement operations on data and new data types). The valid states of the data base are described by a set of integrity constraints. We propose a document model capable to manage structured documents and to index them with a superimposed codes method. We present as well the associated data manipulation language with a navigational interface and content search operators
62487	Information retrieval using a singular value decomposition model of latent semantic structure In a new method for automatic indexing and retrieval, implicit higher-order structure in the association of terms with documents is modeled to improve estimates of term-document association, and therefore the detection of relevant documents on the basis of terms found in queries. Singular-value decomposition is used to decompose a large term by document matrix into 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination; both documents and terms are represented as vectors in a 50- to 150- dimensional space. Queries are represented as pseudo-documents vectors formed from weighted combinations of terms, and documents are ordered by their similarity to the query. Initial tests find this automatic method very promising.
62491	Retrieving documents by plausible inference: a priliminary study Choosing an appropriate document representation and search strategy for document retrieval has been largely guided by achieving good average performance instead of optimizing the results for each individual query. A model of retrieval based on plausible inference gives us a different perspective and suggests that techniques should be found for combining multiple sources of evidence (or search strategies) into an overall assessment of a document's relevance, rather than attempting to pick a single strategy. In this paper, we explain our approach to plausible inference for retrieval and describe some preliminary experiments designed to test this approach. The experiments use a spreading activation search to implement the plausible inference process. The results show that significant effectiveness improvements are possible using this approach.
62493	An outline of a general model for information retrieval systems This paper is a contribution to the construction of a general model for information retrieval. As in the paper of Van Rijsbergen ([RIJ86]), the implicit base in all information retrieval systems is considered as a logical implication. The measure of correspondence between a document and a query is transformed into the estimation of the strength (or certainty) of logical implication. The modal logics will show its suitability for representing the behavior of information retrieval systems. In existing Information Retrieval models, several aspects are often mixed. A part of this paper is contributed to separate these aspects to give a clearer view of information retrieval systems. This general model is also compared with some existing models to show its generality.
62494	French textual information systems: the contribution of extensional and intentional logics An abstract is not available.
62495	An information structure dealing with term dependance and polysemy An information structure (IS) that is regarded as a formal description of a domain of discourse is proposed. This IS is aimed at increasing the effectiveness of an information retrieval system. It is shown how the retrieval algorithm can take into account the term dependencies that are provided by the IS. Moreover, these term dependencies can be used by an automatic indexing procedure in order to interpret polysemic terms. The theoretical framework of our IS has some favorable properties. As a consequence, the construction and maintenance of such an IS is simpler than that of a thesaurus.
62496	Planning in an expert system for automated information retrieval Searching online databases requires an information retrieval strategy formalized in the EURISKO expert system. This search strategy is based on different kinds of planning: at the highest level a plan orders a linear and hierarchical planning for the request interrogation and a dynamic planning for the request modification. The recent development of the system has allowed to supply some new judgements on this approach.
62497	Conceptual representation for knowledge bases and << intelligent >> information retrieval systems This paper describes the “conceptual” Knowledge Representation Language (KRL) proper to an environment for the construction and use of large Knowledge Bases and/or “Intelligent” Information Retrieval Systems. In the KRL, we separate the treatment of the episodic memory (extensional, assertional data = “Snoopy is Charlie Brown's beagle”) from the treatment of the semantic memory (intensional, terminological data = A beagle is a sort of hound / a hound is a dog …). A compromise between an “object-oriented approach” and a “logic-oriented approach” is proposed for implementation purposes.
62498	Rough sets and information retrieval The theory of rough sets was introduced [PAWLAK82]. It allows us to classify objects into sets of equivalent members based on their attributes. We may then examine any combination of the same objects (or even their attributes) using the resultant classification. The theory has direct applications in the design and evaluation of classification schemes and the selection of discriminating attributes. Pawlak's papers discuss its application in the domain of medical diagnostic systems. Here we apply it to the design of information retrieval systems accessing collections of documents. Advantages offered by the theory are: the implicit inclusion of Boolean logic; term weighting; and the ability to rank retrieved documents. In the first section we describe the theory. This is derived from the work by [PAWLAK84, PAWLAK82] and includes only the most relevant aspects of the theory. In the second we apply it to information retrieval. Specifically, we design the approximation space, search strategies as well as illustrate the application of relevance feedback to improve document indexing. Following this in section three we compare the rough set formalism to the Boolean, vector and fuzzy models of information retrieval. Finally we present a small scale evaluation of rough sets which indicates its potential in information retrieval.
62499	Set oriented retrieval The broad way in which we look at how an IRS functions influences the types of questions we ask about it and the ways we try to improve performance. In the recent past, retrieval methodologies have been based on retrieving documents one at a time. In this paper we are introducing a set oriented view. We observe that this view is quite consistent with the single-document or sequential methods, and define a precise model to capture the set-oriented approach. We then examine a number consequences of the model, such as the limitations implied by a finite index vocabulary. Finally, we discuss various ways in which the set orientation can influence our thinking about IR.
62500	Compression of concordances in full-text retrieval systems The concordance of a full-text information retrieval system contains for every different word W of the data base, a list L(W) of “coordinates”, each of which describes the exact location of an occurrence of W in the text. The concordance should be compressed, not only for the savings in storage space, but also in order to reduce the number of I/O operations, since the file is usually kept in secondary memory. Several methods are presented, which efficiently compress concordances of large fulltext retrieval systems. The methods were tested on the concordance of the Responsa Retrieval Project and yield savings of up to 49% relative to the non-compressed file; this is a relative improvement of about 27% over the currently used prefix-omission compression technique.
62501	Active memory for text information retrieval A Symbolic Associative Processor (SAP), capable of supporting parallel Keyword Match and Record Match functions, is proposed to select and streamline textual data for information retrieval. Consequently, high volume text data could be analysed on-the-fly before being channelled to CPU, and thus, cushion the impact of Von Neumann bottleneck commonly experienced in applications requiring high I/O bandwidth. This paper identifies some of the system requirements to support text information retrieval using SAP with the aid of simplified examples.
62502	Access by content of documents in an office information system This paper presents the integration of retrieval functions of an Information Retrieval System, IOTA, in an Office Information Server. Besides the linear scanning of the text (using a software and a hardware filter), two access methods are proposed. The first one is based on a simple indexing of documents based on signatures. Here, texts are treated as character strings. We call this method Textual Search. The second one is based on the extention of Signature Methods for implementing the Indexing Relation of IOTA, where meaningful terms (noun groups, for example) are identified in the text together with grammatical information. We call this method of signature computation the Indexing-Term Signature. The resulting access method is called Semantic Search. We present the current experimentations using the SCHUSS hardware filter as a scanning accelerator and the results of different alternatives of implementation of these Retrieval functions .
62503	Development of a large, concept-oriented database for information retrieval The development of concept-oriented databases using AI knowledge representation schemes is proposed as a step towards improving the precision and recall of information retrieval systems. Currently underway is the augmentation of a 238,000 citation database, Chemical Abstracts (CA) Volume 105, by addition of detailed conceptual information in the form of frames and hierarchies. The initial text data is parsed using natural language processing (NLP) techniques to create frames describing the semantics of the index entries in the database, with the slots in the frames being pointers into a very large semantic network of conceptual objects (956,000 objects). To examine the resultant knowledge base (KB), a simple hypertext system is proposed, with the conceptual information serving as pathways to connect related citations .
62505	Integrated information retrieval for law in a hypertext environment A prototype information retrieval system for lawyers, Justus , has been developed on a Sun workstation to run in a Guide hypertext environment. The hypertext database is created automatically by Justus from machine readable versions of the ordinary printed texts, ideally the publisher's typesetting tapes. The database incorporates primary legal sources, such as statutes and cases, and secondary sources, such as textbooks and a dictionary. Initially, the lawyer may select any document in the system. From this initial document, he may access any other document, or part of any other document, to which reference is made. Reference selection is by a pointing device, such as a mouse. There is no limit on the number of selections that can be made, and no restrictions on the path through the system.
317885	The economics of search An abstract is not available.
317560	Applying user research directly to information system design (panel session) An abstract is not available.
511286	Introduction and perspectives for the 1971 ACM Information Storage and Retrieval Symposium An introduction and some prospectives are provided for the 1971 ACM Information Storage and Retrieval Symposium held at the University of Maryland on April 1 and 2, 1971. The symposium, sponsored by the University of Maryland, the National Aeronautics and Space Administration and the Special Interest Group on Information Retrieval (SIGIR) of the ACM, focuses on advances in techniques in the computer oriented technology of information retrieval. Early developments and the status of recent efforts in document retrieval, quesiton-answering and data management systems are reviewed briefly.
511288	The function of semantics in automated language processing This paper is a survey of some of the major semantic models that have been developed for automated semantic analysis of natural language. Current approaches to semantic analysis and logical inference are based mainly on models of human cognitive processes such as Quillian's semantic memory, Simmon's Protosynthex III and others. All existing systems and/or models, more or less experimental, were applied to a small subset of English. They are highly tentative because the definitions of semantic processes and semantically structured lexicons are not formulated rigorously. This is due mainly to the fact that it is unknown whether a unique, consistent hierarchization of the semantic features of language is possible.However, the models described are significant contributions to an unexplored field called semantics. The progressive development of a sophisticated, semantically based system for automated processing of natural language is a realistic goal. It should not be neglected, despite the fact that it is difficult to predict when this goal will be achieved.
511289	How features resolve syntactic ambiguity Ambiguity is a pervasive and important aspect of natural language. Ambiguities, which are disambiguated by context, contribute powerfully to the expressiveness of natural language as compared to formal languages. In computational systems using natural language, problems of properly controlling ambiguity are particularly large, partially because of the necessity to circumvent parsings due to multiple orderings in the application of rules.Features, that is, subcategorizations of parts-of-speech, constitute an effective means for controlling syntactic ambiguity through ordering the hierarchical organization of syntactic constituents. This is the solution adopted for controlling ambiguity in REL English, which is part of the REL (Rapidly Extensible Language) System. REL is a total software system for facilitating man/machine communications. The efficiency of processing natural language in REL English is achieved both by the detailed syntactic aspects which are incorporated into the REL English grammar, and by means of the particular implementation for processing features in the parsing algorithm.
511290	The converse natural language data management system: current status and plans This paper presents an overview of research in progress in which the principal aim is the achievement of more natural and expressive modes of on-line communication with complexly structured data bases. A natural-language compiler has been constructed that accepts sentences in a user-extendable English subset, produces surface and deep-structure syntactic analyses, and uses a network of concepts to construct semantic interpretations formalized as computable procedures. The procedures are evaluated by a data management system that updates, modifies, and searches data bases that can be formalized as finite models of states of affairs. The system has been designed and programmed to handle large vocabularies and large collections of facts efficiently. Plans for extending the research vehicle to interface with a deductive inference component and a voice input-output effort are briefly described.
511292	CUE: a preprocessor system for restricted, natural English CUE, an input interface system which permits the computer to utilize natural but restricted English as input, is presented. In addition, an experimental model for CUE, Proto-RELADES, which can "understand" and execute English sentences about the content of the library at IBM's Boston Programming Center is described. These sentences can be query, command, or conditional sentences. The linguistic component of the system is based on a transformational grammar of English that performs a full syntactic and semantic analysis of each input sentence and translates it into relevant computer operations. The capabilities and limitations of this system are described.
511293	Full text document retrieval: Hebrew legal texts (report on the first phase of the responsa retrieval project) A full text retrieval system was designed for the responsa literature, which is a large corpus of Hebrew legal cases. The unique problems of the data base --- mixture of Hebrew, Aramaic and vernaculars, lack of vowels and punctuation, extreme language inflection problems, homographs, existence of thousands of grammatical variants of any given keyword --- dictated development of new methods. Among them we list "grammatical synthesis", which synthesizes all grammatical variants of a given keyword; "Compact KWIC", which enables the user to have a glimpse of the nature of the search before having performed it; effective citation index imbedded in full text searches; and, in general, extensive use of both positive and negative feedback within a single search run. A number of searches performed on a relatively small data base gave in each case a recall of 100%. The average precision was 34%. A KWIC of strategic portions of retrieved documents usually enables a quick disposal of non-relevant material.
511295	Quantification in query systems Questions which involve 'all', 'every', 'some', or the indefinite article, pose some peculiar problems when presented to a computerized question-answering system where ambiguities cannot be tolerated. These problems vary from the nature of the correct answer in special cases to the very admissibility of the question itself. To deal with these problems it is convenient to divide questions into two classes---extensional questions whose answers are to name things or truth values, intensional questions whose answers are to give meanings. This paper examines extensional questions. For these, the interpretative problems arising with 'all' and 'every' can be solved by introducing a new kind of quantification, extensional universal quantification, that has the meaning of 'all F' together with a secondary meaning that the class F is not empty. Formal rules for this quantification are given, and it is shown that the so-called definite formulas (which explicate permissible queries) are closed under the new operator.
511296	The relational data file and the decision problem for classes of proper formulas The Relational Data File (RDF) of The Rand Corporation is among the most developed of question-answering systems. The "information language" of this system is an applied predicate calculus. The atomic units of information are binary relational sentences. The system has an inference-making capacity.As part of the actual construction and implementation of the RDF, a theory was developed by J. L. Kuhns to identify those formulas of the predicate calculus which represent the "reasonable" inquiries to put to this system. Accordingly, the classes of definite and proper formulus were defined, and their properties studied. The definite formulas share a semantic property Kuhns judged as necessarily possessed by a reasonable question to be processed by the RDF. The author has previously shown that the decision problem for the class of definite formulas is recursively unsolvable. The proper formulas are definite, and satisfy additional syntactic conditions intended to make them especially suitable for machine processing. The class of proper formulas depends on which logical primitives are employed. Different primitives give rise to different classes of formulas. A formula which can be effectively transformed into a proper equivalent is admissible. Kuhns conjectures that with respect to one particular class of proper formulas, all definite formulas are admissible. In the paper it is shown that the decision problem for several classes of proper formulas is solvable. The following results are established. Theorem 1: The class of proper formulas in prenex form on any complete set of connectives is recursive. Theorem 2: The class of proper formulas on ¬, ?, ? is recursive. Theorem 3: The class of proper formulas on ¬, ?, ? is recursive. Theorem 4: The class of proper formulas on ¬, ?, ?, ?, is recursive. Thus, there is a mechanical decision procedure which determines whether an arbitrary formula is a member of the class. It follows that the analogues of Kuhns' conjecture for these classes are false.
511298	Managing semantic data in an associative net This paper describes the design and implementation of a general associative net structure to be used in an interactive information system, and presents a scheme designed to manage large quantities of semantic data stored in a data base on disc. The associative-net-structured data base is functionally divided into two pools: the hierarchy pool and the linguistic pool. The network of items in the hierarchy pool represents the descriptive information about documents and the network of items in the linguistic pool represents the syntactic and semantic properties of the items in the hierarchy pool. Two search functions and a general search algorithm are presented in this paper. In the implementation, the data base is a regional data set on disc. Items and their associated labeled links are stored on disc tracks. The system establishes a directory to keep track of the items which have associated information stored on more than one track. The use of the directory eliminates unnecessary disc accesses and allows the system to move a proper track into core storage for data processing.
511299	A heathkit method for building data management programs One of the difficulties faced in implementing information management and retrieval systems is that each case seems to present its own special complexities. As a result information retrieval systems typically fall behind their programming schedule and have many bugs when delivered. In this paper a set of basic operations on types of files are defined. These operations are intended to fulfill the same role for information retrieval systems programmers that functions such as LOG(X) fill for mathematical applications programmers.. they should make the job very much easier. The file operations have been implemented as a run-time package written in FORTRAN IV and Burroughs Extended Algol. The approach has been used to develop three different information management systems; an APL interactive computing system, a generalized information retrieval system, and a specialized information retrieval system for map oriented data. These systems are described.
511300	File structure determination An approach to determining an appropriate file structure for a given application is presented, by outlining a methodology for comparing some important aspects of data management system performance. The aspect chosen for analysis is the processing time required to evaluate Boolean functions defined on data values contained within a file structure and select elements from the structure satisfying the expression.Two file structures are studied. The structures are each combinations of hierarchical and inverted file organizations, which differ in the use of the pointers contained in the inverted file. In one case they link a value to nodes corresponding to its occurrence in the data hierarchy and in the second they link a value to the entry which contains the node corresponding to an occurrence.Algorithms for processing within each of the structures are discussed. Each algorithm is then modeled, and approximating models developed for simulation of the algorithms.
511302	An approach to research in file organization Research in file organization is a problem in which the unstructured efforts of many individual researchers have not produced results commensurate with the effort expended. The paper briefly examines some of the reasons for this and suggests a structure consisting of a language and terminology for communication, a model of information processing systems and tools for studying and analyzing problems related to such systems. The ISDOS project is outlined as a coordinated approach which may serve as the beginning in the development of a satisfactory structure.
511303	Quantitative evaluation of design tradeoffs in file systems The design of a file system has never been a simple nor a straightforward task because of its complexity. Heuristics and experience still play a major role in guiding the design process. To organize the entire design process in a more systematic manner, large scale simulation has proved to be an effective technique. The FOREM models developed during the past several years (specifically for the evaluation of file system designs) represent facilities of this type. This paper utilizes the FOREM model as the principal tool and presents a hypothetical design example dealing with many essential issues of the design process. Evaluation of designs of several other actual file systems are also being carried out and will be reported at a later date. Only through quantitative evaluation can each design decision be arrived at correctly and the possible tradeoffs be identified.
511304	Elements of the randomized combinatorial file structure A file structure designed to provide rapid, random access with minimum storage overhead is presented. Storage and retrieval are achieved by direct attribute combination-to-address transformation thereby negating the necessity for large file dictionaries or list-pointer structures. The attribute combination-to-address transformation is conceptually similar to key-to-address transformation techniques, but the transformation is not limited to operations on a single key but operates on the combination of several independent keys (or any subset of the combination) describing an item or request.A storage and retrieval system utilizing the combinatorial file structure is developed. Storage and retrieval results derived from a simulated document library of 4000 items are presented. The new file organization is shown to have marked value with respect to minimum storage overhead and high retrieval speed.
511305	A balanced tree storage and retrieval algorithm A storage and retrieval scheme which places items to be stored at the nodes of a binary tree is discussed. The tree is always balanced in a certain sense thus insuring that no excessively long search paths can exist. In addition to presenting the storage and retrieval algorithms, the deletion problem is also solved. The programming approaches involved yield a non-trivial case study of list-processing techniques. Finally, a cost analysis is given.
511307	Efficient utilization of limited access archival storage in a time shared environment The public storage in any time sharing system tends to continually grow. This necessitates the implementation of certain measures to maintain public storage. One of these possibilities is creation of an archival level of storage called "migrated" storage. Data that have not been referenced recently are moved or "migrated" to a less accessible level of external storage. Since these data are not accessed by the users directly, i.e., the data must be restored to public storage before being used, a certain variable length coding technique, viz., Huffman Coding, is used to compact and store these data. The ideas presented have been implemented on a version of TSS/360 Time Sharing System and are presently being used in a real environment. The overall compaction rate achieved was 3. 16 to 1. Further details on compaction rates and timings are also presented.
511308	Data compression techniques for economic processing of large commercial files The application of compact coding, differencing and other techniques to indexed sequential files is discussed. The effects on system performance are discussed and reductions of almost 80% in mass storage requirements for a particular file are reported.
511309	Optimal classification and its consequences A particular classification and retrieval model are considered. A notion is introduced which indicates the extent to which retrieval performance may be improved by a suitable choice of classification within the model. A method for determining the optimal performance for the model is outlined together with an algorithm for constructing the classification which allows this limit to be attained. A treatment of the mathematical preliminaries for a particular class of match function is given. The relevance of the analysis in research on information retrieval systems is discussed.
511311	Introduction to the Key Word In Context Index (KWIC) to the ACM IS & R symposium An abstract is not available.
62438	A look back and a look forward This paper is in two parts, following the suggestion that I first comment on my own past experience in information retrieval, and then present my views on the present and future.
62439	Experiments on incorporating syntactic processing of user queries into a document retrieval strategy Traditional information has relied on the extensive use of statistical parameters in the implementation of retrieval strategies. This paper sets out to investigate whether linguistic processes can be used as part of a document retrieval strategy. This is done by predefining a level of syntactic analysis of user queries only, to be used as part of the retrieval process. A large series of experiments on an experimental test collection are reported which use a parser for noun phrases as part of the retrieval strategy. The results obtained from the experiments do yield improvements in the level of retrieval effectiveness and given the crude linguistic process used and the way it was used on queries and not on document texts, suggests that the approach of using linguistic processing in retrieval, is valid.
62440	The use of anaphoric resolution for document description in information retrieval This study investigated two hypotheses concerning the use of anaphors in information retrieval. The first hypothesis, that anaphors tend to refer to integral concepts rather than to peripheral concepts, was well supported. Two samples of documents, one in psychology and the other in computer science, were examined by subject experts who judged the centrality of phrases which were referred to anaphorically. The second hypothesis, that various term weighting schemes are affected differently by anaphoric resolution, was also well supported. It was found that schemes which incorporate document length into the calculations produce much smaller increases in term weights for terms occurring in anaphoric resolutions than do those which do not consider document length. It is concluded that although anaphoric resolution has potential for better representing the “aboutness” of a document, care must be taken in choosing both the anaphoric classes to be resolved and the term weighting schemes to be used in measuring a document's topicality.
62441	A french text recognition model for information retrieval system An abstract is not available.
62442	Natural language techniques for intelligent information retrieval Neither natural language processing nor information retrieval is any longer a young field, but the two areas have yet to achieve a graceful interaction. Mainly, the reason for this incompatibility is that information retrieval technology depends upon relatively simple but robust methods, while natural language processing involves complex knowledge-based systems that have never approached robustness. We provide an analysis of areas in which natural language and information retrieval come together, and describe a system that joins the two fields by combining technology, choice of application area, and knowledge acquisition techniques.
62443	Correction of phonographic errors in natural language interfaces In this paper, we point out that, in applications available to the general public, and/or natural language interfaces, the correction of phonographic errors (which are competence errors) is far more important than the correction of typographical errors (which are simply performance errors). Many studies aimed at the correction of typographical errors have been carried out, but relatively few tackle the problem of phonographic correction, and they are generally based on more or less ad hoc methods. We propose a mathematical framework for phonographic correction by defining a similarity relation between phonetically related substrings and a dissimilarity index between strings. We also provide a simple and efficient algorithm for recognizing words in dictionaries from misspelt inputs including both typographical and phonographic errors.
62444	Precedental data bases: how and why they are worked out and used The concept of a “precedental data base” is introduced. It is a linguistic data base consisting of a dictionary of lexical patterns (clishes) and a dictionary of discourses. Some algorithms for textual information processing using precedental data bases are discussed in detail. These systems are installed on mainframe and minicomputers for test runs.
62445	How do the experts do it? The use of ethnographic methods as an aid to understanding the cognitive processing and retrieval of large bodies of text This paper explores an important problem in information retrieval: that of rapidly increasing amounts of full-text storage that is difficult to file and retrieve effectively. The author suggests that a possible avenue for improving full-text retrieval would include in-depth studies of the ways in which individual users cope with large amounts of written information, stored chiefly on paper in their offices. Relevant literature in cognitive psychology is reviewed and some recent and continuing studies are described that have used anthropological methods to approach this problem. It is argued that historians are a good group to study, due to their reliance on the examination and processing of texts, and the broad scope of their inquiries. Examinations of the ways in which this one group of information workers categorize documents could lead us to a better understanding of human problems in processing and retrieving textual information.
62446	On the nature and fuction of explanation in intelligent information retrieval We discuss the complexity of explanation activity in human-human goal-directed dialogue, and suggest that this complexity ought to be taken account of in the design of explanation in human-computer interaction. We propose a general model of clarity in human-computer systems, of which explanation is one component. On the bases of: this model; of a model of human-intermediary interaction in the document retrieval situation as one of cooperative model-building for the purpose of developing an appropriate search formulation; and, on the results of empirical observation of human user-human intermediary interaction in information systems, we propose a model for explanation by the computer intermediary in information retrieval.
62447	On the use of spreading activation methods in automatic information Spreading activation methods have been recommended in information retrieval to expand the search vocabulary and to complement the retrieved document sets. The spreading activation strategy is reminiscent of earlier associative indexing and retrieval systems. Some spreading activation procedures are briefly described, and evaluation output is given, reflecting the effectiveness of one of the proposed procedures.
62448	Knowledge representation, connectionism and conceptual retrieval Knowledge Representation (KR) systems provide support for Artificial Intelligence systems that reason about relationships between objects in their domains of expertise. Because of their support for inference, KR systems appear to have potential to enrich the kind of retrievals that IR systems might make. Ironically, however, the most useful KR systems are limited to reasoning based on a rigid notion of validity, and thus are awkward to use when relevant but inexact retrievals are desired. We have been exploring the potential of a “connectionist” model—the Boltzmann Machine—to overcome this limitation. We report on a number of experiments in which we use a connectionist simulator to support similarity-based reasoning in a frame representation. We draw some tentative, mixed conclusions on the potential for a union of KR, IR, and connectionism.
62449	BABEL: a base for an experimental library This report discusses the implementation of a knowledge base for a library information system. It is done using a typed logic programming language—LOGIN—where type inheritance is built in. The knowledge base is structured in a hierarchical taxonomy of library object classes where each class is represented in a FRAME style knowledge structure and inherits the properties of its parents, and where infrastructural inference rules have been established through typed Horn clauses. Also in this document, some programming techniques aimed at using the power of inheritance as taxonomic inference are discussed.
62450	ALLOY: an amalgamation of expert, linguistic and statistical indexing methods In this paper we report progress on the development of ALLOY, a system that simplifies automatic document indexing and retrieval by combining techniques from several different approaches: expert, linguistic and statistical. The system is being designed to allow a panel of experts to create an ALLOY system for a given field by providing the necessary input that ALLOY needs to automatically index documents and to set up a convenient user interface. The input provided by the experts includes a hierarchy of concepts and an expert dictionary. The amount of information that the panel must provide for given field is considerably less than the amount required to build a complete thesaurus or knowledge base about that field.
62451	Two learning schemes in information retrieval Two methods are given to improve weighting schemes by using relevance information of a set of queries. The first method is to estimate parameter values of two independence models in information retrieval — the binary independence model and the non-binary independence model. The parameters estimated here are used to calculate optimal weights for terms in a different set of queries. Performance of this estimation is compared to the inverse document frequency method, the cosine measure, and the statistical similarity measure. The second method is to learn optimal weights of the non-binary independence model adaptively by a learning formula. Experiments are performed on three different document collections CISI, MEDLARS, and CRN4NUL for both methods, and results are reported. Both methods show improvements compared to the existing weighting schemes. Experimental results show that the second method gives slightly better performance than the first one, and has simpler implementation.
62452	Linear structure in information retrieval Based on the concept of user preference, we investigate the linear structure in information retrieval. We also discuss a practical procedure to determine the linear decision function and present an analysis of term weighting. Our experimental results seem to demonstrate that our model provides a useful framework for the design of an adaptive system.
62455	Information retrieval using impression of documents as a clue Proposed here is an internal representation and mapping method for multimedia information in which retrieval is based on the impression documents are desired to make. A user interface design for a system using this method is also proposed. The proposed internal representation and mapping method represents each desired document impression as an axis in a semantic space. Documents are represented as points in the space. Queries are represented as subspaces. The proposed user interface design employs a method of visual presentation of the semantic space. For evaluation purposes, a prototype system has been developed. An image retrieval experiment shows that the proposed internal representation and mapping method and the user interface design provide effective tools for information retrieval.
62457	A utility-theoretic analysis of expected search length In this paper the expected search length, which is a measure of retrieval system performance, is investigated from the viewpoint of axiomatic utility theory. Necessary and sufficient criteria for the expected search length to be an ordinal scale and sufficient criteria that it is a ratio scale are given.
75336	In search of knowledge-based search tactics Knowledge-based search tactics are discussed in terms of their role in the functioning of a semantically-based search system for bibliographic information retrieval. This prototype system, EP-X, actively assists users in defining or refining their topics of interest. It does so by applying search tactics to a knowledge-base describing topics in a particular domain and a database describing the contents of individual documents. This paper reviews the empirical studies that lead to the two central concepts implemented in EP-X: Semantically-based search; Knowledge-based search tactics.
75337	Adaptive information retrieval: using a connectionist representation to retrieve and learn about documents AIR represents a connectionist approach to the task of information retrieval. The system uses relevance feedback from its users to change its representation of authors, index terms and documents so that, over time, AIR improves at its task. The result is a representation of the consensual meaning of keywords and documents shared by some group of users. The central focus goal of this paper is to use our experience with AIR to highlight those characteristics of connectionist representations that make them particularly appropriate for IR applications. We argue that this associative representation is a natural generalization of traditional IR techniques, and that connectionist learning techniques are effective in this setting.
75338	A neural network for probabilistic information retrieval This paper demonstrates how a neural network may be constructed, together with learning algorithms and modes of operation, that will provide retrieval effectiveness similar to that of the probabilistic indexing and retrieval model based on single terms as document components.
75339	Design of a browsing interface for information retrieval In conventional Boolean retrieval systems, users have difficulty controlling the amount of output obtained from a given query. This paper describes the design of a user interface which permits gradual enlargement or refinement of the user's query by browsing through a graph of term and document subsets. This graph is obtained from a lattice automatically generated from the usual document-term relation. The major design features of the proposed interface are the integration of menu, fill-in the blank and direct manipulation modes of interaction within the “fisheye view” [Furnas, 1986] paradigm. A prototype user interface incorporating some of these ideas has been implemented on a microcomputer. The resulting interface is well adapted to various kinds of users and needs. More experienced users with a particular subject in mind can directly specify a query which results into a jump to a particular vertex in the graph. From there, the user can refine his initial query by browsing through the graph from that point on. On the other hand, casual users without any prior knowledge of the contents of the system or users without any particular subject in mind can freely navigate through the graph without ever specifying any query.
75340	A library system for information retrieval based on a cognitive task analysis and supported by an icon-based interface An abstract is not available.
75341	Integrated information retrieval in a knowledge worker support system This paper describes the design of the information retrieval facilities of an integrated information system called EUROMATH. EUROMATH is an example of a Knowledge Worker Support System: it has been designed specifically to support mathematicians in their research work. EUROMATH is required to provide uniform retrieval facilities for searching in a user's personal data, in a shared database of structured documents and in public, bibliographic databases. The design of information retrieval facilities that satisfy these and other requirements posed several interesting design issues regarding the integration of various retrieval techniques. As well as a uniform query language, designed to be highly usable by the target user group, the retrieval facilities provide expert intermediary functions, i.e. sophisticated support for the retrieval of bibliographic data. This support is achieved using a model of the user, a model of the user's information need and a set of search strategies based on those used by human intermediaries. The expert intermediary facilities include extensive help facilities, automatic query reformulation and browsing of a variety of sources of query terms.
75342	Retrieval system evaluation using recall and precision: problems and answers An abstract is not available.
75343	Optimum polynomial retrieval functions We show that any approach to develop optimum retrieval functions is based on two kinds of assumptions: first, a certain form of representation for documents and requests, and second, additional simplifying assumptions that predefine the type of the retrieval function. Then we describe an approach for the development of optimum polynomial retrieval functions: request-document pairs (ƒ l , d m ) are mapped onto description vectors @@@@ (ƒ l , d m ), and a polynomial function of the form @@@@ T · @@@@ ( @@@@ ) is developed such that it yields estimates of the probability of relevance P ( R | @@@@ (ƒ l , d m )) with minimum square errors. We give experimental results for the application of this approach to documents with weighted indexing as well as to documents with complex representations. In contrast to other probabilistic models, our approach yields estimates of the actual probabilities, it can handle very complex representations of documents and requests, and it can be easily applied to multi-valued relevance scales. On the other hand, this approach is not suited to log-linear probabilistic models, and it needs large samples of relevance feedback data for its application.
75344	Towards an information logic 'Probability is expectation founded upon partial knowledge.' (Boole, 1854) Information retrieval based on stored program electronic computers has been an active area of research since the time these machines were invented. It is therefore somewhat surprising that even now no formal computational model for IR exists. There is no well-defined logic to describe information retrieval, and there is no proof or model theory to talk about the truths of IR. This paper argues that much of the research work in the past has been steps in the direction of a logic for IR. These steps have been taken by developing formal models for information retrieval, but to date none of these are complete nor could any claim to be a computational model for IR. To appreciate this development I shall present a picture of IR, describing bits of a puzzle which may fit together to point to a new framework within which a computational model or logic could be described.
75345	A parallel indexed algorithm for information retrieval In this paper we present a parallel document ranking algorithm suitable for use on databases of 1-1000 GB, resident on primary or secondary storage. The algorithm is based on inverted indexes, and has two advantages over a previously published parallel algorithm for retrieval based on signature files. First, it permits the employment of ranking strategies which cannot be easily implemented using signature files, specifically methods which depend on document-term weighting. Second, it permits the interactive searching of databases resident on secondary storage. The algorithm is evaluated via a mixture of analytic and simulation techniques, with a particular focus on how cost-effectiveness and efficiency change as the size of the database, number of processors, and cost of memory are altered. In particular, we find that if the ratio of the number of processors and/or disks to the size of the database is held constant, then the cost-effectiveness of the resulting system remains constant. Furthermore, for a given size of database, there is a number of processors which optimizes cost-effectiveness. Estimated response times are also presented. Using these methods, it appears that cost-effective interactive access to databases in the 100-1000 GB range can be achieved using current technology.
75346	An optical system for full text search In this paper we propose a full text search system based on optics. The storage and processing of the textual data are performed by an optical back-end system to an electronic computer. In this way we can take advantage of the speed and parallelism of digital optical processing. Using the proposed configuration we show how one might implement a set of text processing operations using lasers, spatial light modulators and photodetectors .
75347	Retrieving highly dynamic widely distributed information Wide area networks provide a variety of information sources which can be exploited only by appropriate information retrieval techniques such as repeated automatic query of remote databases and bulletin boards. Distinctive features of the content and access methods of information on wide area nets are discussed from an IR perspective. The development, algorithms, and analysis of a functioning system are also presented.
75348	The constituent object parser: syntactic structure matching for information retrieval An abstract is not available.
75349	Word sense disambiguation using machine-readable dictionaries An abstract is not available.
75350	File organizations and access methods for CLV disks A large and important class of optical disc technology are CLV format discs such as CD ROM and WORM. In this paper, we examine the issues related to the implementation and performance of several different file organizations on CLV format optical discs such as CD ROM and WORM. The organizations examined are based on hashing and trees. The CLV recording scheme is shown to be a good environment for efficiently implementing hashing. Single seek access and storage utilization levels approaching 100% can be achieved for CD ROM's. It is shown that a B-tree organization is not a good choice for WORM discs (both CAV and CLV), but a modified ISAM approach can be appropriate for WORM discs. We describe clustered BIM's, a class of tree organizations appropriate for CD ROMS. Expressions for the expected retrieval performance of both hashing and trees are also given. The paper concludes by outlining recent results and future directions on buffered implementations of access methods for WORM discs, as well as advantages of signature based access methods for text retrieval in WORM disc architectures.
75351	Storing text retrieval systems on CD-ROM: compression and encryption considerations An abstract is not available.
75352	A new approach to text searching We introduce a family of simple and fast algorithms for solving the classical string matching problem, string matching with don't care symbols and complement symbols, and multiple patterns. In addition we solve the same problems allowing up to k mismatches. Among the features of these algorithms are that they are real time algorithms, they don't need to buffer the input, and they are suitable to be implemented in hardware.
75353	Multikey access methods based on term discrimination and signature clustering In order to improve the two-level signature file method designed by Sacks-Davis et al. [20], we propose new multikey access methods based on term discrimination and signature clustering. By term discrimination, we create separate, efficient access methods for the terms frequently used in user queries. We in addition cluster similar signatures by means of these terms so that we may achieve good performance on retrieval. Meanwhile we provide the space-time analysis of the proposed methods and compare them with the two-level signature file method. We show that the proposed methods achieve 15-30% savings in retrieval time and require 3-9 % more storage overhead.
75354	Indexing medical reports in a multimedia environment: the RIME experimental approach This paper focuses on the RIME system aimed to the indexing of medical reports in a multimedia environment. This particular application is viewed as representative of a large set of still unanswered needs of large communities of users: domain experts dealing with on-line specialized documentation such as software engineers, medical specialists and so on. In this application textual information appears as an interesting media to access related pictures in the data base. After the presentation of the application and a study of the particular corpus involved we define a semantic model for the documents which is based on a Conceptual Language. Then we detail the indexing process and its various linguistic components which perform the translation of every medical report according to this semantic model.
75355	Full text indexing based on lexical relations an application: software libraries In contrast to other kinds of libraries, software libraries need to be conceptually organized. When looking for a component, the main concern of users is the functionality of the desired component; implementation details are secondary. Software reuse would be enhanced with conceptually organized large libraries of software components. In this paper, we present GURU, a tool that allows automatical building of such large software libraries from documented software components. We focus here on GURU's indexing component which extracts conceptual attributes from natural language documentation. This indexing method is based on words' co-occurrences. It first uses EXTRACT, a co-occurrence knowledge compiler for extracting potential attributes from textual documents. Conceptually relevant collocations are then selected according to their resolving power, which scales down the noise due to context words. This fully automated indexing tool thus goes further than keyword-based tools in the understanding of a document without the brittleness of knowledge based tools. The indexing component of GURU is fully implemented, and some results are given in the paper.
75356	How a personal document's intended use or purpose affects its classification in an office This paper reports on one of the findings of a larger case study that attempts to describe how people organize documents in their own offices. In that study, several dimensions along which people make classificatory decisions were identified. Of these, the use to which a document is put emerged as a strong determiner of that document's classification. The method of analysis is reviewed, and examples of different kinds of uses are presented, demonstrating that it is possible to describe a wide variety of specific instances using a closed set of descriptors. The suggestion is made that, in designing systems for organizing materials, it might be advantageous to incorporate information about contextual variables, such as use, since these seem to be particularly important in classification decisions made within personal environments.
75357	Information retrieval using a hypertext-based help system Hypertext offers users a simple, flexible way to navigate through electronic information systems but at the potential risk of becoming lost in the network of interconnected pieces of information. A study was conducted on information retrieval using a commercial hypertext based help system. It was found that the predominant search strategy was “browsing” (characterized by scanning tables of contents and paging through topics), rather than employing the indexes (“analytical search”). Although subjects did not become lost, individuals with better spatial visualization ability, as measured by a standardized test, were faster at retrieving information and returned to the top of the information hierarchy less often than those with poorer spatial visualization ability. These results support previous studies that have found a strong preference by users to browse in hypertext systems and extend those findings to a new domain (help), a different type of user interface, and a different information architecture. In addition, the results demonstrate the importance of spatial visualization ability for efficient navigation and information retrieval in a hierarchical hypertext system.
75358	A hypertext knowledge based for primary care - LIMEDS in LINCKS In organized health care, primary care is the first level. It is characterized by the wide span of health problems managed as well as remote location from traditional medical information and knowledge sources. The LIMEDS project has formulated the special requirements for integrated knowledge and data base management in primary care. This paper presents Gösta's book , a hypertext knowledge base implemented in LINCKS, an object oriented, networked database system. Firstly, aspects which make integrated hypermedia systems particularly suitable for application in primary health care are explored. We then describe the hypertext knowledge base, consisting of 500 basic text objects and 3000 links, and current implementations using the NODE data model. NODE is implemented on a SUN III fileserver, and the user interface for the hypertext context on Apple Macintosh (TM). Combination of design methods towards a parallel means-ends strategy was found to be necessary to achieve Gösta's book . Design groups need to be composed of computer science, medical, psychological and organizational competences.
75359	Settings and the setting structure: the description and automated propagation of networks for perusing videodisk image states This paper describes a system for formally representing spatial relationships between videodisc image states called settings . A number of setting relations are defined, these being based on the manipulations of the camera typically used in the production of the moving film: zooming in or out, panning etc.. An algorithm is presented which, given a limited level of initial specification by a describer, will constrain, where possible, the setting relations holding between all pairs of settings. The resulting network is called the settings structure . The paper begins by placing the settings structure into the context of its being one part of the CLORIS system.
75360	Research toward the development of a lexical knowledge base for natural language processing This paper documents research toward building a complete lexicon containing all the words found in general newspaper text. It is intended to provide the reader with an understanding of the inherent limitations of existing vocabulary collection methods and the need for greater attention to multi-word phrases as the building blocks of text. Additionally, while traditional reference books define many proper nouns, they appear to be very limited in their coverage of the new proper nouns appearing daily in newspapers. Proper nouns appear to require a grammar and lexicon of components much the way general parsing of text requires syntactic rules and a lexicon of common nouns.
75361	Information retrieval and software reuse Software reuse is widely believed to be the most promising technology for improving software quality and productivity. There are many technical and non-technical problems to be solved, however, before widespread reuse of software lifecycle objects becomes a reality. One class of problem concerns the classification, storage, and retrieval of reusable components. Panel members will discuss these problems and some approaches to solving them.
75479	On the application of syntactic methodologies in automatic text analysis This study summarizes various linguistic approaches proposed for document analysis in information retrieval environments. Included are standard syntactic methods to generate complex content identifiers, and the use of semantic know-how obtained from machine-readable dictionaries and from specially constructed knowledge bases. A particular syntactic analysis methodology is also outlined and its usefulness for the automatic construction of book indexes is examined.
803132	WEIRD: An approach to concept-based information retrieval WEIRD is an automatic document retrieval system designed and implemented at Syracuse University, which attempts to advance the art of computerized retrieval from word-matching to judging conceptual similarity. WEIRD uses a vector space model to represent the relations among terms and documents. Items in the space are located according to their -&-ldquo;meaning-&-rdquo;, which is their proximity to all other items in the data base as measured by co-occurrence frequencies. This is done without manipulating large matrices. The dimensions of the space are not used to define relations; items are defined solely by their position relative to the other items. Retrieval is determined by Euclidean distance from the plotted query. In the first section of the paper the basic characteristics of WEIRD are described. Second, the results of a preliminary evaluation are reported. Alternatives for further development of WEIRD are then considered.
803133	Augmented Transition Networks as a design tool for personalized database systems This paper illustrates the use of Augmented Transition Networks (ATNs) as a design tool for constructing document retrieval systems for those personalized applications which are too small or specialized to attract a commercial vendor. ATNs, which are explained in the context of this application, are used not only to improve the human/computer interface with the retrieval system but also to conceptually organize its structure.
803134	Mediator: An integrated approach to Information Retrieval Mediator: An Integrated Approach to Information Retrieval The Mediator is a pseudo intelligent software controller which accomplishes two ends. First, it -&-ldquo;mediates-&-rdquo; between an Information Retrieval System and its end-user. On the assumption that the user of such a system will have at best a minimal knowledge of the operations of computers, it hides from him the internal complexities of the system, and presents to him a simplified -&-ldquo;abstract-&-rdquo; of the operations of the system. The Mediator allows the end-user to communicate with any application program in his own terms and to carry out operations of any degree of complexity which can be defined within those terms. Secondly, the controller enables a single system to extract unified information from data-banks of both data base management and textual environments. The Mediator is driven by a combination of hierarchically structured internal and external tables. The external tables contain a vocabulary selected by the user for his personal communication with the system; the internal tables contain directives which determine the appropriate path to be followed by the retrieval system in accomplishing the user's request.
803135	Analysis of an inverted data base structure An inverted data base organization is analyzed. The inverted directory is viewed realistically as another large data base. Algorithms and formulations are derived to estimate the average number of accesses for insertion, retrieval and deletion of items from the data base. An average load time is also presented for the inverted data base.
803136	A file organization for cluster-based retrieval A file organization for cluster-based retrieval is presented and tested. This file organization is based on the bottom-up search which, in contrast to the more usual top-down search, starts at the lowest level of a cluster hierarchy (the documents) and looks at progressively larger clusters. This approach enables most of the efficiency problems previously associated with clustered file organizations to be avoided. There are two parts to this file organization - a compact cluster hierarchy representation which does not store cluster representatives and a compact inverted file which is used to provide a starting point for the bottom-up search. Retrieval experiments show that the bottom-up search using this file organization can be more effective than a serial search, especially if high precision results are required.
803137	Record block allocation for retrieval on secondary keys Query retrieval based on secondary keys is an important operation in retrieval systems. Such a query generally retrieves more than one data record which satisfies the query criterion. This paper studies the problem of record address allocation in disk-like devices so as to facilitate the fast retrieval of a set of records which are jointly accessed by a query. A heuristic scheme, using the proposed minimal access retrieval property, is designed to assign records to blocks. Some experimental results are also presented.
803138	A Block Structured Query Language for accessing a relational data base This paper describes a B lock S tructured Q uery L anguage (BSQL) to be used with relational data bases. The syntax of the language is presented and discussed. Facilities of the language are illustrated by examples of actual queries. In particular we demonstrate the ability of BSQL to obtain useful information from a relational data base that is incomplete. A relational calculus is then presented which forms a basis for a formalism which precisely describes the semantics of BSQL. Finally, comparative examples with other query languages are given.
803139	A tree algorithm for nearest neighbor searching in document retrieval systems The problem of finding nearest neighbors to a query in a document collection is a special case of associative retrieval, in which searches are performed using more than one key. A nearest neighbors associative retrieval algorithm, suitable for document retrieval using similarity matching, is described. The basic structure used is a binary tree, at each node a set of keys (concepts) is tested to select the most promising branch. Backtracking to initially rejected branches is allowed and often necessary. Under certain conditions, the search time required by this algorithm is 0(log 2 N) k . N is the number of documents, and k is a system-dependent parameter. A series of experiments with a small collection confirm the predictions made using the analytic model; k is approximately 4 in this situation. This algorithm is compared with two other searching algorithms; sequential search and clustered search. For large collections, the average search time for this algorithm is less than that for a sequential search and greater than that for a clustered search. However, the clustered search, unlike the sequential search and this algorithm, does not guarantee that the near neighbors found are actually the nearest neighbors.
803140	Experiments on the determination of the relationships between terms The retrieval effectiveness of an automatic method that uses relevance judgements for the determination of positive as well as negative relationships between terms is evaluated. The term relationships are incorporated into the retrieval process by using a generalized similarity function that has a term match component, a positive term relationship component, and a negative term relationship component. Two strategies, query partitioning and query clustering, for the evaluation of the effectiveness of the term relationships are investigated. The latter appears to be more attractive from linguistic as well as economic points of view. The positive and the negative relationships are verified to be effective both when used individually, and in combination. The importance attached to the term relationship components relative to that of term match component is found to have a substantial effect on the retrieval performance. The usefulness of discriminant analysis as a technique for determining the relative importance of these components is investigated.
803141	Does relevance feedback improve document retrieval performance? Many authors (1, 2, 3, 5, 6, 7) have suggested that overall performance of a document retrieval system is improved by relevance feedback. Relevance feedback denotes the last three steps in the following process: 1) the searcher enters a query, 2) the system prepares a ranked list of suggested documents, 3) the searcher judges some of the documents for relevancy, 4) the searcher informs the system of these documents judged and of the judgement, 5) the system constructs a new query based on the descriptors used in the original query and the descriptors used in the documents judged, 6) the system prepares a second ranked list of suggested documents. The presumption is that the second list is better than the first. By all performance measures (e.g. -&-ldquo;fluid ranking-&-rdquo; and -&-ldquo;frozen ranking-&-rdquo;), the second list is better than the first. However, if one reranks documents in the original list so as to reflect the searcher's efforts (step 3), the corresponding performance measures are comparable to those for the second list. The marginal difference between the performance measures for the -&-rdquo;reranked original-&-rdquo; list (searcher's efforts alone) and the second list (which includes computer efforts) makes it unclear if the cost of steps 4 through 6 above can be justified. It is hoped that advocates of relevance feedback will present -&-ldquo;reranked original-&-rdquo; performance measures as a basis for any performance improvement claims. This paper also presents three reasonable, easily understood retrieval procedures for which the frozen ranking, the fluid ranking, and the reranked original evaluations are -&-ldquo;obviously-&-rdquo; the pertinent way to evaluate. Relevance feedback techniques as implemented in Salton's SMART DRS appear to show that it is worthwhile for user's to read abstracts prior to evaluation of full texts. The last indication presented in this paper is that the relevance feedback performance improvements noted using SMART are due mostly to the user making assessments; subsequent computer efforts appear to be most likely to result in no further change. For a query for which there is a subsequent change, the change is as likely to be harmful as helpful.
803142	INQUIRE system overview INQUIRE is a versatile database management system with integrated information retrieval and full-text processing capabilities. Designed primarily for the end-user of information, INQUIRE features rapid start-up of applications and has a broad range of facilities for both technical and non-technical users. INQUIRE is operational on IBM System 360 or 370, Amdahl 470, or equivalent, under OS, VS, MVS, or CMS.
803143	INSPECTOR INSPECTOR is a proprietary software system that is designed to be used in an information retrieval environment. Specifically, it is oriented toward the on-line retrieval of microfilmed documents through the indexing of certain key terms relating to the document itself. Items such as date, account number, name, customer name or number, purchase order number, etc. might be considered as key descriptive terms. Thus by indexing these elements on a randomly accessible disk drive, the location of the filmed image of all original documents pertaining to a particular descriptive term may be quickly located by the computer and the location displayed to the operator. Alternatively, if used in conjunction with the Eastman Kodak IC-5/PR-1 microfilm retrieval unit, the computer system will cause the film display unit to automatically advance to the correct frame(s), keeping operator intervention to an absolute minimum.
803144	MAGIC MAGIC is a simple, elegant, and time-saving system for retrieving, manipulating, and displaying time series data. The user of MAGIC is not required to know anything about file structures or computer programming. Finished reports and graphs suitable for reproduction are attainable after minimal experience with the MAGIC system.
803145	SIRE SIRE (Syracuse Information Retrieval Experiment) is an interactive bibliographic retrieval system. It was developed at the School of Information Studies at Syracuse University. It is implemented in SAIL (Stanford Artificial Intelligence Language), an ALGOL like language, on a DEC KL-10. SIRE presently has two data bases; 1) one issue of Physics Abstracts with 7146 documents; and 2) a sign language linguistics data base from Gallaudet College with 490 documents.
803146	WEIRD: An approach to concept-based information retrieval WEIRD is an automatic document retrieval system designed and implemented at Syracuse University, which attempts to advance the art of computerized retrieval from word-matching to judging conceptual similarity. WEIRD uses a vector space model to represent the relations among terms and documents. Items in the space are located according to their -&-ldquo;meaning-&-rdquo;, which is their proximity to all other items in the data base as measured by co-occurrence frequencies. This is done without manipulating large matrices. The dimensions of the space are not used to define relations; items are defined solely by their position relative to the other items. Retrieval is determined by Euclidean distance from the plotted query. In the first section of the paper the basic characteristics of WEIRD are described. Second, the results of a preliminary evaluation are reported. Alternatives for further development of WEIRD are then considered.
42006	A statistical similarity measure Within the framework of the vector space models, a statistical similarity measure between document and query is proposed. In this approach the assumption that term (or atomic) vectors are pairwise orthogonal is not required. In addition, it provides a natural and consistent interpretation of term occurrence frequencies obtained from autoindexing.
42007	Probabilistic search term weighting - some negative results The effect of probabilistic search term weighting on the improvement of retrieval quality has been demonstrated in various experiments described in the literature. In this paper, we investigate the feasibility of this method for boolean retrieval with terms from a prescribed indexing vocabulary. This is a quite different test setting in comparison to other experiments where linear retrieval with free text terms was used. The experimental results show that in our case no improvement over a simple coordination match function can be achieved. On the other hand, models based on probabilistic indexing outperform the ranking procedures using search term weights.
42008	Some considerations for using approximate optimal queries An optimal query has been defined as one which will recover all the known relevant documents of a query in their best probability of relevance ranking. We have slightly modified the definition so that it also allows one to trace its evolution from the original to the optimal via the various feedback stages. Such a query can be constructed by modifying the original query with terms from the known relevant documents. It is pointed out that such a term addition strategy differs materially from other approaches that add terms based on term association with all query terms, and calculated from the whole document collection. The effect of viewing a document as constituted of components, and hence affecting the weighting and retreival results of of the optimal query, is also discussed.
42009	An approach to natural language for document retrieval Document retrieval systems have been restricted, by the nature of the task, to techniques that can be used with large numbers of documents and broad domains. The most effective techniques that have been developed are based on the statistics of word occurrences in text. In this paper, we describe an approach to using natural language processing (NLP) techniques for what is essentially a natural language problem - the comparison of a request text with the text of document titles and abstracts. The proposed NLP techniques are used to develop a request model based on “conceptual case frames” and to compare this model with the texts of candidate documents. The request model is also used to provide information to statistical search techniques that identify the candidate documents. As part of a preliminary evaluation of this approach, case frame representations of a set of requests from the CACM collection were constructed. Statistical searches carried out using dependency and relative importance information derived from the request models indicate that performance benefits can be obtained.
42010	Outline of a knowledge base model for an intelligent information retrieval system We attempt in this paper to outline a method for the automatic construction of a knowledge base. We propose some methods and a domain knowledge model. A new idea is to conceive a system that is able to each phase of its construction to acquire domain knowledge from all new information that it is building, in particular the indexing terms; the last section is an attempt in this sense.
42011	Enriched knowledge representation for information retrieval In this paper we identify the need for a new theory of information. An information model is developed which distinguishes between data, as directly observable facts, information, as structured collections of data, and knowledge as methods of using information. The model is intended to support a wide range of information systems. In the paper we develop the use of the model for a semantic information retrieval system using the concept of semantic categories. The likely benefits of this area discussed, though as yet no detailed evaluation has been conducted.
42012	Informational zooming: an interaction model for the graphical access to text knowledge bases An abstract is not available.
42013	Generating an individualized user interface A model of the interface to an information retrieval system is developed based on the semantic data model. Using this framework, a method of developing customized user interfaces is described, in general terms and in a specific implementation in the Interface Builder module of the Western Information Retrieval System.
42014	Why do some people have more difficulty learning to use an information retrieval system than others? The population using information retrieval systems is becoming increasingly diverse. We find a wide range of skills in ability to use these systems; this diverse population must be accommodated by the next generation of systems. This paper reports on a study to identify variables related to information retrieval aptitude, based on results from earlier studies of searchers and programmers. A sample of undergraduate subjects from English, psychology, and engineering majors was given a series of psychometric tests and compared to known populations. We find that engineering majors exhibit academic background and personality characteristics most like those of skilled searchers and programmers, with contrasting patterns or no discernible patterns in English and psychology majors. The strength of most associations increases when restricted to subjects who have either stayed in one major or who have changed major only within one disciplinary area. About half the variance in choice of major can be explained by scores on the tests administered, and a comparable amount of variance in test scores can be explained by the academic background variables.
42015	Illustrated description of an interactive knowledge based indexing system This report discusses the Indexing Aid Project for conducting research in interactive knowledge-based indexing of the medical literature. After providing an overview and background, we describe and illustrate the Indexing Aid System using an extended example, highlighting the knowledge-based capabilities of the system, namely, inheritance and internal retrieval, enforcement of restrictions, and other functions implemented by procedural attachments, which are characteristic of frame-based knowledge representation languages. A feature which generates reports for evaluating the system is also shown. The paper concludes with discussion of the research plan. The project is part of the Automated Classification and Retrieval Program at the Lister Hill National Center for Biomedical Communications, the research and development arm of the National Library of Medicine.
42016	Automatic phrase indexing for document retrieval An automatic phrase indexing method based on the term discrimination model is described, and the results of retrieval experiments on five document collections are presented. Problems related to this non-syntactic phrase construction method are discussed, and some possible solutions are proposed that make use of information about the syntactic structure of document and query texts.
42017	A failure analysis of the limitation of suffixing in an online environment The interaction of suffixing algorithms and ranking techniques in retrieval performance, particularly in an online environment, was investigated. Three general purpose suffixing algorithms were used for retrieval on the Cranfield 1400, Medlars, and CACM collections, and the results analysed with several standard evaluation measures. An examination of the retrieval performance using suffixing suggested two modifications to ranking techniques: variable weighting of word variants and selective stemming depending on query length. The experimental data is presented, and the limitations of suffixing in an online environment is discussed.
42018	Fast object partitioning using Stochastic learning automata Let &OHgr; = {A 1 , …, A W } be a set of W objects to be partitioned into R classes {P 1 , …, P R }. The objects are accessed in groups of unknown size and the size of these groups need not be equal. Additionally, the joint access probabilities of the objects are unknown. The intention is that the objects accessed more frequently together are located in the same class. This problem has been shown to be NP-hard [15, 16]. In this paper, we propose two stochastic learning automata solutions to the problem. Although the first one is relatively fast, its accuracy is not so remarkable in some environments. The second solution, which uses a new variable structure stochastic automation, demonstrates an excellent partitioning capability. Experimentally, this solution converges an order of magnitude faster than the best known algorithm in the literature [15, 16].
42019	A dynamic cluster maintenance system for information retrieval Partitioning by clustering of very large databases is a necessity to reduce the space/time complexity of retrieval operations. However, the contemporary and modern retrieval environments demand dynamic maintenance of clusters. A new cluster maintenance strategy is proposed and its similarity/stability characteristics, cost analysis, and retrieval behavior in comparison with unclustered and completely reclustered database environments have been examined by means of a series of experiments.
42020	Non-hierarchical document clustering using the ICL distribution array processor This paper considers the suitability and efficiency of a highly parallel computer, the ICL Distributed Array Processor (DAP), for document clustering. Algorithms are described for the implementation of the single-pass and reallocation clustering methods on the DAP and on a conventional mainframe computer. These methods are used to classify the Cranfield, Vaswani and UKCIS document test collections. The results suggest that the parallel architecture of the DAP is not well suited to the variable-length records which characterise bibliographic data.
42021	Optimal determination of user-oriented clusters User-oriented clustering schemes enable the classification of documents based upon the user perception of the similarity between documents, rather than on some similarity function presumed by the designer to represent the user criteria. In this paper, an enhancement of such a clustering scheme is presented. This is accomplished by the formulation of the user-oriented clustering as a function-optimization problem. The problem formulated is termed the Boundary Selection Problem (BSP). Heuristic approaches to solve the BSP are proposed and a preliminary for evaluation of these approaches is provided.
42022	A formal treatment of missing & imprecise information Missing, non-applicable and imprecise values arise frequently in Office Information Systems. There is a need to treat them in a consistent and useful manner. This paper proposes a method and gives the precise semantics of the retrieval operations in a system where imprecision is allowed. It also suggests a way to handle the uncertainty introduced by imprecise data values.
42023	Adaptive linear information retrieval models Missing, non-applicable and imprecise values arise frequently in Office Information Systems. There is a need to treat them in a consistent and useful manner. This paper proposes a method and gives the precise semantics of the retrieval operations in a system where imprecision is allowed. It also suggests a way to handle the uncertainty introduced by imprecise data values.
42024	Random and best-first document selection models Most document retrieval systems based on probabilistic models of feature distributions assume random selection of documents for retrieval. The assumptions of these models are met when documents are randomly selected from the database or when retrieving all available documents. A more suitable model for retrieval of a single document assumes that the best document available is to be retrieved first. Models of document retrieval systems assuming random selection and best-first selection are developed and compared under binary independence and two Poisson independence feature distribution models. Under the best-first model, feature discrimination varies with the number of documents in each relevance class in the database. A weight similar to the Inverse Document Frequency weight and consistent with the best-first model is suggested which does not depend on knowledge of the characteristics of relevant documents.
42025	TIRS: a topological information retrieval system satisfying the requirements of the Waller-Kraft wish list Most document retrieval systems based on probabilistic models of feature distributions assume random selection of documents for retrieval. The assumptions of these models are met when documents are randomly selected from the database or when retrieving all available documents. A more suitable model for retrieval of a single document assumes that the best document available is to be retrieved first. Models of document retrieval systems assuming random selection and best-first selection are developed and compared under binary independence and two Poisson independence feature distribution models. Under the best-first model, feature discrimination varies with the number of documents in each relevance class in the database. A weight similar to the Inverse Document Frequency weight and consistent with the best-first model is suggested which does not depend on knowledge of the characteristics of relevant documents.
42026	A retrieval system for on-line English-Japanese dictionaries An abstract is not available.
42027	An advanced full-text retrieval and analysis system MICROARRAS is an advanced full-text retrieval and analysis system. It supports fast, efficient browsing of a document's vocabulary as well as its text, recursive analytic categories, Boolean search with flexible context specifications, evaluation of arithmetic expressions, and graphical display of various numeric distributions. The system is designed to work with large textbases stored on remote mainframes or on a local store for a micro-computer or workstation. The description covers system architecture, design principals, as well as user functions.
42028	A relational model for unstructured documents The logical structure of a document is usually a tree in which the order of the nodes is important at least at some level of the tree. We call a document unstructured if its structure is a single-level ordered tree. The purpose of this paper is to present a many-sorted algebra for handling unstructured documents. The documents in the model are represented by relations. An algebra for handling documents of one type can be extended to an algebra for handling documents of several types. Further, an algebra for handling documents can be extended by the relational algebra for handling documents and relations in a common algebra. The model of this paper can be regarded as a part of a general document model. On the other hand, unstructured documents themselves are an important group of documents. We will show by examples that the simple model covers a wide range of document handling and information retrieval problems.
42029	A VLSI chip for efficient transmission and retrieval of information In this paper, we present a functional description of a VLSI chip aimed at reducing the cost of data transmission and data access within information processing machines and distributed information systems. The chip maps standard character codes (e.g., ASCII) into more efficient codes (e.g., Huffman's codes) using a tree module of basic cells. In bit-serial communication controllers, for example, the parallel-to-serial transformation unit can be simply replaced by the proposed chip. The VLSI design can provide speeds that far exceed current and projected peak transfer rates of high-speed disks and communication controllers.
42030	File organizations & incrementally specified queries Queries to information retrieval systems are often incrementally specified as a result of user interaction with the system. However, most discussions of file organizations consider only completely specified queries. The choice of file organizations to support such incremental specification is discussed qualitatively in this extended abstract. (Quantitative comparisons are partially complete and are not presented here.) Organizations which are advantageous for completely specified queries are not necessarily so for incrementally specified queries (and vice versa).
42031	Predictive test compression by hashing The knowledge of a short substring constitutes a good basis for guessing the next character in a natural language text. This observation, i.e. repeated guessing and encoding of subsequent characters, is very fundamental for the predictive text compression. The paper describes a family of such compression methods, using a hash table for searching the prediction information. The experiments show that the methods produce good compression gains and, moreover, are very fast. The one-pass versions are especially apt for “on-the-fly” compression of transmitted data, and could be a basis for specialized hardware.
42032	Determining online retrieval system display size This paper outlines a problem in commercial online retrieval systems, provides a review of the relevant literature, and presents a solution for a special case of the problem. Previous investigators have considered how to best determine, for a ranked list of records retrieved from an online retrieval system, whether or not the user should continue to display the output. This paper examines the problem of how effective display size can be estimated as a means of assisting the users of commercial online retrieval systems. Although no experimental results are as yet available, the approach presented here will provide a guide to and prolegomenon for systematic study of the problem, as well as a method for providing the estimated number of relevant records remaining in a retrieved set ranked by a retrieval status value.
42033	Conceptual information retrieval using RUBRIC An abstract is not available.
42034	Thesaurus based concept spaces An abstract is not available.
42035	EP-X: a demonstration of semantically based search of bibliographic databases EP-X (Environmental Pollution eXpert) is a prototype knowledge-based system that assists users in conducting bibliographic searches of the environmental pollution literature. This system combines artificial intelligence and human factors engineering techniques, allowing us to redesign traditional bibliographic information retrieval interfaces. The result supports semantically-based search as opposed to the typical character-string matching approach. This paper discusses a sample interaction with EP-X, the knowledge representations necessary to support this semantically-based interaction, preliminary results of empirical studies to evaluate the interface, and recommendations for future directions
42036	Towards an expert system for bibliographical retrieval: a Prolog prototype A prototype Prolog system has been developed for online bibliographic retrieval. Most online bibliographic retrieval systems may be characterized by queries based on the occurrence of keywords and by databases consisting of possibly millions of records. Such systems have very fast response times but generally lack any deductive reasoning capability. An expert system for online bibliographic retrieval, developed in Prolog, would provide enhanced retrieval capabilities through the application of deductive reasoning. Such a system would permit knowledge-type queries to be asked in addition to the traditional keyword-type of queries. A concern with using Prolog to perform an online search of a million-record data base is that the response time would be unacceptable. In order to overcome this drawback two alternatives are examined: a special-purpose hardware device and an extended Prolog capability.
42037	An approach to image retrieval from large image databases In this paper we address the problem of retrieving images from large image databases, giving a partial description of the image content. This approach allows a limited automatic analysis for image belonging to a domain described in advance to the system using a formalism based on fuzzy sets. The image query processing is based on special access structures generated from the image analysis process.
42038	Data cashing in IR systems Information retrieval (IR) systems provide individual remote access to centrally managed data. The current proliferation of personal computer systems, as well as advances in storage and communication technology, have created new possibilities for designing information systems which are easily accessible, economical, and responsive to user needs. This paper outlines methods of integrating personal computers (PCs) into large information systems, with emphasis on effective use of the storage and processing capabilities of these computers. In particular we discuss means for caching retrieved data at PC-equipped user sites, noting that caching in this environment poses unique problems. An event-driven simulation program is described which models information system operation. This simulator is being used to examine caching strategies. Some results of these studies are presented.
42039	Improved techniques for processing queries in full-text systems In static full-text retrieval systems, which accommodate metrical as well as Boolean operators, the traditional approach to query processing uses a “concordance”, from which large sets of coordinates are retrieved and then merged and/or collated. Alternatively, in a system with l documents, the concordance can be replaced by a set of bit-maps of fixed length l , which are constructed for every different word of the database and serve as occurrence maps. We propose to combine the concordance and bit-map approaches, and show how this can speed up the processing of queries: fast ANDing and ORing of the maps in a preprocessing stage, lead to large I/O savings in collating coordinates of keywords needed to satisfy the metrical and Boolean constraints. Moreover, the bit-maps give partial information on the distribution of the coordinates of the keywords, which can be used when queries must be processed by stages, due to their complexity and the sizes of the involved sets of coordinates. The new techniques are partially implemented at the Responsa Retrieval Project.
98006	Inference networks for document retrieval The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models.
98007	A retrieval model based on an extended modal logic and its application to the RIME experimental approach This paper focuses on the query processing module of RIME, an experimental prototype of an intelligent information retrieval system designed to manage high-precision queries on a corpus of medical reports. Though highly specific this particular corpus is representative of an important class of applications: information retrieval among full-text specialized documents which constitute critical sources of information in several organizations (medicine, law, space industry…). This experience allowed us to design and implement an elaborate model for the semantic content of the documents which is an extension of the Conceptual Dependency approach. The underlying retrieval model is inspired from the Logic model proposed by C.J. Van Rijsbergen, which has been considerably refined using an Extended Modal Logic. After presenting the context of the RIME project, we briefly describe the models designed for the internal representation of medical reports and queries. The main part of the paper is then devoted to the retrieval model and its application to the query processing module of RIME which has a natural language interface. Processing a query involves two main phases: the interpretation which transforms the natural language query into a search expression, and the evaluation phases which retrieves the corresponding medical reports. We focus here on the evaluation phases and show its relationship with the underlying retrieval model. Evaluations from practical experiments are also given, along with indications about current developments of the project.
98008	Probabilistic document indexing from relevance feedback data Based on the binary independence indexing model, we apply three new concepts for probabilistic document indexing from relevance feedback data: Abstraction from specific terms and documents, which overcomes the restriction of limited relevance information for parameter estimation. Flexibility of the representation, which allows the integration of new text analysis and knowledge-based methods in our approach as well as the consideration of more complex document structures or different types of terms (e.g. single words and noun phrases). Probabilistic learning or classification methods for the estimation of the indexing weights making better use of the available relevance information. We give experimental results for five test collections which show improvements over other indexing methods.
98009	EXPRESS: an experimental interface for factual information retrieval The EXPRESS system has been designed and implemented in order to explore methods for user assistance in accessing complexly structured factual databases, e.g. relational product databases. Terminological support in this area has to take into account that different controlled vocabularies may be used in a variety of attributes spread over several relations. In our approach, traditional thesaurus structures are extended in order to cope with these problems and to encode further domain-specific knowledge. User support in query reformulation is based on this enriched thesaurus as well as on the local evaluation of the retrieved data sets. Concepts for the representation of retrieval strategies in the form of plans and their potential use in future systems are discussed.
98010	Hypertext, full text, and automatic linking Current computing systems typically support only mid-century information structures: simple hierarchies. Hypertext technologies enable users to impose many structures on document sets and, consequently, provide many paths to desired information, but they require that users work their way through some structure. Full-text search eliminates this requirement by ignoring structure altogether. The search strategy can also be restricted to work within specified contexts. The architecture provided for search readily supports automatic linking. These ideas have been tested in IRIS Intermedia.
98011	Machine learning and vectorial matching for an image retrieval model: EXPRIM and the system RIVAGE An abstract is not available.
98013	Online query refinement on information retrieval systems: a process model of searcher/system interactions This article reports findings of empirical research that investigated information searchers' online query refinement process. Prior studies have recognized the information specialists' role in helping searchers articulate and refine queries. Using a semantic network and a Problem Behavior Graph to represent the online search process, our study revealed that searchers also refined their own queries in an online task environment. The information retrieval system played a passive role in assisting online query refinement, which was, however, or that confirmed Taylor's four-level query formulation model. Based on our empirical findings, we proposed using a process model to facilitate and improve query refinement in an online environment. We believe incorporating this model into retrieval systems can result in the design of more “intelligent” and useful information retrieval systems.
98015	A direct manipulation interface for boolean information retrieval via natural language query This paper describes the design of a direct manipulation user interface for Boolean information retrieval. Intended to overcome the difficulties of manipulating explicit Boolean queries as well as the “black box” drawbacks of so-called natural language query systems, the interface presents a two-dimensional graphical representation of a user's natural language query which not only exposes heuristic query transformations performed by the system, but also supports query reformulation by the user via direct manipulation of the representation. The paper illustrates the operation of the interface as implemented in the AI-STARS full-text information retrieval system.
98219	Using syntactic analysis in a document retrieval system that uses signature files Our work involves the study of the extent to which natural language processing techniques aid the automatic indexing and retrieval of documents. In this paper we describe the use of signature files in large text retrieval systems. We show that good performance can be obtained without requiring the significant overheads required for the inverted file technique. We examine the use of syntactic analysis of the text in all stages of retrieval and argue that an initial Boolean query should be performed that provides a subset of documents, which are then ranked. We then give an algorithm for generating such queries, taking into account the syntactic structure of the queries.
98223	A dynamic signature technique for multimedia databases A signature file acts as a filtering mechanism to reduce the amount of data that needs to be searched during query evaluation. Even though several techniques for organizing and searching signature files have been proposed in literature, they have serious limitations when applied to multimedia databases, where integrated access methods to text and image content are needed. A new signature technique, called Quick Filter, is proposed in the paper. According to this technique, signatures are divided into partitions, each of which holds signatures sharing the same characteristic key. As a result, it is possible to determine if the signatures in a partition satisfy a query by merely examining the key. Partitions not matching the key need not be searched. This method is based on dynamic hashing since signatures are hashed into partitions according to the keys and the file size, computed algorithmically from the signatures. Implementation of this technique is illustrated using an example and is verified by analytical performance evaluation. The result is a signature technique which satisfies the requirements for access methods in multimedia databases: dynamicity, with respect to insertions and updates, good query processing performance on large databases for high-weight queries .
98226	Surrogate subsets: a free space management strategy for the index of a text retrieval system This paper presents a new data structure and an associated strategy to be utilized by indexing facilities for text retrieval systems. The paper starts by reviewing some of the goals that may be considered when designing such an index and continues with a small survey of various current strategies. It then presents an indexing strategy referred to as surrogate subsets discussing its appropriateness in the light of the specified goals. Various design issues and implementation details are discussed. Our strategy requires that a surrogate file be divided into a large number of subsets separated by free space which will allow the index to expand when new material is appended to the database. Experimental results report on the utilization of free space when the database is enlarged.
98228	Construction of a dynamic Thesaurus and its use for associated information retrieval An information retrieval system based on a dynamic thesaurus was developed utilizing the connectionist approach. The dynamic thesaurus consists of nodes, which represent each term of a thesaurus, and links, which represent the connections between nodes. Term information that is automatically extracted from user's relevant documents is used to change node weights and generate links. Node weights and links reflect a user's particular interest. A document retrieval experiment was conducted in which both a high recall rate and a high precision rate were achieved. The topics discussed in this paper: Connectionist Model, Automatic Indexing, Information Retrieval, and Thesaurus.
98229	Knowledge-based retrieval of office documents Document classification and retrieval systems for office applications require knowledge management to describe the semantics of documents related to procedural and domain dependent aspects of the office work to be described: operational dependencies, documents relationships and references to regulations and laws are concepts which are not explicitly stated in the text. This paper presents a semantic model for office documents classification and retrieval based on knowledge representation of the office procedural environment and of the application domain. Navigation along knowledge networks for document retrieval and browsing is described.
98231	Evaluation of an expert system for searching in full text This paper presents a prototype expert system which provides online search assistance. The expert system automatically reformulates queries, using an online thesaurus as the source of domain knowledge, and a knowledge base of domain-independent search tactics. The expert system works with a full-text database which requires no syntactic or semantic pre-processing. In addition, the expert system ranks the retrieved passages in decreasing order of probable relevance. Users' search performance using the expert system was compared with their search performance on their own, and their search performance using the online thesaurus. The following conclusions were reached: 1) The expert system significantly reduced the number of queries necessary to find relevant passages compared with the user searching alone or with the thesaurus. 2) The expert system produced marginally significant improvements in precision compared with the user searching on their own. There was no significant difference in the recall achieved by the three system configurations. 3) Overall, the expert system ranked relevant passages above irrelevant passages.
98233	Order preserving minimal perfect hash functions and information retrieval Rapid access to information is essential for a wide variety of retrieval systems and applications. Hashing has long been used when the fastest possible direct search is desired, but is generally not appropriate when sequential or range searches are also required. This paper describes a hashing method, developed for collections that are relatively static, that supports both direct and sequential access. Indeed, the algorithm described gives hash functions that are optimal in terms of time and hash table space utilization, and that preserve any a priori ordering desired. Furthermore, the resulting order preserving minimal perfect hash functions (OPMPHFs) can be found using space and time that is on average linear in the number of keys involved.
98234	On the interrelationship of dictionary size and completeness When dictionaries for specific applications or subject fields are derived from a text collection, the frequency distribution of the terms in the collection gives information about the expected completeness of the dictionary. If only a subset of the terms in the collection is to be included in the dictionary, the completeness of the dictionary can be optimized with respect to dictionary size. In this paper, formulas for the relationship between the frequency distribution of the terms in the collection and expected dictionary completeness are derived. First we regard one-dimensional dictionaries where the (non-trivial) terms occurring in the texts are to be included in the dictionary. Then we describe the case of two-dimensional dictionaries, which are needed for example for automatic indexing with a controlled vocabulary; here relationships between text terms and descriptors from the prescribed vocabulary have to be stored in the dictionary. For both cases, formulas for the interpolation and extrapolation with respect to different collection sizes are derived. We give experimental results for one-dimensional dictionaries and show how the completeness can be estimated and optimized.
98236	Construction of optimal graphs for bit-vector compression Bitmaps are data structures occurring often in information retrieval. They are useful; they are also large and expensive to store. For this reason, considerable effort has been devoted to finding techniques for compressing them. These techniques are most effective for sparse bitmaps. We propose a preprocessing stage, in which bitmaps are first clustered and the clusters used to transform their member bitmaps into sparser ones, that can be more effectively compressed. The clustering method efficiently generates a graph structure on the bitmaps. The results of applying our algorithm to the Bible is presented: for some sets of bitmaps, our method almost doubled the compression savings.
98237	On hypertext This panel will employ two different interpretations of the phrase “growing up” to address areas of common interest between hypertext and information retrieval researchers. First, the panelists will question whether or not hypertext is “growing up” as a scientific discipline; They will discuss characteristics that separate hypertext research from other related disciplines. Second, the panelists will discuss the problems encountered when a hypertext system “grows up” in size and complexity; They will discuss the very real problems expected when representing and integrating large knowledge bases, accommodating multiple users, and distributing single logical hypertexts across multiple physical sites. The panelists will not lecture, but they will advance a number of themes including “the Myth of Modularity” (Frisse), “New Architectures Employing Hyperconcept Databases” (Agosti), “Hypertext in Software Engineering” (Bruandet), “Automatic Hypertext Generation” (Hahn), and “Large-Scale Hypertexts” (Weiss).
98240	Experiments with query acquisition and use in document retrieval systems In some recent experimental document retrieval systems, emphasis has been placed on the acquisition of a detailed model of the information need through interaction with the user. It has been argued that these “enhanced” queries, in combination with relevance feedback, will improve retrieval performance. In this paper, we describe a study with the aim of evaluating how easily enhanced queries can be acquired from users and how effectively this additional knowledge can be used in retrieval. The results indicate that significant effectiveness benefits can be obtained through the acquisition of domain concepts related to query concepts, together with their level of importance to the information need.
98242	The automatic generation of extended queries In the extended vector space model, each document vector consists of a set of subvectors representing the multiple concepts or concept classes present in the document. Typical information concepts, in addition to the usual content terms or descriptors, include author names, bibliographic links, etc. The extended vector space model is known to improve retrieval effectiveness. However, a major impediment to the use of the extended model is the construction of an extended query. In this paper, we describe a method for automatically extending a query containing only content terms (a single concept class) to a representation containing multiple concept classes. No relevance feedback is involved. Experiments using the CACM collection resulted in an average precision 34% better than that obtained using the standard single-concept term vector model.
98244	Term clustering of syntactic phrases Term clustering and syntactic phrase formation are methods for transforming natural language text. Both have had only mixed success as strategies for improving the quality of text representations for document retrieval. Since the strengths of these methods are complementary, we have explored combining them to produce superior representations. In this paper we discuss our implementation of a syntactic phrase generator, as well as our preliminary experiments with producing phrase clusters. These experiments show small improvements in retrieval effectiveness resulting from the use of phrase clusters, but it is clear that corpora much larger than standard information retrieval test collections will be required to thoroughly evaluate the use of this technique.
98245	Optimization for dynamic inverted index maintenance For free-text search over rapidly evolving corpora, dynamic update of inverted indices is a basic requirement. B-trees are an effective tool in implementing such indices. The Zipfian distribution of postings suggests space and time optimizations unique to this task. In particular, we present two novel optimizations, merge update , which performs better than straight forward block update, and pulsing which significantly reduces space requirements without sacrificing performance.
98247	Partitioned posting files: a parallel inverted file structure for information retrieval This paper describes algorithms and data structures for applying a parallel computer to information retrieval. Previous work has described an implementation based on overlap encoded signatures. That system was limited by 1) the necessity of keeping the signatures in primary memory, and 2) the difficulties involved in implementing document-term weighting. Overcoming these limitations requires adapting the inverted index techniques used on serial machines. The most obvious adaptation, also previously described, suffers from the fact that data must be sent between processors at query-time. Since interprocessor communication is generally slower than local computation, this suggests that an algorithm which does not perform such communication might be faster. This paper presents a data structure, called a partitioned posting file, in which the interprocessor communication takes place at database-construction time, so that no data movement is needed at query-time. Algorithms for constructing the data structure are also described. Performance characteristics and storage overhead are established by benchmarking against a synthetic database.
98249	Parallel text searching in serial files using a processor farm This paper discusses the implementation of a parallel text retrieval system using a microprocessor network. The system is designed to allow fast searching in document databases organised using the serial file structure, with a very rapid initial text signature search being followed by a more detailed, but more time-consuming, pattern matching search. The network is built from transputers, high performance microprocessors developed specifically for the construction of highly parallel computing systems, which are linked together in a processor farm. The paper discusses the design and implementation of processor farms, and then reports our initial studies of the efficiency of searching that can be achieved using this approach to text retrieval from serial files.
98252	An architecture for probabilistic concept-based information retrieval An abstract is not available.
98254	A new method for information retrieval, based on the theory of relative concentration This paper introduces a new method for information retrieval of documents that are represented by a vector. The novelty of the algorithm lies in the fact that no (generalized) p-norms are used as a matching function between the query and the document (as is done e.g. by Salton and others) but a function that measures the relative dispersion of the terms between a document and a query. This function originates from an earlier paper of the author where a good measure of relative concentration was introduced, used in informetrics to measure the degree of specialization of a journal w.r.t. the entire subject. This new information retrieval algorithm is shown to have many desirable properties (in the sense of the new Cater-Kraft wish list) including those of the original cosine-matching function of Salton. In addition the property of the cosine-matching function that, if one only uses weights 0 to 1, one is reduced to Boolean IR, is refined in the sense that one takes into consideration the broadness or specialization of a document and a query. Our new matching function satisfies these additional properties.
98255	Extended boolean retrieval: a heuristic approach? We show that the similarity measures for p-norm retrieval, as defined by Salton, Fox and Wu have some undesirable mathematical properties. We propose a new function that remedies some of these drawbacks. Still, even for this new similarity measure the extended Boolean model has some properties which can only be described as 'heuristic'.
98901	Determining the functionality features of an intelligent interface to an information retrieval system In this paper, we propose a method for specifying the functionality of an intelligent interface to large-scale information retrieval systems, and for implementing those functions in an operational environment. The method is based on a progressive, three-stage model of intelligent information support; a high-level cognitive task analysis of the information retrieval problem; a low-level specification of the host system functionality; and, derivation of explicit relations between the system functions and the cognitive tasks. This method is applied, by example, in the context of the European Space Agency Information Retrieval Service, with some specific suggestions for implementation of a stage one intelligent interface to that system.

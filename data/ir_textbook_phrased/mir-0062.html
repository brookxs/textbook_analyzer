3.3.1 the trec collection research in information-retrieval has frequently been criticized on two fronts . first , that it lacks a solid formal-framework as a basic foundation . second , that it lacks robust and consistent testbeds and benchmarks . the first of these criticisms is difficult to dismiss entirely due to the inherent degree of psychological suhjectiveness associated with the task of deciding on the relevance of a given document (which characterizes information , as opposed to data , retrieval) . thus , at least for now , research in information-retrieval will have to proceed without a solid formal underpinning . the second of these criticisms , however , can be acted upon . for three decades , experimentation in information-retrieval was based on relatively small test-collections which did not reflect the main issues present in a large bibliographical environment . further , comparisons between various retrieval-systems were difficult to make because distinct groups conducted experiments focused on distinct aspects of retrieval (even when the same test-collection was used) and there were no widely accepted benchmarks . reference collections 85 in the early 1990s , a reaction to this state of disarray was initiated under the leadership of donna harman at the national institute of standards and technology (nist) , in maryland . such an effort consisted of promoting a yearly conference , named trec for text-retrieval-conference , dedicated to experimentation with a large test collection comprising over a million documents . for each trec conference , a set of reference experiments is designed . the research groups which participate in the conference use these reference experiments for comparing their retrieval-systems . a clear statement of the purpose of the trec conferences can be found in the nist trec web-site [768] and reads as follows . the trec conference series is co-sponsored by the national institute of standards and technology (nist) and the information-technology office of the defense advanced research projects agency (darpa) as part of the tipster text program . the goal of the conference series is to encourage research in information-retrieval from large text applications by providing a large test collection , uniform-scoring procedures , and a forum for organizations interested in comparing their results . attendance at trec conferences is restricted to those researchers and developers who have performed the trec retrieval tasks and to selected government personnel from sponsoring agencies . participants in a trec conference employ a wide variety of retrieval techniques , including methods using automatic thesauri , sophisticated term-weighting , natural-language techniques , relevance-feedback , and advanced pattern-matching . each system works with the same test-collection that consists of about 2 gigabytes of text (over 1 million documents) and a given set of information-needs called `` topics . ' results are run through a common evaluation package so that groups can compare the effectiveness of different techniques and can determine how differences between systems affect performance . since the collection was built under the tipster program , it is frequently referred to as the tipster or the tipster/trec test-collection . here , however , for simplicity we refer to it as the trec collection . the first trec conference was held at nist in november 1992 , while the second trec conference occurred in august 1993 . in november 1997 , the sixth trec conference was held (also at nist) and counted the following participating organizations (extracted from [794]) : apple computer city univ. , london att labs research claritech corporation australian national univ. . cornell ltniv . / sabir research , inc. . carnegie mellon univ. . csiro (australia) cea (france) daimler benz res . center , ulm center for inf . res. , russia dublin ltniv . center 86 retrieval-evaluation duke univ. / univ. of colorado/bellcore eth (switzerland) fs consulting , inc. . ge corp. / rutgers univ. . george mason univ. / ncr corp. . harris corp. . ibm t.j. watson res . (2 groups) iss (singapore) iti (singapore) apl , johns hopkins univ. . lexis-nexis mds at rmit , australia mit/ibm almaden res . center msi/irit/univ . toulouse nec corporation new mexico state univ. (2 groups) nsa (speech-research group) open text corporation oregon health sciences univ. . queens college , cuny rutgers univ. (2 groups) siemens ag sri international twentyone univ. . california , berkeley univ. . california , san diego univ. . glasgow univ. . maryland , college park univ. . massachusetts , amherst univ. . montreal univ. . north carolina (2 groups) univ. . sheffield/univ . cambridge univ. . waterloo verity , inc. . xerox res . centre europe the seventh trec conference was held again at nist in november of 1998 . in the following , we briefly discuss the trec document-collection and the (benchmark) tasks at the trec conferences . as with most test-collections , the trec collection is composed of three parts : the documents , the example information requests (called topics in the trec nomenclature) , and a set of relevant documents for each example information request . further , the trec conferences also include a set of tasks to be used as a benchmark . the document collection the trec collection has been growing steadily over the years . at trec-3 , the collection-size was roughly 2 gigabytes while at trec-6 it had gone up to roughly 5.8 gigabytes . in the beginning , copyright restrictions prevented free-distribution of the collection and , as a result , the distribution cd-rom disks had to be bought . in 1998 , however , an arrangement was made which allows free access to the documents used in the most recent trec conferences . as a result , trec disk 4 and trec disk 5 are now available from nist at a small fee (us$ 200 in 1998) to cover distribution costs . information on how to obtain the collection (which comes with the disks) and the topics with their relevant-document sets (which have to be retrieved through the network) can be obtained directly from the nist trec web-site [768] . the trec collection is distributed in six cd-rom disks of roughly 1 gigabyte of compressed text each - the documents come from the following sources : wsj - ? wail street journal ^ p __ , associated press (news wire) ziff - * computer selects (articles) , ziff-davis fr ^ federal register reference collections 87 doe - + us doe publications (abstracts) sjmn ó ¶ gt ; san jose mercury news pat - + us patents ft ó * financial times cr óª congressional record fbis ógt ; foreign broadcast-information-service lat - gt ; l.a times table 3.1 illustrates the contents of each disk and some simple statistics regarding the collection (extracted from [794]) . documents from all subcollections are disk contents size number words/doc . words/doc . mb docs (median) (mean) 1 wsj , 1987-1989 267 98,732 245 434.0 ap , 1989 254 84,678 446 473.9 ziff 242 75,180 200 473.0 fr , 1989 260 25,960 391 1315.9 doe 184 226,087 111 120.4 2 wsj , 1990-1992 242 74,520 301 508.4 ap , 1988 237 79,919 438 468.7 ziff 175 56,920 182 451.9 fr , 1988 209 19,860 396 1378.1 3 sjmn , 1991 287 90,257 379 453.0 ap , 1990 237 78,321 451 478.4 ziff 345 161,021 122 295.4 pat , 1993 243 6,711 4,445 5391.0 4 ft , 1991-1994 564 210,158 316 412.7 fr , 1994 395 55,630 588 644.7 cr , 1993 235 27,922 288 1373.5 5 fbis 470 130,471 322 543.6 lat 475 131,896 351 526.5 6 fbis 490 120,653 348 581.3 table 3.1 document-collection used at trec-6 . stopwords are not removed and no stemming is performed (see chapter 7 for details on stemming) . tagged with sgml (see chapter 6) to allow easy-parsing (which implies simple coding for the groups participating at trec conferences) . major structures such as a field for the document number (identified by lt ; docnogt ;-rrb- and a field for the document text (identified by lt ; textgt ;-rrb- are common to all documents . minor structures might be different across subcollections to preserve parts of the structure in the original document . this has been the philosophy for formatting decisions at nist : preserve as much of the original structure as possible while providing a common framework which allows simple decoding of the data . an example of a trec document is the document numbered 880406-0090 retrieval-evaluation lt ; docgt ; lt ; docnogt ; wsj880406-0090 lt ; / docnogt ; lt ; hlgt ; att unveils services to upgrade phone networks under global plan lt ; / hlgt ; lt ; authorgt ; janet guyon (wsj staff) lt ; / authorgt ; lt ; dateline gt ; new york lt ; / dateline gt ; lt ; textgt ; american telephone ; telegraph co. introduced the first of a new generation of phone services with broad ... lt ; / textgt ; lt ; / docgt ; figure 3.7 trec document numbered wsj880406-0090 . in the wall-street-journal subcollection which is shown in figure 3.7 (extracted from [342]) . further details on the trec document-collection can be obtained from [794 , 768] . the example information requests (topics) the trec collection includes a set of example information requests which can be used for testing a new ranking-algorithm . each request is a description of an information-need in natural-language . in the trec nomenclature , each test information request is referred to as a topic . an example of an information request in trec is the topic numbered 168 (prepared for the trec-3 conference) which is illustrated in figure 3.8 (extracted from [342]) . the task of converting an information request (topic) into a system query (i.e. , a set of index-terms , a boolean-expression , a fuzzy expression , etc.) must be done by the system itself and is considered to be an integral part of the evaluation-procedure . the number of topics prepared for the first six trec conferences goes up to 350 . the topics numbered 1 to 150 were prepared for use with the trec-1 and trec-2 conferences . they were written by people who were experienced users of real systems and represented long-standing information-needs . the topics numbered 151 to 200 were prepared for use with the trec-3 conference , are shorter , and have a simpler structure which includes only three subfields (named title , description , and narrative as illustrated in the topic 168 above) . the topics numbered 201 to 250 were prepared for use with the trec-4 conference and are even shorter . at the trec-5 (which included topics 251-300) and trec-6 (which included topics 301-350) conferences , the topics were prepared with a composition similar to the topics in trec-3 (i.e. , they were expanded with respect to the topics in trec-4 which were considered to be too short) . reference collections 89 lt ; topgt ; lt ; numgt ; number : 168 lt ; titlegt ; topic : financing amtrak lt ; descgt ; description : a document will address the role of the federal-government in financing the operation of the national railroad transportation corporation (amtrak) . lt ; narrgt ; narrative : a relevant-document must provide information on the government 's responsibility to make amtrak an economically viable entity . it could also discuss the privatization of amtrak as an alternative to continuing government subsidies . documents comparing government subsidies given to air and bus transportation with those provided to amtrak would also be relevant . lt ; / topgt ; figure 3.8 topic numbered 168 in the trec collection . the relevant documents for each example information request at the trec conferences , the set of relevant documents for each example information request (topic) is obtained from a pool of possible relevant documents . this pool is created by taking the top k documents (usually , k = 100) in the rankings generated by the various participating retrieval-systems . the documents in the pool are then shown to human assessors who ultimately decide on the relevance of each document . this technique for assessing relevance is called the pooling method [794] and is based on two assumptions . first , that the vast majority of the relevant documents is collected in the assembled pool . second , that the documents which are not in the pool can be considered to be not relevant . both assumptions have been verified to be accurate in tests done at the trec conferences . a detailed description of these relevance-assessments can be found in [342 , 794] . the (benchmark) tasks at the trec conferences the trec conferences include two main information-retrieval-tasks [342] . in the first , called ad-hoc task , a set of new (conventional) requests are run against a fixed document-database . this is the situation which normally occurs in a library where a user is asking new queries against a set of static documents . in the second , called routing task , a set of fixed requests are run against a database whose documents are continually changing . this is like a filtering task in which the same questions are always being asked against a set of dynamic-documents (for instance , news clipping services) . unlike a pure filtering task , however , the retrieved documents must be ranked . 90 retrieval-evaluation for the ad-hoc task , the participant systems receive the test information requests and execute them on a pre-specified document-collection . for the routing task , the participant systems receive the test information requests and two distinct document-collections . the first collection is used for training and allows the tuning of the retrieval algorithm . the second collection is used for testing the tuned retrieval algorithm . starting at the trec-4 conference , new secondary-tasks , besides the ad-hoc and routing tasks , were introduced with the purpose of allowing more specific comparisons among the various systems . at trec-6 , eight (specific) secondary-tasks were added in as follows . ª chinese ad-hoc task in which both the documents and the topics are in chinese . ï filtering routing task in which the retrieval algorithm has only to decide whether a new incoming document is relevant (in which case it is taken) or not (in which case it is discarded) . no ranking of the documents taken needs to be provided . the test-data (incoming documents) is processed in time-stamp order . ï interactive task in which a human searcher interacts with the retrieval-system to determine the relevant documents . documents are ruled relevant or not relevant (i.e. , no ranking is provided) . ï nlp-task aimed at verifying whether retrieval algorithms based on natural-language-processing offer advantages when compared to the more traditional retrieval algorithms based on index-terms . ï cross languages ad-hoc task in wrhich the documents are in one language but the topics are in a different language . ï high precision task in which the user of a retrieval-system is asked to retrieve ten documents that answer a given (and previously unknown) information request within five minutes (wall clock time) . ï spoken-document-retrieval task in which the documents are written transcripts of radio broadcast-news shows . intended to stimulate research on retrieval techniques for spoken documents . ï very-large corpus ad-hoc task in which the retrieval-systems have to deal with collections of size 20 gigabytes (7.5 million documents) . for trec-7 , the nlp and the chinese secondary-tasks were discontinued . additionally , the routing task was retired as a main task because there is a consensus that the filtering task is a more realistic type of routing task . trec-7 also included a new task called query task in which several distinct query versions were created for each example information request [794] . the main goal of this task is to allow investigation of query-dependent retrieval-strategies , a well known problem with the trec collection due to the sparsity of the given information requests (which present very little overlap) used in past trec conferences . reference collections 91 besides providing detailed descriptions of the tasks to be executed , the trec conferences also make a clear distinction between two basic techniques for transforming the information requests (which are in natural-language) into query statements (which might be in vector form , in boolean form , etc.) . in the trec-6 conference , the allowable query-construction methods were divided into automatic methods , in which the queries were derived completely automatically from the test information requests , and manual methods , in which the queries were derived using any means other than the fully automatic method [794] . evaluation-measures at the trec conferences at the trec conferences , four basic types of evaluation-measures are used : summary table statistics , recall-precision averages , document level averages , and average-precision histograms . briefly , these measures can be described as follows (see further details on these measures in section 3.2) . ï summary table statistics consists of a table which summarizes statistics relative to a given task . the statistics included are : the number of topics (information requests) used in the task , the number of documents retrieved over all topics , the number of relevant documents which were effectively retrieved for all topics , and the number of relevant documents which could have been retrieved for all topics . ï recall-precision averages consists of a table or graph with average-precision (over all topics) at 11 standard recall levels . since the recall levels of the individual queries are seldom equal to the standard recall levels , interpolation is used to define the precision at the standard recall levels . further , a non-interpolated average-precision over seen relevant documents (and over all topics) might be included . ï document level averages in this case , average-precision (over all topics) is computed at specified document cutoff values (instead of standard recall levels) . for instance , the average-precision might be computed when 5 , 10 , 20 , 100 relevant documents have been seen . further , the average r-precision value (over all queries) might also be provided . ï average-precision histogram consists of a graph which includes a single measure for each separate topic . this measure (for a topic ti) is given , for instance , by the difference between the r-precision (for topic tz) for a target retrieval algorithm and the average r-precision (for topic t %) computed from the results of all participating retrieval-systems .
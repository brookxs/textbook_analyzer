naive-bayes text-classification multinomial naive-bayes multinomial nb (113) in text-classification , our goal is to find the best class for the document . the best class in nb classification is the most likely or maximum a posteriori (map) class : (114) in equation 114 , many conditional-probabilities are multiplied , one for each position . this can result in a floating-point underflow . it is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities . the class with the highest log probability-score is still the most probable ; and the logarithm function is monotonic . hence , the maximization that is actually done in most implementations of nb is : (115) equation 115 has a simple interpretation . each conditional parameter is a weight that indicates how good an indicator is for . similarly , the prior is a weight that indicates the relative-frequency of . more frequent classes are more likely to be the correct class than infrequent classes . the sum of log prior and term-weights is then a measure of how much evidence there is for the document being in the class , and equation 115 selects the class for which we have the most evidence . we will initially work with this intuitive interpretation of the multinomial nb model and defer a formal-derivation to section 13.4 . how do we estimate the parameters and ? we first try the maximum-likelihood-estimate (mle ; probtheory) , which is simply the relative-frequency and corresponds to the most likely value of each parameter given the training-data . for the priors this estimate is : (116) we estimate the conditional-probability as the relative-frequency of term in documents belonging to class : (117) positional independence-assumption the problem with the mle estimate is that it is zero for a term-class combination that did not occur in the training-data . if the term wto in the training-data only occurred in china documents , then the mle estimates for the other classes , for example uk , will be zero : (118) 113 sparseness figure 13.2 : naive-bayes algorithm (multinomial-model) : training and testing . to eliminate zeros , we use add-one or laplace-smoothing , which simply adds one to each count (cf. section 11.3.2) : (119) term class 116 we have now introduced all the elements we need for training and applying an nb classifier . the complete algorithm is described in figure 13.2 . table 13.1 : data for parameter-estimation examples . docid words in document in china ? training-set 1 chinese beijing chinese yes 2 chinese chinese shanghai yes 3 chinese macao yes 4 tokyo japan chinese no test-set 5 chinese chinese chinese tokyo japan ? worked example . for the example in table 13.1 , the multinomial parameters we need to classify the test document are the priors and and the following conditional-probabilities : 119 we then get : end worked example . table 13.2 : training and test times for nb . mode time-complexity training testing we use as a notation for here , where is the length of the training collection . this is nonstandard ; is not defined for an average . we prefer expressing the time-complexity in terms of and because these are the primary statistics used to characterize training collections . the time-complexity of applymultinomialnb in figure 13.2 is . and are the numbers of tokens and types , respectively , in the test document . applymultinomialnb can be modified to be (exercise 13.6) . finally , assuming that the length of test documents is bounded , because for a fixed constant . table 13.2 summarizes the time complexities . in general , we have , so both training and testing complexity are linear in the time it takes to scan the data . because we have to look at the data at least once , nb can be said to have optimal time-complexity . its efficiency is one reason why nb is a popular text-classification method . subsections relation to multinomial unigram-language-model
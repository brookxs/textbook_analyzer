4.3.10 other measures the performance-measures already listed are by far the most common ways in which search-engines are evaluated in the literature . several others , however , have been important in the past and may again prove useful in some situations . expected search length the ordered-list of relevance-assessments described in section 4.3.5 also recommends another , holistic evaluation of the entire retrieval 's behavior ; this method is known as expected search length (esl) . esl considers the length of a `` path '' as users walk down the ordered hitlist , measuring how many irrelevant documents were seen on this path before each relevant-document ; `` expected '' refers to the average length of each path ending in a relevant-document . cooper initially proposed this model to measure the work a search-engine saves , in comparison to searching the entire collection at random [cooper , 1968] . given that a search-engine retrieves documents in hitlist order , esl also requires a criterion by which the users ' wandering paths are stopped . van rijsbergen , p. 160 discusses a number of predicates that 138 finding out about might be used to terminate the search : some fixed number of relevant documents , some fraction of all relevant documents , etc. . because the generality of queries can vary considerably , it is desirable to terminate the esl after some fixed fraction e of relevant documents has been retrieved . for this same reason , it makes sense to normalize esl with respect to the number of relevant documents we might expect to retrieve if we were retrieving at random . if we use nret for the number of retrieved documents (i.e. , those satisfying the predicate mentioned earlier) , we can estimate the expected random-search length randsl as : nret - (ndoc - rel) óknó (4 - `` gt ; then the expected search length reduction factor randsl ~ esl eslrf - ----- óó ----- (4.20) randsl captures the amount a real search-method improves over the random case . operating characteristic curves swets [swets , 1963] enumerated a number of abstract desiderata (quoted by van rijsbergen , p. 155) that we might wish for any assessment measure . according to these , ir 's standard re/pre plot leaves much to be desired , in particular because this two-dimensional assessment makes direct comparison impossible . swets therefore recommended an analysis from the perspective of signal-detection , based on several key assumptions : assumption 1 there is a `` relevant '' signal we wish to distinguish from background-noise . we can consider the worst-case to be comparison against an `` irrelevant '' signal with both signals imposed over the data-collection . we can imagine that this signal is generated by the presence or absence of some keywords . assessing the retrieval 139 figure 4.17 distinguishing between overlapping distributions assumption 2 these two signals are to be discriminated according to only a single dimension . assumption 3 these signals are both distributed normally across the corpus . in this idealized case , we get a picture similar to figure 4.17 . then , because our corpus has been ordered by the ranking , the goal becomes to select a value rankt that best separates these two modal distributions . using a simple retrieval rule that retrieves a document if its value is above the threshold ranktgt ; wherever we place this threshold we are bound to make two types of errors . there will be some rel documents that fall below our threshold (shaded in figure 4.17) and some irrelevant documents that fall above it (cross-hatched) . following signal-detection-theory we can call the first set `` falseó '' errors and the second `` false + '' errors . (these are often called type 1 and type-2 errors , respectively .) note that the ratio of the right tail of the rel curve (the area not cross-hatched in in figure 4.17) to the total-area under the rel curve corresponds exactly to the recall measure defined earlier (equation 4.2) , while the ratio of the right tail of the nrel curve (cross-hatched in figure 4.17) to the total-area under the nrel curve corresponds exactly to fallout (equation 4.4) . 140 finding out about 0.8 0.6 0.4 0.2 fraction rel gt ; % fraction nrel gt ; x 0.2 0.4 0.6 0.8 figure 4.18 operating characteristic curve the parametric-curve defined by the percentage of rel versus nrel documents retrieved as r is varied is called the operating characteristic curve . obviously , if these two distributions are identical , this curve will be exactly a diagonal line , from (0,0) to (1,1) . if the mean value of the rel distribution is greater than that of the nrel the operating characteristic curve is moved closer to the upper-left corner , as shown in figure 4.18 . while swets (and subsequently others [robertson , 1969 ; bookstein and kraft , 1977]) then considered fairly elaborate tests to discriminate the relative-performance of retrieval-systems with respect to such curves , it is fair to say that the 1979 assessment of van rijsbergen , p. 154 still stands : ... although the swets model is theoretically attractive and links ir measurements to ready-made and well-developed statistical-theory , it has not found general acceptance amongst workers in the field . assessing the retrieval 141 optimal selection of rankr depends on specification of the costs (losses) of making false + or falseó errors . for example , if you are an overworked and underpaid law clerk and you read an irrelevant document (false +) , youve wasted precision attention , but that 's all ; if you miss a reference you should have found (falseó) the cost might be huge . but if you 're a partying undergraduate with one more term paper between you and summer vacation , your assessments might be quite different . section 5.5.6 gives an example of how explicit models of these various costs can be incorporated within a bayesian decision-making framework .
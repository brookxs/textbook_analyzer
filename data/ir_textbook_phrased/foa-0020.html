1.7 how well are we doing ? suppose you and i each build an foa search-tool ; how might we decide which does the better job ? how might a potential customer decide on their relative values ? if we use a new search engine that seems to work much better , how can we determine which of its many features are critical to this success ? if we are to make a science of foa , or even if we only wish to build consistent , reliable tools , it is vital that we establish a methodology by which the performance of search-engines can be rigorously evaluated . just as your evaluation of a human question-answerer (professor , reference librarian , etc.) might well depend on subjective factors (how well you `` communicate '') and factors that go beyond the performance of the search-engine (does any available document contain a satisfying answer ?) , evaluation of search-engines is notoriously difficult . the field of ir has made great progress , however , by adopting a methodology for search-engine-evaluation that has allowed objective assessment of a task that is closely related to foa . here we will sketch this simplified notion of the foa task . the first step is to focus on a particular query . with respect to this query , we identify the set of documents rel that are determined to be omniscient relevant to it . '' ' '' then a good search-engine is one that can retrieve all relevance an (j onjy ^ documents in jrel . figure 1.10 shows both rel and retr , the set of documents actually retrieved in response to the query , in terms of a venn-diagram . clearly , the number of documents that were designated both relevant and retrieved , retr d rel , will be a key measure of success . but we must compare the size of the set \ retr n rel | to something , and several standards of comparison are possible . for example , if we are very concerned that the search-engine retrieve every relevant-document , overview 35 high-recall retrieval ^ - '' * '' corpus figure 1.10 comparison of retrieved versus relevant documents then it is appropriate to compare the intersection to the number of documents marked as relevant , | rel | . this measure of search-engine-performance is known as recall : \ retr n rel \ recall = however , we might instead be worried about how much of what the users see is relevant , so an equally reasonable standard of comparison is what number of the documents retrieved , | retr | , are in fact relevant . this measure is known as precision : \ retr fl rel \ precision = \ retr (1.2) note that even in this simple measure of search-engine-performance , we have identified two legitimate criteria . in real applications , our users will often vary as to whether high precision or high recall is more important . for example , a lawyer looking for every prior ruling (i.e. , judicial opinions , retrievable as separate documents) that is on point for his or her case will be more interested in high-recall behavior . the typical undergraduate , on the other hand , who is quickly searching the-web for a term paper due the next day , knows all too well that there may be many , many relevant documents somewhere out there . but the student cares much more that the first screen of hits be full of relevant leads . 36 finding out about examples of high-recall and high-precision retrievals are also shown in figure 1.10 . to be useful , this same analysis must be extended to consider the order in which documents are retrieved , and it must consider performance across a broad range of typical queries rather than just one . these and other issues of evaluation are taken up in chapter 4 .
7.4 selective-dissemination-of-information search selective-dissemination-of-information , frequently called dissemination systems , are becoming more prevalent with the growth of the-internet . a dissemination system is sometimes labeled a `` push '' system while a search-system is called a `` pull '' system . the differences are that in a search-system the user proactively makes a decision that he needs information and directs the query to the information-system to search . in a dissemination system , the user defines a profile (similar to a stored query) and as new information is added to the system it is automatically compared to the user 's profile . if it is considered a match , it is asynchronously sent to the user 's `` mail '' file (see chapter 1) . 180 chapter 7 one concept that ties together the two search statements (query and profile) is the introduction of a time parameter associated with a search statement . as long as the time is in the future , the search statement can be considered active and disseminating as items arrive . once the time parameter is past , the user 's need for the information is no longer exists except upon demand (i.e. , issuing the search statement as an ad-hoc query) . the differences between the two functions lie in the dynamic nature of the profiling process , the size and diversity of the search statements and number of simultaneous searches per item . in the search-system , an existing database exists . as such , corpora statistics exist on term-frequency within and between terms . these can be used for weighting factors in the indexing process and the similarity comparison (e.g. , inverse-document-frequency algorithms) . a dissemination system does not necessarily have a retrospective database associated with it . thus its algorithms need to avoid dependency upon previous data or develop a technique to estimate terms for their formula . this class of system is also discussed as a binary-classification system because there is no possibility for real-time feedback from the user to assist in search statement refinement . the system makes a binary-decision to reject or file the item (lewis-95) . profiles are relatively static search statements that cover a diversity of topics . rather than specifying a particular information-need , they usually generalize all of the potential information-needs of a user . they are focused on current information-needs of the user . thus profiles have a tendency to contain significantly more terms than an ad-hoc query (hundreds of terms versus a small number) . the size tends to make them more complex and discourages users from wanting to change them without expert advice . one of the first commercial search techniques for dissemination was the logicon message-dissemination system (lmds) . the system originated from a system created by chase , rosen and wallace (crw inc.) . it was designed for speed to support the search of thousands of profiles with items arriving every 20 seconds . it demonstrated one approach to the problem where the profiles were treated as the static database and the new item acted like the query . it uses the terms in the item to search the profile structure to identify those profiles whose logic could be satisfied by the item . the system uses a least frequently occurring trigraph (three character) algorithm that quickly identifies which profiles are not satisfied by the item . the potential profiles are analyzed in detail to confirm if the item is a hit . another example of a dissemination approach is the personal library software (pls) system . it uses the approach of accumulating newly received items into the database and periodically running user 's profiles against the database . this makes maximum use of the retrospective search software but loses near-real-time delivery of items . more recent examples of a similar approach are the retrievalware and the inroute software-systems . in these systems the item is processed into the searchable form . since the profiles are relatively static , some use is made in identifying all the terms used in all the profiles . any words in the items that are members of this list can not contribute to the similarity process and user-search techn iques 181 thus are eliminated from the search structure . every profile is then compared to the item . retrievalware uses a statistical algorithm but it does not include any corpora data . thus not having a database does not affect its similarity-measure . inroute , like the inquery-system used against retrospective database , uses inverse-document-frequency information . it creates this information as it processes items , storing and modifying it for use as future items arrive . this would suggest that the values would be continually changing as items arrive until sufficient items have arrived to stabilize the inverse-document-frequency weights . relevance-feedback has been proven to enhance the search capabilities of ad-hoc queries against retrospective databases . relevance-feedback can also be applied to dissemination systems . unlike an ad-hoc query situation , the dissemination process is continuous , and the issue is the practicality of archiving all of the previous relevance-judgments to be used in the relevance-feedback process . allan performed experiments on the number of items that have to arrive and be judged before the effects of relevance-feedback stabilize (allan-96) . previous work has been done on the number of documents needed to generate a new query and the amount of training needed (buckley-94 , aalbersberg-92 , lewis-94) . the two major choices are to save relevant items or relevance statistics for words . by saving dissimilar items , allan demonstrated that the system sees a 2-3 per cent loss in effectiveness by archiving 10 per cent of the relevance-judgments . this still requires significant storage space . he was able to achieve high effectiveness by storing information on as few as 250 terms . another approach to dissemination uses a statistical classification-technique and explicit error-minimization to determine the decision criteria for selecting items for a particular profile (schutze-95) . in this case , the classification-process is related to assignment for each item into one of two classes : relevant to a user 's profile or non-relevant . error-minimization encounters problems in high-dimension spaces . the dimensionality of an information-space is defined by the number of unique terms where each term is another dimension . this is caused by there being too many dimensions for a realistic training-set to establish the error-minimization parameters . to reduce the dimensionality , a version of latent-semantic-indexing (lsi) can be used . the process requires a training-data-set along with its associated profiles . relevance-feedback is an example of a simple case of a learning-algorithm that does not use error-minimization . other examples of algorithms used in linear-classifiers that perform explicit error-minimization are linear-discriminant-analysis , logistic-regression and linear neural-networks . schutze et al. used two approaches to reduce the dimensionality : selecting a set of existing features to use or creating a new much smaller set-of-features that the original features are mapped into . a x2 measure was used to determine the most important features . the test was applied to a table that contained the number of relevant (nr) and non-relevant (nnr) items in which a term occurs plus the number of relevant and non-relevant items in which the term does not occur (nr. , nnr . respectively) . the formula used was : 182 chapter 7 2 ___________ n (nrnnr _ - nr_nnr) 2 ___________ x (nr + nr _) {nnr + nnrj (nr + nnr) (nr _ + nnr _) to focus the analysis , only items in the local-region defined by a profile were analyzed . the chi-squared technique provides a more effective mechanism than frequency of occurrence of terms . a high x2 score indicates a feature whose frequency has a significant dependence on occurrence in a relevant or non-relevant item . an alternative technique to identify the reduced feature (vector) set is to use a modified latent-semantic index (lsi) technique to determine a new reduced set of concept-vectors . the technique varies from the lsi technique described in chapter 5 by creating a separate representation of terms and items by each profile to create the `` local '' space of items likely to be relevant (i.e. , local lsi) . the results of the analysis go into a learning-algorithm associated with the classification-technique (hull-94) . the use of the profile to define a local-region is essential when working with large-databases . otherwise the number of lsi factors is in the hundreds and the ability to process them is currently unrealistic . rather than keeping the lsi factors separate per profile , another approach is to merge the results from all of the queries into a single lsi analysis (dumais-93) . this increases the number of factors with associated increase in computational-complexity . once the reduced vector set has been identified , then learning-algorithms can be used for the classification-process . linear-discriminate-analysis , logistic-regression and neural-networks are three possible techniques that were compared by schutze et al. . other possible techniques are classification-trees (tong-94 , lewis-94a) , bayesian-networks (croft-94) , bayesian-classifiers (lewis-92) , rules induction (apte-94) , nearest-neighbor techniques (masand-92 , yang-94) , and least-square methods (fuhr-89) . linear discrimination-analysis uses the covariance class for each document-class to detect feature dependence (gnanadesikan-79) . assuming a sample of data from two groups with ns and ni members , mean vectors jc j and x2 and covariance-matrices d and c2 respectively , the objective is to maximize the separation between the two groups . this can be achieved by maximizing the distance between the vector means , scaling to reflect the structure in the pooled covariance-matrix . thus choose a such that : a a = arga max user-search techniques 183 is maximized where t is the transpose and (ª , + n2 - 2) c = (wj - l) ci + 0 ? 2 1) c2 . since c is positive , the cholesky-decomposition of c = rt. . let b = ra ; then the formula becomes ; btrt-lcxl-x2) a = arg bmax ---------- j == -------- which is maximized by choosing b oc rt ~ l (xr x 2) . this means : a * = rô16 = c-1 (x1 - x2) the one dimensional space defined by y = atx should cause the group means to be well separated - to produce a non-linear classifier , a pair of shrinkage parameters is used to create a very general family of estimators for the group covariance-matrix (freidman-89) . this process called regularized discriminant-analysis looks at a weighted-combination of the pooled and unpooled covariance-matrices . the optimal values of the shrinkage parameters are selected based upon the cross-validation over the training-set . the non-linear classifier produced by this technique has not been shown to make major improvements in the classification-process (hull-95) . a second approach is to use logistic-regression (cooper-94a) . it models a binary response variable by a linear combination of one or more predictor variables , using a logit link fiinction : g (7l) = l0g (7c / (l-7t)) and modeling variance with a binomial random-variable . this is achieved by modeling the dependent variable log (7i / (l - 7c)) as a linear combination of independent variables using a form g {%) = x $ gt ; . in this formula n is the estimated response probability (probability-of-relevance) , x , is the feature-vector (reduced vector) for document / , and (3 is the weight-vector which is estimated from the-matrix of feature-vectors . the optimal value of p can be calculated using the maximum-likelihood and the newton-raphson-method of numerical-optimization (mcculiagh-89) . the major difference from previous experiments using logistic-regression is that schutze et al. do not use information from all the profiles but restrict the analysis for each profile . a third technique is to use neural-networks for the learning function . a neural-network is a network of input-and-output cells (based upon neuron functions in the brain) originating with the work of mcculloch and pitts (mcculloch-43) . each input-pattern is propagated forward through the network . when an error is detected it is propagated backward adjusting the cell parameters to reduce the 184 chapter 7 error , thus achieving learning . this technique is very flexible and can accommodate a wide range of distributions . a major risk of neural-networks is that they can overfit by learning the characteristics of the training-data-set and not be generalized enough for the normal input of items . in applying training to a neural-network-approach , a validation set of items is used in addition to the training items to ensure that overfitting has not occurred . as each iteration of parameter-adjustment occurs on the training-set , the validation set is retested . whenever the errors on the validation set increase , it indicates that overfitting is occurring and establishes the number of iterations on training that improve the parameter-values while not harming generalization . the linear-and-non-linear architectures for an implementation of neural-nets is shown in figure 7.9 . output unit output unit hidden unit block for lsi hidden unit block for terms lsi representation term-representation lsi representation term-representation linear neural-network non-linear neural-network figure 7.9 linear-and-non-linear networks in the non-linear network , each of the hidden blocks consists of three hidden units . a hidden unit can be interpreted as feature-detectors that estimate the probability of a feature being present in the input . propagating this to the output unit can improve the overall estimation of relevance in the output unit . the networks show input of both terms and the lsi representation (reduced feature-set) . in both architectures , all input units are directly connected to the output units . relevance is computed by setting the activations of the input units to the document 's representation and propagating the activation through the network to user-search techniques 185 the output unit , then propagating the error back through the network using a gradient-descent-algorithm (rumelhart-95) . a sigmoid was chosen as : as the activation function for the units of the network (schutze-95) . in this case backpropagation minimizes the same error as logistic-regression (rumelhart-95a) . the cross-entropy error is : til0gctj + 1 - tj) l0g (l - lt ; ji) where t [is the relevance for document / and a \ is the estimated relevance (or activation of the output unit) for document / . the definition of the sigmoid is equivalent to : which is the same as the logit link function . schutze et al. performed experiments with the tipster test database to compare the three algorithms . they show that the linear-classification schemes perform 10-15 per cent better than the traditional relevance-feedback . to use the learning-algorithms based upon error-minimization and numerical-computation one must use some technique of dimensionality-reduction . their experiments show that local latent-semantic-indexing is best for linear discrimination-analysis and logistic-regression since they have no mechanism for protecting against overfitting . when there are mechanisms to avoid overfitting such as in neural-networks , other less precise techniques of dimension-reduction can be used . this work suggests that there are alternatives to the statistical-classification scheme associated with profiles and dissemination . an issue with mail files is the logical reorganization associated with display of items . in a retrospective query , the search is issued once and the hit list is a static file that does not change in size or order of presentation . the dissemination function is always adding items that satisfy a user 's profile to the user 's mail file . if the items are stored sorted by rank , then the relative order of items can always be changing as new items are inserted in their position based upon the rank value . this constant reordering can be confusing to the user who remembers items by spatial-relationships as well as naming . thus the user may remember an item next to another item is of significant interest . but in trying to 186 chapter 7 retrieve it at a later time , the reordering process can make it significantly harder to find .
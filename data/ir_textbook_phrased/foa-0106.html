5.5.4 binary independence-model perhaps the simplest model proceeds by imagining binary , independent features ; it is conventionally called (surprise !) the binary independence-model (bimj [robertsonandsparck jones , 1976 ; van rijsbergen , 1977] . first , the binary assumption is that all the features xt are binary . this is not a very restrictive assumption and is used only to simplify the derivation . mathematical foundations 171 the much bigger assumption is that the documents ' features occur independently of one another . we have discussed the problems with such an assumption before . van rijsbergen , p. 120 quotes j. h. williams 's expression of the paradox : the assumption of independence of words in a document is usually made as a matter of mathematical convenience . without the assumption , many of the subsequent mathematical relations could not be expressed . with it , many of the conclusions should be accepted with extreme caution . [williams , 1965 , emphasis in original] the key advantage it allows is that the probability of a feature-vector x becomes simply the product of the marginal probabilities of the individual features : pr (x \ rel) = f {pr (x , - \ rel) (5.40) i very convenient - and very unrealistic . ^ '' maybe this applying this decomposition to our odds calculation gives : assumption is n't so bad ? odds (rel | x) = odds (rel) ï ft pr {xi ^ it will be convenient to introduce the variables pi and qi to capture the probabilities that feature xx is present , given that a document is or is not relevant : xi = l \ rel) (5.42) qi = pr (xi = l \ rd) (5.43) the complementary probabilities concerning documents in which the feature is absent can also be defined easily : 1 - pi = pr {xi = 0 \ rel) (5.44) 1 - ca = pr (xi = 0 \ rel) (5.45) these definitions break the product into two portions , the first having to do with those features that are present in a particular document 172 finding out about rel rel q d figure 5.6 random variables underlying binary independence-model and the second with those that are not : odds (rel \ x) = odds (rel) yl ~ yl ^ ~ ~ ^ (5 * 46) recall that both queries and documents live within the same vector-space defined over the features x \ . the two products of equation 5.46 (defined in terms of presence or absence of a feature in a document) can be further broken into four subcases , depending on whether the features occur in the query . we next make another `` background '' assumption concerning all the features x \ that are not in both the query and the document of current interest ; we assume that the probability of these features being present in relevant and irrelevant documents is equal : pi = qi . in other words , for those terms we do n't care about (because they do n't affect this query/document comparison) , we are happy to think that their occurrence is independent of their relevance . consider the sets d and q shown in figure 5.6 defined in terms of those features xj present and absent in the document and query , respectively . * regrouping the two products of equation 5.46 into four products created by the two sets d and q , the ^ terms cancel except in the intersection of the query and document (where the feature xj is present in both) and in q \ dgt ; the set-difference of q less d : odds (rel \ x) = odds (rel) ' yl ~ ' yl ^ '' ^ (5 * 47) xtedf \ q 4 * xteq \ d l ~ * apologies for the unfortunate overuse of the same letter ` q ' for denoting both the set q of features contained in the query and the probability qt of the presence of a feature in irrelevant documents , but there is no intended , direct-connection between these two quantities . mathematical foundations 173 in the retrieval situation we will exploit the sparseness that makes it much more efficient to keep track of where a feature does occur (x , - = 1) than all the places it does not (x \ = 0) . since the second product is defined over all the features of q except those in d , if we are careful to `` premultiply '' each feature in their intersection by a reciprocal , we can then safely multiply everything in the query by the same ratio : odds (rel \ x) = odds (rel) - u] óll - t \ the next section will show the utility of separating the last term , which depends on features of the document in question , from the first two , which do not , as part of an online retrieval calculation . but first , it is worthwhile considering how we might attempt to estimate some of the required statistics [robertson and sparck jones , 1976] . fuhr [fuhr , 1992] , for example , considers the retrospective case when we have relevance-feedback from a user who has evaluated each of the top n documents in an initial retrieval and has found r of these to be relevant (as well as evaluating all the n ó r remaining and found them to be irrelevant !) . if a particular feature x \ is present in n of the retrieved documents with r of these relevant , then this bit of relevance-feedback provides reasonable estimates for pi and q \ \ p , = j (5.49)
1.5.1 automatically selecting keywords we begin by considering the document at its most mechanical level , as a string of characters . our first candidates for keywords will be tokens , things broken by white-space . that is , each token in the document could be considered one of its keywords . how good is this simple solution ? suppose users ask for documents about cars and the document we are currently indexing has the string car . it seems reasonable to assume that users are interested in this document , despite the fact that the query happens to contain the plural form cars while the document contains the singular car . for many queries we might like to consider occurrences of the words car and cars , or even retrieval and retrieve , as roughly interchangeable with one another ; the suffixes do not affect meaning dramatically . and of course our problem does n't end with plurals ; we could make similar arguments concerning past-tense - ed endings and - ing participles . this simple solution also depends too much on where spaces occur . consider the german noun geschwiisroigkeitsbescrajsiiaimg , corresponding to the english phrase speed limit . in many ways , the fact that english happens to put a white-space between the words while german does not is not semantically critical to the meaning of these descriptors or the documents in which they might occur . such morphological-features - used to mark relatively superficial , surfacestructure features (such as tense or singular versus plural) - can be considered less important to the meaning . and differences between german and english are trivial when they are compared to asian texts , where the relationship between characters and words is radically different . what about hyphenation ? use of the word database , the phrase data-base , and the hyphenated phrase data-base is highly variable , depending on author preference and current practice at the time and place of publication . yet we would hope that all occurrences of any of these tokens would be treated as references to approximately the same semantic-category . similarly , we hope that the end-of-line hyphenation 28 finding out about (breaking long words at syllable boundaries) would not create two keywords when we would expect only one . but simply adding '' - '' to the set of white-space characters defining tokens would make clinton-dole and a-z keywords , too ! hyphenation is concerned with the situation in which a potential keyword is broken up by punctuation ; what about those situations where a space also breaks up a semantic unit ? speed limit seems semantically cohesive , but what algorithm could distinguish it from other bigrams (consecutive pairs-of-words) that happen to occur sequentially ? the problem only becomes that much more complicated if we attempt to consider longer noun-phrases like apples and oranges or back-propagation-neural-network , let alone more complicated syntactic compounds such as verb-phrases , clauses , or sentences . identifying phrases is an important and active area of research from the perspectives of both ir and computational-linguistics . summarizing , we will take a token to be our default keyword because this is straightforward . more sophisticated solutions will handle hyphenation , multiword phrases , subtoken stems , and so on (cf. section 2.3.1) .
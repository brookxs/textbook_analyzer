3.3.2 indexing by concept the basis for concept-indexing is that there are many ways to express the same idea and increased retrieval-performance comes from using a single representation . indexing by term treats each of these occurrences as a different index and then uses thesauri or other query-expansion techniques to expand a query to find the different ways the same thing has been represented . concept-indexing determines a canonical set of concepts based upon a test-set of terms and uses them as a basis for indexing all items this is also called latent-semantic-indexing because it is indexing the latent-semantic information in items . the determined set of concepts does not have a label associated with each concept (i.e. , a word or set of words that can be used to describe it) , but is a mathematical representation (e.g. , a vector) . an example of a system that uses concept-indexing is the matchplus system developed by hnc inc. . the matchplus system uses neural-networks to facilitate machine-learning of concept/word relationships and sensitivity to similarity of use (caid-93) . the systems goal is to be able to determine from the corpus of items , word relationships (e.g. , synonyms) and the strength of these relationships and use that information in generating context-vectors . two neural-networks are used . one neural-network learning-algorithm generates stem context-vectors that are sensitive to similarity of use and another one performs query-modification based upon user-feedback . word stems , items and queries are represented by high-dimensional (at least 300 dimensions) vectors called context-vectors . each dimension in a vector could be viewed as an abstract concept class . the approach is based upon cognitive-science work by waltz and pollack (waltx-85) . to define context-vectors , a set of n features are selected on an ad-hoc basis (e.g. , high-frequency terms after removal of stop words) . the selection of the initial features is not critical since they evolve and expand to the abstract concept classes used in the indexing process . for any word-stem kf its context-vector v * is an / i-dimensional vector with each component . / interpreted as follows : 64 chapter 3 yk positive if k is strongly associated with feature j \ k Â´ 0 if word k is not associated with feature j yk negative if word k contradicts feature y the interpretation of components for concept-vectors is exactly the same as weights in neural-networks . each of the n features is viewed as an abstract concept class . then each word-stem is mapped to how strongly it reflects each concept in the items in the corpus . there is overlap between the concept classes (features) providing a distributed-representation and insulating against a small number of entries for context-vectors that could have no representation for particular stems (hinton-84) . once the context-vectors for stems are determined , they are used to create the index for an item . a weighted sum of the context-vectors for all the stems in the item is calculated and normalized to provide a vector-representation of the item in terms of the n concept classes (features) . chapter 5 provides additional detail on the specific algorithms used . queries (natural-language only) go through the same analysis to determine vector-representations . these vectors are then compared to the item vectors .
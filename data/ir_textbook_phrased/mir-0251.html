13.3.1 measuring the-web measuring the-internet and in particular the-web , is a difficult task due to its highly dynamic nature . nowadays , there are more than 40 million computers in more than 200 countries connected to the-internet , many of them hosting web-servers . the estimated number of web-servers ranges from 2.4 million according to netsizer [597] (november 1998) to over three million according to the netcraft web-survey [596] (october 1998) . this wide range might be explained when we consider that there are many web-sites that share the same web-server using virtual-hosts , that not all of them are fully accessible , that many of them are provisional , etc. . other estimations were made by sampling 0.1 % of all internet numeric addresses obtaining about 2 million unique web-sites [619] or by counting domain-names starting with www which in july 1998 were 780,000 according to the-internet domain survey [599] . however , since not all web-servers have this prefix , the real number is even higher . considering that in july 1998 the number of internet hosts was estimated at 36.7 million [599] , there is about one web-server per every ten computers connected to the 370 searching the-web internet . the characterization of the-web is a new task of the-web consortium [797] . in two interesting articles , already (sadly) outdated , bray [114] and woodruff et al. [834] studied different statistical measures of the-web . the first study uses 11 million pages while the second uses 2.6 million pages , with both sets gathered in november 1995 . their characterization of web-pages is partially reproduced in the following paragraphs . a first question is how many different institutions (not web-servers) maintain web-data . this number is smaller than the number of servers , because many places have multiple-servers . the exact number is unknown , but should be more than 40 % of the number of web-servers (this percentage was the value back in 1995) . the exact number of web-pages is also not known . estimates at the beginning of 1998 ranged from 200 to 320 million , with 350 million as the best current estimate (july 1998 [91]) . the latter study used 20,000 random queries based on a lexicon of 400,000 words extracted from yahoo! . those queries were submitted to four search-engines and the union of all the answers covered about 70 % of the-web . figure 13.1 gives an approximation of how the number of web-servers and the number of pages have changed in recent years . between 1997 and 1998 , the size-of-the-web doubled in nine months and is currently growing at a rate of 20 million pages per month . on the other hand , it is estimated that the 30,000 largest web-sites (about 1 % of the-web) account for approximately 50 % of all web-pages [619] . the most popular formats for web-documents are html , followed by gif and jpg (both for images) , ascii text , and postscript , in that order . the most popular compression tools used are gnu zip , zip , and compress . what is a typical html page ? first , most html pages are not standard , meaning that they do not comply with all the html specifications . in ad300 200 number of web-pages (millions) _ cl 19 % 1997 1998 figure 13.1 approximate growth of the-web . characterizing the-web 371 dition , although html is an instance of sgml , html-documents seldom start with a formal document-type-definition . second , they are small (around 5 kbs on average with a median of 2 kbs) and usually contain few images (between one and two on average with an average size of 14 kb) . the pages that have images use them for presentation issues such as colored bullets and lines . an average page has between five and 15 hyperlinks (more than eight links on average) and most of them are local (that is , they point to pages in their own web-server hierarchy) . on average , no external server points to any given page (typically , there are only local links pointing to a given page) . this is true even for home-pages of web-sites . in fact , in 1995 , around 80 % of these home-pages had fewer than ten external-links pointing to each of them . the top ten most referenced sites are microsoft , netscape , yahoo! , and top us universities . in these cases we are talking about sites which are referenced by at least 100,000 places . on the other hand , the site with most links to outside sites is yahoo! . in some sense , yahoo! and other directories are the glue of the-web . without them we would have many isolated portions (which is the case with many personal-web pages) . if we assume that the average html page has 5 kb and that there are 300 million web-pages , we have at least 1.5 terabytes of text . this is consistent with other measures obtained from search-engines . note that this volume does not include non-textual documents . regarding the languages used in web-pages , there have been three studies made . the first study was done by funredes [637] from 1996 to 1998 . it uses the altavista search-engine and is based on searching different words in different languages . this technique might not be significant statistically , but the results are consistent with the second study wrhich was carried out by alis technology [11] and is based on automatic software that can detect the language used . one of the goals of the study was to test such software (done in 8000 web-servers) . the last study was done by oclc in june of 1998 [619] by sampling internet numeric addresses and using the silc language-identification software . table 13.1 gives the percentages of web-pages written in each language (with the exception of the oclc data that counts web-sites) , as well as the number of people (millions) who speak the language . the variations for japanese might be due to an inability to detect pages written in kanji . some languages , in particular spanish and portuguese , are growing fast and will surpass french in the near future . the total number of languages exceeds 100 .
low-rank approximations we next state a matrix-approximation problem that at first seems to have little to do with information-retrieval . we describe a solution to this matrix problem using singular-value decompositions , then develop its application to information-retrieval . given an matrix and a positive integer , we wish to find an matrix of rank at most , so as to minimize the frobenius norm of the-matrix difference , defined to be (238) low-rank-approximation the singular-value-decomposition can be used to solve the low-rank-matrix-approximation problem . we then derive from it an application to approximating term-document matrices . we invoke the following three-step procedure to this end : given , construct its svd in the form shown in (232) ; thus , . derive from the-matrix formed by replacing by zeros the smallest singular-values on the diagonal of . compute and output as the rank - approximation to . 18.1 theorem . (239) end theorem . recalling that the singular-values are in decreasing order , we learn from theorem 18.3 that is the best rank - approximation to , incurring an error (measured by the frobenius norm of) equal to . thus the larger is , the smaller this error (and in particular , for , the error is zero since ; provided , then and thus) . to derive further insight into why the process of truncating the smallest singular-values in helps generate a rank - approximation of low error , we examine the form of : (240) (241) (242) exercises . compute a rank 1 approximation to the-matrix in example 235 , using the svd as in exercise 236 . what is the frobenius norm of the error of this approximation ? consider now the computation in exercise 18.3 . following the schematic in figure 18.2 , notice that for a rank 1 approximation we have being a scalar . denote by the first column of and by the first column of . show that the rank-1 approximation to can then be written as . reduced can be generalized to rank approximations : we let and denote the `` reduced '' matrices formed by retaining only the first columns of and , respectively . thus is an matrix while is a matrix . then , we have (243) where is the square submatrix of with the singular-values on the diagonal . the primary advantage of using (243) is to eliminate a lot of redundant columns of zeros in and , thereby explicitly eliminating multiplication by columns that do not affect the low-rank-approximation ; this version of the svd is sometimes known as the reduced svd or truncated svd and is a computationally simpler representation from which to compute the low-rank-approximation . for the-matrix in example 18.2 , write down both and .
4.1.1 cognitive assumptions `` garbage in , garbage out '' is one of the first insights every software-developer learns , and foa is no exception . the primary source of data considered by traditional ir methods , and the focus of chapters 2 and 3 of this text , are the documents of the corpus , particularly the keywords they contain . (chapter 6 will consider the use of other document attributes .) a fundamental feature of the broader foa view is that browsing users provide an equally important source of data concerning portions previously published with john hatton [belew and hatton , 1996] . assessing the retrieval 107 what keywords mean and what documents are about . it is therefore appropriate to begin by characterizing who these users we will be watching are . we begin with one important cognitive assumption we must make about our users : how thorough an foa search do they wish to perform ? is this an important search to which the users are willing to dedicate a great deal of time and attention , or will a quick , cursory answer suffice ? for example , chapter 1 mentioned how much less thorough the typical undergraduate (doing some quick research before submitting a term paper) is than the ph.d. candidate (who wants to ensure his or her proposed dissertation topic is new) . the typical www searcher seems satisfied with only a few useful leads , but the professional searcher (a lawyer looking for any case that might help , a doctor looking for any science that might heal a patient) will search diligently if there is even a small chance of finding another relevant-document . this kind of variability can be observed not only across different classes of users but even across the same user at different times.f in for a fact , stay for a lesson prototypic retrievals from the perspective of cognitive-psychology , the task facing users who are asked to produce relevance-feedback (relevance-feedback) can best be described as one of object-recognition , in the tradition of rosch and others [rosch and mervis , 1975 ; rosch , 1977] . the object to be recognized is an internally represented prototypic document satisfying the users ' information-need . in this case , the prototype corresponds to the model the subjects maintain of ideally relevant documents . as users consider an actual retrieved document 's relevance , they evaluate how well it matches the prototype . barry and others have suggested the many and varied features over which prototypes can be defined [barry , 1994] . only a small number of these may be revealed by any one of the users ' queries , of course . here we will simply assume that users are capable of grading the quality of this match . users might be asked to score the quality of relevance match according to a five-point scale like that shown in figure 4.1 . users can qualify the middle relevant response either by weakening it (possibly relevant) or strengthening it (critically relevant) . such distinctions are often made in experimental settings (e.g. , in the use of the stairs 108 finding out about not possibly relevant critically i - e (no response) figure 4.1 relevance scale retrieval-system by lawyers [blair and maron , 1985]) and relate to the different purposes for foa that different users may have . to make these distinctions concrete , we might imagine `` critically relevant '' to apply only to those documents that must be read even for an undergrad term paper , while `` possibly relevant '' would be much more broadly applied to those a ph.d. student-needs as part of his or her literature-review . for now , however , we will simplify the types of relevance-feedback to allow users to reply with only a single grade of `` relevant '' (©) or `` not relevant '' (0) . these two assessments require overt action on the part of subjects ; `` no response '' (#) is the default relevance-feedback assessment for documents not receiving any other responses . again , this frees users from the much more cognitively demanding task of exhaustively assessing every retrieved document those documents that `` jump out '' at users as particularly good or especially bad - examples of the prototype they seek provide the most informative relevance-feedback . figure 4.1 also introduces a color-code convention in the electronic version : © will be used to indicate positive relevance-feedback and 0 to indicate negative-relevance-feedback . relevance-feedback is nonmetric as we move from a cognitive-understanding of the users ' tasks to statistical analyses of their behaviors , we must understand one important feature of the relevance-feedback data-stream : relevance-feedback is nonmetric data . that is , while users find it easy and natural to critique retrieved documents with © , © , and # , they would find it much more difficult to reliably assign numeric quantities reflecting something like the relative applicability of each retrieval . think for a moment just why this is hard , by imagining your reactions to a typical retrieval is the first document to be rated 10 or 6743 ? if you rate the first document as 10 , the second as 6ª and the third as 2 , then you must ensure that the third document is exactly as much less relevant assessing the retrieval 109 figure 4.2 relevance-feedback labeling of the retr set than the second as the second is from the first . trying to keep all rel (di) assessments consistent in the metric sense , for many retrieved documents or any other set , makes people crazy . this is not only a property of relevance-assessments . a large literature on psychological-assessment [kruskal , 1977a ; kruskal , 1977b ; shepard et al , 1972] has demonstrated that human-subjects can quite easily and reliably sort objects into `` piles , '' and that they like one pile better than another . yet the same people find it more difficult to quantify how much they like each object , let alone make these quantitative assessments consistent with one another . reliable estimates of both cognitive qualities would be necessary if we are to have a true preference metric * rather than assuming that users can provide a separate score for each retrieved document , we will treat this as an ordered nonmetric scale of increasing preference : #lt ; @ (4.1) each of these assumptions about what `` relevance '' is and how it can be measured is a matter of considerable debate [wilson , 1973 ; froehlich , 1994] and is likely to be the topic of much future work . it is also interesting to note that in our later attempts at a comprehensive model of how and why humans use language , `` relevance '' again plays a central role (cf. section 8.2.2) .
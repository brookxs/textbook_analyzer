7.1.3 sources of feedback two distinct classes of machine-learning-techniques can be applied to the foa problem . these can be distinguished on the basis of the type of training feedback given to the learning-system . the most powerful and best-understood are supervised-learning methods , where every training-instance given to the learning-system comes with an explicit label as to what the learning-system should do . using an email example , suppose we want talk announcements to consistently go into one folder , mail from our family to go in another , and spam to be deleted . in terms of supervised-learning , this regime requires that we first provide a training-set (cf. section 7.4) . in our case , the training-set would consist of email-messages and the c mail categories we have used to classify them in the past . after training on this data-set , we hope that our classifier generalizes to new , previously unseen messages and classifies them correctly as well . a second class of machine-learning-techniques makes weaker assumptions concerning the availability of training feedback . reinforcement-learning assumes only that a positive/negative signal tells the learning-system when it is doing a good/bad job . in the foa process , for example , relfhk generates a reinforcement signal (saying whether it was a good or bad thing that a document was retrieved) . note that relevance-feedback does not count as supervised-learning ; in general , we do not know all of the documents that should have been retrieved with respect to a particular query . supervised-training provides more information in the sense that every aspect of the learaer * s action (retrieval) can be contrasted with corresponding features of the correct action . reinforcement information , on the other hand * aggregates all of these features into a single measure of performance . the difference between these two kinds of learning is especially stark in the foa context . to provide reinforcement information , users need adaptive-information-retrieval 257 figure 7.2 browsing across queries in same session yes ! later ó stupid # @ $ $ ## ! machines only react to each document and say whether they are happy or sad it was retrieved . in order to do supervised-training , the user would need to identify the perfect retrieval , requiring the user to evaluate each and every document in the corpus ! the distinction between the supervised retrieval and that shaped by relevance-feedback highlights the need to be explicit about which kinds of feedback are hard for the user and which are easier . the discussion of rave made some of our assumptions concerning cognitive overhead clear (section 4.4) , but this is another important area for further study . here we will continue to assume that relevance-feedback is easy to acquire . learning from the entire session note that each iteration of the foa conversation is but a link in the larger dialog leading from the users ' initial information-need to the end of their search ; cf. figure 7.2 . first , the continuity of the same search pursuing the same information-need from iteration of the foa dialog to the next iteration helps to constrain interpretations of the users ' relevance-feedback ; we know which documents they have seen previously , we may know something about how a document retrieved in iteration i + 1 is related to one from iteration i and why it was retrieved , and so on . but perhaps the most important reason to model multiple-queries as part of a single session is that it is the total , aggregate difference between their cognitive-states at the beginning of the session and at the end of the session that should most concern us . users begin the search with their best characterization of what it is they want to foa * and they leave for one of a number of potential reasons : 258 finding out about ï because they ran out of time (but would like to continue this same session at a later time) , ï because they found what they were looking for , ï because they reached ultimate frustration , etc. . our interpretation of what the system could or should learn from this termination will be radically different , and so determining which of these reasons pertains becomes particularly important to establish . distributions over learning instances anyone familiar with basic statistics will appreciate how sensitive our estimate of an average is on the set of examples we happen to select for our sample . learning-methods are similarly sensitive to the amount and distribution of training-data . for example , the ratio of the number of features to the number of instances can affect learning-performance significantly [moulinier et al. , 1996 ; dagan et al , 1997] . define a positive training-instance æ to be one that has been identified as a member of a class and a negative training-instance 0 to be one that is identified as not a member of that class . it is less obvious that the ratio of positive to negative training instances is also important . in binary-classification tasks we can expect an even number of positive and negative training instances . but when classifying into a larger number of categories , it is common to treat each training-instance as a positive instance of one class and a negative instance of all the others . continuing our email-classification example , we can expect that every classified message in our training-set corresponds to a positive instance of one classification and simultaneously as a negative example of all the others . this makes efficient use of the training-data , but also generates a ^ - j - skew to the positive versus negative training-instance distribution .
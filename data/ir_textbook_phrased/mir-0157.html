8.8.1 sequential-searching a few approaches to directly searching-compressed-text exist . one of the most successful techniques in practice relies on huffman-coding taking words as symbols . that is , consider each different text word as a symbol , count their frequencies , and generate a huffman codefor the words . then , compress the text by replacing each word with its code . to improve compression/decompression efficiency , the huffman-code uses an alphabet of bytes instead of bits . this scheme compresses faster and better than known commercial systems , even those based on ziv-lempel coding . since huffman-coding needs to store the codes of each symbol , this scheme has to store the whole vocabulary of the text , i.e. the list of all different text words . this is fully exploited to efficiently search complex-queries . although according to heaps ' law the vocabulary (i.e. , the alphabet) grows as 0 {n) for 0 lt ; (3 lt ; 1 , the generalized zipf 's law shows that the distribution is skewed enough so that the entropy remains constant (i.e. , the compression-ratio will not degrade as the text grows) . those laws are explained in chapter 6 . any single-wrord or pattern query is first searched in the vocabulary . some queries can be binary searched , while others such as approximate searching or regular-expression searching must traverse sequentially all the vocabulary . this vocabulary is rather small compared to the text size , thanks to heaps ' law . notice that this process is exactly the same as the vocabulary searching performed by inverted-indices , either for simple or complex-pattern-matching . once that search is complete , the list of different words that match the query is obtained . the huffman-codes of all those wrords are collected and they are searched in the compressed text . one alternative is to traverse byte-wise the compressed text and traverse the huffman-decoding tree in synchronization , so that each time that a leaf is reached , it is checked whether the leaf (i.e. , word) was marked as ` matching ' the query or not . this is illustrated in figure 8.24 . boyer-moore filtering can be used to speed up the search . solving phrases is a little more difficult . each element is searched in the vocabulary . for each word of the vocabulary we define a bit mask . we set the / - th bit in the mask of all words which match with the i-th element of the phrase-query . this is used together with the shift-or algorithm . the text is traversed byte-wise , and only when a leaf is reached , does the shift-or algorithm consider that a new text symbol has been read , whose bit mask is that of the leaf (see figure 8.24) . this algorithm is surprisingly simple and efficient . 224 indexing and searching eh cm cm cm cm m cm cm cm cm huffman-tree cm vocabulary marks cm cm cm cm cm cm n cm huffman-tree rrooi rrfoi [`` tool czh vocabulary marks figure 8.24 on the left , searching for the simple pattern ` rose ' allowing one error . on the right , searching for the phrase ` ro * rose is , ' where ` ro * ' represents a prefix search . this scheme is especially fast when it comes to solving a complex-query (regular-expression , extended pattern , approximate-search , etc.) that would be slow with a normal algorithm . this is because the complex search is done only in the small-vocabulary , after which the algorithm is largely insensitive to the complexity of the originating query . its cpu times for a simple pattern are slightly higher than those of agrep (briefly described in section 8.5.6) . however , if the i/o times are considered , compressed searching is faster than all the online algorithms . for complex-queries , this scheme is unbeaten by far . on the reference machine , the cpu times are 14 mb/sec for any query , while for simple queries this improves to 18 mb/sec if the speedup-technique is used . agrep , on the other hand , runs at 15 mb/sec on simple searches and at 1-4 mb/sec for complex ones . moreover , i/o times are reduced to one third on the compressed text .
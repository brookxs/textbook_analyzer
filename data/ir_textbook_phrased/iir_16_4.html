k-means 6 6.4.4 centroid (190) the definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way . we used centroids for rocchio classification in chapter 14 (page 14.2) . they play a similar role here . the ideal cluster in - means is a sphere with the centroid as its center of gravity . ideally , the clusters should not overlap . our desiderata for classes in rocchio classification were the same . the difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster . a measure of how well the centroids represent the members of their clusters is the residual sum-of-squares or rss , the squared distance of each vector from its centroid summed over all vectors : (191) objective-function seeds 16.5 16.6 17.2 17.2 we can apply one of the following termination-conditions . a fixed number of iterations has been completed . this condition limits the runtime of the clustering algorithm , but in some cases the quality of the clustering will be poor because of an insufficient number of iterations . assignment of documents to clusters (the partitioning function) does not change between iterations . except for cases with a bad local minimum , this produces a good clustering , but runtimes may be unacceptably long . centroids do not change between iterations . this is equivalent to not changing (exercise 16.4.1) . terminate when rss falls below a threshold . this criterion ensures that the clustering is of a desired quality after termination . in practice , we need to combine it with a bound on the number of iterations to guarantee termination . terminate when the decrease in rss falls below a threshold . for small , this indicates that we are close to convergence . again , we need to combine it with a bound on the number of iterations to prevent very long runtimes . we now show that - means converges by proving that monotonically decreases in each iteration . we will use decrease in the meaning decrease or does not change in this section . first , rss decreases in the reassignment step since each vector is assigned to the closest centroid , so the distance it contributes to decreases . second , it decreases in the recomputation step because the new centroid is the vector for which reaches its minimum . (192) (193) (194) since there is only a finite-set of possible clusterings , a monotonically decreasing algorithm will eventually arrive at a (local) minimum . take care , however , to break ties consistently , e.g. , by assigning a document to the cluster with the lowest index if there are several equidistant centroids . otherwise , the algorithm can cycle forever in a loop of clusterings that have the same cost . while this proves the convergence of - means , there is unfortunately no guarantee that a global-minimum in the objective-function will be reached . this is a particular problem if a document-set contains many outliers , documents that are far from any other documents and therefore do not fit well into any cluster . frequently , if an outlier is chosen as an initial seed , then no other vector is assigned to it during subsequent iterations . thus , we end up with a singleton cluster (a cluster with only one document) even though there is probably a clustering with lower rss . figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds . another type of suboptimal clustering that frequently occurs is one with empty clusters (exercise 16.7) . effective heuristics for seed-selection include (i) excluding outliers from the seed set ; (ii) trying out multiple starting-points and choosing the clustering with lowest cost ; and (iii) obtaining seeds from another method such as hierarchical-clustering . since deterministic hierarchical-clustering-methods are more predictable than - means , a hierarchical-clustering of a small random sample of size (e.g. , for or) often provides good seeds (see the description of the buckshot algorithm , chapter 17 , page 17.8) . other initialization methods compute seeds that are not selected from the vectors to be clustered . a robust method that works well for a large variety of document distributions is to select (e.g. ,) random vectors for each cluster and use their centroid as the seed for this cluster . see section 16.6 for more sophisticated initializations . what is the time-complexity of - means ? most of the time is spent on computing vector distances . one such operation costs . the reassignment step computes distances , so its overall complexity is . in the recomputation step , each vector gets added to a centroid once , so the complexity of this step is . for a fixed number of iterations , the overall complexity is therefore . thus , - means is linear in all relevant factors : iterations , number-of-clusters , number of vectors and dimensionality of the space . this means that - means is more efficient than the hierarchical-algorithms in chapter 17 . we had to fix the number of iterations , which can be tricky in practice . but in most cases , - means quickly reaches either complete convergence or a clustering that is close to convergence . in the latter case , a few documents would switch membership if further iterations were computed , but this has a small effect on the overall quality of the clustering . there is one subtlety in the preceding argument . even a linear algorithm can be quite slow if one of the arguments of is large , and usually is large . high-dimensionality is not a problem for computing the distance between two documents . their vectors are sparse , so that only a small fraction of the theoretically possible componentwise differences need to be computed . centroids , however , are dense since they pool all terms that occur in any of the documents of their clusters . as a result , distance computations are time consuming in a naive implementation of - means . however , there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities . truncating centroids to the most significant terms (e.g. ,) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in section 16.6) . the same efficiency problem is addressed by k-medoids , a variant of - means that computes medoids instead of centroids as cluster centers . we define the medoid of a cluster as the document vector that is closest to the centroid . since medoids are sparse document-vectors , distance computations are fast . estimated minimal residual sum-of-squares as a function of the number of clusters in - means . in this clustering of 1203 reuters-rcv1 documents , there are two points where the curve flattens : at 4 clusters and at 9 clusters . the documents were selected from the categories china , germany , russia and sports , so the clustering is closest to the reuters classification . subsections cluster cardinality in k-means
ir in perspective this section is not meant to constitute an attempt at an exhaustive and complete account of the historical development of ir . in any case , it would not be able to improve on the accounts given by cleverdon [8] and salton [9]] . although information-retrieval can be subdivided in many ways , it seems that there are three main areas of research which between them make up a considerable portion of the subject . they are : content-analysis , information-structures , and evaluation . briefly the first is concerned with describing the contents of documents in a form suitable for computer-processing ; the second with exploiting relationships between documents to improve the efficiency-and-effectiveness of retrieval-strategies ; the third with the measurement of the effectiveness of retrieval . since the emphasis in this book is on a particular approach to document-representation , i shall restrict myself here to a few remarks about its history . i am referring to the approach pioneered by luhn [10] . he used frequency-counts of words in the document-text to determine which words were sufficiently significant to represent or characterise the document in the computer (more details about this in the next chapter) . thus a list of what might be called ` keywords ' was derived for each document . in addition the frequency of occurrence of these words in the body of the text could also be used to indicate a degree of significance . this provided a simple weighting-scheme for the ` keywords ' in each list and made available a document representative in the form of a ` weighted keyword description ' . at this point , it may be convenient to elaborate on the use of ` keyword ' . it has become common practice in the ir literature to refer to descriptive items extracted from text as keywords or terms . such items are often the outcome of some process such as , for example , the gathering together of different morphological variants of the same word . in this book , keyword and term will be used interchangeably . the use of statistical-information about distributions of words in documents was further exploited by maron and kuhns [11] and stiles [12] who obtained statistical associations between keywords . these associations provided a basis for the construction of a thesaurus as an aid to retrieval . much of this early research was brought together with the publication of the 1964 washington symposium on statistical association methods for mechanized documentation (stevens et al. [13]) . sparck jones has carried on this work using measures of association between keywords based on their frequency of co-occurrence (that is , the frequency with which any two keywords occur together in the same document) . she has shown [14] that such related words can be used effectively to improve recall , that is , to increase the proportion of the relevant documents which are retrieved . interestingly , the early ideas of luhn are still being developed and many automatic methods of characterisation are based on his early work . the term information-structure (for want of better words) covers specifically a logical organisation of information , such as document representatives , for the purpose of information-retrieval . the development in information-structures has been fairly recent . the main reason for the slowness of development in this area of information-retrieval is that for a long time no one realised that computers would not give an acceptable retrieval-time with a large document set unless some logical-structure was imposed on it . in fact , owners of large data-bases are still loath to try out new organisation techniques promising faster and better retrieval . the slowness to recognise and adopt new techniques is mainly due to the scantiness of the experimental-evidence backing them . the earlier experiments with document-retrieval systems usually adopted a serial file organisation which , although it was efficient when a sufficiently large number of queries was processed simultaneously in a batch mode , proved inadequate if each query required a short real-time response . the popular organisation to be adopted instead was the inverted-file . by some this has been found to be restrictive (salton [15]) . more recently experiments have attempted to demonstrate the superiority of clustered files for on-line retrieval . the organisation of these files is produced by an automatic-classification method . good [16] and fairthorne [17] were among the first to suggest that automatic-classification might prove useful in document-retrieval . not until several years later were serious experiments carried out in document-clustering (doyle [18] ; rocchio [19]) . all experiments so far have been on a small scale . since clustering only comes into its own when the scale is increased , it is hoped that this book may encourage some large-scale experiments by bringing together many of the necessary tools . evaluation of retrieval-systems has proved extremely difficult . senko [20] in an excellent survey-paper states : ` without a doubt system-evaluation is the most troublesome area in isr ... ' , and i am inclined to agree . despite excellent pioneering work done by cleverdon et al. [21] in this area , and despite numerous measures-of-effectiveness that have been proposed (see robertson [22 , 23] for a substantial list) , a general theory of evaluation had not emerged . i attempt to provide foundations for such a theory in chapter 7 (page 168) . in the past there has been much debate about the validity of evaluations based on relevance-judgments provided by erring human beings . cuadra and katter [24] supposed that relevance was measurable on an ordinal scale (one which arises from the operation of rank-ordering) but showed that the position of a document on such a scale was affected by external variables not usually controlled in the laboratory . lesk and salton [25] subsequently showed that a dichotomous scale on which a document is either relevant or non-relevant , when subjected to a certain probability of error , did not invalidate the results obtained for evaluation in terms of precision (the proportion of retrieved documents which are relevant) and recall (the proportion of relevant documents retrieved) . today effectiveness of retrieval is still mostly measured in terms of precision-and-recall or by measures_based thereon . there is still no adequate statistical treatment showing how appropriate significance-tests may be used (i shall return to this point in the chapter on evaluation , page 178) . so , after a few decades of research in this area we basically have only precision-and-recall , and a working hypothesis which states , quoting cleverdon [26] : ` within a single system , assuming that a sequence of sub-searches for a particular question is made in the logical order of expected decreasing precision , and the requirements are those stated in the question , there is an inverse relationship between recall and precision , if the results of a number of different searches are averaged . '
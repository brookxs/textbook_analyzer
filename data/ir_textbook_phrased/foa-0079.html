4.4.1 raveunion it would be most useful if , for every query , the relevance of every document could be assessed . however , the collection of this many assessments , 142 finding out about more elaborate ways of merging ranked-lists for a corpus large enough to provide a real retrieval test , quickly becomes much too expensive . but if the evaluation goal is relaxed to being the relative comparison of one retrieval-system to one or more alternative systems , assessments can be constrained to only those documents retrieved by one of the systems . we therefore follow the pooling procedure used by many other evaluators , viz. , using the proposed retrieval-methods themselves as procedures for identifying which documents are worth assessing . the first step in constructing a rave experiment is to combine the ranked-retrieval lists of the two or more retrieval-methods , creating a single list of documents ordered according to how interested we are in having them assessed by a subject . ravetjnion produces the most straightforward `` zipper '' merge of the lists , beginning with the most highly ranked and alternating one . ^ the output of raveunion is a file of (query , document) pairs along with a field that indicates if the pair was uniquely suggested by only one of the methods . this last information can be used to compare the average relevance-scores of documents suggested by one method alone to those retrieved by more than one .
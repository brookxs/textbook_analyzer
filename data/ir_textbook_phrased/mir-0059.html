3.2.1 recall and precision consider an example information request / (of a test-reference-collection) and its set r of relevant documents . let \ r \ be the number of documents in this set . assume that a given retrieval-strategy (which is being evaluated) processes the information request / and generates a document answer set a. let \ a \ be the number of documents in this set . further , let \ ra \ be the number of documents in the intersection of the sets r and a. figure 3.1 illustrates these sets . the recall and precision measures are defined as follows . ï recall is the fraction of the relevant documents (the set r) which has been retrieved i.e. , recall = \ r \ ï precision is the fraction of the retrieved documents (the set a) which is relevant i.e. , precision = \ a \ recall and precision , as defined above , assume that all the documents in the answer set a have been examined (or seen) . however , the user is not usually presented with all the documents in the answer set a at once . instead , the relevant docs in answer set ^ ------- _ ^ collection relevant docs answer set ' | * | \ a \ figure 3.1 precision-and-recall for a given example information request . 76 retrieval-evaluation documents in a are first sorted according to a degree of relevance (i.e. , a ranking is generated) . the user then examines this ranked list starting from the top document . in this situation , the recall and precision measures vary as the user proceeds with his examination of the answer set a. thus , proper evaluation requires plotting a precision versus recall curve as follows . as before , consider a reference collection and its set of example information requests . let us focus on a given example information request for which a query q is formulated . assume that a set rq containing the relevant documents for q has been defined . without loss of generality , assume further that the set rq is composed of the following documents rq = {^ 3 , ^ 5 , ^ 9 , ^ 25 , ^ 39 ^ 44 , ^ 56 ^ 71) ^ 89 ? d \ 2z] (3.1) thus , according to a group of specialists , there are ten documents which are relevant to the query q. consider now a new retrieval algorithm which has just been designed . assume that this algorithm returns , for the query g , a ranking of the documents in the answer set as follows . ranking for query q : 1 . d \ 2z ï 6 . ci9 ï 11 . `` 38 2 . 7 . d5n 12 . d48 3 . d56 ï 8 . di29 13 . 4 . de 9 - ^ 187 14 . dn 5 . d8 10 . ^ 25 ï 15 . the documents that are relevant to the query q are marked with a bullet after the document number . if we examine this ranking , starting from the top document , we observe the following points . first , the document di23 which is ranked as number 1 is relevant . further , this document corresponds to 10 % of all the relevant documents in the set rq . thus , we say that we have a precision of 100 % at 10 % recall . second , the document d $ ß which is ranked as number 3 is the next relevant-document . at this point , we say that we have a precision of roughly 66 % (two documents out of three are relevant) at 20 % recall (two of the ten relevant documents have been seen) . third , if we proceed with our examination of the ranking generated we can plot a curve of precision versus recall as illustrated in figure 3.2 . the precision at levels of recall higher than 50 % drops to 0 because not all relevant documents have been retrieved . this precision versus recall curve is usually based on 11 (instead often) standard recall levels which are 0 % , 10 % , 20 % , ... , 100 % . for the recall level 0 % , the precision is obtained through an interpolation procedure as detailed below . in the above example , the precision-and-recall figures are for a single query . usually , however , retrieval algorithms are evaluated by running them for several distinct queries . in this case , for each query a distinct precision versus recall curve is generated . to evaluate the retrieval-performance of an algorithm over retrieval-performance evaluation 77 figure 3.2 precision at 11 standard recall levels . all test queries , we average the precision figures at each recall level as follows . p (r) = pi (r) (3.2) t = l where p (r) is the average-precision at the recall level r , nq is the number of queries used , and pi (r) is the precision at recall level r for the i-th query . since the recall levels for each query might be distinct from the 11 standard recall levels , utilization of an interpolation procedure is often necessary . for instance , consider again the set of 15 ranked documents presented above . assume that the set of relevant documents for the query q has changed and is now given by (3.3) in this case , the first relevant-document in the ranking for query q is d5e which provides a recall level of 33.3 % (with precision also equal to 33.3 %) because , at this point , one-third of all relevant documents have already been seen . the second relevant-document is di29 which provides a recall level of 66.6 % (with precision equal to 25 %) . the third relevant-document is d $ which provides a recall level of 100 % (with precision equal to 20 %) . the precision figures at the 11 standard recall levels are interpolated as follows . let fj , j e {0,1,2 , ... , 10} , be a reference to the j-th standard recall level (i.e. , rs is a reference to the recall level 50 %) . then , p (rj) - = max r3lt ; rlt ; r ^ t p (r) (3.4) 78 retrieval-evaluation 120 100 - gt ; 80 -60 20 40 60 recall 80 100 120 figure 3.3 interpolated precision at 11 standard recall levels relative to rq = which states that the interpolated precision at the j-th standard recall level is the maximum known precision at any recall level between the j-th recall level and the (j - f l) - th recall level . in our last example , this interpolation rule yields the precision-and-recall figures illustrated in figure 3.3 . at recall levels 0 % , 10 % , 20 % , and 30 % , the interpolated precision is equal to 33.3 % (which is the known precision at the recall level 33.3 %) . at recall levels 40 % , 50 % , and 60 % , the interpolated precision is 25 % (which is the precision at the recall level 66.6 %) . at recall levels 70 % , 80 % , 90 % , and 100 % , the interpolated precision is 20 % (which is the precision at recall level 100 %) . the curve of precision versus recall which results from averaging the results for various queries is usually referred to as precision versus recall figures . such average figures are normally used to compare the retrieval-performance of distinct retrieval algorithms . for instance , one could compare the retrieval-performance of a newly proposed retrieval algorithm with the retrieval-performance of the classic vector-space-model . figure 3.4 illustrates average-precision versus recall figures for two distinct retrieval algorithms . in this case , one algorithm has higher precision at lower recall levels while the second algorithm is superior at higher recall levels . one additional approach is to compute average-precision at given document cutoff values . for instance , we can compute the average-precision when 5 , 10 , 15 , 20 , 30 , 50 , or 100 relevant documents have been seen . the procedure is analogous to the computation of average-precision at 11 standard recall levels but provides additional-information on the retrieval-performance of the ranking-algorithm . average-precision versus recall figures are now a standard evaluation-strategy for information-retrieval-systems and are used extensively in the information-retrieval literature . they are useful because thev allow us to evaluate retrieval-performance evaluation 79 100 120 figure 3.4 average recall versus precision figures for two distinct retrieval algorithms . quantitatively both the quality of the overall answer set and the breadth of the retrieval algorithm . further , they are simple , intuitive , and can be combined in a single curve . however , precision versus recall figures also have their disadvantages and their widespread usage has been criticized in the literature . we return to this point later on . before that , let us discuss techniques for summarizing precision versus recall figures by a single numerical value . single value summaries average-precision versus recall figures are useful for comparing the retrieval-performance of distinct retrieval algorithms over a set of example queries . however , there are situations in which we would like to compare the retrieval-performance of our retrieval algorithms for the individual queries . the reasons are twofold . first , averaging precision over many queries might disguise important anomalies in the retrieval algorithms under study . second , when comparing two algorithms , we might be interested in investigating whether one of them outperforms the other for each query in a given set of example queries (notice that this fact can be easily hidden by an average-precision computation) . in these situations , a single-precision value (for each query) can be used . this single value should be interpreted as a summary of the corresponding precision versus recall curve . usually , this single value summary is taken as the precision at a specified recall level . for instance , we could evaluate the precision when we observe the first relevant-document and take this precision as the single value summary . of course , as seems obvious , this is not a good approach . more interesting strategies can be adopted as we now discuss . 80 retrieval-evaluation average-precision at seen relevant documents the idea here is to generate a single value summary of the ranking by averaging the precision figures obtained after each new relevant-document is observed (in the ranking) . for instance , consider the example in figure 3.2 . the precision figures after each new relevant-document is observed are 1 , 0.66 , 0.5 , 0.4 , and 0.3 . thus , the average-precision at seen relevant documents is given by (l +0.66 +0.5 - h) .4 - h) .3) / 5 or 0.57 . this measure favors systems which retrieve relevant documents quickly (i.e. , early in the ranking) . of course , an algorithm might present a good average-precision at seen relevant documents but have a poor performance in terms of overall recall . r-precision the idea here is to generate a single value summary of the ranking by computing the precision at the r-th position in the ranking , where r is the total number of relevant documents for the current query (i.e. , number of documents in the set rq) . for instance , consider the examples in figures 3.2 and 3.3 . the value of r-precision is 0.4 for the first example (because r = 10 and there are four relevant documents among the first ten documents in the ranking) and 0.33 for the second example (because r = 3 and there is one relevant-document among the first three documents in the ranking) . the r-precision measure is a useful parameter for observing the behavior of an algorithm for each individual query in an experiment . additionally , one can also compute an average r-precision figure over all queries . however , using a single number to summarize the full behavior of a retrieval algorithm over several queries might be quite imprecise . precision histograms the r-precision measures for several queries can be used to compare the retrieval history of two algorithms as follows . let rpa {i) and rpb (i) be the r-precision values of the retrieval algorithms a and b for the i-th query . define , for instance , the difference rpa/b (i) = rpa (i) - rpb (i) (3.5) a value of rpa/b ^) equal to 0 indicates that both algorithms have equivalent performance (in terms of r-precision) for the i-th query . a positive value of rpa/b (^) indicates a better retrieval-performance by algorithm a (for the i-th query) while a negative value indicates a better retrieval-performance by algorithm b. figure 3.5 illustrates the rpaib ^) values (labeled r-precision a/b) for two hypothetical retrieval algorithms over ten example queries . the algorithm .4 is superior for eight queries while the algorithm b performs better for the two other queries (numbered 4 and 5) . this type of bar graph is called a precision histogram and allows us to quickly compare the retrieval-performance history of two algorithms through visual-inspection . summary table statistics single mine measures can also be stored in a table to provide a statistical summary regarding the set of all the queries in a retrieval-task . for instance , these retrieval-performance evaluation 81 1,5 , 1,0 0,5 q. n u 1 2 3 -1,0 ' igt ; 6 7 8 9 10 -1,5 query number figure 3.5 a precision histogram for ten hypothetical-queries . summary table statistics could include : the number of queries used in the task , the total number of documents retrieved by all queries , the total number of relevant documents which were effectively retrieved when all queries are considered , the total number of relevant documents which could have been retrieved by all queries , etc. . precision-and-recall appropriateness precision-and-recall have been used extensively to evaluate the retrieval-performance of retrieval algorithms . however , a more careful reflection reveals problems with these two measures [451 , 664 , 754] . first , the proper estimation of maximum recall for a query requires detailed knowledge of all the documents in the collection . with large-collections , such knowledge is unavailable which implies that recall can not be estimated precisely . second , recall and precision are related measures which capture different aspects of the set of retrieved documents . in many situations , the use of a single measure which combines recall and precision could be more appropriate . third , recall and precision-measure the effectiveness over a set of queries processed in batch mode . however , with modern systems , interactivity (and not batch-processing) is the key aspect of the retrieval-process . thus , measures which quantify the informativeness of the retrieval-process might now be more appropriate . fourth , recall and precision are easy to define when a linear ordering of the retrieved documents is enforced . for systems which require a weak-ordering though , recall and precision might be inadequate . 82 retrieval-evaluation
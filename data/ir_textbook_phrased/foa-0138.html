6.9 text-based intelligence knowledge-representation has always been a central issue for ai , and as a subdiscipline within computer-science its primary contribution is probably the beginnings of a computational-theory of knowledge . although it is still too early to speak of such a theory , some key aspects of good knowledge-representation are becoming clear [belew and forrest , 1988] . the text captured in document corpora was not entered with the intention of being part of a knowledge-base . these are documents written by someone as part of a natural-communication process , and any search-engine technology simply gives this document added life . alternatively , we can say that the document was intended to become part of a `` knowledge-base , '' but one that predates (at least the ai) use of that term : people publish their documents with the explicit hope that their ideas can become part of our collective-wisdom and be used by others . 248 finding out about note the ease with which an author-as-knowledge engineer can express his or her knowledge . hypertext knowledge-bases are accessible to every writer . in this view , hypertext solves the key ai problem of the knowledge-acquisition-bottleneck , providing a knowledge-representation-language with the ease , flexibility , and expressiveness of natural-language - by actually using natural-language ! the cost paid is the weakness of the inferences that can be made from a textual foundation : contrast the strong theorem-proving notions of inference of section 6.5.1 with the many confounded associations that arise in swanson 's analysis of latent knowledge in section 6.5.3 . grounding symbols in texts according to hamad 's grounding hypothesis , if computers are ever to understand natural-language as fully as humans , they must have an equally vast corpus of experience from which to draw [harnad , 1987] . we propose that the huge volumes of natural-language-text managed by hypertext-systems provide exactly the corpus of `` experience '' needed for such understanding . each word in every document in a hypertext system constitutes a separate experiential `` data point '' about what that word means . the exciting prospect of using search-engines as a basis for natural language-understanding systems is that their understanding of words , and concepts built from these words , will reflect the richness of this huge base of textual `` experience . '' there are , of course , differences between the text-based `` experience '' and first-person , human experience , and these imply fundamental-limits on language-understanding derived from this source . in this view , the computer 's experience of the world is secondhand , from documents written by people about the world and subsequently through users ' queries of the system . the `` trick '' is to learn what words mean by interacting with users who already know what the words mean , with the documents of the textual corpus forming the common referential base of experience . the hypertext itself is in fact only the first source of information , viz. , how authors use and juxtapose words . the second , ongoing source of experience is the subsequent interactions with users , a new population of people who use these same words and then react positively or negatively to the system 's interpretation of them . both the original authors and the inference beyond the index 249 browsing users function as the text-based intelligent-system 's `` eyes '' into the real-world and how it looks to humans . that insight is something no video-camera will ever give any robot .
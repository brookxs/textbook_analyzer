references and further reading definition and implementation of the notion of relevance to a query got off to a rocky start in 1953 . swanson (1988) reports that in an evaluation in that year between two teams , they agreed that 1390 documents were variously relevant to a set of 98 questions , but disagreed on a further 1577 documents , and the disagreements were never resolved . rigorous formal-testing of ir systems was first completed in the cranfield experiments , beginning in the late 1950s . a retrospective discussion of the cranfield test-collection and experimentation with it can be found in (cleverdon , 1991) . the other seminal series of early ir experiments were those on the smart system by gerard salton and colleagues (salton , 1971b ; 1991) . the trec evaluations are described in detail by voorhees and harman (2005) . online-information is available at http://trec.nist.gov/ . initially , few researchers computed the statistical-significance of their experimental-results , but the ir community increasingly demands this (hull , 1993) . user-studies of ir-system effectiveness began more recently (saracevic and kantor , 1988 ; 1996) . the notions of recall and precision were first used by kent et al. (1955) , although the term-precision did not appear until later . the (or , rather its complement) was introduced by van rijsbergen (1979) . he provides an extensive theoretical discussion , which shows how adopting a principle of decreasing marginal relevance (at some point a user will be unwilling to sacrifice a unit of precision for an added unit of recall) leads to the harmonic mean being the appropriate method for combining precision-and-recall (and hence to its adoption rather than the minimum or geometric-mean) . buckley and voorhees (2000) compare several evaluation-measures , including precision at , map , and r-precision , and evaluate the error-rate of each measure . was adopted as the official evaluation-metric in the trec-hard track (allan , 2005) . aslam and yilmaz (2005) examine its surprisingly close correlation to map , which had been noted in earlier studies (buckley and voorhees , 2000 , tague-sutcliffe and blustein , 1995) . a standard program for evaluating ir systems which computes many measures of ranked-retrieval effectiveness is chris buckley 's trec_eval program used in the trec evaluations . it can be downloaded from : http://trec.nist.gov/trec_eval/ . kekäläinen and järvelin (2002) argue for the superiority of graded-relevance judgments when dealing with very-large document-collections , and järvelin and kekäläinen (2002) introduce cumulated gain-based methods for ir-system evaluation in this context . sakai (2007) does a study of the stability and sensitivity of evaluation-measures based on graded-relevance judgments from ntcir tasks , and concludes that ndcg is best for evaluating document-ranking . schamber et al. (1990) examine the concept of relevance , stressing its multidimensional and context-specific nature , but also arguing that it can be measured effectively . (voorhees , 2000) is the standard article for examining variation in relevance-judgments and their effects on retrieval-system scores and ranking for the trec ad-hoc task . voorhees concludes that although the numbers change , the rankings are quite stable . hersh et al. (1994) present similar analysis for a medical ir collection . in contrast , kekäläinen (2005) analyze some of the later trecs , exploring a 4-way relevance-judgment and the notion of cumulative-gain , arguing that the relevance-measure used does substantially affect system rankings . see also harter (1998) . zobel (1998) studies whether the pooling method used by trec to collect a subset of documents that will be evaluated for relevance is reliable and fair , and concludes that it is . the and its use for language-related purposes is discussed by carletta (1996) . many standard sources (e.g. , siegel and castellan , 1988) present pooled calculation of the expected agreement , but di eugenio (2004) argue for preferring the unpooled agreement (though perhaps presenting multiple measures) . for further discussion of alternative measures of agreement , which may in fact be better , see lombard et al. (2002) and krippendorff (2003) . text-summarization has been actively explored for many years . modern work on sentence-selection was initiated by kupiec et al. (1995) . more recent work includes (barzilay and elhadad , 1997) and (jing , 2000) , together with a broad selection of work appearing at the yearly duc conferences and at other nlp venues . tombros and sanderson (1998) demonstrate the advantages of dynamic summaries in the ir-context . turpin et al. (2007) address how to generate snippets efficiently . clickthrough-log analysis is studied in (joachims , 2002b , joachims et al. , 2005) . in a series of papers , hersh , turpin and colleagues show how improvements in formal retrieval-effectiveness , as evaluated in batch experiments , do not always translate into an improved system for users (hersh et al. , 2000b , turpin and hersh , 2002 , hersh et al. , 2000a ; 2001 , turpin and hersh , 2001) . user-interfaces for ir and human-factors such as models of human information seeking and usability-testing are outside the scope of what we cover in this book . more information on these topics can be found in other textbooks , including (baeza-yates and ribeiro-neto , 1999 , ch . 10) and (korfhage , 1997) , and collections focused on cognitive-aspects (spink and cole , 2005) .
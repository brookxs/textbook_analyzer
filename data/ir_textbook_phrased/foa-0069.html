43.2 consensual relevance in most search-engine-evaluation , the assumption has been that a single expert can be trusted to provide reliable relevance-assessments . whether any one , `` omniscient '' individual is capable of providing reliable data about the appropriate set of documents to be retrieved remains a foundational issue within ir . for example , a number of papers in a recent special issue ofthe journal ofthe american society for information-systems devoted to relevance advocated a move toward a more `` user-centered , situational '' view of relevance [froehlich , 1994] . our attention to the opinions of individual users suggests the possibility of combining evidence from multiple human judges . rather than having relevance be a boolean determination made by a single expert , we will consider `` relevance '' to be a consensual , central tendency ofthe searching users ' opinions . the relevance-assessments of individual users and the resulting central tendency of relevance is suggested by figure 4.8 . two features of this definition are significant . first , consensual relevance posits a `` consumers ' '' perspective on what will count as ir-system success . a document 's relevance to a query is not going to be determined by an expert in the topical area , but by the users who are doing the searching . if they find it relevant , it 's relevant , whether or not some domain expert thinks the document `` should '' have been retrieved . second , consensual relevance becomes a statistical , aggregate property of multiple-users ' reactions rather than a discrete feature elicited from any one individual . by making relevance a statistical measure , our confidence in the relevance of a document (with respect to a query) assessing the retrieval 119 consensually relevant ivant respect to user , . figure 4.8 consensual relevance increases as more relevance-assessment data are collected . this reliance on statistical stability creates a strong link between ir and machine-learning (cf. chapter 7) . allen 's investigation into idiosyncratic cognitive-styles of browsing users [allen , 1992] and wilbur 's assessment of the reliability of relevance-feedback across users [wilbur , 1998] provide a more textured view of how multiple relevance-assessments can be compared and combined . it seems , however , that our move from omniscient to consensual relevance has only made the problem of evaluation that much more difficult . test-corpora must be large enough to provide robust tests for retrieval-methods , and multiple-queries are necessary to evaluate the overall performance of a search-engine . getting even a single person 's opinion about the relevance of a document to a particular query is hard , and we are now interested in getting many ! however , software like rave (cf. section 4.4) allows an ir experimenter to effectively collect large numbers of relevance-assessments for an arbitrary document-corpus .
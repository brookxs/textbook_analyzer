4.8 hidden-markov-models hidden-markov-models (hmm) have been applied for the last 20 years to solving problems in speech-recognition and to a lesser extent in the areas locating named-entities (bikel-97) , optical-character-recognition (bazzi-98) and topic-identification (kubaia-97) . more recently hmms have been applied more generally to information-retrieval search with good results . one of the first comprehensive and practical descriptions of hidden-markov-models was written by dr. lawrence rabiner (rabiner-89) a hmm can best be understood by first defining a discrete markov process . the easiest way to understand it is by an example . lets take the example of a three state markov-model of the stock-market . the states will be one of the following that is observed at the closing of the market : state 1 (si) : market decreased state 2 (s2) : market did not change state 3 (s3) : market increased in value the movement between states can be defined by a state-transition matrix with state-transitions (this assumes you can go from any state to any other state) : .5 .3 .4 a ={ * u} = .1 .6 .3 .6 .7 .5 given that the market fell on one day (state 1) , the-matrix suggests that the probability of the market not changing the next day is .1 . this then allows questions such as the probability that the market will increase for the next 4 days then fall . this would be equivalent to the sequence of seq = {s3 , s3 , s3 , s3 , si} . in order to simplify our model , lets assume that instead of the current-state being dependent upon all the previous states , lets assume it is only dependent upon the 100 chapter 4 last state (discrete , first order , markov-chain .) this would then be calculated by the formula : p (seq) =p [s3 , s3 , s3 , s3.s1] = p [s3] * p [s3/s3] * p [s3/s3} * p [s3/s3] * p [s1/s3] = s3 (init) * a3 ,3 * a3 ,3 * a3 ,3 * a1gt ; 3 = (1.0) * (.5) * (.5) * (.5) * (.4) = .05 in the equation we also assume the probability of the initial-state of s3 is s3 (init) = l . the following graph depicts the model . the directed lines indicate the state transition-probabilities a , j . there is also an implicit loop from every state back to itself in the example every state corresponded to an observable event (change in the market) . when trying to apply this model to less precise world problems such as in speech-recognition , this model was too restrictive to be applicable . to add more flexibility a probability-function was allowed to be associated with the state . the result is called the hidden-markov-model . it gets its name from the fact that there are two stochastic-processes with the underlying stochastic-process not being observable (hidden) , but can only be analyzed by observations which originate from another stochastic-process . thus the system will have as input a series of results , but it will not know the-markov-model and the number of states that were associated with generating the results . so part of the hmm process is in determining which model of states best explains the results that are being observed . amore formal-definition of a discrete hidden-markov-model is summarized by mittendorf and schauble (mittendorf-94) : as consisting of the following : data-structure 101 1 . s = {so , , ... , sn.i} as a finite-set of states where s0 always denotes the initial-state . typically the states are interconnected such that any state can be reached from any other state . 2 . v = {v0 , ... , vm_j} is a finite-set of output symbols . this will correspond to the physical output from the system being modeled . 3 . a = s x s a transition-probability matrix where ay represents the n - \ probability of transitioning from state i to state] such that / \ #i , j = 1 for all i = 0 , ... , n - 1 . every value in the matrix is a positive value between 0 and 1 . for the case where every state can be reached from every other state every value in the matrix will be non-zero . 4 . b = s x v is an output probability-matrix where element bjtk is a m - \ function determining the probability and j ^ b jtk = 1 for all # = 0 j = 0 , ... , n-l . 5 . the initial-state distribution . the hmm will generate an output symbol at every state-transition . the transition-probability is the probability of the next state given the current-state . the output probability is the probability that a given output is generated upon arriving at the next state . given the hmm definition , it can be used as both a generator of possible sequences of outputs and their probabilities (as shown in example above) , or given a particular out sequence it can model its generation by an appropriate hmm-model . the complete specification of a hmm requires specification of the states , the output symbols and three probability measures for the state transitions , output probability functions and the initial states . the distributions are frequently called a , b , and 7t , and the following notation is used to define the model : i = (a , b , 7i) . one of the primary problems associated with hmm is how to efficiently calculate the probability of a sequence of observed outputs given the hmm-model . this can best be looked at as how to score a particular model given a series of outputs . or another way to approach it is how to determine which of a number of competing models should be selected given an observed set of outputs . this is in effect 102 chapter 4 uncovering the hidden part of the model . they typical approach is to apply an `` optimality criterion '' to select the states . but there are many such algorithms to choose from . once you have selected the model that you expect corresponds to the output , then there is the issue of determining which set of state-sequences best explains the output . the final issue is how best to tune the x model to maximize the probability of the output-sequence given x . this is called the training sequence and is crucial to allow the models to adapt to the particular problem being solved . more details can be found in rabiner 's paper (rabiner-89) .
7.4.1 modeling documents the general-framework of empirical bayesian-estimation is broad and powerful enough that it has been applied in many contexts . the hard 270 finding out about work comes , however , in specifying just how the parametric-model Â© is to be constructed from a set of individual parameters 0 ; and how these can be estimated from the training-data . principled approaches to the text-classification problem require specification of explicit models of how documents are generated . two models of the event-space underlying our construction of hypothetical documents have been proposed [mccallum and nigam , 1998] , and we consider each of these below . one critical , simplifying assumption shared by both models is that the features occur independently in the documents . as we have discussed a number of times , any such naive-bayesian-model will miss a great many of the interactions arising among real words in real documents . it is somewhat curious , then , that such naive classifiers do as well as they do [domingos and pazzani , 1997] . multivariate bernoulli arguably the simplest model captures only the presence or absence of words in the document . that is , the document is modeled as the composition of k keywords drawn from the vocab as so many independent bernoulli-trials . we imagine that a document d is constructed by repeatordered edly selecting | d | words for each position in the document . ^ sequence jf we associate a biased coin with each keyword k , we can decompose the desired model 0 into two sets of parameters : 6c = pr (c) (7.10) eck = pr (k \ c) (7.11) i.e. , the prior-probability of each class c and the probability that a keyword is present given that we know a document containing it is in class c . then the naive-bayesian assumption allows us to assume that the keywords occur at each positional location independently of one another : eck (7.12) jt = i adaptive-information-retrieval 271 multinomial an alternative model of document-generation is as repeated draws from an urn of words containing all the keywords in the vocab . this gives rise to a multinomial-model that is sensitive to the frequency fa of keywords ' occurrence in dy rather than just their presence or absence : * gfdk f] - f - (7.13)
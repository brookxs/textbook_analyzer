3.4 information-extraction there are two processes associated with information-extraction : determination of facts to go into structured fields in a database and extraction of text that can be used to summarize an item . in the first case only a subset of the important facts in an item may be identified and extracted . in summarization all of the major concepts in the item should be represented in the summary . the process of extracting facts to go into indexes is called automatic file build in chapter 1 . its goal is to process incoming items and extract index-terms that will go into a structured database . this differs from indexing in that its objective is to extract specific types of information versus understanding all of the t ^ xt of the document . an information-retrieval-system 's goal is to provide an indepth representation of the total contents of an item (sundheim-92) . an 66 chapter 3 information-extraction-system only analyzes those portions of a document that potentially contain information relevant to the extraction criteria . the objective of the data-extraction is in most cases to update a structured database with additional facts . the updates may be from a controlled-vocabulary or substrings from the item as defined by the extraction rules . the term `` slot '' is used to define a particular category of information to be extracted . slots are organized into templates or semantic frames . information-extraction requires multiple levels-of-analysis of the text of an item . it must understand the words and their context (discourse-analysis) . the processing is very similar to the natural-language-processing described under indexing . in establishing metrics to compare information-extraction , the previously defined measures of precision-and-recall are applied with slight modifications to their meaning . recall refers to how much information was extracted from an item versus how much should have been extracted from the item . it shows the amount of correct and relevant data extracted versus the correct and relevant data in the item . precision refers to how much information was extracted accurately versus the total information extracted . additional metrics used are overgeneration and fallout . overgeneration measures the amount of irrelevant information that is extracted . this could be caused by templates filled on topics that are not intended to be extracted or slots that get filled with non-relevant data . fallout measures how much a system assigns incorrect slot fillers as the number of potential incorrect slot fillers increases (lehnert-91) . these measures are applicable to both human and automated-extraction processes . human beings fall short of perfection in data-extraction as well as automated systems . the best source of analysis-of-data extraction is from the message-understanding-conference proceedings . conferences (similar to trec) were held in 1991 , 1992 , 1993 and 1995 . the conferences are sponsored by the advanced research project agency/software and intelligent-systems technology office of the department-of-defense . large test databases are made available to any organization interested in participating in evaluation of their algorithms . in muc-5 (1993) , four experienced human analysts performed detailed extraction against 120 documents and their performance was compared against the top three information-extraction-systems . the humans achieved a 79 per cent recall with 82 per cent precision . that is , they extracted 79 per cent of the data they could have found and 18 per cent of what they extracted was erroneous . the automated programs achieved 53 per cent recall and 57 per cent precision . the other mediating factor is the costs associated with information-extraction . the humans required between 15 and 60 minutes to process a single item versus the 30 seconds to three minutes required by the computers . thus the existing algorithms are not operating close to what a human can achieve , but they are significantly cheaper . a combination of the two in a computer-assisted information-extraction-system appears the most reasonable solution in the foreseeable future . another related information-technology is document-summarization . rather than trying to determine specific facts , the goal of document-summarization cataloging and indexing 67 is to extract a summary of an item maintaining the most important ideas while significantly reducing the size . examples of summaries that are often part of any item are titles , table-of-contents , and abstracts with the abstract being the closest . the abstract can be used to represent the item for search purposes or as a way for a user to determine the utility of an item without having to read the complete item . it is not feasible to automatically generate a coherent narrative summary of an item with proper discourse , abstraction and language-usage (sparck jones-93) . restricting the domain of the item can significantly improve the quality of the output (paice-93 , reimer-88) . the more restricted goals for much of the research is in finding subsets of the item that can be extracted and concatenated (usually extracting at the sentence-level) and represents the most important concepts in the item . there is no guarantee of readability as a narrative abstract and it is seldom achieved . it has been shown that extracts of approximately 20 per cent of the complete item can represent the majority of significant concepts (morris-92) . different algorithms produces different summaries . just as different humans create different abstracts for the same item , automated techniques that generate different summaries does not intrinsically imply major deficiencies between the summaries . most automated algorithms approach summarization by calculating a score for each sentence and then extracting the sentences with the highest scores . some examples of the scoring-techniques are use of rhetorical-relations (e.g. , reason , direction , contrast : see miike-94 for experiments in japanese) , contextual inference and syntactic-coherence using cue-words (rush-71) , term location (salton-83) , and statistical-weighting properties discussed in chapter 5 . there is no overall theoretic basis for the approaches leading to many heuristic-algorithms . kupiec et al. are pursuing statistical-classification approach based upon a training-set reducing the heuristics by focusing on a weighted-combination of criteria to produce `` optimal '' scoring-scheme (kupiec-95) . they selected the following five feature-sets as a basis for their algorithm : sentence length feature that requires sentence to be over five words in length fixed phrase feature that looks for the existence of phrase `` cues '' (e.g. , `` in conclusion) paragraph feature that places emphasis on the first tee and last five paragraphs in an item and also the location of the sentences within the paragraph thematic word feature that uses word-frequency uppercase word feature that places emphasis on proper-names and acronvms . 68 chapter 3 as with previous experiments by edmundson , kupiec et al. discovered that location-based heuristics gives better results than the frequency_based features (edmundson-69) . although there is significant overlap in the algorithms and techniques for information-extraction and indexing items for information-retrieval , this text does not present more detail on information-extraction . for additional-information , the muc proceedings from morgan kaufman publishers , inc. in san francisco is one source of the latest detailed information on information-extraction .
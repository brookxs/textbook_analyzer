group-average agglomerative-clustering group-average agglomerative-clustering gaac 17.3 all group-average-clustering average-link clustering sim-ga (203) the motivation for gaac is that our goal in selecting two clusters and as the next merge in hac is that the resulting merge cluster should be coherent . to judge the coherence of , we need to look at all document-document similarities within , including those that occur within and those that occur within . we can compute the measure sim-ga efficiently because the sum of individual vector similarities is equal to the similarities of their sums : (204) (205) sim fficient 17.8 205 equation 204 relies on the distributivity of the dot-product with respect to vector-addition . since this is crucial for the efficient-computation of a gaac clustering , the method can not be easily applied to representations of documents that are not real-valued vectors . also , equation 204 only holds for the dot-product . while many algorithms introduced in this book have near-equivalent descriptions in terms of dot-product , cosine-similarity and euclidean-distance (cf. simdisfigs) , equation 204 can only be expressed using the dot-product . this is a fundamental difference between single-link/complete-link clustering and gaac . the first two only require a square matrix of similarities as input and do not care how these similarities were computed . to summarize , gaac requires (i) documents represented as vectors , (ii) length-normalization of vectors , so that self-similarities are 1.0 , and (iii) the dot-product as the measure of similarity between vectors and sums of vectors . the merge algorithms for gaac and complete-link-clustering are the same except that we use equation 205 as similarity-function in figure 17.8 . therefore , the overall time-complexity of gaac is the same as for complete-link-clustering : . like complete-link-clustering , gaac is not best-merge persistent (exercise 17.10) . this means that there is no algorithm for gaac that would be analogous to the algorithm for single-link in figure 17.9 . we can also define group-average similarity as including self-similarities : (206) 139 139 self-similarities are always equal to 1.0 , the maximum possible value for length-normalized vectors . the proportion of self-similarities in equation 206 is for a cluster of size . this gives an unfair advantage to small clusters since they will have proportionally more self-similarities . for two documents , with a similarity , we have . in contrast , . this similarity of two documents is the same as in single-link , complete-link and centroid-clustering . we prefer the definition in equation 205 , which excludes self-similarities from the average , because we do not want to penalize large clusters for their smaller proportion of self-similarities and because we want a consistent similarity value for document pairs in all four hac algorithms . exercises . apply group-average-clustering to the points in and 17.7 . map them onto the surface of the unit-sphere in a three-dimensional-space to get length-normalized vectors . is the group-average-clustering different from the single-link and complete-link clusterings ?
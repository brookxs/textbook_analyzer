7.2.1 feature-selection when you are thinking about how you classifiy your email , keywords contained in your email are almost certainly some of the features you think of first . recall , however , that the keyword vocabulary can be very-large . using this feature-space , then , individual document representations will be very sparse . in terms of the vector-space-model of section 3.4 , many of the vector elements will be zero . to use littlestone 's lovely expression , `` irrelevant attributes abound '' [littlestone , 1988] , and so it should come as no surprise that his learning-techniques are especially appropriate in the foa learning applications discussed in section 7.5.3 . efforts to control the keyword vocabulary and make the lexical-features as meaningful as possible are therefore important preconditions for good classification-performance . for example , name-tagging techniques (cf. section 6.6.1) that reliably identify proper-names can provide valuable classification features . a proper-name tagger would be one that was especially sophisticated about capitalization , name order , and abbreviation conventions . when both people 's proper-names and institutional names (government-agencies , universities , corporations , etc.) are capitalized , the recognition of complex , multitoken phrases becomes possible . in part because of the difficult issues lexical , keyword-based representations entail , it is worth thinking briefly about some of the alternatives . there are also less-obvious features we might use to classify documents . meta-data associated with the document , for example , information about its date and place of publication , is one possibility . geographic place information associated with a document can also be useful ; cf. section 6.6.1 . finally , recall the bibliographic-citations that many documents contain (cf. section 6.1) . the set of references one 260 finding out about document makes to others (representable as links in a graph) can be used as the basis of classification in much the same way as its keywords . in summary , while keywords provide the most obvious set-of-features on which classifications can be based , these result in very-large and sparse-learning problems . other features are also available and may be more useful . it is important to note , however , that careful bayesian-reasoning about dependencies among keyword features is a very difficult problem , as discussed in section 5.5.7 . attempting to extend this inference to include other heterogeneous-types of features must be done carefully . distribution-based selection because our typical assumption has been that keywords occur independently , it should come as no surprise that when we try to reduce from the full set of all keywords in the vocab to a smaller set , a good way to decide which features are most useful is to pick those that are most independent of any others . that is , we can hope that two keywords that are statistically dependent can be merged into a single one . as mentioned in section 3.3.6 , entropy captures the amount of randomness in (or uncertainty about) some random-variable : h (x) = - j ^ pr (x = x) log (pr (x)) (7.1) when the distribution of a random-variable x is conditionally dependent on that of a second random-variable y , the conditional-entropy of x on y (a.la . post-y entropy of x) can be similarly defined [papoulis , 1991 , pp. 549-54] : h (x \ y) = - # pr (x \ y) log (pr (x = x \ y = y)) (7.2) x = x , y = y if knowledge of values of y reduces our uncertainty about the distribution of xy it is natural to think that y informs us about x. mutual-information i captures this notion : - h (x \ y) pr (x) log (pr (x)) + j2pr (x \ y) log (pr (x \ y)) (7.3) x x , y this information is mutual in the sense that it is a symmetric relation adaptive-information-retrieval 261 between the two variables ; y tells us as much about x as x does about y : i (x , y) = i {y , x) . if the mutual-information i (k {-rcb- kj) between all pairs of keywords in k isknown , vanrijsbergen , p. 123 recommends selecting the maximum-spanning-tree , which maximizes the total information across all edges of the tree : the mst ... incorporates the most significant of the dependencies between the variables subject to the global-constraint that the sum of them should be a maximum , [van rijsbergen , p. 123] note , however , that the mutual-information statistic has an intrinsic bias toward keyword pairs kj , kj in which the individual keyword frequencies fi and fj are intermediate . the post-y entropy of x can only reduce it [papoulis , 1991] : h (x \ y) lt ; h (x) (7.4) in terms of keyword frequencies , then , the mutual-information of a pair of keywords is limited by their marginal entropies . this implies that very rare and very common keywords are `` penalized '' with respect to the mutual-information measure . for this reason , it is worthwhile considering the relation between mutual-information as a measure of keyword interdependencies and the eigenstructure analysis (cf. section 5.2.3) of the keyword cross-correlation matrix / . mutual-information considers the full joint-probability between the two keywords , while methods like singular-value-decomposition (svd) and principal-component-analysis (pca) consider only the crosscorrelation matrix . when random variables happen to fit a normal-distribution , these correlation statistics are sufficient , but that is unlikely in the case of our keywords . a second important difference is that correlationbased methods construct new feature variables , out of linear-combinations of the initial keyword tokens . mutual information-based methods (or at least van rijsbergen , p. 123 's mst-based construction) select the best variables from a constant set . selection based on `` fit '' to a classification-task rather than using distributional statistics among the keywords themselves as the basis for feature-selection , it is also possible to look for those 262 finding out about features that are most `` fit '' with respect to some classification-task [lewis and hayes , 1994] (cf. section 7.4) . both mutual-information and correlation statistics can be used in either supervised or unsupervised-learning situations , considering either the mutual-information j (fc , c) of each keyword k with respect to the class c in the former case or fisher 's linear-discriminant [duda and hart , 1973 , p. 114] in the latter . using the classification-performance criterion , yang and pederson considered a wide range of potential measures and found that simply measuring / * , the document frequency of keyword fc , provided an effective measure over potential keyword features [yang and pedersen , 1997] . in fact , using document-frequency as a criterion , they were able to remove 98 percent of all keywords while retaining (even improving slightly !?) classification-performance . given the efficient way in which fk can be collected (relative to ml , x2gt ; and other potential measures) , the level of performance maintained by such aggressive dimensionality-reduction is indeed striking . but because the features selected are sensitive to the particular classification-task considered , their utility for other purposes may be suspect . distribution-based selection-methods may therefore be more robust .
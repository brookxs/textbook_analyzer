8.2.2 construction building and maintaining an inverted-index is a relatively low-cost task . in principle , an inverted-index on a text of n characters can be built in o (n) time . all the vocabulary known up to now is kept in a trie data-structure , storing for each word a list of its occurrences (text positions) . each word of the text is read and searched in the trie . if it is not found , it is added to the trie with an empty list of occurrences . once it is in the trie , the new position is added to the end of its list of occurrences . figure 8.3 illustrates this process . once the text is exhausted , the trie is written to disk together with tiie list of occurrence . it is good practice to split the index into two files . in the inverted-files 197 1 6 9 11 17 19 24 28 33 40 46 50 55 60 this is a text . a text has many words . words are made from letters . text vocabulary-trie figure 8.3 building an inverted-index for the sample text . first file , the lists of occurrences are stored contiguously . in this scheme , the file is typically called a ` posting file ' . in the second file , the vocabulary is stored in lexicographical order and , for each word , a pointer to its list in the first file is also included . this allows the vocabulary to be kept in memory at search-time in many cases . further , the number of occurrences of a word can be immediately known from the vocabulary with little or no space-overhead . we analyze now the construction time under this scheme . since in the trie 0 (1) operations are performed per text character , and the positions can be inserted at the end of the lists of occurrences in 0 (1) time , the overall process is o (n) worst-case time . however , the above algorithm is not practical for large texts where the index does not fit in main-memory . a paging mechanism will severely degrade the performance of the algorithm . we describe an alternative which is faster in practice . the algorithm already described is used until the main-memory is exhausted (if the trie takes up too much space it can be replaced by a hash-table or other structure) . when no more memory is available , the partial index j2 obtained up to now is written to disk and erased from main-memory before continuing with the rest of the text . finally , a number of partial indices iz exist on disk . these indices are then merged in a hierarchical fashion . indices i \ and i2 are merged to obtain the index j1 . .2 ; / .3 and i4 produce i `` 3 . .4 ; and so on . the resulting partial indices are now approximately twice the size . when all the indices at this level have been merged in this way , the merging proceeds at the next level , joining the index i \ ,2 with the index j3 . .4 to form / 1 . .4 . this is continued until there is just one index comprising the whole text , as illustrated in figure 8.4 . merging two indices consists of merging the sorted vocabularies , and whenever the same word appears in both indices , merging both lists of occurrences . by construction , the occurrences of the smaller-numbered index are before those of the larger-numbered index , and therefore the lists are just concatenated . this is a very fast process in practice , and its complexity is o (n \ + n2k where ii \ and fi-gt ; are the sizes of the indices . 198 indexing and searching i-1 . .8 level 4 (final index) i-1 . .4 i-5 . .8 level-3 level 2 level 1 (initial dumps) figure 8.4 merging the partial indices in a binary fashion . rectangles represent partial indices , while rounded rectangles represent merging operations . the numbers inside the merging operations show a possible merging order . the total time to generate the partial indices is o (n) as before . the number of partial indices is o (n/m) . each level of merging performs a linear process over the whole index (no matter how it is split into partial indices at this level) and thus its cost is o (n) . to merge the 0 {n/m) partial indices , iog2 (n/a /) merging levels are necessary , and therefore the cost of this algorithm is o (nlog (n/m)) . more than two indices can be merged at once . although this does not change the complexity , it improves efficiency since fewer merging levels exist . on the other hand , the memory buffers for each partial index to merge will be smaller and hence more disk seeks will be performed . in practice it is a good idea to merge even 20 partial indices at once . real times to build inverted-indices on the reference machine are between 4-8 mb/min for collections of up to 1 gb (the slowdown factor as the text grows is barely noticeable) . of this time , 20-30 % is spent on merging the partial indices . to reduce build-time space requirements , it is possible to perform the merging in-place . that is , when two or more indices are merged , write the result in the same disk blocks of the original indices instead of on a new file . it is also a good idea to perform the hierarchical merging as soon as the files are generated (e.g. . collapse /) and / 2 into j1gt ; gt ; 2 as soon as i2 is produced) . this also reduces space requirements because the vocabularies are merged and redundant words are eliminated (there is no redundancy in the occurrences) . the vocabulary can be a significative part of the smaller partial indices , since they represent a small text . this algorithm changes very little if block addressing is used . index-maintenance is also cheap . assume that a new text of size n * is added to the database , tlip inverted-index for the new text is built and then both indices are merged other indices for text 199 as is done for partial indices . this takes o (n - f n ' log (n'jm)) . deleting text can be done by an o (n) pass over the index eliminating the occurrences that point inside eliminated text areas (and eliminating words if their lists of occurrences disappear in the process) .
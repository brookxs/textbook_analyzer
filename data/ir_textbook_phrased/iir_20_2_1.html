crawler architecture the simple scheme outlined above for crawling demands several modules that fit together as shown in figure 20.1 . the url-frontier , containing urls yet to be fetched in the current crawl (in the case of continuous crawling , a url may have been fetched previously but is back in the frontier for re-fetching) . we describe this further in section 20.2.3 . a dns resolution module that determines the-web server from which to fetch the page specified by a url . we describe this further in section 20.2.2 . a fetch module that uses the http-protocol to retrieve the-web page at a url . a parsing-module that extracts the text and set of links from a fetched web-page . a duplicate-elimination module that determines whether an extracted link is already in the url-frontier or has recently been fetched . figure 20.1 : the basic crawler architecture . crawling is performed by anywhere from one to potentially hundreds of threads , each of which loops through the logical cycle in figure 20.1 . these threads may be run in a single process , or be partitioned amongst multiple processes running at different nodes of a distributed-system . we begin by assuming that the url-frontier is in place and non-empty and defer our description of the implementation of the url-frontier to section 20.2.3 . we follow the progress of a single url through the cycle of being fetched , passing through various checks and filters , then finally (for continuous crawling) being returned to the url-frontier . a crawler thread begins by taking a url from the frontier and fetching the-web page at that url , generally using the http-protocol . the fetched page is then written into a temporary store , where a number of operations are performed on it . next , the page is parsed and the text as well as the links in it are extracted . the text (with any tag-information - e.g. , terms in boldface) is passed on to the indexer . link-information including anchor-text is also passed on to the indexer for use in ranking in ways that are described in chapter 21 . in addition , each extracted link goes through a series of tests to determine whether the link should be added to the url-frontier . first , the thread tests whether a web-page with the same content has already been seen at another url . the simplest implementation for this would use a simple fingerprint such as a checksum (placed in a store labeled `` doc fp 's '' in figure 20.1) . a more sophisticated test would use shingles instead of fingerprints , as described in chapter 19 . next , a url filter is used to determine whether the extracted url should be excluded from the frontier based on one of several tests . for instance , the crawl may seek to exclude certain domains (say , all . com urls) - in this case the test would simply filter out the url if it were from the . com domain . a similar test could be inclusive rather than exclusive . many hosts on the web place certain portions of their websites off-limits to crawling , under a standard known as the robots-exclusion-protocol , except for the robot called `` searchengine '' . user-agent : * disallow : / yoursite/temp / user-agent : searchengine disallow : the robots.txt file must be fetched from a website in order to test whether the url under consideration passes the robot restrictions , and can therefore be added to the url-frontier . rather than fetch it afresh for testing on each url to be added to the frontier , a cache can be used to obtain a recently fetched copy of the file for the host . this is especially important since many of the links extracted from a page fall within the host from which the page was fetched and therefore can be tested against the host 's robots.txt file . thus , by performing the filtering during the link-extraction process , we would have especially high locality in the stream of hosts that we need to test for robots.txt files , leading to high cache hit rates . unfortunately , this runs afoul of webmasters ' politeness expectations . a url (particularly one referring to a low-quality or rarely changing document) may be in the frontier for days or even weeks . if we were to perform the robots filtering before adding such a url to the frontier , its robots.txt file could have changed by the time the url is dequeued from the frontier and fetched . we must consequently perform robots-filtering immediately before attempting to fetch a web-page . as it turns out , maintaining a cache of robots.txt files is still highly effective ; there is sufficient locality even in the stream of urls dequeued from the url-frontier . next , a url should be normalized in the following sense : often the html encoding of a link from a web-page indicates the target of that link relative to the page . thus , there is a relative link encoded thus in the html of the page en.wikipedia.org/wiki/main_page: disclaimers http://en.wikipedia.org/wiki/wikipedia:general_disclaimer finally , the url is checked for duplicate-elimination : if the url is already in the frontier or (in the case of a non-continuous crawl) already crawled , we do not add it to the frontier . when the url is added to the frontier , it is assigned a priority based on which it is eventually removed from the frontier for fetching . the details of this priority-queuing are in section 20.2.3 . certain housekeeping tasks are typically performed by a dedicated thread . this thread is generally quiescent except that it wakes up once every few seconds to log crawl progress statistics (urls crawled , frontier size , etc.) , decide whether to terminate the crawl , or (once every few hours of crawling) checkpoint the crawl . in checkpointing , a snapshot of the crawler 's state (say , the url-frontier) is committed to disk . in the event of a catastrophic crawler failure , the crawl is restarted from the most recent checkpoint . subsections distributing the crawler
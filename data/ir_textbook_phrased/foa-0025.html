23 imtradocumeiit parsing having now focused our attention on a particular file and on the beginning and ending locations within that file associated with a particular document , we can consider this file segment simply a stream of characters . reading each and every character of each and every document , deciding whether it is part of a meaningful token , and deciding whether these tokens are worth indexing will be the most computationally intensive aspect of the indexing chore ; this is our `` inner loop '' for that reason , we will devote some real attention to making this lexical-analysis as efficient as possible . several general criteria will shape our design . first , because we are assuming that our textual corpus is very-large , we will do our best to avoid duplicating this primary text . that is , we will attempt to deal with the text in-situ and not make a second copy for use by the indexing-and-retrieval system . thus , we will be creating a system of pointers into locations within the corpora directories and files . extracting lexical-features 43 figure 2.2 finite-state-machine a wide range of alternative designs are possible even at this early stage , and so we desire as much flexibility as possible in the specification of the lexical-analyzer . a lexical-analyzer generator , such as the lex tool in unix , allows the specification of very complicated lexical analyzers for very elaborate languages . the fundamental representation used by all such algorithms is a finite-state-machine , like that shown in figure 2.2 . this simple representation breaks the set of possible characters coming from a text-stream into classes (drawn as circular states) , with transitions from one state to the next on the occurrence of particular characters . by careful construction of the sets of characters (e.g. , white-space characters corresponding to state 0 in figure 2.2) , arbitrary text sequences can be handled very efficiently . for our two example corpora and many other situations , the stream of characters , a straightforward analysis in terms of a simple finite-state-machine , will suffice . we will depend on a utility written by christopher fox [fox , 1992] . this utility simultaneously achieves two critical goals . first , the lexical-analyzer tokenizes the stream of characters into a sequence of wordlike elements . at first blush this seems straightforward : a token is anything separated by white-space * where the standard definition of white-space is used . but what about hyphens ? should the hyphenated phrase data-base be treated as two separate tokens or as a single one ? should a file name * like wimdows.exe be treated as a single token ? which host , directory , and file elements in a full url like 44 finding out about www.cs.ucsd.edu/-rik are to be kept intact as individual tokens ? more elaborate elements such as these can quickly demand the sophistication of a tool like lex . the presence of digits among the alphabetic characters presents another problem . are numbers to be allowed as tokens ? perhaps we only want to allow `` special '' numbers (e.g. , 1776 , 1984 , 2001 , 3.14159) . perhaps we want to use rules similar to those for programming-language identifiers and require that a token begin with an alphabetic character , which may then be followed by numbers or letters . we must also worry about the case of the characters at this earliest lexical-analysis stage . are we to treat capitalization as significant in distinguishing tokens from one another ? an enormous reduction in vocabulary-size is possible if we fold case so as to treat upper - and lowercase characters interchangeably . but of course then we have also precluded the possibility of many proper-name analyses that may be useful for identifying singular people , places , or events (see chapter 6) . in some cases the semantics of the documents make decisions about case automatic . for example , if the documents are program source files , the language in question may or may not treat differences in case as significant .
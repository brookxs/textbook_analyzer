6.3 item clustering clustering of items is very similar to term-clustering for the generation of thesauri . manual item clustering is inherent in any library or filing-system . in this case someone reads the item and determines the category or categories to which it belongs . when physical clustering occurs , each item is usually assigned to one category . with the advent of indexing , an item is physically stored in a primary category , but it can be found in other categories as defined by the index-terms assigned to the item . with the advent of electronic holdings of items , it is possible to perform automatic-clustering of the items . the techniques described for the clustering of terms in sections 6.2.2.1 through 6.2.2.3 also apply to item clustering . similarity between documents is based upon two items that have terms in common versus terms with items in common . thus , the similarity-function is performed between rows of the item matrix . using figure 6.2 as the set of items and their terms and similarity equation : sim (item , , itemj) = i (term ^) (teraijj #) as k goes from 1 to 8 for the eight terms , an item-item matrix is created (figure 6.9) . using a threshold of 10 produces the item relationship matrix shown in figure 6.10 . document and term-clustering 155 item 1 item 2 item 3 item 4 item 5 item 1 11 3 6 22 item 2 11 12 10 36 item 3 3 12 6 9 item 4 6 10 6 11 item 5 22 36 9 11 figure 6.9 item/item matrix iteml item 2 item3 item 4 items iteml 1 0 0 1 item2 1 1 1 1 item 3 0 1 0 0 item4 0 1 0 3 item5 1 1 0 1 figure 6.10 item relationship matrix using the clique algorithm for assigning items to classes produces the following classes based upon figure 6.10 : class 1 = item 1 , item 2 , item 5 class 2 = item 2 , item 3 class 3 = item 2 , item 4 , item 5 application of the single link technique produces : class 1 = item 1 , item 2 , item 5 , item 3 , item 4 all the items are in this one cluster , with item 3 and item 4 added because of their similarity to item 2 . the star technique (i.e. , always selecting the lowest nonassigned item) produces : class 1 - item 1 , item 2 , item 5 class 2 - item 3 , item 2 class 3 - item4 , itera2 , items using the string technique and stopping when all items are assigned to classes produces the following : class 1 - item 1 , item 2 , item 3 class 2 - item 4 , item 5 156 chapter 6 in the vocabulary domain homographs introduce ambiguities and erroneous hits . in the item domain multiple topics in an item may cause similar problems . this is especially true when the decision is made to partition the document space . without precoordination of semantic-concepts , an item that discusses `` politics '' in `` america '' and `` economics '' in `` mexico '' could get clustered with a class that is focused around `` politics '' in `` mexico . '' clustering by starting with existing clusters can be performed in a manner similar to the term model . lets start with item 1 and item 3 in class 1 , and item 2 and item 4 in class 2 . the centroids are : class 1 = 3/2 , 4/2 , 0/2 , 0/2 , 3/2 , 2/2 , 4/2 , 3/2 class 2 = 3/2 , 2/2 , 4/2 , 6/2 , 1/2 , 2/2 , 2/2 , 1/2 the results of recalculating the similarities of each item to each centroid and reassigning terms is shown in figure 6.11 . class 1 class 2 assign class 1 class 2 class 2 class 2 class 2 figure 6.11 item clustering with initial clusters finding the centroid for class 2 , which now contains four items , and recalculating the similarities does not result in reassignment for any of the items . instead of using words as a basis for clustering items , the acquaintance system uses n-grams (damashek-95 , cohen-95) . not only does their algorithm cluster items , but when items can be from more than one language , it will also recognize the different languages .
33.3 resolving power zipf observed that the frequency of words ' occurrence varies dramatically , andpoisson models explore deviations of these occurrence patterns from purely random-processes . we now make the first important move toward a theory of why some words occur more frequently and how such statistics can be exploited when building an index automatically . luhn , as far back as 1957 , said clearly : it is hereby proposed that the frequency of word occurrence in an article furnishes a useful measurement of word significance . [luhn ,1957] that is , if a word occurs frequently , more frequently than we would expect it to occur within a corpus , then it is reflecting emphasis on the part of the author about that topic . but the raw frequency of occurrence in a document is only one of two critical statistics recommending good keywords . consider a document taken from our ait corpus , and imagine using the keyword artificial-intelligence with it . by construction , virtually every document in the ait is about artificial-intelligence !? assigning the keyword artificial-intelligence to every document in ait would be a mistake , not because this document is n't about artificial-intelligence , but because this term can not help us discriminate one subset of our corpus as relevant to any query . if we change our search-task to looking not only in our ait corpus but through a much larger collection (for example , all computer industry newsletters) , then associating artificial-intelligence with those articles in our ait subcorpus becomes a good idea . this term helps to distinguish ai documents from others . the second critical characteristic of good indices now becomes clear : a good index term not only characterizes a document absolutely , as a feature of a document in isolation , but also allows us to discriminate it relative to other documents in the corpus . hence keywords are not strictly properties of any single-document , but they reflect a relationship between an individual document and the collection from which it might be selected . these two countervailing considerations suggest that the best keywords will not be the most ubiquitous , frequently occurring terms , nor weighting and matching against indices 77 upper cutott lower cutoff zipf s first law rank-order of words too common significant too rare figure 3.3 resolving power those that occur only once or twice , but rather those occurring a moderate number of times . using zipf 's rank ordering of words as a baseline , luhn hypothesized a modal function of a word 's rank he called resolving power , centered exactly at the middle of this rank ordering . if resolving power is defined as a word 's ability to discriminate content , luhn assumed that this quantity is maximal at the middle and falls off at either very high or very low frequency extremes , as shown in figure 3.3 . * the next step is then to establish maximal and minimal occurrence thresholds defining useful , midfrequency index-terms . unfortunately , luhn 's view does not provide theoretical grounds for selecting these bounds , so we are reduced to the engineering task of tuning them for optimal-performance . we 'll begin with the maximal-frequency threshold , which is used to exclude words that occur too frequently . for any particular corpus , it is interesting to contrast this set of most-common words with the negative dictionary of noise words , defined in section 2.3.2 . while there is often great overlap , the negative dictionary list has proven itself to be useful across many different corpora , while the most frequent tokens in a particular corpus may be quite specific to it . establishing the low-frequency threshold is less intuitive . assuming that our index is to be of limited size , including a certain keyword means we must exclude some other . this suggests that a word that occurs in * after [van rijsbergen , p. 16 , figure 2.1] . 78 finding out about specificity exhaustivity ¶ i discritninability representation of _______________ of few '' ' ¶ ª '' ¶ ¶ ¶ ´ ¶ ... . . ¶ ¶ ¶ ... ... ... ... ... ... ... ... ... . . i many doc/jkw kw/doc leads to high precision high recall figure 3.4 specificity/exhaustivity trade-offs exactly one document ca n't possibly be used to help discriminate that document from others regularly . for example , imagine a word - suppose it is derivative - that occurs exactly once , in a single-document . if we took out that word derivative and put in any other word , for example , foobar , in terms of the word-frequency co-occurrence-statistics that are the basis of all our indexing-techniques , the relationship between that document and all the other documents in the collection will remain unchanged . in terms of overlap between what the word derivative means , in the foa sense of what this and other documents are about , a single word occurrence has no meaning ! the most useful words will be those that are not used so often as to be roughly common to all of the documents , and not so rarely as to be (nearly) unique to any one (or a small set of) document . we seek those keywords whose combinatorial properties , when used in concert with one another as part of queries , help to compare and contrast topical areas-of-interest against one another .
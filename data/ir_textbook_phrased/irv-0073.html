discrimination-power of an index term on p. 120 i defined and in fact there made the comment that it was a measure of the power of term i to discriminate between relevant and non-relevant documents . the weights in the weighting-function derived from the independence-assumption a1 are exactly these ki 's . now if we forget for the moment that these weights are a consequence of a particular model and instead consider the notion of discrimination-power of an index term on its own merits . certainly this is not a novel thing to do , salton in some of his work has sought effective ways of measuring the ` discrimination value ' of index-terms [24] . it seems reasonable to attach to any index term that enters into the retrieval-process a weight related to its discrimination-power . ki as a measure of this power is slightly awkward in that it becomes undefined when the argument of the log function becomes zero . we therefore seek a more ` robust ' function for measuring discrimination-power . the function i am about to recommend for this purpose is indeed more robust , has an interesting interpretation , and enables me to derive a general result of considerable interest in the next section . however , it must be emphasised that it is only an example of a function which enables some sense to be make of the notion ` discrimination-power ' in this and the next section . it should therefore not be considered unique although it is my opinion that any alternative way of measuring discrimination-power in this context would come very close to the measure i suggest here . instead of ki i suggest using the information radius , defined in chapter 3 on p. 42 , as a measure of the discrimination power of an index term . it is a close cousin of the expected mutual-information measure a relationship that will come in useful later on . using u and v as positive weights such as u + v = 1 and the usual notation for the probability functions we can write the information radius as follows : the interesting interpretation of the information radius that i referred to above is illustrated most easily in terms of continuous-probability functions . instead of using the densities p (. / w1) and p (. / w2) i shall use the corresponding probability measure u1 and u2 . first we define the average of two directed divergencies [25] , r (u1 , u2/v) = ui (u1/v) + vi (u2/v) where i (ui/v) measures the expectation on ui of the information in favour of rejecting v for ui given by making an observation ; it may be regarded as the information gained from being told to reject v in favour of ui . now the information radius is the minimum thereby removing the arbitrary v . in fact it turns out that the minimum is achieved when v = u u1 + v u2 that is , an average of the two distributions to be discriminated . if we now adopt u and v as the prior-probabilities then v is in fact given by the density p (x) = p (x/w1) p (w1) + p (x/w2) p (w2) defined over the entire collection without regard to relevance . now of this distribution we are reasonably sure , the distribution u1 and u2 we are only guessing at ; therefore it is reasonable when measuring the difference between u1 and u2 that v should incorporate as much of the information that is available . the information radius does just this . there is one technical problem associated with the use of the information radius , or any other ` discrimination measure ' based on all four cells of the contingency-table , which is rather difficult to resolve . as a measure of discrimination-power it does not distinguish between the different contributions made to the measure by the different cells . so , for example , an index term might be a good discriminator because it occurs frequently in the non-relevant documents and infrequently in the relevant documents . therefore , to weight an index term proportional to the discrimination measure whenever it is present in a document is exactly the wrong thing to do . it follows that the data contained in the contingency-table must be used when deciding on a weighting-scheme .
presentation of experimental-results in my discussion of micro - , macro-evaluation , and expected search length , various ways of averaging the effectiveness measure of the set of queries arose in a natural way . i now want to examine the ways in which we can summarise our retrieval results when we have no a priori reason to suspect that taking means is legitimate . in this section the discussion will be restricted to single number measures such as a normalised symmetric difference , normalised recall , etc. . let us use z to denote any arbitrary measure . the test queries will be qi and n in number . our aim in all this is to make statements about the relative merits of retrieval under different conditions a , b , c , ... in terms of the measure of effectiveness z . the ` conditions ' a , b , c , ... may be different search-strategies , or information-structures , etc. . in other words , we have the usual experimental set-up where we control a variable and measure how its change influences retrieval-effectiveness . for the moment we restrict these comparisons to one set of queries and the same document-collection . the measurements we have therefore are {za (q1) , za (q2) , ...} , {zb (q1) , zb (q2) , ...} , {zc (q1) , zc (q2) , ...} , ... where zx (q1) is the value of z when measuring the effectiveness of the response to qi under conditions x . if we now wish to make an overall comparison between these sets of measurements we could take means and compare these . unfortunately , the distributions of z encountered are far from bell-shaped , or symmetric for that matter , so that the mean is not a particularly good ` average ' indicator . the problem of summarising ir data has been a hurdle every since the beginning of the subject . because of the non-parametric nature of the data it is better not to quote a single statistic but instead to show the variation in effectiveness by plotting graphs . should it be necessary to quote ` average ' results it is important that they are quoted alongside the distribution from which they are derived . there are a number of ways of representing sets of z-values graphically . probably the most obvious one is to use a scatter diagram , where the x-axis is scaled for za and the y-axis for zb and each plotted point is the pair (za (qi) , zb (qi)) . the number of points plotted will equal the number of queries . if we now draw a line at 45 [[ring]] to the x-axis from the origin we will be able to see what proportion of the queries did better under condition a than under condition b . there are two disadvantages to this method of representation : the comparison is limited to two conditions , and it is difficult to get an idea of the extent to which two conditions differ . a more convenient way of showing retrieval results of this kind is to plot them as cumulative frequency distributions , or as they are frequently called by statisticians empirical distribution-functions . let {z (q1) , z (q2) , ... , z (qn)} be a set of retrieval results then the empirical distribution-function f (z) is a function of z which equals the proportion of z (qi) 's which are less than or equal to z. to plot this function we divide the range of z into intervals . if we assume that 0 lt ; = z lt ; = 1 , then a convenient set of intervals is ten . the distributions will take the general shape as shown in figure 7.14 . when the measure z is such that the smaller its value the more effective the retrieval , then the higher the curve the better . it is quite simple to read off the various quantiles . for example , to find the median we only need to find the z-value corresponding to 0.5 on the f (z) axis . in our diagrams they are 0.2 and 0.4 respectively for conditions a and b. i have emphasised the measurement of effectiveness from the point-of-view of the user . if we now wish to compare retrieval on different document-collections with different sets of queries then we can still use these measures to indicate which system satisfies the user more . on the other hand , we can not thereby establish which system is more effective in its retrieval operations . it may be that in system a the sets of relevant documents constitute a smaller proportion of the total set of documents than is the case in system b . in other words , it is much harder to find the relevant documents in system b than in system a. so , any direct comparison must be weighted by the generality measure which gives the number of relevant documents as a proportion of the total number of documents . alternatively one could use fallout which measures the proportion of non-relevant documents retrieved . the important point here is to be clear about whether we are measuring user-satisfaction or system effectiveness .
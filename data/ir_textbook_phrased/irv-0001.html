one introduction information-retrieval is a wide , often loosely-defined term but in these pages i shall be concerned only with automatic-information-retrieval systems . automatic as opposed to manual and information as opposed to data or fact . unfortunately the word-information can be very misleading . in the context of information-retrieval (ir) , information , in the technical meaning given in shannon 's theory of communication , is not readily measured (shannon and weaver [1]) . in fact , in many cases one can adequately describe the kind of retrieval by simply substituting ` document ' for ` information ' . nevertheless , ` information-retrieval ' has become accepted as a description of the kind of work published by cleverdon , salton , sparck jones , lancaster and others . a perfectly straightforward definition along these lines is given by lancaster [2] : ` information-retrieval is the term conventionally , though somewhat inaccurately , applied to the type of activity discussed in this volume . an information-retrieval-system does not inform (i.e. change the knowledge of) the user on the subject of his inquiry . it merely informs on the existence (or non-existence) and whereabouts of documents relating to his request . ' this specifically excludes question-answering-systems as typified by winograd [3] and those described by minsky [4]] . it also excludes data-retrieval systems such as used by , say , the stock exchange for on-line quotations . to make clear the difference between data retrieval (dr) and information retrieval (ir) , i have listed in table 1.1 some of the distinguishing properties of data and information retrieval . one table 1.1 data-retrieval or information-retrieval ? data-retrieval (dr) information-retrieval (ir) matching exact-match partial-match , best match inference deduction induction-model deterministic probabilistic-classification monothetic polythetic query-language artificial natural query-specification complete incomplete items wanted matching relevant error response sensitive insensitive may want to criticise this dichotomy on the grounds that the boundary between the two is a vague one . and so it is , but it is a useful one in that it illustrates the range of complexity associated with each mode of retrieval . let us now take each item in the table in turn and look at it more closely . in data-retrieval we are normally looking for an exact-match , that is , we are checking to see whether an item is or is not present in the file . in information-retrieval this may sometimes be of interest but more generally we want to find those items which partially match the request and then select from those a few of the best matching ones . the inference used in data-retrieval is of the simple deductive kind , that is , arb and brc then arc . in information-retrieval it is far more common to use inductive-inference ; relations are only specified with a degree of certainty or uncertainty and hence our confidence in the inference is variable . this distinction leads one to describe data-retrieval as deterministic but information-retrieval as probabilistic . frequently bayes ' theorem is invoked to carry out inferences in ir , but in dr probabilities do not enter into the processing . another distinction can be made in terms of classifications that are likely to be useful . in dr we are most likely to be interested in a monothetic-classification , that is , one with classes defined by objects possessing attributes both necessary and sufficient to belong to a class . in ir such a classification is one the whole not very useful , in fact more often a polythetic classification is what is wanted . in such a classification each individual in a class will possess only a proportion of all the attributes possessed by all the members of that class . hence no attribute is necessary nor sufficient for membership to a class . the query-language for dr will generally be of the artificial kind , one with restricted syntax and vocabulary , in ir we prefer to use natural-language although there are some notable exceptions . in dr the query is generally a complete specification of what is wanted , in ir it is invariably incomplete . this last difference arises partly from the fact that in ir we are searching for relevant documents as opposed to exactly matching items . the extent of the match in ir is assumed to indicate the likelihood of the relevance of that item . one simple consequence of this difference is that dr is more sensitive to error in the sense that , an error in matching will not retrieve the wanted item which implies a total failure of the system . in ir small errors in matching generally do not affect performance of the system significantly . many automatic-information-retrieval systems are experimental . i only make occasional reference to operational systems . experimental ir is mainly carried on in a ` laboratory ' situation whereas operational systems are commercial systems which charge for the service they provide . naturally the two systems are evaluated differently . the ` real-world ' ir systems are evaluated in terms of ` user-satisfaction ' and the price the user is willing to pay for its service . experimental ir systems are evaluated by comparing the retrieval experiments with standards specially constructed for the purpose . i believe that a book on experimental information-retrieval , covering the design and evaluation of retrieval-systems from a point-of-view which is independent of any particular system , will be a great help to other workers in the field and indeed is long overdue . many of the techniques i shall discuss will not have proved themselves incontrovertibly superior to all other techniques , but they have promise and their promise will only be realised when they are understood . information about new techniques has been so scattered through the literature that to find out about them you need to be an expert before you begin to look . i hope that i will be able to take the reader to the point where he will have little trouble in implementing some of the new techniques . also , that some people will then go on to experiment with them , and generate new , convincing evidence of their efficiency-and-effectiveness . my aim throughout has been to give a complete coverage of the more important ideas current in various special areas of information-retrieval . inevitably some ideas have been elaborated at the expense of others . in particular , emphasis is placed on the use of automatic-classification techniques and rigorous methods of measurement of effectiveness . on the other hand , automatic-content-analysis is given only a superficial coverage . the reasons are straightforward , firstly the material reflects my own bias , and secondly , no adequate coverage of the first two topics has been given before whereas automatic-content-analysis has been documented very well elsewhere . a subsidiary reason for emphasising automatic-classification is that little appears to be known or understood about it in the context of ir so that research workers are loath to experiment with it .
7.2.2 hypothesis spaces however they are selected , the features discussed in the previous section must now be composed into hypotheses concerning how we might describe documents . there is an extraordinary range of alternatives represented here . some of these are shown in figure 7.3 . decision-trees are formed by asking a question about individual features and using the answers to these questions to navigate through a series of tests until documents are ultimately classified at the leaves . weighted , linear-combinations of the features can also be formed . neural-networks are best viewed as nonlinear compositions of weighted features [crestani , 1993 ; crestani , 1994 ; gallant , 1991 ; kwok , 1995 ; wong et al , 1993] . boolean formulas can be formed from sentences using simple conjunctive or disjunctive combinations . our focus here will be on bayesian-networks , which attempt to represent probabilistic relationships among the features . in any of these cases , machine-learning-techniques must be sensitive to their inductive-bias . that is , given a fixed amount of data , we must adaptive-information-retrieval 263 decision-trees inductive small bias : trees linear-combinations neural-networks few , small smooth weights mappings figure 7.3 inductive-bias boolean formulas small con/disjuncts bayesian-networks sparse , acyclic have some a priori preference for some kinds of hypotheses over others . for example , decision-tree-learning algorithms [quinlan , 1993] prefer small trees and neural-networks prefer smooth mappings [mitchell , 1997] . a common feature of all of these learning-algorithms is a general preference for parsimony , or simplicity . this preference is typically attributed first to william of occam (ca. 1330) . occam 's razor has been used since then to cleave simpler hypotheses from more complex ones . another motivation for the parsimony bias has been realized more recently within machine-learning : simple hypotheses are more likely to accurately go beyond the data used to train them to classify other unseen data . that is , very complicated hypotheses have a tendency to over-fit to the training-data given a learning-algorithm , but the good fit they can accomplish on this set is not matched when the same classification is done on new data . the issues involved in evaluating a classifier 's performance are an important topic within machine-learning [mitchell , 1997] .
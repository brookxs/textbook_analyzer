6.3.4 similarity models in this section we define notions of syntactic similarity between strings or documents . similarity is measured by a distance-function . for example , if we have strings of the same length , we can define the distance between them as the number of positions that have different characters . then , the distance is 0 if they are equal . this is called the hamming-distance . a distance-function should also be symmetric (that is , the order of the arguments does not matter) and should satisfy the triangle-inequality (that is , distance (a , c) lt ; distance (a , b) 4 - distance (b , c)) . an important distance over strings is the edit or levenshtein-distance mentioned earlier . the edit-distance is defined as the minimum number of characters , insertions , deletions , and substitutions that we need to perform in any of the strings to make them equal . for instance , the edit-distance between ` color1 and ` colour1 is one , while the edit-distance between ` survey1 arid ` surgery ' is two . the edit-distance is considered to be superior for modeling syntactic errors than other more complex methods such as the soundex system , which is based on phonetics [595] . extensions to the concept of edit-distance include different weights for each operation , adding transpositions , etc. . there are other measures . for example , assume that we are comparing two given strings and the only operation allowed is deletion of characters . then , after all non-common characters have been deleted , the remaining sequence of characters (not necessarily contiguous in the original string , but in the same order) is the longest-common-subsequence (lcs) of both strings . for example , the lcs of ` survey ' and `` surgery ' is % surey . ' markup-languages 149 similarity can be extended to documents . for example , we can consider lines as single symbols and compute the longest common sequence of lines between two files . this is the measure used by the dif f command in unix-like operating-systems . the main problem with this approach is that it is very time consuming and does not consider lines that are similar . the latter drawback can be fixed by taking a weighted-edit-distance between lines or by computing the lcs over all the characters . other solutions include extracting fingerprints (any piece of text that in some sense characterizes it) for the documents and comparing them , or finding large repeated pieces . there are also visual-tools to see document-similarity . for example , dotplot draws a rectangular map where both coordinates are file lines and the entry for each coordinate is a gray pixel that depends on the edit-distance between the associated lines .
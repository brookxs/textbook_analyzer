the cooper model - expected search length in 1968 , cooper [20] stated : ` the primary function of a retrieval-system is conceived to be that of saving its users to as great an extent as is possible , the labour of perusing and discarding irrelevant documents , in their search for relevant ones ' . it is this ` saving ' which is measured and is claimed to be the single index of merit for retrieval-systems . in general the index is applicable to retrieval-systems with ordered (or ranked) output . it roughly measures the search effort which one would expect to save by using the retrieval-system as opposed to searching the collection at random . an attempt is made to take into account the varying difficulty of finding relevant documents for different queries . the index is calculated for a query of a precisely specified type . it is assumed that users are able to quantify their information-need according to one of the following types : (a) only one relevant-document is wanted ; (b) some arbitrary number n is wanted ; (c) all relevant documents are wanted ; (4) a given proportion of the relevant documents is wanted , etc. . thus , the index is a measure of performance for a query of given type . here we shall restrict ourselves to type-2 queries . for further details the reader is referred to cooper [20] . the output of a search-strategy is assumed to be a weak-ordering of documents . i have defined this concept on page 118 in a different context . we start by first considering a special case , namely a simple ordering , which is a weak-ordering such that for any two distinct-elements e1 and e2 it is never the case that e1 r e2 and e2 r e1 (where r is the order relation) . this simply means that all the documents in the output are ordered linearly with no two or more documents at the same level of the ordering . the search length is now defined as the number of non-relevant documents a user must scan before his information-need (in terms of the type quantification above) is satisfied . for example , consider a ranking of 20 documents in which the relevant ones are distributed as in figure 7.7 . a type-2 query with n = 2 would have search length 2 , with n = 6 it would have search length 3 . unfortunately the ranking generated by a matching-function is rarely a simple ordering , but more commonly a weak-ordering . this means that at any given level in the ranking , there is at l ; east one document (probably more) which makes the search length inappropriate since the order of documents within a level is random . if the information-need is met at a certain level in the ordering then depending on the arrangement of the relevant documents within that level we shall get different search lengths . nevertheless we can use an analogous quantity which is the expected search length . for this we need to calculate the probability of each possible search length by juggling (mentally) the relevant and non-relevant documents in the level at which the user-need is met . for example , consider the weak-ordering in figure 7.8 . if the query is of type-2 with n = 6 then the need is met at level-3 . the possible search lengths are 3 , 4 , 5 or 6 depending on how many non-relevant documents precede the sixth relevant-document . we can ignore the possible arrangements within levels 1 and 2 ; their contributions are always the same . to compute the expected search length we need the probability of each possible search length . we get at this by considering first the number of different ways in which two relevant documents could be distributed among five , it is ([5] 2) = 10 . of these 4 would result in a search length of 3 , 3 in a search length of 4 , 2 in a search length of 5 and 1 in a search length of 6 . their corresponding probabilities are therefore , 4/10 , 3/10 , 2/10 and 1/10 . the expected search length is now : (4/10) . 3 + (3/10) . 4 + (2/10) . 5 + (1/10) . 6 = 4 the above procedure leads immediately to a convenient ` intuitive ' derivation of a formula for the expected search length . it seems plausible that the average results of many random searches through the final level (level at which need is met) will be the same as for a single search with the relevant documents spaced ` evenly ' throughout that level . first we enumerate the variables : (a) q is the query of given type ; (b) j is the total number of documents non-relevant to q in all levels preceding the final ; (c) r is the number of relevant documents in the final level ; (d) i is the number of non-relevant documents in the final level ; (e) s is the number of relevant documents required from the final level to satisfy the need according its type . now , to distribute the r relevant documents evenly among the non-relevant documents , we partition the non-relevant documents into r + 1 subsets each containing i / (r + 1) documents . the expected search length is now : as a measure of effectiveness esl is sufficient if the document collection and test queries are fixed . in that case the overall measure is the mean expected search length where q is the set of queries . this statistic is chosen in preference to any other for the property that it is minimised when the total expected search length to extend the applicability of the measure to deal with varying test queries and document-collections , we need to normalise the esl in some way to counter the bias introduced because : (1) queries are satisfied by different numbers of documents according to the type of the query and therefore can be expected to have widely differing search lengths ; (2) the density of relevant documents for a query in one document-collection may be significantly different from the density in another . the first item suggests that the esl per desired relevant-document is really what is wanted as an index of merit . the second suggests normalising the esl by a factor proportional to the expected number of non-relevant documents collected for each relevant one . luckily it turns out that the correction for variation in test queries and for variation in document-collection can be made by comparing the esl with the expected random-search length (ersl) . this latter quantity can be arrived at by calculating the expected search length when the entire document-collection is retrieved at one level . the final measure is therefore : which has been called the expected search length reduction factor by cooper . roughly it measures improvement over random retrieval . the explicit form for ersl is given by : where (1) r is the total number of documents in the collection relevant to q ; (2) i is the total number of documents in the collection non-relevant to q ; (3) s is the total desired number of documents relevant to q . the explicit form for esl was given before . finally , the overall measure for a set of queries q is defined , consistent with the mean esl , to be which is known as the mean expected search length reduction factor . within the framework as stated at the head of this section this final measure meets the bill admirably . however , its acceptability as a measure of effectiveness is still debatable (see , for example , senko [21]) . it totally ignores the recall aspect of retrieval , unless queries are evaluated which express the need for a certain proportion of the relevant documents in the system . it therefore seems to be a good substitute for precision , one which takes into account order of retrieval and user-need . for a further defence of its subjective nature see cooper [1] . a spirited attack on cooper 's position can be found in soergel [22] .
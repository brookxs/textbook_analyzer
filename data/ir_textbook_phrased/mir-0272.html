13.9 trends and research-issues the-future of the-web might surprise us , considering that its massive use started less than five years ago . there are many distinct trends and each one opens up new and particular research problems . what follows is a compilation of the major trends as we have perceived them . ï modeling : special ir-models tailored for the-web are needed [308 , 155 , 652] . as we have seen , web user queries are different . we also have the pull/push dichotomy : will we search for information or will the information reach us ? in both cases we need better search paradigms and better information-filtering [782] . ï querying : further work on combining structure-and-content in the queries is needed as well as new visual-metaphors to pose those queries and visualize the answers [44] . future query-languages may include concept-based-search and natural-language-processing , as well as searching by example (this implies document-clustering and categorization on the web [810,120 , 157]) . ï distributed-architectures : new distributed schemes to traverse and search the-web must be devised to cope with its growth . this will have an impact on current crawling and indexing-techniques , as well as caching 394 searching the-web techniques for the-web . which will be the bottleneck in the future ? server capacity or network bandwidth ? æ ranking : better ranking schemes are needed , exploiting both content-and-structure (internal to a page and hyperlinks) ; in particular , combining and comparing query-dependent and independent techniques . one problem related to advertisements is that search-engines may rank some pages higher due to reasons that are not based on the real relevance of a page (this is called the search-engine persuasion problem in [543]) . 9 indexing : which is the best logical view for the text ? what should be indexed ? how to exploit better text-compression schemes to achieve fast searching and get lower network-traffic ? how to compress efficiently word-lists , url tables , etc. and update them without significant run-time-penalty ? many implementation details must be improved . ï dynamic-pages : a large number of web-pages are created on-demand and current techniques are not able to search on those dynamic-pages . this is called the hidden-web . æ duplicated data : better mechanisms to detect and eliminate repeated web-pages (or pages that are syntactically very similar) are needed . initial approaches are based on resemblance measures using document fingerprints [121 , 120] . this is related to an important problem in databases : finding similar objects . ï multimedia : searching for non-textual objects will gain importance in the near future . there are already some research results in the literature [579 , 80 , 136] . ï user-interfaces : better user-interfaces are clearly needed . the output should also be improved , for example allowing better extraction of the main-content of a page or the formulation of content-based-queries [766] . ï browsing : more tools will appear , exploiting links , popularity of web-pages , content-similarity , collaboration , 3d , and virtual-reality [384 , 638 , 385 , 421] . an important trend would be to unify further searching with browsing . an important issue to be settled in the future is a standard protocol to query-search engines . one proposal for such a protocol is starts [316] , which could allow us to choose the best sources for querying , evaluate the query at these sources , and merge the query results . this protocol would make it easier to build metasearchers , but at the same time that is one of the reasons for not having a standard . in that way , metasearchers can not profit from the work done by search-engines and web-directories . this is a particular case of the federated-searching problem from heterogeneous-sources as it is called in the database community [656] . this is a problem already studied in the case of the-web , including discovery and ranking of sources [161 , 845 , 319] . these issues are also very important for digital-libraries [649] (see also chapter 15) and visualization issues [15] . a related topic is metadata-standards for the-web (see chapter 6) bibliographic discussion 395 and their limitations [544] . xml helps [436 , 213 , 306] , but semantic-integration is still needed . hyperlinks can also be used to infer information about the-web . although this is not exactly searching the-web , this is an important trend called web-mining . traditionally , web-mining had been focused on text-mining , that is , extracting information from web-pages . however , the hyperlink-structure can be exploited to obtain useful information . for example , the parasite system [736] uses hyperlink information to find pages that have moved , related-pages , and personal-web pages . hits , already mentioned in section 13.4.4 , has also been used to find communities and similar pages [444 , 298] . other results on exploiting-hyperlink-structure can be found in [639 , 543 , 154] . farther improvements in this problem include web-document clustering [810 , 120 , 162] (already mentioned) , connectivity services (for example , asking which web-pages point to a given page [92]) , automatic-link-generation [320] , extracting information [100 , 115] , etc. . another trend is intranet applications . many companies do not want their private networks to be public . however , for business reasons they want to allow web users to search inside their intranets obtaining partial-information . this idea leads to the concept of portals for which there are already several commercial products . new models to see web-sites as databases and/or information-systems are also important .
evaluation of text-classification historically , the classic reuters-21578 collection was the main benchmark for text-classification evaluation . this is a collection of 21,578 newswire articles , originally collected and labeled by carnegie group , inc. and reuters , ltd. in the course of developing the construe text-classification system . it is much smaller than and predates the reuters-rcv1 collection discussed in chapter 4 (page 4.2) . the articles are assigned classes from a set of 118 topic categories . a document may be assigned several classes or none , but the commonest case is single-assignment (documents with at least one class received an average of 1.24 classes) . the standard approach to this any-of problem (chapter 14 , page 14.5) is to learn 118 two-class classifiers , one for each class , where the two-class classifier for class is the classifier for the two classes and its complement . table 13.7 : the ten largest classes in the reuters-21578 collection with number of documents in training and test sets . class # train # test class # train # test earn 2877 1087 trade 369 119 acquisitions 1650 179 interest 347 131 money-fx 538 179 ship 197 89 grain 433 149 wheat 212 71 crude 389 189 corn 182 56 for each of these classifiers , we can measure recall , precision , and accuracy . in recent work , people almost invariably use the modapte split , which includes only documents that were viewed and assessed by a human indexer , and comprises 9,603 training-documents and 3,299 test documents . the distribution of documents in classes is very uneven , and some work evaluates systems on only documents in the ten largest classes . they are listed in table 13.7 . a typical document with topics is shown in figure 13.9 . in section 13.1 , we stated as our goal in text-classification the minimization of classification-error on test-data . classification-error is 1.0 minus classification-accuracy , the proportion of correct decisions , a measure we introduced in section 8.3 (page 8.3) . this measure is appropriate if the percentage of documents in the class is high , perhaps 10 % to 20 % and higher . but as we discussed in section 8.3 , accuracy is not a good measure for `` small '' classes because always saying no , a strategy that defeats the purpose of building a classifier , will achieve high accuracy . the always-no classifier is 99 % accurate for a class with relative-frequency 1 % . for small classes , precision , recall and are better measures . we will use effectiveness as a generic term for measures that evaluate the quality of classification decisions , including precision , recall , , and accuracy . performance refers to the computational-efficiency of classification and ir systems in this book . however , many researchers mean effectiveness , not efficiency of text-classification when they use the term performance . figure 13.9 : a sample document from the reuters-21578 collection . when we process a collection with several two-class classifiers (such as reuters-21578 with its 118 classes) , we often want to compute a single aggregate measure that combines the measures for individual classifiers . there are two methods for doing this . macroaveraging computes a simple average over classes . microaveraging pools per-document decisions across classes , and then computes an effectiveness measure on the pooled contingency-table . table 13.8 gives an example . the differences between the two methods can be large . macroaveraging gives equal weight to each class , whereas microaveraging gives equal weight to each per-document classification decision . because the measure ignores true negatives and its magnitude is mostly determined by the number of true positives , large-classes dominate small classes in microaveraging . in the example , microaveraged precision (0.83) is much closer to the precision of (0.9) than to the precision of (0.5) because is five times larger than . microaveraged results are therefore really a measure of effectiveness on the large-classes in a test-collection . to get a sense of effectiveness on small classes , you should compute macroaveraged results . table 13.8 : macro - and microaveraging . `` truth '' is the true class and `` call '' the decision of the classifier . in this example , macroaveraged precision is . microaveraged precision is . class 1 truth : truth : yes no call : yes 10 10 call : no 10 970 class 2 truth : truth : yes no call : yes 90 10 call : no 10 890 pooled table truth : truth : yes no call : yes 100 20 call : no 20 1860 table 13.9 : text-classification effectiveness numbers on reuters-21578 for f (in percent) . results from li and yang (2003) (a) , joachims (1998) (b : knn) and dumais et al. (1998) (b : nb , rocchio , trees , svm) . (a) nb rocchio knn svm micro-avg-l (90 classes) 80 85 86 89 macro-avg (90 classes) 47 59 60 60 (b) nb rocchio knn trees svm earn 96 93 97 98 98 acq 88 65 92 90 94 money-fx 57 47 78 66 75 grain 79 68 82 85 95 crude 80 70 86 85 89 trade 64 65 77 73 76 interest 65 63 74 67 78 ship 85 49 79 74 86 wheat 70 69 77 93 92 corn 65 48 78 92 90 micro-avg (top 10) 82 65 82 88 92 micro-avg-d (118 classes) 75 62 n/a n/a 87 in one-of classification (more-than-two-classes) , microaveraged is the same as accuracy (exercise 13.6) . table 13.9 gives microaveraged and macroaveraged effectiveness of naive-bayes for the modapte split of reuters-21578 . to give a sense of the relative effectiveness of nb , we compare it with linear svms (rightmost column ; see chapter 15) , one of the most effective classifiers , but also one that is more expensive to train than nb . nb has a microaveraged of 80 % , which is 9 % less than the svm (89 %) , a 10 % relative decrease (row `` micro-avg-l (90 classes) '') . so there is a surprisingly small effectiveness penalty for its simplicity and efficiency . however , on small classes , some of which only have on the order of ten positive examples in the training-set , nb does much worse . its macroaveraged is 13 % below the svm , a 22 % relative decrease (row `` macro-avg (90 classes) '') . the table also compares nb with the other classifiers we cover in this book : rocchio and knn . in addition , we give numbers for decision-trees , an important classification-method we do not cover . the bottom part of the table shows that there is considerable variation from class to class . for instance , nb beats knn on ship , but is much worse on money-fx . comparing parts (a) and (b) of the table , one is struck by the degree to which the cited papers ' results differ . this is partly due to the fact that the numbers in (b) are break-even scores (cf. page 8.4) averaged over 118 classes , whereas the numbers in (a) are true scores (computed without any knowledge of the test-set) averaged over ninety classes . this is unfortunately typical of what happens when comparing different results in text-classification : there are often differences in the experimental setup or the evaluation that complicate the interpretation of the results . these and other results have shown that the average effectiveness of nb is uncompetitive with classifiers like svms when trained and tested on independent and identically distributed (i.i.d.) data , that is , uniform data with all the good properties of statistical-sampling . however , these differences may often be invisible or even reverse themselves when working in the real-world where , usually , the training sample is drawn from a subset of the data to which the classifier will be applied , the nature of the data drifts over time rather than being stationary (the problem of concept-drift we mentioned on page 13.4) , and there may well be errors in the data (among other problems) . many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than nb . our conclusion from the results in table 13.9 is that , although most researchers believe that an svm is better than knn and knn better than nb , the ranking of classifiers ultimately depends on the class , the document collection , and the experimental setup . in text-classification , there is always more to know than simply which machine-learning-algorithm was used , as we further discuss in section 15.3 (page) . when performing evaluations like the one in table 13.9 , it is important to maintain a strict separation between the training-set and the test-set . we can easily make correct classification decisions on the test-set by using information we have gleaned from the test-set , such as the fact that a particular term is a good predictor in the test-set (even though this is not the case in the training-set) . a more subtle example of using knowledge about the test-set is to try a large number of values of a parameter (e.g. , the number of selected features) and select the value that is best for the test-set . as a rule , accuracy on new data - the type of data we will encounter when we use the classifier in an application - will be much lower than accuracy on a test-set that the classifier has been tuned for . we discussed the same problem in ad-hoc-retrieval in section 8.1 (page 8.1) . in a clean statistical text classification experiment , you should never run any program on or even look at the test-set while developing a text-classification system . instead , set aside a development-set for testing while you develop your method . when such a set serves the primary purpose of finding a good value for a parameter , for example , the number of selected features , then it is also called held-out data . train the classifier on the rest of the training-set with different parameter-values , and then select the value that gives best results on the held-out part of the training-set . ideally , at the very end , when all parameters have been set and the method is fully specified , you run one final experiment on the test-set and publish the results . because no information about the test-set was used in developing the classifier , the results of this experiment should be indicative of actual performance in practice . this ideal often can not be met ; researchers tend to evaluate several systems on the same test-set over a period of several years . but it is nevertheless highly important to not look at the test-data and to run systems on it as sparingly as possible . beginners often violate this rule , and their results lose validity because they have implicitly tuned their system to the test-data simply by running many variant systems and keeping the tweaks to the system that worked best on the test-set . exercises . assume a situation where every document in the test-collection has been assigned exactly one class , and that a classifier also assigns exactly one class to each document . this setup is called one-of classification more-than-two-classes . show that in one-of classification (i) the total number of false-positive decisions equals the total number of false-negative decisions and (ii) microaveraged and accuracy are identical . the class priors in figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class . why ? the function applymultinomialnb in figure 13.2 has time-complexity . how would you modify the function so that its time-complexity is ? table 13.10 : data for parameter-estimation exercise . docid words in document in china ? training-set 1 taipei taiwan yes 2 macao taiwan shanghai yes 3 japan sapporo no 4 sapporo osaka taiwan no test-set 5 taiwan taiwan sapporo ? based on the data in table 13.10 , (i) estimate a multinomial naive-bayes-classifier , (ii) apply the classifier to the test document , (iii) estimate a bernoulli nb classifier , (iv) apply the classifier to the test document . you need not estimate parameters that you do n't need for classifying the test document . your task is to classify words as english or not english . words are generated by a source with the following distribution : event word english ? probability 1 ozb no 4/9 2 uzu no 4/9 3 zoo yes 1/18 4 bun yes 1/18 (i) compute the parameters (priors and conditionals) of a multinomial nb classifier that uses the letters b , n , o , u , and z as features . assume a training-set that reflects the probability distribution of the source perfectly . make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text-classification . compute parameters using smoothing , in which computed-zero probabilities are smoothed into probability 0.01 , and computed-nonzero probabilities are untouched . (this simplistic smoothing may cause . solutions are not required to correct this .) (ii) how does the classifier classify the word zoo ? (iii) classify the word zoo using a multinomial classifier as in part (i) , but do not make the assumption of positional independence . that is , estimate separate parameters for each position in a word . you only need to compute the parameters you need for classifying zoo . what are the values of and if term and class are completely independent ? what are the values if they are completely dependent ? the feature-selection-method in equation 130 is most appropriate for the bernoulli model . why ? how could one modify it for the multinomial-model ? features can also be selected according to information-gain (ig) , which is defined as : (138) where is entropy , is the training-set , and , and are the subset of with term , and the subset of without term , respectively . is the class-distribution in (sub) collection , e.g. , if a quarter of the documents in are in class . show that mutual-information and information gain are equivalent . show that the two formulas (and 137) are equivalent . in the example on page 13.5.2 we have . show that this holds in general . and mutual-information do not distinguish between positively and negatively correlated features . because most good text-classification features are positively correlated (i.e. , they occur more often in than in) , one may want to explicitly rule out the selection of negative indicators . how would you do this ?
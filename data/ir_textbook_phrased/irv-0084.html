the robertson model - the logistic transformation robertson in collaboration with teather has developed a model for estimating the probabilities corresponding to recall and fallout [17] . the estimation-procedure is unusual in that in making an estimate of these probabilities for a single query it takes account of two things : one , the amount of data used to arrive at the estimates , and two , the averages of the estimates over all queries . the effect of this is to ` pull ' an estimate closer to the overall mean if it seems to be an outlyer whilst at the same time counterbalancing the ` pull ' in proportion to the amount of data used to make the estimate in the first place . there is now some evidence to show that this pulling-in-to-the-mean is statistically a reasonable thing to do [18] . using the logit transformation for probabilities , that is the basic quantitative-model for a single query j they propose is logit [[theta]] j1 = [[alpha]] j + [[delta]] j logit [[theta]] j2 = [[alpha]] j - [[delta]] j here [[theta]] j1 and [[theta]] j2 are probabilities corresponding to recall and fallout respectively as defined in the previous section . the parameters [[alpha]] j and [[delta]] j are to be interpreted as follows : [[alpha]] j measures the specificity of the query-formulation ; [[delta]] j measures the separation of relevant and non-relevant documents . for a given query j if the query i has been formulated in a more specific way than j , one would expect the recall and fallout to decrease , i.e. [[theta]] i1 lt ; [[theta]] j1 and [[theta]] i2 lt ; [[theta]] j2 also , if for query i the system is better at separating the non-relevant from the relevant documents than it is for query j one would expect the recall to increase and the fallout to decrease , i.e. [[theta]] i1 gt ; [[theta]] j1 and [[theta]] i2 lt ; [[theta]] j2 given that logit is a monotonic-transformation , these interpretations are consistent with the simple quantitative-model defined above . to arrive at an estimation-procedure for [[alpha]] j and [[delta]] j is a difficult technical problem and the interested reader should consult robertson 's thesis [19] . it requires certain assumptions to be made about [[alpha]] j and [[delta]] j , the most important of which is that the {[[alpha]] j} and {[[delta]] j} are independent and normally distributed . these assumptions are rather difficult to validate . the only evidence produced so far derives the distribution of {[[alpha]] j} for certain test-data . unfortunately , these estimates , although they are unimodally and symmetrically distributed themselves , can only be arrived at by using the normality assumption . in the case of [[delta]] j it has been found that it is approximately constant across queries so that a common - [[delta]] model is not unreasonable : logit [[theta]] j1 = [[alpha]] j1 + [[delta]] logit [[theta]] j2 = [[alpha]] j2 - [[delta]] from them it would appear that [[delta]] could be a candidate for a single number measure of effectiveness . however , robertson has gone to some pains to warn against this . his main argument is that these parameters are related to the behavioural characteristics of an ir-system so that if we were to adopt [[delta]] as a measure of effectiveness we could be throwing away vital information needed to make an extrapolation to the performance of other systems .
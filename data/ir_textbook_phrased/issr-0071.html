5.2.1 probabilistic weighting the probabilistic-approach is based upon direct application of the theory of probability to information-retrieval-systems . this has the advantage of being able to use the developed formal-theory of probability to direct the algorithmic development . it also leads to an invariant result that facilitates integration of results from different databases . the use of probability-theory is a natural choice because it is the basis of evidential-reasoning (i.e. , drawing conclusions from evidence) . this is summarized by the-probability-ranking-principle (prp) and its plausible corollary (cooper-94) : automatic-indexing 109 hypothesis : if a reference retrieval-system 's response to each request is a ranking of the documents in the collection in order of decreasing probability of usefulness to the user who submitted the request , where the probabilities are estimated as accurately as possible on the basis of whatever data is available for this purpose , then the overall effectiveness of the system to its users is the best obtainable on the basis of that data . plausible corollary : the most promising source of techniques for estimating the probabilities of usefulness for output ranking in ir is standard probability-theory and statistics . there are several factors that make this hypothesis and its corollary difficult (gordon-92 , gordon-91 , robertson-77) . probabilities are usually based upon a binary condition ; an item is relevant or not . but in information-systems the relevance of an item is a continuous function from non-relevant to absolutely useful . a more complex theory of expected-utility (cooper-78) is needed to address this characteristic . additionally , the output ordering by rank of items based upon probabilities , even if accurately calculated , may not be as optimal as that defined by some domain-specific heuristic (stirling-77) . the domains in which probabilistic-ranking are suboptimal are so narrowly focused as to make this a minor issue . but these issues mentioned are not as compelling as the benefit of a good probability-value for ranking that would allow integration of results from multiple sources . the source of the problems that arise in application of probability-theory come from a lack of accurate data and simplifying assumptions that are applied to the mathematical-model . if nothing else , these simplifying assumptions cause the results of probabilistic-approaches in ranking items to be less accurate than other approaches . the advantage of the probabilistic-approach is that it can accurately identify its weak assumptions and work to strengthen them . in many other approaches , the underlying weaknesses in assumptions are less obvious and harder to identify and correct . even with the simplifying assumption , results from comparisons of approaches in the trec conferences have shown that the probabilistic-approaches , while not scoring highest , are competitive against all other approaches . there are many different areas in which the probabilistic-approach may be applied . the method of logistic-regression is described as an example of how a probabilistic-approach is applied to information-retrieval (gey-94) . the approach starts by defining a `` model 0 '' system which exists before specific probabilistic-models are applied . in a retrieval-system there exist query terms q , and document terms djª which have a set of attributes (vi , ... , vn) from the query (e.g. , counts of term-frequency in the query) , from the document (e.g. , counts of term-frequency in the document) and from the database (e.g. , total number of documents in the database divided by the number of documents indexed by the term) . the logistic reference-model uses a random-sample of query-documcutterm triples for which binary relevance-judgments have been made from a training 110 chapters sample . log 0 is the logarithm of the odds (logodds) of relevance for term tk which is present in document dj and query q , : log (0 (r | qi , dj , t0) = co + c , v , + ... + cnvn the logarithm that the ith query is relevant to the jth document is the sum of the logodds for all terms : log (0 (r | q , , dj)) = j tlog (∞ (r i q ^ djgt ; w) - log (o (r))] k =\ where o (r) is the odds that a document chosen at random from the database is relevant to query qx . the coefficients c are derived using logistic-regression which fits an equation to predict a dichotomous independent variable as a function of independent variables that show statistical variation (hosmer-89) . the inverse logistic transformation is applied to obtain the probability of relevance of a document to a query : p (r | qisdj) = i \ (i + e-log (o (riqi'di))) the coefficients of the equation for logodds is derived for a particular database using a random-sample of query-document-term-relevance quadruples and used to predict odds of relevance for other query-document pairs . gey applied this methodology to the cranfield collection (gey-94) . the collection has 1400 items and 225 queries with known results . additional attributes of relative-frequency in the query (qrf) , relative-frequency in the document (drf) and relative-frequency of the term in all the documents (rfad) were included , producing the following logodds formula : z} , log (o (r | tj)) = cq + cilog (qaf) + c2log (qrf) + c3log (daf) + c4log (drf) + c5iog (idf) + c6log (rfad) where qaf , daf , and idf were previously defined , qrf = qaf \ (total number of terms in the query) , drf = daf \ (total number of words in the document) and rfad = (total number of term occurrences in the c!atabase) \ (total number of all words in the database) . logs are used to reduce the impact of frequency information ; then smooth out skewed distributions . a higher maximum-likelihood is attained for logged attributes . the coefficients and log (o (r)) were calculated creating the final formula for ranking for query-vector q , which contains q terms : automatic-indexing 111 log (o (r | 0)) = -5.138 + 2 , (zj + 5.138) k =\ the logistic inference method was applied to the test database along with the cornell smart vector system which uses traditional term-frequency , inverse-document-frequency and cosine relevance-weighting formulas (see section 5.2.2) . the logistic inference method outperformed the vector method . thus the index that supports the calculations for the logistic reference-model contains the o (r) constant value (e.g. , -5.138) along with the coefficients c0 through c6 . additionally , it needs to maintain the data to support daf , drf , idf and rfad . the values for qaf and qrf are derived from the query . attempts have been made to combine the results of different probabilistic techniques to get a more accurate value . the objective is to have the strong points of different techniques compensate for weaknesses . to date this combination of probabilities using averages of log-odds has not produced better results and in many cases produced worse results (hull-96) .
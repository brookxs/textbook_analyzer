optimality of hac to state the optimality conditions of hierarchical-clustering precisely , we first define the combination similarity comb-sim of a clustering as the smallest combination similarity of any of its clusters : (210) 17.1 we then define to be optimal if all clusterings with clusters , , have lower combination similarities : (211) figure 17.12 shows that centroid-clustering is not optimal . the clustering (for) has combination similarity and (for) has combination similarity -3.46 . so the clustering produced in the first merge is not optimal since there is a clustering with fewer clusters (-rrb- that has higher combination similarity . centroid-clustering is not optimal because inversions can occur . the above definition of optimality would be of limited use if it was only applicable to a clustering together with its merge history . however , we can show (exercise 17.5) that for the three non-inversion algorithms can be read off from the cluster without knowing its history . these direct definitions of combination similarity are as follows . single-link the combination similarity of a cluster is the smallest similarity of any bipartition of the cluster , where the similarity of a bipartition is the largest similarity between any two documents from the two parts : (212) where each is a bipartition of . complete-link the combination similarity of a cluster is the smallest similarity of any two points in : . gaac the combination similarity of a cluster is the average of all pairwise similarities in (where self-similarities are not included in the average) : equation 205 . we can now prove the optimality of single-link-clustering by induction over the number of clusters . we will give a proof for the case where no two pairs of documents have the same similarity , but it can easily be extended to the case with ties . the inductive basis of the proof is that a clustering with clusters has combination similarity 1.0 , which is the largest value possible . the induction-hypothesis is that a single-link-clustering with clusters is optimal : for all . assume for contradiction that the clustering we obtain by merging the two most similar clusters in is not optimal and that instead a different sequence of merges leads to the optimal clustering with clusters . we can write the assumption that is optimal and that is not as . case 1 : the two documents linked by are in the same cluster in . they can only be in the same cluster if a merge with similarity smaller than has occurred in the merge sequence producing . this implies . thus , . contradiction . case 2 : the two documents linked by are not in the same cluster in . but , so the single-link merging rule should have merged these two clusters when processing . contradiction . thus , is optimal . in contrast to single-link-clustering , complete-link-clustering and gaac are not optimal as this example shows : both algorithms merge the two points with distance 1 (and) first and thus can not find the two-cluster clustering . but is optimal on the optimality criteria of complete-link-clustering and gaac . however , the merge criteria of complete-link-clustering and gaac approximate the desideratum of approximate sphericity better than the merge criterion of single-link-clustering . in many applications , we want spherical clusters . thus , even though single-link-clustering may seem preferable at first because of its optimality , it is optimal with respect to the wrong criterion in many document-clustering applications . table 17.1 : comparison of hac algorithms . method-combination similarity time compl . optimal ? comment single-link max inter-similarity of any 2 docs yes chaining-effect complete-link min inter-similarity of any 2 docs no sensitive to outliers group-average average of all sims no best choice for most applications centroid average inter-similarity no inversions can occur table 17.1 summarizes the properties of the four hac algorithms introduced in this chapter . we recommend gaac for document-clustering because it is generally the method that produces the clustering with the best properties for applications . it does not suffer from chaining , from sensitivity to outliers and from inversions . there are two exceptions to this recommendation . first , for non-vector representations , gaac is not applicable and clustering should typically be performed with the complete-link method . second , in some applications the purpose of clustering is not to create a complete hierarchy or exhaustive partition of the entire document-set . for instance , first-story-detection or novelty-detection is the task of detecting the first occurrence of an event in a stream of news stories . one approach to this task is to find a tight cluster within the documents that were sent across the wire in a short period of time and are dissimilar from all previous documents . for example , the documents sent over the wire in the minutes after the world trade center attack on september 11 , 2001 form such a cluster . variations of single-link-clustering can do well on this task since it is the structure of small parts of the vector-space - and not global structure - that is important in this case . similarly , we will describe an approach to duplicate-detection on the web in section 19.6 (page 19.6) where single-link-clustering is used in the guise of the union-find algorithm . again , the decision whether a group of documents are duplicates of each other is not influenced by documents that are located far away and single-link-clustering is a good choice for duplicate-detection . exercises . show the equivalence of the two definitions of combination similarity : the process-definition on page 17.1 and the static definition on page 17.5 .
6.3.2 information-theory written text has a certain semantics and is a way to communicate information . although it is difficult to formally capture how much information is there in a given text , the distribution of symbols is related to it . for example , a text where one symbol appears almost all the time does not convey much information . information-theory defines a special concept , entropy , to capture information-content (or equivalently , information uncertainty) . if the alphabet has a symbols , each one appearing with probability pz (probability here is defined as the symbol frequency over the total number of symbols) in a text , the entropy of this text is defined as e = - ' 2 = 1 in this formula the a symbols of the alphabet are coded in binary , so the entropy is measured in bits . as an example , for a = 2 , the entropy is 1 if both symbols appear the same number of times or 0 if only one symbol appears . we say that the amount of information in a text can be quantified by its entropy . the definition of entropy depends on the probabilities (frequencies) of each symbol . to obtain those probabilities we need a text model . so we say that the amount of information in a text is measured with regard to the text model . this concept is also important , for example , in text-compression , where the entropy is a limit on how much the text can be compressed , depending on the text model . in our case we are interested in natural-language , as we now discuss .
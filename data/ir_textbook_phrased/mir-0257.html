13.4.4 ranking most search-engines use variations of the boolean or vector-model (see chapter 2) to do ranking . as with searching , ranking has to be performed without accessing the text , just the index . there is not much public-information about the specific ranking-algorithms used by current search-engines . further , it is difficult to compare fairly different search-engines given their differences , and continuous improvements . more important , it is almost impossible to measure recall , as the number of relevant pages can be quite large for simple queries . some inconclusive studies include [327 , 498] . yuwono and lee [844] propose three ranking-algorithms in addition to the classical tf ~ idf scheme (see chapter 2) . they are called boolean spread , vector spread , and most-cited . the first two are the normal ranking-algorithms of the boolean and vector-model extended to include pages pointed to by a page in the answer or pages that point to a page in the answer . the third , most-cited , is based only on the terms included in pages having a link to the pages in the answer . a comparison of these techniques considering 56 queries over a collection of 2400 web-pages indicates that the vector-model yields a better recall-precision-curve , with an average-precision of 75 % . some of the newr ranking-algorithms also use hyperlink information . this is an important difference between the-web and normal ir databases . the number of hyperlinks that point to a page provides a measure of its popularity and-quality . also , many links in common between pages or pages referenced by the same page often indicates a relationship between those pages . we now present three examples of ranking-techniques that exploit these facts , but they differ in that two of them depend on the query and the last does not . the first is webquery [148] , which also allows visual-browsing of web-pages . webquery takes a set of web-pages (for example , the answer to a query) and ranks them based on how connected each web-page is . additionally , it extends the set by finding web-pages that are highly connected to the original set . a related approach is presented by li [512] , a better idea is due to kleinberg [444] and used in hits (hypertext induced topic search) . this ranking-scheme depends on the query and considers the set of pages 5 that point to or are pointed by pages in the answer . pages that have many links pointing to them in s are called authorities (that is . they should have relevant content) . pages that have many outgoing links are called hubs (they should point to similar-content) . a positive two-way feedback exists : search-engines 381 better authority pages come from incoming edges from good hubs and better hub pages come from outgoing edges to good authorities . let h (p) and a (p) be the hub and authority value of page p . these values are defined such that the following equations are satisfied for all pages p : ues i pó + u v # s | vógt ;p where h (p) and a (p) for all pages are normalized (in the original paper , the sum of the squares of each measure is set to one) . these values can be determined through an iterative-algorithm , and they converge to the principal-eigenvector of the link matrix of s . in the case of the-web , to avoid an explosion of the size of 5 , a maximal number of pages pointing to the answer can be defined . this technique does not work with non-existent , repeated , or automatically generated links . one solution is to weight each link based on the surrounding content . a second problem is that the topic of the result can become diffused . for example , a particular query is enlarged by a more general topic that contains the original answer . one solution to this problem is to analyze the content of each page and assign a score to it , as in traditional ir ranking . the link weight and the page score can be included on the previous formula multiplying each term of the summation [154 , 93 , 153] . experiments show that the recall and precision on the first ten answers increases significantly [93] . the order of the links can also be used by dividing the links into subgroups and using the hits-algorithm on those subgroups instead of the original web-pages [153] . the last example is pagerank , which is part of the ranking-algorithm used by google [117] . pagerank simulates a user navigating randomly in the web who jumps to a random page with probability q or follows a random hyperlink (on the current page) with probability 1 ó q . it is further assumed that this user never goes back to a previously visited page following an already traversed hyperlink backwards . this process can be modeled with a markov-chain , from where the stationary-probability of being in each page can be computed . this value is then used as part of the ranking mechanism of google . let c (a) be the number of outgoing links of page a and suppose that page a is pointed to by pages p \ to pn . then , the pagerank , pr (a) of a is defined as pr {a) = q + (l-q) j2 pr (pi) / c (pi) where q must be set by the system (a typical value is 0.15) . notice that the ranking (weight) of other pages is normalized by the number of links in the page . pagerank can be computed using an iterative-algorithm , and corresponds to the principal-eigenvector of the normalized link matrix of the-web (which is the transition-matrix of the markov chain) . crawling the-web using this ordering has been shown to be better than other crawling schemes [168] (see next section) . 382 searching the-web therefore , to help ranking-algorithms , page designers should include informative titles , headings , and meta fields , as well as good links . however , keywords should not be repeated as some search-engines penalize repeating words (spam-ming) . using full terms instead of indirect ways to refer to subjects should also be considered .
estimation of parameters the use of a weighting-function of the kind derived above in actual retrieval requires the estimation of pertinent parameters . i shall here deal with the estimation of ti and ri for the non-linear case , obviously the linear case will follow by analogy . to show what is involved let me given an example of the estimation-process using simple maximum-likelihood estimates . the basis for our estimates is the following 2-by-2 table . here i have adopted a labelling-scheme for the cells in which [x] means the number of occurrences in the cell labelled x. ignoring for the moment the nature of the set on which this table is based ; our estimates might be as follows : in general we would have two tables of this kind when setting-up our function g (x) , one for estimating the parameters associated with p (x/w1) and one for p (x/w2) . in the limit we would have complete knowledge of which documents in the collection were relevant and which were not . were we to calculate the estimates for this limiting case , this would only be useful in showing what the upper-bound to our retrieval would be under this particular model . more realistically , we would have a sample of documents , probably small (not nesessarily random) , for which the relevance status of each document was known . this small set would then be the source data for any 2-by-2 tables we might wish to construct . the estimates therefore would be biased in an unavoidable way . the estimates shown above are examples of point estimates . there are a number of ways of arriving at an appropriate rule for point-estimation . unfortunately the best form of estimation rule is still an open problem [14] . in fact , some statisticians believe that point-estimation should not be attempted at all [15] . however in the context of ir it is hard to see how one can avoid making point estimates . one major objection to any point-estimation rule is that in deriving it some ` arbitrary ' assumptions are made . fortunately in ir there is some chance of justifying these assumptions by pointing to experimental-data gathered from retrieval-systems , thereby removing some of the arbitrariness . two basic assumptions made in deriving any estimation rule through bayesian-decision-theory are : (1) the form of the prior-distribution on the parameter-space , i.e. in our case the assumed probability-distribution on the possible values of the binomial parameter ; and (2) the form of the loss-function used to measure the error made in estimating the parameter . once these two assumptions are made explicit by defining the form of the distribution and loss-function , then , together with bayes ' principle which seeks to minimise the posterior conditional expected loss given the observations , we can derive a number of different estimation rules . the statistical literature is not much help when deciding which rule is to be preferred . for details the reader should consult van rijsbergen [2] where further references to the statistical literature are given . the important rules of estimating a proportion p all come in the form where x is the number of successes in n trials , and a and b are parameters dictated by the particular combination of prior and loss-function . thus we have a whole class of estimation rules . for example when a = b = 0 we have the usual estimate x/n , and when a = b = [1] / 2 we have a rule attributed to sir harold jeffreys by good [16] . this latter rule is in fact the rule used by robertson and sparck jones [1] in their estimates . each setting of a and b can be justified in terms of the reasonableness of the resulting prior-distribution . since what is found reasonable by one man is not necessarily so for another , the ultimate choice must rest on performance in an experimental test . fortunately in ir we are in a unique position to do this kind of test . one important reason for having estimation rules different from the simple x/n , is that this is rather unrealistic for small samples . consider the case of one sample (n = 1) and the trial result x = 0 (or x = 1) which would result in the estimate for p as p = 0 (or p = 1) . this is clearly ridiculous , since in most cases we would already know with high probability that 0 lt ; p lt ; 1 . to overcome this difficulty we might try and incorporate this prior-knowledge in a distribution on the possible values of the parameter we are trying to estimate . once we have accepted the feasibility of this and have specified the way in which estimation-error is to be measured , bayes ' principle (or some other principle) will usually lead to a rule different from x/n . this is really as much as i wish to say about estimation rules , and therefore i shall not push the technical discussion on this points any further ; the interested reader should consult the readily accessible statistical literature .
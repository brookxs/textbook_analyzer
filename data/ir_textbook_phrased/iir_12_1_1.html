finite-automata and language-models what do we mean by a document-model generating a query ? a traditional generative-model of a language , of the kind familiar from formal-language-theory , can be used either to recognize or to generate strings . for example , the finite-automaton shown in figure 12.1 can generate strings that include the examples shown . the full set of strings that can be generated is called the language of the automaton . if instead each node has a probability-distribution over generating different terms , we have a language-model . the notion of a language-model is inherently probabilistic . a language-model is a function that puts a probability measure over strings drawn from some vocabulary . that is , for a language-model over an alphabet : (90) 12.2 worked example . to find the probability of a word-sequence , we just multiply the probabilities which the model gives to each word in the sequence , together with the probability of continuing or stopping after producing each word . for example , (91) (92) (93) frog 90 stop likelihood-ratio 12.1.3 end worked example . figure 12.3 : partial-specification of two unigram language models . worked example . suppose , now , that we have two language-models and , shown partially in figure 12.3 . each gives a probability-estimate to a sequence of terms , as already illustrated in m1probability . the language-model that gives the higher probability to the sequence of terms is more likely to have generated the term sequence . this time , we will omit stop probabilities from our calculations . for the sequence shown , we get : and we see that . we present the formulas here in terms of products of probabilities , but , as is common in probabilistic-applications , in practice it is usually best to work with sums of log probabilities (cf. page 13.2) . end worked example .
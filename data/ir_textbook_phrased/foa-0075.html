4.3.8 one-parameter criteria this section began with recall and precision , the two most typical measures of search-engine-performance . from that beginning , richer , more elaborate characterizations of how well the system is performing have been considered . but even having the two measures of recall and precision , it is not a simple matter to decide whether one system is better or worse than another . what are we to think of a system that has good recall but poor precision , relative to another with the opposite feature ? for example , if we wish to optimize a search-engine with respect to one or more design-parameters (e.g. , the exact form of the query / document-matching function , cf. section 5.3.1) , effective optimization becomes much more difficult in multicriterial cases . such thinking has generated composite measures based on the basic components of recall and precision . for example , jardine and van rijsbergen [jardine and van rijsbergen , 1971 ; van rijsbergen , 1973] originally proposed the f-measure for this purpose : p ~ ~ (@ 2 + l) * precision-recall p '' ^ precision + recall * j van rijsbergen , p. 174 * has since defined the closely related effectiveness * van rijsbergen 's original paper used the function symbol e rather than the f we use here . the substitution is made to maintain the e ó `` effectiveness '' mnemonic . assessing the retrieval 133 of measure # , which uses a to smoothly vary the emphasis given to precision versus recall : / a 1-ca '' '' ea = 1 - . . + - rój : (4.12) \ prectswn recall / the transform a = - ott ^ converts easily between the two formulations , with e = 1 ó f. van rijsbergen , p. 174 also presents an argument that a perfectly even-handed balance of precision against recall at a = 0.5 is most appropriate . setting a = 0.5 , f5 = 1 also has the pleasing consequence that the f , statistic corresponds to the harmonic mean of precision-and-recall as discussed at some length in section 7.4 , it is possible to view retrieval as a type of classification-task : given a set-of-features for each document (e.g. , the keywords it contains) , classifiy it as either rel or rel with respect to some query . lewis and gale [lewis and gale , 1994] used the fp measure in the context of text-classification tasks , and they recommend a focus on the same j8 = 1.0 balance . classification-accuracy measures how often the classification is correct . if we associate the choice to retrieve a document with classifying it as rel , we can use the variables defined in the contingency-table of (table 4.1) : i retr h rel i + i retr d rel \ accuracy - ----------------- ó -------------------- (4.13) ndoc sliding ratio the fact that the retr set is ordered makes it useful to compare two rank orderings directly . if the `` correct , '' idealized ranking is known (for example , one corresponding to perfectly decreasing probability-of-relevance) , then an actual search-engine 's hitlist ranking can be compared against this standard . more typically , the rankings of two retrieval-systems are compared to one another . given two rankings , we will prefer the one that ranks relevant documents ahead of irrelevant ones . if our relevance-assessments are binary , with each document simply marked as relevant or irrelevant , * the normalized recall measure considered in section 4.3.6 (or the * as always , these assessments of relevance are with respect to some particular query . 134 finding out about expected search length measure to be described in section 4.3.10) is the best we can do in distinguishing the two rankings . but if we assume instead that it is possible to impose a more refined measure rel (di) than simply rel/rel (e.g. , recall the richer preference scale of figure 4.1) , more sophisticated measures are possible . in this case , we prefer a ranking that ranks d \ ahead of dj just in case rel (d {-rrb- gt ; rel {dj) . one way to quantify this preference is to sum the rel {d \) for the nret most highly ranked documents : nret (4.14) the ratio of this measure , computed for each of the two systems ' rankings , is called the sliding ratio score [pollack , 1968] : rank \ {dt) lt ; nret e rd (dt) rank2 (dl) lt ; nret e wdi) f = i as nret increases , this ratio comes closer to unity : ranki (di) lt ; nret e rekdi) lim ------ ó --------------- = 1 (4.16) nret - + ndoc ranked ,) lt ; nret i = l and so it is most useful for distinguishing between two rankings when only a small nret is considered . point alienation the sliding ratio measure provides a more discriminating measure but depends entirely on the availability of metric rel (dj) measures for retrieved documents . as discussed in section 4.1.1 , it is much easier to derive nonmetric assessments directly from relevance-feedback data given naturally as part of users ' browsing : e - lt ; # - lt ; © (4.17) in an effort to exploit the nonmetric preferences often provided by human subjects5 guttman [guttman , 1978] defined a measure known assessing the retrieval 135 as point alienation . bartell has pioneered a variation of it for use with document rankings rated by relevance-feedback [bartell et al. , 1994a] . the basic idea is deceptively simple : compare the difference in rank between two differentially preferred documents to the absolute-difference of these ranks : y. rank (d) ~ rank (d') ~ f ^ d , \ rank {d) - rank (df) \ l ' j if d is really preferred over d ! - (d y d !) - (e.g. , if some user has marked d as rel but said nothing about df) , we can hope that rank (d) lt ; rank (df) , * and so the numerator (rank (d) ~ rank (df)) whl be negative ; if , on the other hand , the two documents are incorrectly ordered by the ranking , the numerator will be positive . comparing this arithmetic difference to its absolute value , and then summing over the rankings for all pairs of documents (d , df) that are differentially preferred {dy dr) gives equation 4.18 .
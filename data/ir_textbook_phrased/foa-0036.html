3.2 remember zipf looking at our corpus as a very long string of characters , something that even a monkey could generate , provides a useful baseline against which we can evaluate larger constructs . associate with each word w its frequency f {w) gt ; the number of times it occurs anywhere in the corpus . now imagine that we 've sorted the vocabulary according to frequency so that the most frequently occurring word will have rank r = 1 , the next most frequently used word will have r = 2 , and so on . george kingsley zipf (1902-50) has become famous for noticing that the distribution we find true of our corpus is in fact very reliably true of any large sample of natural-language we might consider . zipf {zipf , 1949] observed that the words ' rank-frequency distribution can be fit very closely by the relation : f (r) = ó , a ´ l , c ´ 0.1 (3.1) this empirical rule is now known as zipf 's law . but why should this pattern of word-usage , something we can reasonably expect to vary with author or type of publication , be so weighting and matching against indices 63 100,000 y log (freq) 10,000 - 1000 - 100 - 10 - log (rank) h 10 100 1000 figure 3.1 zipfian distribution of ait words 10,000 universal ? what 's more , the notion of `` word '' used in this formula has varied radically : in tabulations of word-frequencies by yule and thorndike , words were stemmed to their root form ; yule counted only nouns [yule , 1924 ; thorndike , 1937] . dewey [dewey , 1929] and thorndike collected statistics from multiple sources ; others were collected from a single work (for example , james joyce 's ulysses) . the frequency-distribution for a small subset of (nonnoise words in) our ait corpus is shown in figure 3.1 . note the nearly linear , negatively sloped relation when frequency is plotted as a function of rank , and both are plotted on log scales .
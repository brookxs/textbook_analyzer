7.4.3 statistical-methods in a statistical method , a probability is estimated for each symbol (the modeling task) and , based on this probability , a code is assigned to each symbol at a time (the coding task) . shorter codes are assigned to the most likely symbols . the relationship between probabilities and codes was established by claude shannon in his source-code theorem [718] . he showed that , in an optimal encoding-scheme , a symbol that is expected to occur with probability p should be assigned a code of length iog2 ^ bits . the number of bits in which a symbol is best coded represents the information-content of the symbol . the average amount of information per symbol over the whole alphabet is called the entropy of the probability distribution , and is given by : 1 2 pi e is a lower-bound on compression , measured in bits per symbol , which applies to any coding method based on the probability distribution pt . it is important to note that e is calculated from the probabilities and so is a property of the model . see chapter 6 for more details on this topic . text-compression 177 modeling the basic function of a model is to provide a probability assignment for the next symbol to be coded . high compression can be obtained by forming good models of the text that is to be coded . the probability assignment is explained in the following section . compression models can be adaptive , static , or semi-static . adaptive-models start with no information about the text and progressively learn about its statistical distribution as the compression-process goes on . thus , adaptive-models need only one pass over the text and store no additional-information apart from the compressed text . for long enough texts , such models converge to the true statistical distribution of the text . one major disadvantage , however , is that decompression of a file has to start from its beginning , since information on the distribution of the data is stored incrementally inside the file . adaptive-modeling is a good option for general purpose compression programs , but an inadequate alternative for full-text-retrieval where random-access to compressed patterns is a must . static models assume an average distribution for all input texts . the modeling phase is done only once for all texts to be coded in the future (i.e. , somehow a probability-distribution is estimated and then used for all texts to be compressed in the future) . these models tend to achieve poor compression ratios when the data deviates from initial statistical assumptions . for example , a model adequate for english literary texts will probably perform poorly for financial texts containing a lot of different numbers , as each number is relatively rare and so receives long codes . semi-static models do not assume any distribution on the data , but learn it in a first pass . in a second pass , they compress the data by using a fixed code derived from the distribution learned from the first pass . at decoding time , information on the data-distribution is sent to the decoder before transmitting the encoded symbols . the disadvantages of semi-static models are that they must make two passes over the text and that information on the data-distribution must be stored to be used by the decoder to decompress . in situations where interactive data communications are involved it may be impractical to make two passes over the text . however , semi-static models have a crucial advantage in ir contexts : since the same codes are used at every point in the compressed file , direct-access is possible . word-based models take words instead of characters as symbols . usually , a word is a contiguous string of characters in the set {a. . z , a. . z} separated by other characters not in the set {a. . z , a. . z} . there are many good reasons to use word-based models in an ir-context . first , much better compression rates are achieved by taking words as symbols because words carry a lot of meaning in natural-languages and , as a result , their distribution is much more related to the semantic structure of the text than the individual letters . second , words are the atoms on which most information-retrieval-systems are built . words are already stored for indexing purposes and so might be used as part of the model for compression . third , the word-frequencies are also useful in answering queries involving combinations of words because the best strategy is to start with the 178 text operations least frequent words first . since the text is not only composed of words but also of separators , a model must also be chosen for them . there are many different ways to deal with separators . as words and separators always follow one another , two different alphabets are usually used : one for words and one for separators . consider the following example : each rose , a rose is a rose . in the word-based-model , the set of symbols of the alphabet is {a , each , is , rose} , whose frequencies are 2 , 1 , 1 , and 3 , respectively , and the set of separators is {' , lj \ u} , whose frequencies are 1 and 5 , respectively (where u represents a space) . once it is known that the text starts with a word or a separator , there is confusion about which alphabet to use . in natural-language-texts , a word is followed by a single space in most cases . in the texts of the trec-3 collection [342] (see chapter 3) , 70-80 % of the separators are single spaces . another good alternative is to consider the single space that follows a word as part of the same word . that is , if a word is followed by a space , we can encode just the word . if not , we can encode the word and then the following separator . at decoding time , we decode a word and assume that a space follows unless the next symbol corresponds to a separator . notice that now a single alphabet for words and separators (single space excluded) is used . for instance , in the example above , the single alphabet is {' , lj ' , a , each , is , rose} and there is no longer an alphabet for separators . as the alphabet excludes the single space then the words are called spaceless words . in some situations word-based models for full-text databases have a potential to generate a great quantity of different codes and care must be exercised to deal with this fact . for instance , as discussed in the section on lexical-analysis (at the beginning of this chapter) , one has to consider whether a sequence of digits is to be considered as a word . if it is , then a collection which contains one million documents and includes document numbers as identifiers will generate one million words composed solely of digits , each one occurring once in the collection . this can be very inefficient for any kind of compression-method available . one possible good solution is to divide long numbers into shorter ones by using a null (or implicit) punctuation marker in between . this diminishes the alphabet size resulting in considerable improvements in the compression-ratio and in the decoding time . another important consideration is the size of the alphabet in word-based schemes . how large is the number of different words in a full-text database ? it is empirically known that the vocabulary v of natural-language-texts with n words grows sublinearly . heaps [352] shows that v = o (nd) , where 0 is a constant dependent on the particular text . for the 2 gigabyte trec-3 collection [342] , p is between 0.4 and 0.6 which means that the alphabet size grows roughly proportional to the square-root of n. even for this growth of the alphabet , the generalized zipf law shows that the probability distribution is skewed so that the entropy remains constant . this implies that the compression-ratio does not degrade as the text (and hence the number of different symbols) grows . heaps ' and zipfs ' laws are explained in chapter 6 . finally , it is important to mention that word-based huffman methods need large texts to be effective (i.e. , they are not adequate to compress and transmit text-compression 179 a single web-page over a network) . the need to store the vocabulary represents an important space-overhead when the text is small (say , less than 10 megabytes) . however , this is not a concern in ir in general as the texts are large and the vocabulary is needed anyway for other purposes such as indexing and querying . coding coding corresponds to the task of obtaining the representation (code) of a symbol based on a probability-distribution given by a model . the main goal of a coder is to assign short-codes to likely symbols and long codes to unlikely ones . as we have seen in the previous section , the entropy of a probability-distribution is a lower-bound on how short the average length of a code can be , and the quality of a coder is measured in terms of how close to the entropy it is able to get . another important consideration is the speed of both the coder and the decoder . sometimes it is necessary to sacrifice the compression-ratio to reduce the time to encode and decode the text . a semi-static huffman compression-method works in two passes over the text . in a first pass , the modeler determines the probability distribution of the symbols and builds a coding tree according to this distribution . in a second pass , each next symbol is encoded according to the coding tree . adaptive huffman compression methods , instead , work in one single pass over the text updating the coding tree incrementally . the encoding of the symbols in the input-text is also done during this single pass over the text . the main problem of adaptive huffman methods is the cost of updating the coding tree as new symbols are read . as with huffman-based methods , arithmetic-coding methods can also be based on static , semi-static or adaptive-algorithms . the main strength of arithmetic-coding methods is that they can generate codes which are arbitrarily close to the entropy for any kind of probability-distribution . another strength of arithmetic-coding methods is that they do not need to store a coding tree explicitly . for adaptive-algorithms , this implies that arithmetic-coding uses less memory than huffman-based coding . for static or semi-static algorithms , the use of canonical huffman codes overcomes this memory problem (canonical huffman trees are explained later on) . in arithmetic-coding , the input-text is represented by an interval of real numbers between 0 and 1 . as the size of the input becomes larger , the interval becomes smaller and the number of bits needed to specify this interval increases . compression is achieved because input symbols with higher probabilities reduce the interval less than symbols with smaller probabilities and hence add fewer bits to the output code . arithmetic-coding presents many disadvantages over huffman-coding in an ir environment . first , arithmetic-coding is much slower than huffman-coding , especially with static and semi-static algorithms . second , with arithmetic-coding , decompression can not start in the middle of a compressed file . this contrasts with huffman-coding , in which it is possible to index and to decode from 180 text operations any position in the compressed text if static or semi-static algorithms are used . third , word-based huffman-coding methods yield compression ratios as good as arithmetic-coding ones . consequently , huffman-coding is the method of choice in full-text-retrieval , where both speed and random-access are important . thus , we will focus the remaining of our discussion on semi-static word-based huffman-coding . huffman-coding huffman-coding is one of the best known compression methods [386] . the idea is to assign a variable-length encoding in bits to each symbol and encode each symbol in turn . compression is achieved by assigning shorter codes to more frequent symbols . decompression uniqueness is guaranteed because no code is a prefix of another . a word-based semi-static model and huffman-coding form a good compression-method for text . figure 7.2 presents an example of compression using huffman-coding on words . in this example the set of symbols of the alphabet is {l , lj \ a , each , for , is , rose} , whose frequencies are 1 , 2 , 1 , 1 , 1 , and 3 , respectively . in this case the alphabet is unique for words and separators . notice that the separator * u ' is not part of the alphabet because the single space that follows a word is considered as part of the word . these words are called spaceless words (see more about spaceless words in section 7.4.3) . the huffman-tree shown in figure 7.2 is an example of a binary trie built on binary codes . tries are explained in chapter 8 . decompression is accomplished as follows . the stream of bits in the compressed file is traversed from left to right . the sequence of bits read is used to also traverse the huffman compression tree , starting at the root . whenever a leaf node is reached , the corresponding word (which constitutes the decompressed symbol) is printed out and the tree traversal is restarted . thus , according to the tree in figure 7.2 , the presence of the code 0110 in the compressed file leads to the decompressed symbol for . to build a huffman-tree , it is first necessary to obtain the symbols that constitute the alphabet and their probability-distribution in the text to be compressed . the algorithm for building the tree then operates bottom up and starts original text : for each , rose , a rose is a rose compressed text : 0110 0100 1 0101 00 1 0111 00 1 figure 7.2 huffman-coding tree for spaceless words . text-compression 181 by creating for each symbol of the alphabet a node containing the symbol and its probability (or frequency) . at this point there is a forest of one-node trees whose probabilities sum up to 1 . next , the two nodes with the smallest probabilities become children of a newly created parent node . with this parent node is associated a probability equal to the sum of the probabilities of the two chosen children . the operation is repeated ignoring nodes that are already children , until there is only one node , which becomes the root of the decoding-tree . by delaying the pairing of nodes with high probabilities , the algorithm necessarily places them closer to the root-node , making their code smaller . the two branches from every internal node are consistently labeled 0 and 1 (or 1 and 0) . given s symbols and their frequencies in the text , the algorithm builds the huffman-tree in o (slogs) time . the number of huffman trees which can be built for a given probability-distribution is quite large . this happens because interchanging left and right subtrees of any internal node results in a different tree whenever the two subtrees are different in structure , but the weighted-average code length is not affected . instead of using any kind of tree , the preferred choice for most applications is to adopt a canonical tree which imposes a particular order to the coding bits . a huffman-tree is canonical when the height of the left subtree of any node is never smaller than that of the right subtree , and all leaves are in increasing order of probabilities from left to right . figure 7.3 shows the canonical tree for the example of figure 7.2 . the deepest leaf at the leftmost position of the huffman canonical tree , corresponding to one element with smallest probability , will contain only zeros , and the following codes will be in increasing order inside each level . at each change of level we shift left one bit in the counting . the table in figure 7.3 shows the canonical codes for the example of figure 7.2 . a canonical code can be represented by an ordered sequence s of pairs (xl , yl) , 1 lt ; i lt ; t , where xi represents the number of symbols at level ? \ y % represents the numerical value of the first code at level i , and # is the height of the tree . for our example in figure 7.3 , the ordered sequence is s = ((1,1) , (1,1) , ((-rrb- , oo) , (4,0)) . for instance , the fourth pair (4,0) in 5 corresponds to the fourth level and indicates that there are four nodes at this level and that to the node most to the left is assigned a code , at this level , with value 0 . since this is the fourth level , a value 0 corresponds to the codeword 0000 . symbol prob . old can . code code each 1/9 0100 0000 , u 1/9 0101 0001 for 1/9 0110 0010 is 1/9 0111 0011 a 2/9 00 01 rose 3/9 1 1 figure 7.3 canonical code . 182 text operations (a) non-optimal tree l 254 empty-nodes 256 elements 256 elements (b) optimal byte tree 254 elements 256 elements 2 elements 254 empty-nodes figure 7.4 example of byte huffman-tree . one of the properties of canonical codes is that the set of codes having the same length are the binary representations of consecutive integers . interpreted as integers , the 4-bit codes of the table in figure 7.3 are 0 , 1 , 2 , and 3 , the 2-bit code is 1 and the 1-bit code is also 1 . in our example , if the first character read from the input stream is 1 , a codeword has been identified and the corresponding symbol can be output . if this value is 0 , a second bit is appended and the two bits are again interpreted as an integer and used to index the table and identify the corresponding symbol once we read c00 ! we know that the code has four bits and therefore we can read two more bits and use them as an index into the table . this fact can be exploited to enable efficient encoding and decoding with small overhead . moreover , much less memory is required , which is especially important for large vocabularies . byte-oriented huffman-code the original method proposed by huffman [386] leads naturally to binary coding trees . in [577] , however , it is proposed to build the code assigned to each symbol as a sequence of whole bytes . as a result , the huffman-tree has degree 256 instead of 2 . typically , the code assigned to each symbol contains between 1 and o bytes . for example , a possible code for the word rose could be the 3-byte code k47 131 8 / the construct ion of byte huffman trees involves some details which must be dealt with . care must be exercised to ensure that the first levels of the tree have no empty-nodes when the code is not binary . figure 7.4 (a) illustrates a ease where a naive extension of the binary huffman-tree construction algorithm might generate a non-optimal byte tree . in this example the alphabet has 512 symbols , all with tiie same probability . the root-node has 254 empty spaces that could be occupied by symbols from the second level of the tree , changing their code lengths from 2 bytes to 1 byte . a way to ensure that the empty-nodes always go to the lowest level of the tree follows . we calculate beforehand the number of empty-nodes that will arise . text-compression 183 we then compose these empty-nodes with symbols of smallest probabilities (for moving the empty-nodes to the deepest level of the final tree) . to accomplish this , we need only to select a number of symbols equal to 1 + ((v - 256) mod 255) , where v is the total number of symbols (i.e. , the size of the vocabulary) , for composing with the empty-nodes . for instance , in the example in figure 7.4 (a) , we have that 2 elements must be coupled with 254 empty-nodes in the first step (because , 1 - f ((512 - 256) mod 255) = 2) . the remaining steps are similar to the binary huffman-tree construction algorithm . all techniques for efficient encoding and decoding mentioned previously can easily be extended to handle word-based byte huffman-coding . moreover , no significant decrease of the compression-ratio is experienced by using bytes instead of bits when the symbols are words . further , decompression of byte huffman-code is faster than decompression of binary huffman-code . in fact , compression and decompression are very fast and compression ratios achieved are better than those of the ziv-lempel family [848 , 849] . in practice byte processing is much faster than bit processing because bit shifts and masking operations are not necessary at decoding time or at searching time . one important consequence of using byte huffman-coding is the possibility of performing direct searching on compressed text . the searching algorithm is explained in chapter 8 . the exact search can be done on the compressed text directly , using any known sequential-pattern matching-algorithm . moreover , it allows a large number of variations of the exact and approximate compressed pattern-matching problem , such as phrases , ranges , complements , wild cards , and arbitrary regular-expressions . the algorithm is based on a word-oriented shift-or algorithm and on a fast boyer-moore-type filter . for approximate searching on the compressed text it is eight times faster than an equivalent approximate searching on the uncompressed text , thanks to the use of the vocabulary by the algorithm [577 , 576] . this technique is not only useful in speeding up sequential search . it can also be used to improve indexed schemes that combine inverted-files and sequential search , like glimpse [540] .
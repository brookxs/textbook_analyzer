multinomial distributions over words under the unigram-language-model the order of words is irrelevant , and so such models are often called `` bag-of-words '' models , as discussed in chapter 6 (page 6.2) . even though there is no conditioning on preceding context , this model nevertheless still gives the probability of a particular ordering of terms . however , any other ordering of this bag of terms will have the same probability . so , really , we have a multinomial-distribution over words . so long as we stick to unigram models , the language-model name and motivation could be viewed as historical rather than necessary . we could instead just refer to the model as a multinomial-model . from this perspective , the equations presented above do not present the multinomial probability of a bag-of-words , since they do not sum over all possible orderings of those words , as is done by the multinomial coefficient (the first term on the right-hand-side) in the standard presentation of a multinomial-model : (97) stop 13.2 the fundamental problem in designing language-models is that we do not know what exactly we should use as the model . however , we do generally have a sample of text that is representative of that model . this problem makes a lot of sense in the original , primary uses of language-models . for example , in speech-recognition , we have a training sample of (spoken) text . but we have to expect that , in the future , users will use different words and in different sequences , which we have never observed before , and so the model has to generalize beyond the observed data to allow unknown words and sequences . this interpretation is not so clear in the ir case , where a document is finite and usually fixed . the strategy we adopt in ir is as follows . we pretend that the document is only a representative sample of text drawn from a model distribution , treating it like a fine-grained topic . we then estimate a language-model from this sample , and use that model to calculate the probability of observing any word-sequence , and , finally , we rank documents according to their probability of generating the query . exercises . including stop probabilities in the calculation , what will the sum of the probability estimates of all strings in the language of length 1 be ? assume that you generate a word and then decide whether to stop or not (i.e. , the null string is not part of the language) . if the stop probability is omitted from calculations , what will the sum of the scores assigned to strings in the language of length 1 be ? what is the likelihood-ratio of the document according to and in m1m2compare ? no explicit stop probability appeared in m1m2compare . assuming that the stop probability of each model is 0.1 , does this change the likelihood-ratio of a document according to the two models ? how might a language-model be used in a spelling-correction-system ? in particular , consider the case of context-sensitive-spelling-correction , and correcting incorrect usages of words , such as their in are you their ? (see section 3.5 (page) for pointers to some literature on this topic .)
5.1 derivation of zipf 's law for random texts as before , we begin by defining a word to be any sequence of characters separated by spaces . let us therefore consider an alphabet of m characters , interspersed with a specially designated space character 0 . we will consider an especially simple model (similar to that used by many others [li , 1992 ; miller , 1957 ; hill , 1970 ; hill , 1974]) in which a random monkey generates words by hitting all keys - space and letters - with equal probability p : p = pt {%) = pr (a) == ... = pr (z) = ó ^ ó (5.1) m + 1 we can use lexicographic trees to conveniently organize words of length fc , say , by the order in which the k characters occur prior to the terminating space , as shown in figure 5.1 this shows a set of m + 1 trees , each rooted in the words ' starting character . leaf nodes at level k 149 150 finding out about o pr (`` a '') figure 5.1 lexicographic tree underlying zipfian distribution double counting spaces ?! are all labeled with the probability of the sequence of k ó 1 characters prior to the space occurring at level k . one immediate observation is that njt , the number of words w {of length k or less , is : nk = number (wi i i lt ; k) = y '' nf = m (1 ~ m} (5gt ; 2) in an infinitely long-sequence of characters generated according to equation 5.1 , we will expect to find a `` word '' wk terminating at level k (i.e. , a string of k unbroken nonspace characters bracketed by two spaces) with probability defined in terms of the independent character probabilities p : l ,5.3) we can compute c , the constant of proportionality , by including all the mk words of length k and summing these probabilities over all possible words (including unrealistic , infinitely long ones !) : ^ '' , (m + l) 2 mlcp lgt ; c h ï ¶ ï pk = next consider the rank of these words . because the probability of a word 's occurrence is an exponentially decreasing function of its length , mathematical foundations 151 we know that the m highest ranked words are the one-character words ; next come the m2 two-letter words ; and so on . using equation 5.2 we therefore know how the rank rgt ; of all words wk terminating on level k must be bounded above and below : nfc-i lt ; rklt ; nk where f denotes a compromise `` average '' rank for all the mk equiprobable words . * note that equations 5.4 and 5.5 define the words ' probability and rank , respectively , in terms of the common metric k . as li [li , 1992] notes , zipf 's law is fundamentally about this transformation , from an exponential-distribution onto a rank variable . solving both equations for k : k = ln (m + 1) i __ f 2 (m ó l) f) t i , _ v m + l + ~ lnm we can now set them equal and derive an expression for a word 's probability in terms of its rank : ` z (m - \) fk m + 1 ln (m + l) lnm 1 / 2 (m-l) ffc this has the functional-form required by mandelbrot 's generalized zipf 's law (cf. equation 3.2) : _ c pk ~ (ft + b) ´ where 1 / m + l v m + l ln (af + l) mv2 (m 1) 7 ' 2 (m 1) ' p ; 1 / m + l v mv2 (m - 1) 7 ' 2 (m - 1) ' lnm * the model can be extended by replacing this simple average with distributional-information , for example , incorporating realistic character frequency information . 152 finding out about 1.25 1.15 1.05 80 100 0.95 figure 5.2 a as function of m , number of distinct characters
blocked sort-based indexing the basic steps in constructing a nonpositional index are depicted in figure 1.4 (page) . we first make a pass through the collection assembling all term-docid pairs . we then sort the pairs with the term as the dominant key and docid as the secondary key . finally , we organize the docids for each term into a postings list and compute statistics like term and document-frequency . for small collections , all this can be done in memory . in this chapter , we describe methods for large-collections that require the use of secondary-storage . to make index-construction more efficient , we represent terms as termids (instead of strings as we did in figure 1.4) , where each termid is a unique serial number . we can build the mapping from terms to termids on-the-fly while we are processing the collection ; or , in a two-pass approach , we compile the vocabulary in the first pass and construct the inverted-index in the second pass . the index-construction algorithms described in this chapter all do a single pass through the data . section 4.7 gives references to multipass algorithms that are preferable in certain applications , for example , when disk space is scarce . we work with the reuters-rcv1 collection as our model collection in this chapter , a collection with roughly 1 gb of text . it consists of about 800,000 documents that were sent over the reuters newswire during a 1-year period between august 20 , 1996 , and august 19 , 1997 . a typical document is shown in figure 4.1 , but note that we ignore multimedia-information like images in this book and are only concerned with text . reuters-rcv1 covers a wide range of international topics , including politics , business , sports , and (as in this example) science . some key statistics of the collection are shown in table 4.2 . reuters-rcv1 has 100 million tokens . collecting all termid-docid pairs of the collection using 4 bytes each for termid and docid therefore requires 0.8 gb of storage . typical collections today are often one or two orders of magnitude larger than reuters-rcv1 . you can easily see how such collections overwhelm even large computers if we try to sort their termid-docid pairs in memory . if the size of the intermediate files during index-construction is within a small factor of available memory , then the compression techniques introduced in chapter 5 can help ; however , the postings file of many large-collections can not fit into memory even after compression . table : collection statistics for reuters-rcv1 . values are rounded for the computations in this book . the unrounded values are : 806,791 documents , 222 tokens per document , 391,523 (distinct) terms , 6.04 bytes per token with spaces and punctuation , 4.5 bytes per token without spaces and punctuation , 7.5 bytes per term , and 96,969,056 tokens . the numbers in this table correspond to the third line (`` case folding '') in icompresstb5 . symbol statistic value documents 800,000 avg . # tokens per document 200 terms 400,000 avg . # bytes per token (incl . spaces/punct .) 6 avg . # bytes per token (without spaces/punct .) 4.5 avg . # bytes per term 7.5 tokens 100,000,000 figure 4.1 : document from the reuters newswire . with main-memory insufficient , we need to use an external-sorting algorithm , that is , one that uses disk . for acceptable speed , the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks as we explained in section 4.1 . one solution is the blocked sort-based indexing-algorithm or bsbi in figure 4.2 . bsbi (i) segments the collection into parts of equal size , (ii) sorts the termid-docid pairs of each part in memory , (iii) stores intermediate sorted results on disk , and (iv) merges all intermediate results into the final index . the algorithm parses documents into termid-docid pairs and accumulates the pairs in memory until a block of a fixed size is full (parsenextblock in figure 4.2) . we choose the block-size to fit comfortably into memory to permit a fast in-memory sort . the block is then inverted and written to disk . inversion involves two steps . first , we sort the termid-docid pairs . next , we collect all termid-docid pairs with the same termid into a postings list , where a posting is simply a docid . the result , an inverted-index for the block we have just read , is then written to disk . applying this to reuters-rcv1 and assuming we can fit 10 million termid-docid pairs into memory , we end up with ten blocks , each an inverted-index of one part of the collection . merging in blocked sort-based indexing.two blocks (`` postings lists to be merged '') are loaded from disk into memory , merged in memory (`` merged postings lists '') and written back to disk . we show terms instead of termids for better readability . in the final step , the algorithm simultaneously merges the ten blocks into one large merged index . an example with two blocks is shown in figure 4.3 , where we use to denote the document of the collection . to do the merging , we open all block files simultaneously , and maintain small read buffers for the ten blocks we are reading and a write-buffer for the final merged index we are writing . in each iteration , we select the lowest termid that has not been processed yet using a priority-queue or a similar data-structure . all postings lists for this termid are read and merged , and the merged list is written back to disk . each read buffer is refilled from its file when necessary . how expensive is bsbi ? its time-complexity is because the step with the highest time-complexity is sorting and is an upper-bound for the number of items we must sort (i.e. , the number of termid-docid pairs) . but the actual indexing time is usually dominated by the time it takes to parse the documents (parsenextblock) and to do the final merge (mergeblocks) . exercise 4.6 asks you to compute the total index-construction time for rcv1 that includes these steps as well as inverting the blocks and writing them to disk . notice that reuters-rcv1 is not particularly large in an age when one or more gb of memory are standard on personal-computers . with appropriate compression (chapter 5) , we could have created an inverted-index for rcv1 in memory on a not overly beefy server . the techniques we have described are needed , however , for collections that are several orders of magnitude larger . exercises . if we need comparisons (where is the number of termid-docid pairs) and two disk seeks for each comparison , how much time would index-construction for reuters-rcv1 take if we used disk instead of memory for storage and an unoptimized sorting algorithm (i.e. , not an external-sorting algorithm) ? use the system-parameters in table 4.1 . how would you create the dictionary in blocked sort-based indexing on-the-fly to avoid an extra pass through the data ?
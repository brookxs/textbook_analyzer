48 finding out about 2.3.3 summary we have described the lexical-analyzer in terms of the job it must do processing every document in the corpus , because this task confronts us first . but because our central task will be to match these documents against subsequent users ' queries , it is critical that the identical lexical-analysis be performed on the queries . this creates several implementation-constraints (e.g. , that the same code libraries are available to the indexer and to the query-processing interface) , but these are minor . if the query-language is designed to support any special operators (e.g. , boolean combinators , proximity operators) , the query 's lexical-analyzer may accept a superset of the tokens accepted by the document 's analyzer . in any case , it is imperative if queries and documents are to be matched correctly that the same lexical-analysis be applied to both streams . using an identical code library is the easiest way to ensure this . it may seem nonsensical to worry so much about processing each character efficiently , when we assume that some other previous process has already identified each interdocument break - does n't such processing require the same computational-effort , and , if so , does n't this make our current efficiency worries moot ? perhaps . a conclusive answer depends on many architecture and operating-system specifics . there are two reasons we have made such assumptions . the first is that the practicalities of delivering the foa corpora and code currently make this convenient . but the more serious reason is that the most theoretically and intellectually interesting questions involve analysis of operations downstream from the first stages of interdocument parsing : how to identify tokens , how to count them , etc. . if these latter operations are made especially efficient , it means we can afford to do more experimentation , more playfully . for a text , that is the primary concern .
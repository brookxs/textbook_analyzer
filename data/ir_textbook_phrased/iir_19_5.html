index-size and estimation http://www.yahoo.com/any_string spider traps 20 we could ask the following better-defined question : given two search-engines , what are the relative sizes of their indexes ? even this question turns out to be imprecise , because : in response to queries a search-engine can return web-pages whose contents it has not (fully or even partially) indexed . for one thing , search-engines generally index only the first few thousand words in a web-page . in some cases , a search-engine is aware of a page that is linked to by pages it has indexed , but has not indexed itself . as we will see in chapter 21 , it is still possible to meaningfully return in search-results . search-engines generally organize their indexes in various tiers and partitions , not all of which are examined on every search (recall tiered indexes from section 7.2.1) . for instance , a web-page deep inside a website may be indexed but not retrieved on general web-searches ; it is however retrieved as a result on a search that a user has explicitly restricted to that website (such site-specific search is offered by most web-search-engines) . 20 capture-recapture-method suppose that we could pick a random page from the index of and test whether it is in 's index and symmetrically , test whether a random page from is in . these experiments give us fractions and such that our estimate is that a fraction of the pages in are in , while a fraction of the pages in are in . then , letting denote the size of the index of search-engine , we have (245) (246) 246 from outside the search-engine to implement the sampling phase , we might generate a random page from the entire (idealized , finite) web and test it for presence in each search-engine . unfortunately , picking a web-page uniformly at random is a difficult problem . we briefly outline several attempts to achieve such a sample , pointing out the biases inherent to each ; following this we describe in some detail one technique that much research has built on . random searches : begin with a search-log of web-searches ; send a random-search from this log to and a random page from the results . since such logs are not widely available outside a search-engine , one implementation is to trap all search queries going out of a work group (say scientists in a research center) that agrees to have all its searches logged . this approach has a number of issues , including the bias from the types of searches made by the work group . further , a random document from the results of such a random-search to is not the same as a random document from . random ip-addresses : a second approach is to generate random ip-addresses and send a request to a web-server residing at the random address , collecting all pages at that server . the biases here include the fact that many hosts might share one ip (due to a practice known as virtual-hosting) or not accept http requests from the host where the experiment is conducted . furthermore , this technique is more likely to hit one of the many sites with few pages , skewing the document probabilities ; we may be able to correct for this effect if we understand the distribution of the number of pages on websites . random-walks : if the-web graph were a strongly connected directed-graph , we could run a random-walk starting at an arbitrary web-page . this walk would converge to a steady-state distribution (see chapter 21 , section 21.2.1 for more background material on this) , from which we could in principle pick a web-page with a fixed probability . this method , too has a number of biases . first , the-web is not strongly connected so that , even with various corrective rules , it is difficult to argue that we can reach a steady-state distribution starting from any page . second , the time it takes for the random-walk to settle into this steady-state is unknown and could exceed the length of the experiment . clearly each of these approaches is far from perfect . we now describe a fourth sampling approach , random queries . this approach is noteworthy for two reasons : it has been successfully built upon for a series of increasingly refined estimates , and conversely it has turned out to be the approach most likely to be misinterpreted and carelessly implemented , leading to misleading measurements . the idea is to pick a page (almost) uniformly at random from a search-engine 's index by posing a random query to it . it should be clear that picking a set of random terms from (say) webster 's dictionary is not a good way of implementing this idea . for one thing , not all vocabulary terms occur equally often , so this approach will not result in documents being chosen uniformly at random from the search-engine . for another , there are a great many terms in web-documents that do not occur in a standard dictionary such as webster 's . to address the problem of vocabulary terms not in a standard dictionary , we begin by amassing a sample web dictionary . this could be done by crawling a limited portion of the-web , or by crawling a manually-assembled representative subset of the-web such as yahoo! (as was done in the earliest experiments with this method) . consider a conjunctive-query with two or more randomly chosen words from this dictionary . operationally , we proceed as follows : we use a random conjunctive-query on and pick from the top 100 returned results a page at random . we then test for presence in by choosing 6-8 low-frequency terms in and using them in a conjunctive-query for . we can improve the estimate by repeating the experiment a large number of times . both the sampling process and the testing-process have a number of issues . our sample is biased towards longer documents . picking from the top 100 results of induces a bias from the ranking-algorithm of . picking from all the results of makes the experiment slower . this is particularly so because most web-search-engines put up defenses against excessive robotic querying . during the checking phase , a number of additional biases are introduced : for instance , may not handle 8-word conjunctive-queries properly . either or may refuse to respond to the test queries , treating them as robotic spam rather than as bona fide queries . there could be operational problems like connection time-outs . a sequence of research has built on this basic paradigm to eliminate some of these issues ; there is no perfect solution yet , but the level of sophistication in statistics for understanding the biases is increasing . the main idea is to address biases by estimating , for each document , the magnitude of the bias . from this , standard statistical-sampling methods can generate unbiased samples . in the checking phase , the newer work moves away from conjunctive-queries to phrase and other queries that appear to be better-behaved . finally , newer experiments use other sampling-methods besides random queries . the best known of these is document random-walk sampling , in which a document is chosen by a random-walk on a virtual graph derived from documents . in this graph , nodes are documents ; two documents are connected by an edge if they share two or more words in common . the graph is never instantiated ; rather , a random-walk on it can be performed by moving from a document to another by picking a pair of keywords in , running a query on a search-engine and picking a random document from the results . details may be found in the references in section 19.7 . exercises . two web-search-engines a and b each generate a large number of pages uniformly at random from their indexes . 30 % of a 's pages are present in b 's index , while 50 % of b 's pages are present in a 's index . what is the number of pages in a 's index relative to b 's ?
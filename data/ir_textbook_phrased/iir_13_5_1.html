mutual-information a common feature-selection-method is to compute as the expected mutual-information (mi) of term and class . mi measures how much information the presence/absence of a term contributes to making the correct classification decision on . formally : (130) 13.4 for mles of the probabilities , equation 130 is equivalent to equation 131 : (131) (132) 130 131 worked example . consider the class poultry and the term export in reuters-rcv1 . the counts of the number of documents with the four possible combinations of indicator values are as follows : 131 end worked example . to select terms for a given class , we use the feature-selection-algorithm in figure 13.6 : we compute the utility measure as and select the terms with the largest values . mutual-information measures how much information - in the information-theoretic sense - a term contains about the class . if a term 's distribution is the same in the class as it is in the collection as a whole , then . mi reaches its maximum value if the term is a perfect indicator for class-membership , that is , if the term is present in a document if-and-only-if the document is in the class . figure 13.7 : features with high mutual-information scores for six reuters-rcv1 classes . figure 13.7 shows terms with high mutual-information scores for the six classes in figure 13.1 . the selected terms (e.g. , london , uk , british for the class uk) are of obvious utility for making classification decisions for their respective classes . at the bottom of the list for uk we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding whether the document is in the class . as you might expect , keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier 's accuracy . figure 13.8 : effect of feature-set size on accuracy for multinomial and bernoulli models . 13.8
156 finding out about 5.2.3 singular-value-decomposition just as students ' height and weight are correlated along the dimension size , we can guess that (at least some small sets of) keywords are correlated and that the vector-space-representation of a document-corpus (cf. section 3.4) might be simplified in a similar way . the goal is to `` reduce the dimensionality '' of the documents ' representation in the same way we reduced that of our students ' sizes . but although we can draw a simple picture revealing the correlational structure between two dimensions , the picture is much more complicated when we try to conceive of the full v ´ 105 space of index-terms . for the d vectors we seek a smaller k lt ; v-dimensional solution . we still have as many documents as we had before , but we 're going to use a different set of descriptors . one of the most effective ways to characterize the correlational structure among large-sets of objects is via eigenfactor analysis . the technique we consider is called singular-value-decomposition . this factors any rectangular matrix into three terms : / = ulat (5.14) where u and a are each orthonorinal and 1 is a diagonal matrix `` conmathematical necting '' them.t details as before (cf. equation 3.39) , using inner-product to measure simi larity , we can define x : x =] f (5.15) because / is a d x v matrix , x is a d x d symmetric-matrix capturing all (^) interdocument similarities . then u is the system of eigenvectors of x , and l has the square-roots of the eignenvalues , y/tt ^ along its diagonal . by convention , we order these in decreasing order : ' gt ; y/h ; (5.16) large eigenvalues correspond to dominant correlations and so , just as looking for the size dimension that captured the main interaction between height and weight , we will rely on the first k dimensions to mathematical foundations 157 terms figure 5.4 svd decomposition capture the dominant modes of interaction in / : jk = uklkatk (5.17) this operation is shown schematically in figure 5.4.1 '' as always , whenever we throw something away (viz. , the small eigenvectors) , the result must be an approximation . that is , there will be a difference between our reduced-dimension representation jk and the original / . one easy way to measure this discrepancy is by referring to the d2 interdocument similarities latent in the x matrix and considering how different they are in the approximate matrix xk xk = jkj # (5.18) using the li norm | | ï | | 2 to measure deviation . then : err = | | x-xkh (5.19) in fact , approximating x with its reduced fc-rank svd decomposition turns out to be optimal , in the sense that it results in minimal err over all other rank-fc alternatives [bartell et al , 1992] .
weighting and matching against indices 89 3.4.2 vector length-normalization one good example involves the length of document and query-vectors . so far , we have placed no constraint on the number of keywords associated with a document . this means that long documents , which , caeteris paribus , can be expected to give rise to more keyword indices , can be expected to match (more precisely , have nonzero inner-product with) more queries and be retrieved more often . somehow (as discussed in section 1.4) this does n't seem fair : the author of a very short document who worked hard to compress the meaning into a pithy few paragraphs is less likely to have his or her document retrieved , relative to a wordy writer who says everything six times in six different ways ! these possibilities have been captured by robertson and walker in a pair of hypotheses regarding a document 's scope versus its verbosity : some documents may simply cover more material than others ... (the `` scope hypothesis '') . an opposite view would have long documents like short documents but longer : in other words , a long document covers a similar scope to a short document , but simply uses more words (the `` verbosity hypothesis '') . [robertson and walker , 1994 , p. 235] once we have decided that about-ness is conserved across documents , all documents ' vectors will have constant length . if we make the same assumption about the query-vector , then all of the vectors will lie on the surface of a sphere , as shown in figure 3.8 . without loss of generality , we will assume that the radius of the sphere is unity . making weights sensitive to document-length unfortunately , this very simple normalization is often inadequate , as can be shown in terms of the inverse-document-frequency (idf) weights discussed in section 3.3.7 . idf weighting highlights the distinction between inter - and intradocument keyword occurrences . because its primary focus is on discrimination among documents , intradocument occurrences of the same keyword become insignificant . this makes idf very sensitive to the definition of how document boundaries are defined (cf. section 2.2) , as suggested by figure 3.9 . 90 finding out about kw3 kwl figure 3.8 length-normalization of vector-space egt ; oc10g ñ __ __ parallel __ _ ó ó ó ó ó ó doc2 parallel ó . __ - * ó ó ó doqo t - docjooo figure 3.9 sensitivity of idf to `` document '' size the idf weight that results from encapsulating more text within the same `` document '' is , in a sense * the converse of normalizing the number of keywords assigned to every document . in either case , the advantage of using the paragraph as our canonical document (cf. section 1.4) , and/or relying on all documents in the corpus to be of nearly uniform size (as in the ait dissertation abstracts) is apparent . weighting and matching against indices 91 probablity probability / \ i of retrieval of relevance ~ t i pivot 03 b pivoted normalization old normalization / i 77 1 / a / slope = tan (aj pivot document-length old normalization-factor figure 3.10 pivot-based document-length-normalization . from [singhal et al. , 1996] . reproduced with permission of the association of computing-machinery . the okapi retrieval-system of robertson et al. [robertson and walker , 1994] has proven itself successful (in retrieval competitions like trec ; cf. section 4.3.3) by combining idf weightings with corpus-specific sensitivities to the lengths of the documents retrieved . they propose that the average length of all documents in a corpus , digital libraryen , provides a `` natural '' reference-point against which other documents ' lengths can be compared . define len (d) to be the number of keywords associated with the document . okapi then normalizes the first component of our weighting-formula , keyword frequency , by a term that is sensitive to each document 's deviation from this corpuswide average : wkd = fki (kgt ; len (d) / dlen) + fkd log (ndoc - djb) + 0.5 dk + 0.5 (3.30) robertson and walker report that k w 1.0 ó 2.0 | q j seems to work best , where | q | is the number of query terms . singhal et al. [singhal et alo 1996] approach the problem of length-normalization by doing a post hoc analysis of the distributions of retrieved versus relevant documents (in the trec-corpus) as a function of their length . a sketch of typical curves is shown in figure 3.10 . a . the 92 finding out about fact that these two distributions cross suggests a corpus-specific length-normalization pivot value , p , below which match scores are reduced and above which they are increased . the amount of this linear increase or decrease , shown as the length-normalization slope m of the length-normalization function in figure 3.10 . b , is the second corpus-specific parameter of singhal et al. 's model . returning to the `` generic '' form of the weighting-function originally given in equation 3.13 , the pivot-based length-normalization is : wkd = ---------------- ¶ -------------- discrimk (3.31) (1 ó m) ï p + m - norm where norm is whatever other normalization-factor (e.g. , cosine) is already in use ; several possible values are given in the next section . both okapi and pivot-based document-length normalizations rely on the specification of additional corpus-specific parameters (k \ and p , m , respectively) . although the addition of yet more `` knobs to twiddle '' is generally to be avoided in a retrieval-system , recent experience with machine-learning-techniques suggests the possibility of training such parameters to best match each corpus . this approach is sometimes called a regression technique and is discussed more fully in chapter 7 .
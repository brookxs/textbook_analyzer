feature-selection feature-selection noise feature overfitting figure : basic feature-selection-algorithm for selecting the best features . we can view feature-selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features) . it may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification , but when discussing the bias-variance-tradeoff in section 14.6 (page) , we will see that weaker models are often preferable when limited training-data are available . the basic feature-selection-algorithm is shown in figure 13.6 . for a given class , we compute a utility measure for each term of the vocabulary and select the terms that have the highest values of . all other terms are discarded and not used in classification . we will introduce three different utility measures in this section : mutual-information , ; the test , ; and frequency , . of the two nb models , the bernoulli model is particularly sensitive to noise features . a bernoulli nb classifier requires some form of feature-selection or else its accuracy will be low . this section mainly addresses feature-selection for two-class-classification tasks like china versus not-china . section 13.5.5 briefly discusses optimizations for systems with more than two classes . subsections mutual-information feature selectionchi2 feature-selection assessing as a feature-selection methodassessing chi-square as a feature-selection-method frequency-based feature-selection feature-selection for multiple-classifiers comparison of feature-selection methods
linear versus nonlinear classifiers in this section , we show that the two learning-methods naive-bayes and rocchio are instances of linear-classifiers , the perhaps most important group of text classifiers , and contrast them with nonlinear classifiers . to simplify the discussion , we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class-membership by comparing a linear combination of the features to a threshold . figure 14.8 : there are an infinite number of hyperplanes that separate two linearly separable classes . in two dimensions , a linear classifier is a line . five examples are shown in figure 14.8 . these lines have the functional-form . the classification-rule of a linear classifier is to assign a document to if and to if . here , is the two-dimensional vector representation of the document and is the parameter vector that defines (together with) the decision-boundary . an alternative geometric interpretation of a linear classifier is provided in figure 15.7 (page) . we can generalize this 2d linear classifier to higher dimensions by defining a hyperplane as we did in equation 140 , repeated here as equation 144 : (144) decision hyperplane figure 14.9 : linear-classification algorithm . the corresponding algorithm for linear-classification in dimensions is shown in figure 14.9 . linear-classification at first seems trivial given the simplicity of this algorithm . however , the difficulty is in training the linear classifier , that is , in determining the parameters and based on the training-set . in general , some learning-methods compute much better parameters than others where our criterion for evaluating the quality of a learning-method is the effectiveness of the learned linear classifier on new data . we now show that rocchio and naive-bayes are linear-classifiers . to see this for rocchio , observe that a vector is on the decision-boundary if it has equal distance to the two class centroids : (145) 14.8 we can derive the linearity of naive-bayes from its decision-rule , which chooses the category with the largest (figure 13.2 , page 13.2) where : (146) (147) we choose class if the odds are greater than 1 or , equivalently , if the log odds are greater than 0 . it is easy to see that equation 147 is an instance of equation 144 for , number of occurrences of in , and . here , the index , , refers to terms of the vocabulary (not to positions in as does ; cf. variantmultinomial) and and are - dimensional vectors . so in log space , naive-bayes is a linear classifier . prime 0.70 0 1 dlrs -0.71 1 1 rate 0.67 1 0 world -0.35 1 0 interest 0.63 0 0 sees -0.33 0 0 rates 0.60 0 0 year -0.25 0 0 discount 0.46 1 0 group -0.24 0 0 bundesbank 0.43 0 0 dlr -0.24 0 0 a linear classifier . the dimensions and parameters of a linear classifier for the class interest (as in interest rate) in reuters-21578 . the threshold is . terms like dlr and world have negative weights because they are indicators for the competing class currency . worked example . table 14.4 defines a linear classifier for the category interest in reuters-21578 (see section 13.6 , page 13.6) . we assign document `` rate discount dlrs world '' to interest since . we assign `` prime dlrs '' to the complement class (not in interest) since . for simplicity , we assume a simple binary vector representation in this example : 1 for occurring terms , 0 for non-occurring terms . end worked example . a linear problem with noise . in this hypothetical web-page-classification scenario , chinese-only web-pages are solid circles and mixed chinese-english web-pages are squares . the two classes are separated by a linear class boundary (dashed line , short dashes) , except for three noise documents (marked with arrows) . figure 14.10 is a graphical example of a linear problem , which we define to mean that the underlying distributions and of the two classes are separated by a line . we call this separating line the class boundary . it is the `` true '' boundary of the two classes and we distinguish it from the decision-boundary that the learning-method computes to approximate the class boundary . as is typical in text-classification , there are some noise documents in figure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes . in section 13.5 (page 13.5) , we defined a noise feature as a misleading feature that , when included in the document-representation , on average increases the classification-error . analogously , a noise document is a document that , when included in the training-set , misleads the learning-method and increases classification-error . intuitively , the underlying distribution partitions the representation-space into areas with mostly homogeneous class assignments . a document that does not conform with the dominant class in its area is a noise document . noise documents are one reason why training a linear classifier is hard . if we pay too much attention to noise documents when choosing the decision hyperplane of the classifier , then it will be inaccurate on new data . more fundamentally , it is usually difficult to determine which documents are noise documents and therefore potentially misleading . if there exists a hyperplane that perfectly separates the two classes , then we call the two classes linearly separable . in fact , if linear-separability holds , then there is an infinite number of linear separators (exercise 14.4) as illustrated by figure 14.8 , where the number of possible separating hyperplanes is infinite . figure 14.8 illustrates another challenge in training a linear classifier . if we are dealing with a linearly separable problem , then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training-data . in general , some of these hyperplanes will do well on new data , some will not . figure 14.11 : a nonlinear problem . an example of a nonlinear classifier is knn . the nonlinearity of knn is intuitively clear when looking at examples like figure 14.6 . the decision boundaries of knn (the double lines in figure 14.6) are locally linear segments , but in general have a complex shape that is not equivalent to a line in 2d or a hyperplane in higher dimensions . figure 14.11 is another example of a nonlinear problem : there is no good linear separator between the distributions and because of the circular `` enclave '' in the upper left part of the graph . linear-classifiers misclassify the enclave , whereas a nonlinear classifier like knn will be highly accurate for this type of problem if the training-set is large enough . if a problem is nonlinear and its class boundaries can not be approximated well with linear hyperplanes , then nonlinear classifiers are often more accurate than linear-classifiers . if a problem is linear , it is best to use a simpler linear classifier . exercises . prove that the number of linear separators of two classes is either infinite or zero .
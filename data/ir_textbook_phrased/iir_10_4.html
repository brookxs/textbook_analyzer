evaluation of xml-retrieval table 10.2 : inex 2002 collection statistics . 12,107 number of documents 494 mb size 1995-2002 time of publication of articles 1,532 average number of xml nodes per document 6.9 average depth of a node 30 number of cas topics 30 number of co topics figure 10.11 : simplified schema of the documents in the inex collection . the premier venue for research on xml-retrieval is the inex (initiative for the evaluation of xml-retrieval) program , a collaborative effort that has produced reference collections , sets of queries , and relevance-judgments . a yearly inex meeting is held to present and discuss research results . the inex 2002 collection consisted of about 12,000 articles from ieee journals . we give collection statistics in table 10.2 and show part of the schema of the collection in figure 10.11 . the ieee journal collection was expanded in 2005 . since 2006 inex uses the much larger english wikipedia as a test-collection . the relevance of documents is judged by human assessors using the methodology introduced in section 8.1 (page) , appropriately modified for structured-documents as we will discuss shortly . two types-of-information-needs or in inex are content-only or co topics and content-and-structure (cas) topics . co topics are regular keyword-queries as in unstructured-information retrieval . cas topics have structural constraints in addition to keywords . we already encountered an example of a cas topic in figure 10.3 . the keywords in this case are summer and holidays and the structural constraints specify that the keywords occur in a section that in turn is part of an article and that this article has an embedded year attribute with value 2001 or 2002 . since cas queries have both structural and content criteria , relevance-assessments are more complicated than in unstructured retrieval . inex 2002 defined component coverage and topical-relevance as orthogonal dimensions of relevance . the component coverage dimension evaluates whether the element retrieved is `` structurally '' correct , i.e. , neither too low nor too high in the tree . we distinguish four cases : exact coverage (e) . the information sought is the main topic of the component and the component is a meaningful unit of information . too small (s) . the information sought is the main topic of the component , but the component is not a meaningful (self-contained) unit of information . too large (l) . the information sought is present in the component , but is not the main topic . no coverage (n) . the information sought is not a topic of the component . the topical-relevance dimension also has four levels : highly relevant (3) , fairly relevant (2) , marginally relevant (1) and nonrelevant (0) . components are judged on both dimensions and the judgments are then combined into a digit-letter code . 2s is a fairly relevant component that is too small and 3e is a highly relevant component that has exact coverage . in theory , there are 16 combinations of coverage and relevance , but many can not occur . for example , a nonrelevant component can not have exact coverage , so the combination 3n is not possible . the relevance-coverage combinations are quantized as follows : (54) 8.5.1 8.5.1 q the number of relevant components in a retrieved set of components can then be computed as : (55) 8 10.6 one flaw of measuring relevance this way is that overlap is not accounted for . we discussed the concept of marginal relevance in the context of unstructured retrieval in section 8.5.1 (page) . this problem is worse in xml-retrieval because of the problem of multiple nested elements occurring in a search result as we discussed on page 10.2 . much of the recent focus at inex has been on developing algorithms and evaluation-measures that return non-redundant results lists and evaluate them properly . see the references in section 10.6 . table 10.3 : inex 2002 results of the vector-space-model in section 10.3 for content-and-structure (cas) queries and the quantization function q. algorithm average-precision simnomerge 0.242 simmerge 0.271 table 10.3 shows two inex 2002 runs of the vector-space system we described in section 10.3 . the better run is the simmerge run , which incorporates few structural constraints and mostly relies on keyword-matching . simmerge 's median average-precision (where the median is with respect to average-precision numbers over topics) is only 0.147 . effectiveness in xml-retrieval is often lower than in unstructured retrieval since xml-retrieval is harder . instead of just finding a document , we have to find the subpart of a document that is most relevant to the query . also , xml-retrieval effectiveness - when evaluated as described here - can be lower than unstructured retrieval-effectiveness on a standard evaluation because graded judgments lower measured performance . consider a system that returns a document with graded-relevance 0.6 and binary relevance 1 at the top of the retrieved list . then , interpolated precision at 0.00 recall (cf. page 8.4) is 1.0 on a binary evaluation , but can be as low as 0.6 on a graded evaluation . table 10.4 : a comparison of content-only and full-structure search in inex 2003/2004 . content only full structure improvement precision at 5 0.2000 0.3265 63.3 % precision-at-10 0.1820 0.2531 39.1 % precision at 20 0.1700 0.1796 5.6 % precision at 30 0.1527 0.1531 0.3 % table 10.3 gives us a sense of the typical performance of xml-retrieval , but it does not compare structured with unstructured retrieval . table 10.4 directly shows the effect of using structure in retrieval . the results are for a language-model-based system (cf. chapter 12) that is evaluated on a subset of cas topics from inex 2003 and 2004 . the evaluation-metric is precision at as defined in chapter 8 (page 8.4) . the discretization function used for the evaluation maps highly relevant elements (roughly corresponding to the 3e elements defined for q) to 1 and all other elements to 0 . the content-only system treats queries and documents as unstructured bags of words . the full-structure model ranks elements that satisfy structural constraints higher than elements that do not . for instance , for the query in figure 10.3 an element that contains the phrase summer holidays in a section will be rated higher than one that contains it in an abstract . the table shows that structure helps increase precision at the top of the results list . there is a large increase of precision at and at . there is almost no improvement at . these results demonstrate the benefits of structured-retrieval . structured-retrieval imposes additional constraints on what to return and documents that pass the structural filter are more likely to be relevant . recall may suffer because some relevant documents will be filtered out , but for precision-oriented tasks structured-retrieval is superior .
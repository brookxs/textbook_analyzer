cluster representatives before we can sensibly talk about search-strategies applied to clustered document-collections , we need to say a little about the methods used to represent clusters . whereas in a serial-search we need to be able to match queries with each document in the file , in a search of a clustered file we need to be able to match queries with clusters . for this purpose clusters are represented by some kind of profile (a much overworked word) , which here will be called a cluster representative . it attempts to summarise and characterise the cluster of documents . a cluster representative should be such that an incoming query will be diagnosed into the cluster containing the documents relevant to the query . in other words we expect the cluster representative to discriminate the relevant from the non-relevant documents when matched against any query . this is a tall order , and unfortunately there is no theory enabling one to select the right kind of cluster representative . one can only proceed experimentally . there are a number of ` reasonable ' ways of characterising clusters ; it then remains a matter for experimental test to decide which of these is the most effective . let me first give an example of a very primitive cluster representative . if we assume that the clusters are derived from a cluster method based on a dissimilarity-measure , then we can represent each cluster at some level of dissimilarity by a graph (see figure 5.2) . here a and b are two clusters . the nodes represent documents and the line between any two nodes indicates that their corresponding documents are less dissimilar than some specified level of dissimilarity . now , one way of representing a cluster is to select a typical member from the cluster . a simple way of doing this is to find that document which is linked to the maximum number of other documents in the cluster . a suitable name for this kind of cluster representative is the maximally linked document . in the clusters a and b illustrated , there are pointers to the candidates . as one would expect in some cases the representative is not unique . for example , in cluster b we have two candidates . to deal with this , one either makes an arbitrary choice or one maintains a list of cluster representatives for that cluster . the motivation leading to this particular choice of cluster representative is given in some detail in van rijsbergen [3] but need not concern us here . let us now look at other ways of representing clusters . we seek a method of representation which in some way ` averages ' the descriptions of the members of the clusters . the method that immediately springs to mind is one in which one calculates the centroid (or centre of gravity) of the cluster . if {d1 , d2 , ... , dn} are the documents in the cluster and each di is represented by a numerical vector (d1 , d2 , ... , dt) then the centroid c of the cluster is given by where | | di | | is usually the euclidean norm , i.e. . more often than not the documents are not represented by numerical vectors but by binary vectors (or equivalently , sets of keywords) . in that case we can still use a centroid type of cluster representative but the normalisation is replaced with a process which thresholds the components of the sum [[sigma]] di . to be more precise , let di now be a binary vector , such that a 1 in the jth position indicates the presence of the jth keyword in the document and a 0 indicates the contrary . the cluster representative is now derived from the sum vector (remember n is the number of documents in the cluster) by the following procedure . let c = (c1 , c2 , ... ct) be the cluster representative and [di] j the jth component of the binary vector di , then two methods are : so , finally we obtain as a cluster representative a binary vector c . in both cases the intuition is that keywords occurring only once in the cluster should be ignored . in the second case we also normalise out the size n of the cluster . there is some evidence to show that both these methods of representation are effective when used in conjunction with appropriate search-strategies (see , for example , van rijsbergen [4] and murray [5]) . obviously there are further variations on obtaining cluster representatives but as in the case of association measures it seems unlikely that retrieval-effectiveness will change very much by varying the cluster representatives . it is more likely that the way the data in the cluster representative is used by the search-strategy will have a larger effect . there is another theoretical way of looking at the construction of cluster representatives and that is through the notion of a maximal predictor for a cluster [6] . given that , as before , the documents di in a cluster are binary vectors then a binary cluster representative for this cluster is a predictor in the sense that each component (ci) predicts that the most likely value of that attribute in the member documents . it is maximal if its correct predictions are as numerous as possible . if one assumes that each member of a cluster of documents d1 , ... , dn is equally likely then the expected total number of incorrect predicted properties (or simply the expected total number of mismatches between cluster representative and member documents since everything in binary) is , this can be rewritten as the expression (*) will be minimised , thus maximising the number of correct predictions , when c = (c1 , ... , ct) is chosen in such a way that is a minimum . this is achieved by so in other words a keyword will be assigned to a cluster representative if it occurs in more than half the member documents . this treats errors of prediction caused by absence or presence of keywords on an equal basis . croft [7] has shown that it is more reasonable to differentiate the two types of error in ir applications . he showed that to predict falsely 0 (cj = 0) is more costly than to predict falsely a 1 (cj = 1) . under this assumption the value of [1] / 2 appearing is (3) is replaced by a constant less than [1] / 2 , its exact value being related to the relative-importance attached to the two types of prediction-error . although the main reason for constructing these cluster representatives is to lead a search-strategy to relevant documents , it should be clear that they can also be used to guide a search to documents meeting some condition on the matching-function . for example , we may want to retrieve all documents di which match q better than t , i.e. {di | m (q , di) gt ; t} for more details about the evaluation of cluster representative (3) for this purpose the reader should consult the work of yu et al. [8,9] . one major objection to most work on cluster representatives is that it treats the distribution of keywords in clusters as independent . this is not very realistic . unfortunately , there does not appear to be any work to remedy the situation except that of ardnaudov and govorun [10] . finally , it should be noted that cluster methods which proceed directly from document descriptions to the classification without first computing the intermediate dissimilarity coefficient , will need to make a choice of cluster representative ab-initio . these cluster representatives are then ` improved ' as the algorithm , adjusting the classification according to some objective-function , steps through its iterations .
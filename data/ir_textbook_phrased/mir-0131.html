7.5 comparing text-compression techniques table 7.2 presents a comparison between arithmetic-coding , character-based huffman-coding , word-based huffman-coding , and ziv-lempel coding , considering the aspects of compression-ratio , compression speed , decompression speed , memory-space overhead , compressed pattern-matching capability , and random-access capability . one important objective of any compression-method is to be able to obtain good compression ratios . it seems that two bits per character (or 25 % compression-ratio) is a very good result for natural-language-texts . thus , ` very good '' in the context of table 7.2 means a compression-ratio under 30 % - , ` good ' means a compression-ratio between 30 % and 45 % , and ` poor ' means a compression-ratio over 45 % . comparing text-compression techniques 187 character word arithmetic huffman huffman ziv-lempel compression-ratio very good poor very good good compression speed slow fast fast very fast decompression speed slow fast very fast very fast memory-space low low high moderate compressed pat . matching no yes yes yes random-access no yes yes no table 7.2 comparison of the main techniques . two other important characteristics of a compression-method are compression and decompression speeds . measuring the speed of various compression methods is difficult because it depends on the implementation details of each method , the compiler used , the computer-architecture of the machine used to run the program , and so on . considering compression speed , the lz78 methods (unix compress is an example) are among the fastest . considering decompression speed , the lz77 methods (gzip is an example) from the ziv-lempel are among the fastest . for statistical-methods (e.g. , arithmetic and semi-static huffman) the compression time includes the cost of the first pass during which the probability distribution of the symbols are obtained . with two passes over the text to compress , the huffman-based methods are slower than some ziv-lempel methods , but not very far behind . on the other hand , arithmetic methods are slower than huffman methods because of the complexity of arithmetic-coding compared with canonical huffman coding . considering decompression speed , word-based huffman methods are as fast as ziv-lempel methods , while character-based huffman methods are slower than word-based huffman methods . again , the complexity of arithmetic-coding make them slower than huffman-coding during decompression . all ziv-lempel compression methods require a moderate amount of memory during encoding and decoding to store tables containing previously occurring strings . in general , more detailed tables that require more memory for storage yield better compression . statistical-methods store the probability distribution of the symbols of the text during the modeling phase , and the model during both compression and decompression phases . consequently , the amount of memory depends on the size of the vocabulary of the text in each case , which is high for word-based models and low for character-based models . in an ir environment , two important considerations are whether the compression-method allows efficient random-access and direct searching on compressed text (or compressed pattern-matching) . huffman methods allow random-access and decompression can start anywhere in the middle of a compressed file , while arithmetic-coding and ziv-lempel methods can not . more recently , practical , efficient , and flexible direct searching methods on compressed texts have been discovered for word-based huffman compression [575 . 576 , 577] . 188 text operations direct searching has also been proposed for ziv-lempel methods , but only on a theoretical basis , with no implementation of the algorithms [250 , 19] . more recently , navarro and raffinot [592] presented some preliminary implementations of algorithms to search directly ziv-lempel compressed text . their algorithms are twice as fast as decompressing and searching , but slower than searching the decompressed text . they are also able to extract data from the middle of the compressed text without necessarily decompressing everything , and although some previous text has to be decompressed (i.e. , it is not really ` direct-access ') , the amount of work is proportional to the size of the text to be decompressed (and not to its position in the compressed text) .
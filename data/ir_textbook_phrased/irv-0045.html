scatter-storage or hash-addressing one file-structure which does not relate very well to the ones mentioned before is known as scatter-storage . the technique by which the file-structure is implemented is often called hash-addressing . its underlying principle is appealingly simple . given that we may access the data through a number of keys ki , then the address of the data in store is located through a key transformation-function f which when applied to ki evaluates to give the address of the associated data . we are assuming here that with each key is associated only one data item . also for convenience we will assume that each record (data and key) fits into one location , whose address is in the image-space of f . the addresses given by the application of f to the keys ki are called the hash addresses and f is called a hashing-function . ideally f should be such that it spreads the hash addresses uniformly over the available storage . of course this would be achieved if the function were one-to-one . unfortunately this can not be so because the range of possible key values is usually considerably larger than the range of the available storage addresses . therefore , given any hashing-function we have to contend with the fact that two distinct keys ki and kj are likely to map to the same address f (ki) (= f (kj)) . before i explain some of the ways of dealing with this i shall give a few examples of hashing-functions . let us assume that the available storage is of size 2 [m] then three simple transformations are as follows : (1) if ki is the key , then take the square of its binary representation and select m bits from the middle of the result ; (2) cut the binary representation of ki into pieces each of m bits and add these together . now select the m least significant bits of the sum as the hash address ; (3) divide the integer corresponding to ki by the length of the available store 2 [m] and use the remainder as the hash address . each of these methods has disadvantages . for example , the last one may given the same address rather frequently if there are patterns in the keys . before using a particular method , the reader is advised to consult the now extensive literature on the subject , e.g. morris [29] , or lum et al. [30] . as mentioned before there is the problem of collisions , that is , when two distinct keys hash to the same address . the first point to be made about this problem is that it destroys some of the simplicity of hashing . initially it may have been thought that the key need not be stored with the data at the hash address . unfortunately this is not so . no matter what method we use to resolve collisions we still need to store the key with the data so that at search-time when a key is hashed we can distinguish its data from the data associated with keys which have hashed to the same address . there are a number of strategies for dealing with collisions . essentially they fall into two classes , those which use pointers to link together collided keys and those which do not . let us first look at the ones which do not use pointers . these have a mechanism for searching the store , starting at the address where the collision occurred , for an empty storage location if a record needs to be inserted , or , for a matching key value at retrieval-time . the simplest of these advances from the hash address each time moving along a fixed number of locations , say s , until an empty location or the matching key value is found . the collision strategy thus traces out a well defined sequence of locations . this method of dealing with collisions is called the linear method . the tendency with this method is to store collided records as closely to the initial hash address as possible . this leads to an undesirable effect called primary clustering . in this context all this means is that the records tend to concentrate in groups or bunch-up . it destroys the uniform nature of the hashing-function . to be more precise , it is desirable that hash addresses are equally likely , however , the first empty location at the end of a collision sequence increases in likelihood in proportion to the number of records in the collision sequence . to see this one needs only to realise that a key hashed to any location in the sequence will have its record stored at the end of the sequence . therefore big groups of records tend to grow even bigger . this phenomenon is aggravated by a small step-size s when seeking an empty location . sometimes s = 1 is used in which case the collision strategy is known as the open-addressing technique . primary clustering is also worse when the hash-table (available storage) is relatively full . variations in the linear method which avoid primary clustering involve making the step-size a variable . one way is to set s equal to ai + bi [2] on the ith step . another is to invoke a random-number-generator which calculates the step-size afresh each time . these last two collision-handling methods are called the quadratic and random method respectively . although they avoid primary clustering they are nevertheless subject to secondary clustering , which is caused by keys hashing to the same address and following the same sequence in search of an empty location . even this can be avoided , see for example bell and kaman [31] . the second class of collision-handling methods involves extra storage space which is used to chain together collided records . when a collision occurs at a hash address it may be because it is the head of a chain of records which have all hashed to that address , or it may be that a record is stored there which belongs to a chain starting at some other address . in both cases a free location is needed which in the first case is simply linked in and stores the new record , in the second case the intermediate chain element is moved to the free location and the new record is stored at its own hash address thus starting a new chain (a one-element chain so far) . a variation on this method is to use a two-level store . at the first level we have a hash-table , at the second level we have a bump table which contains all the collided records . at a hash address in the hash-table we will find either , a record if no collisions have taken place at that address , or , a pointer to a chain of records which collided at that address . this latter chaining method has the advantage that records need never be moved once they have been entered in the bump table . the storage overhead is larger since records are put in the bump table before the hash-table is full . for both classes of collision strategies one needs to be careful about deletions . for the linear , quadratic etc. collision-handling strategies we must ensure that when we delete a record at an address we do not make records which collided at that address unreachable . similarly with the chaining method we must ensure that a deleted record does not leave a gap in the chain , that is , after deletion the chain must be reconnected . the advantages of hashing are several . firstly it is simple . secondly its insertion and search-strategies are identical . insertion is merely a failed search . if ki is the hashed key , then if a search of the collision sequence fails to turn up a match in ki , its record is simply inserted at the end of the sequence at the next free location . thirdly , the search-time is independent of the number of keys to be inserted . the application of hashing in ir has tended to be in the area of table-construction and look-up procedures . an obvious application is when constructing the set of conflation classes during text-processing . in chapter 2 , i gave an example of a document representative as simply a list of class-names , each name standing for a set of equivalent words . during a retrieval operation , a query will first be converted into a list of class-names . to do this each significant word needs to be looked up in a dictionary which gives the name of the class to which it belongs . clearly there is a case for hashing . we simply apply the hashing-function to the word and find the name of the conflation class to which it belongs at the hash address . a similar example is given in great detail by murray [32] . finally , let me recommend two very readable discussions on hashing , one is in page and wilson [33] , the other is in knuth 's third volume [28] .
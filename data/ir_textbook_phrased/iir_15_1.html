support-vector-machines : the linearly separable case figure 15.1 : the support-vectors are the 5 points right up against the margin of the classifier . for two-class , separable training-data sets , such as the one in figure 14.8 (page) , there are lots of possible linear separators . intuitively , a decision-boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes . while some learning-methods such as the perceptron-algorithm (see references in vclassfurther) find just any linear separator , others , like naive-bayes , search for the best linear separator according to some criterion . the svm in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point . this distance from the decision surface to the closest data point determines the margin of the classifier . this method of construction necessarily means that the decision function for an svm is fully specified by a (usually small) subset of the data which defines the position of the separator . these points are referred to as the support-vectors (in a vector-space , a point can be thought of as a vector between the origin and that point) . figure 15.1 shows the margin and support-vectors for a sample problem . other data points play no part in determining the decision surface that is chosen . an intuition for large-margin classification.insisting on a large-margin reduces the capacity of the model : the range of angles at which the fat decision surface can be placed is smaller than for a decision hyperplane (cf. vclassline) . maximizing the margin seems good because points near the decision surface represent very uncertain classification decisions : there is almost a 50 % chance of the classifier deciding either way . a classifier with a large-margin makes no low certainty classification decisions . this gives you a classification safety-margin : a slight error in measurement or a slight document variation will not cause a misclassification . another intuition motivating svms is shown in figure 15.2 . by construction , an svm-classifier insists on a large-margin around the decision-boundary . compared to a decision hyperplane , if you have to place a fat separator between classes , you have fewer choices of where it can be put . as a result of this , the memory capacity of the model has been decreased , and hence we expect that its ability to correctly generalize to test-data is increased (cf. the discussion of the bias-variance-tradeoff in chapter 14 , page 14.6) . let us formalize an svm with algebra . a decision hyperplane (page 14.4) can be defined by an intercept term and a decision hyperplane normal-vector which is perpendicular to the hyperplane . this vector is commonly referred to in the machine-learning literature as the weight-vector . to choose among all the hyperplanes that are perpendicular to the normal-vector , we specify the intercept term . because the hyperplane is perpendicular to the normal-vector , all points on the hyperplane satisfy . now suppose that we have a set of training-data points , where each member is a pair of a point and a class label corresponding to it.for svms , the two data classes are always named and (rather than 1 and 0) , and the intercept term is always explicitly represented as (rather than being folded into the weight-vector by adding an extra always-on feature) . the math works out much more cleanly if you do things this way , as we will see almost immediately in the definition of functional margin . the linear classifier is then : (165) we are confident in the classification of a point if it is far away from the decision-boundary . for a given data-set and decision hyperplane , we define the functional margin of the example with respect to a hyperplane as the quantity . the functional margin of a data-set with respect to a decision surface is then twice the functional margin of any of the points in the data-set with minimal functional margin (the factor of 2 comes from measuring across the whole width of the margin , as in figure 15.3) . however , there is a problem with using this definition as is : the value is underconstrained , because we can always make the functional margin as big as we wish by simply scaling up and . for example , if we replace by and by then the functional margin is five times as large . this suggests that we need to place some constraint on the size of the vector . to get a sense of how to do that , let us look at the actual geometry . figure 15.3 : the geometric margin of a point (-rrb- and a decision-boundary (-rrb- . what is the euclidean-distance from a point to the decision-boundary ? in figure 15.3 , we denote by this distance . we know that the shortest-distance between a point and a hyperplane is perpendicular to the plane , and hence , parallel to . a unit vector in this direction is . the dotted line in the diagram is then a translation of the vector . let us label the point on the hyperplane closest to as . then : (166) (167) (168) geometric margin 168 15.2 6 since we can scale the functional margin as we please , for convenience in solving large svms , let us choose to require that the functional margin of all data points is at least 1 and that it is equal to 1 for at least one data vector . that is , for all items in the data : (169) is maximized for all , we are now optimizing a quadratic function subject to linear-constraints . quadratic-optimization problems are a standard , well-known class of mathematical-optimization problems , and many algorithms exist for solving them . we could in principle build our svm using standard quadratic-programming (qp) libraries , but there has been much recent research in this area aiming to exploit the structure of the kind of qp that emerges from an svm . as a result , there are more intricate but much faster and more scalable libraries available especially for building svms , which almost everyone uses to build models . we will not present the details of such algorithms here . however , it will be helpful to what follows to understand the shape of the solution of such an optimization-problem . the solution involves constructing a dual problem where a lagrange-multiplier is associated with each constraint in the primal problem : the solution is then of the form : in the solution , most of the are zero . each non-zero indicates that the corresponding is a support-vector . the classification function is then : (170) dot-product to recap , we start with a training-data-set . the data-set uniquely defines the best separating hyperplane , and we feed the data through a quadratic-optimization procedure to find this plane . given a new point to classify , the classification function in either equation 165 or equation 170 is computing the projection of the point onto the hyperplane normal . the sign of this function determines the class to assign to the point . if the point is within the margin of the classifier (or another confidence threshold that we might have determined to minimize classification mistakes) then the classifier can return `` do n't know '' rather than one of the two classes . the value of may also be transformed into a probability of classification ; fitting a sigmoid to transform the values is standard (platt , 2000) . also , since the margin is constant , if the model includes dimensions from various sources , careful rescaling of some dimensions may be required . however , this is not a problem if our documents (points) are on the unit hypersphere . figure 15.4 : a tiny 3 data point training-set for an svm . worked example . consider building an svm over the (very little) data-set shown in figure 15.4 . working geometrically , for an example like this , the maximum-margin weight-vector will be parallel to the shortest line connecting points of the two classes , that is , the line between and , giving a weight-vector of . the optimal decision surface is orthogonal to that line and intersects it at the halfway point . therefore , it passes through . so , the svm decision-boundary is : (171) working algebraically , with the standard constraint that , we seek to minimize . this happens when this constraint is satisfied with equality by the two support-vectors . further we know that the solution is for some . so we have that : the margin is . this answer can be confirmed geometrically by examining figure 15.4 . end worked example . exercises . what is the minimum number of support-vectors that there can be for a data-set (which contains instances of each class) ? the basis of being able to use kernels in svms (see section 15.2.3) is that the classification function can be written in the form of equation 170 (where , for large problems , most are 0) . show explicitly how the classification function could be written in this form for the data-set from small-svm-eg . that is , write as a function where the data points appear and the only variable is . install an svm package such as svmlight (http://svmlight.joachims.org/) , and build an svm for the data-set discussed in small-svm-eg . confirm that the program gives the same solution as the text . for svmlight , or another package that accepts the same training-data format , the training file would be : 1 1:2 2:3 1 1:2 2:0 1 1:1 2:1 the training command for svmlight is then : svm_learn - c 1 - a alphas.dat train.dat model.dat the - c 1 option is needed to turn off use of the slack variables that we discuss in section 15.2.1 . check that the norm of the weight-vector agrees with what we found in small-svm-eg . examine the file alphas.dat which contains the values , and check that they agree with your answers in exercise 15.1 .
tokenization given a character-sequence and a defined document unit , tokenization is the task of chopping it up into pieces , called tokens , perhaps at the same time throwing away certain characters , such as punctuation . here is an example of tokenization : input : friends , romans , countrymen , lend me your ears ; output : token type term 2.2.3 2.2.2 sleep perchance dream the major question of the tokenization phase is what are the correct tokens to use ? in this example , it looks fairly trivial : you chop on whitespace and throw away punctuation characters . this is a starting point , but even for english there are a number of tricky cases . for example , what do you do about the various uses of the apostrophe for possession and contractions ? mr. o'neill thinks that the boys ' stories about chile 's capital are n't amusing . o'neill ? are n't ? these issues of tokenization are language-specific . it thus requires the language of the document to be known . language-identification based on classifiers that use short character subsequences as features is highly effective ; most languages have distinctive signature patterns (see page 2.5 for references) . for most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms , such as the programming-languages c++ and c# , aircraft names like b-52 , or a t.v. show name such as m * a * s * h - which is sufficiently integrated into popular-culture that you find usages such as m * a * s * h-style hospitals . computer-technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token , including email addresses (jblack@mail.yahoo.com) , web urls (http://stuff.big.com/new/specials.html) , numeric ip-addresses (142.32.48.231) , package tracking numbers (1z9999w99845399981) , and more . one possible solution is to omit from indexing tokens such as monetary-amounts , numbers , and urls , since their presence greatly expands the size of the vocabulary . however , this comes at a large cost in restricting what people can search for . for instance , people might want to search in a bug-database for the line number where an error occurs . items such as the date of an email , which have a clear semantic-type , are often indexed separately as document-metadata parametricsection . in english , hyphenation is used for various purposes ranging from splitting up vowels in words (co-education) to joining nouns as names (hewlett-packard) to a copyediting device to show word-grouping (the hold-him-back-and-drag-him-away maneuver) . it is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation) , the last should be separated into words , and that the middle case is unclear . handling hyphens automatically can thus be complex : it can either be done as a classification-problem , or more commonly by some heuristic-rules , such as allowing short hyphenated prefixes on words , but not longer hyphenated forms . conceptually , splitting on white-space can also split what should be regarded as a single token . this occurs most commonly with names (san francisco , los angeles) but also with borrowed foreign phrases (au fait) and compounds that are sometimes written as a single word and sometimes space separated (such as white-space vs. whitespace) . other cases with internal spaces that we might wish to regard as a single token include phone numbers (-lrb-800-rrb-Â 234-2333) and dates (mar 11 , 1983) . splitting tokens on spaces can cause bad retrieval results , for example , if a search for york university mainly returns documents containing new york university . the problems of hyphens and non-separating whitespace can even interact . advertisements for air fares frequently contain items like san francisco-los angeles , where simply doing whitespace splitting would give unfortunate results . in such cases , issues of tokenization interact with handling phrase queries (which we discuss in section 2.4 (page)) , particularly if we would like-queries for all of lowercase , lower-case and lower case to return the same results . the last two can be handled by splitting on hyphens and using a phrase-index . getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way . one effective strategy in practice , which is used by some boolean-retrieval systems such as westlaw and lexis-nexis (westlaw) , is to encourage users to enter hyphens wherever they may be possible , and whenever there is a hyphenated form , the system will generalize the query to cover all three of the one word , hyphenated , and two word forms , so that a query for over-eager will search for over-eager or `` over eager '' or overeager . however , this strategy depends on user-training , since if you query using either of the other two forms , you get no generalization . each new language presents some new issues . for instance , french has a variant use of the apostrophe for a reduced definite article the before a word beginning with a vowel (e.g. , l'ensemble) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g. , donne-moi give me) . getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives : you would want documents mentioning both l'ensemble and un ensemble to be indexed under ensemble . other languages make the problem harder in new ways . german writes compound nouns without spaces (e.g. , computerlinguistik ` computational-linguistics ' ; lebensversicherungsgesellschaftsangestellter ` life insurance-company employee ') . retrieval-systems for german greatly benefit from the use of a compound-splitter module , which is usually implemented by seeing if a word can be subdivided into multiple words that appear in a vocabulary . this phenomenon reaches its limit case with major east asian-languages (e.g. , chinese , japanese , korean , and thai) , where text is written without any spaces between words . an example is shown in figure 2.3 . one approach here is to perform word-segmentation as prior linguistic-processing . methods of word-segmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine-learning sequence models , such as hidden-markov-models or conditional-random-fields , trained over hand-segmented words (see the references in section 2.5) . since there are multiple possible segmentations of character sequences (see figure 2.4) , all such methods make mistakes sometimes , and so you are never guaranteed a consistent unique tokenization . the other approach is to abandon word-based-indexing and to do all indexing via just short subsequences of characters (character - grams) , regardless of whether particular sequences cross word-boundaries or not . three reasons why this approach is appealing are that an individual chinese-character is more like a syllable than a letter and usually has some semantic-content , that most words are short (the commonest length is 2 characters) , and that , given the lack of standardization of word-breaking in the writing-system , it is not always clear where word-boundaries should be placed anyway . even in english , some cases of where to put word-boundaries are just orthographic conventions - think of notwithstanding vs. not to mention or into vs. on to - but people are educated to write the words with consistent use of spaces . the standard unsegmented form of chinese-text using the simplified characters of mainland china.there is no whitespace between words , not even between sentences - the apparent space after the chinese period (-rrb- is just a typographical illusion caused by placing the character on the left side of its square box . the first sentence is just words in chinese-characters with no spaces between them . the second and third sentences include arabic numerals and punctuation breaking up the chinese-characters . ambiguities in chinese-word segmentation.the two characters can be treated as one word-meaning ` monk ' or as a sequence of two words meaning ` and ' and ` still ' .
near-duplicates and shingling 19.5 duplication the simplest approach to detecting duplicates is to compute , for each web-page , a fingerprint that is a succinct (say 64-bit) digest of the characters on that page . then , whenever the fingerprints of two web-pages are equal , we test whether the pages themselves are equal and if so declare one of them to be a duplicate copy of the other . this simplistic approach fails to capture a crucial and widespread phenomenon on the web : near duplication . in many cases , the contents of one web-page are identical to those of another except for a few characters - say , a notation showing the date and time at which the page was last modified . even in such cases , we want to be able to declare the two pages to be close enough that we only index one copy . short of exhaustively comparing all pairs of web-pages , an infeasible task at the scale of billions of pages , how can we detect and filter out such near-duplicates ? we now describe a solution to the problem of detecting near-duplicate web pages . the answer lies in a technique known as shingling . given a positive integer and a sequence of terms in a document , define the - shingles of to be the set of all consecutive sequences of terms in . as an example , consider the following text : a rose is a rose is a rose . the 4-shingles for this text (is a typical value used in the detection of near-duplicate web pages) are a rose is a , rose is a rose and is a rose is . the first two of these shingles each occur twice in the text . intuitively , two documents are near-duplicates if the sets of shingles generated from them are nearly the same . we now make this intuition precise , then develop a method for efficiently computing and comparing the sets of shingles for all web-pages . let denote the set of shingles of document . recall the jaccard-coefficient from page 3.3.4 , which measures the degree of overlap between the sets and as ; denote this by . our test for near duplication between and is to compute this jaccard-coefficient ; if it exceeds a preset threshold (say ,) , we declare them near-duplicates and eliminate one from indexing . however , this does not appear to have simplified matters : we still have to compute jaccard coefficients pairwise . to avoid this , we use a form of hashing . first , we map every shingle into a hash value over a large space , say 64 bits . for , let be the corresponding set of 64-bit hash values derived from . we now invoke the following trick to detect document pairs whose sets have large jaccard overlaps . let be a random-permutation from the 64-bit integers to the 64-bit integers . denote by the set of permuted hash values in ; thus for each , there is a corresponding value . let be the smallest integer in . then theorem . (247) end theorem . proof . we give the proof in a slightly more general setting : consider a family of sets whose elements are drawn from a common universe . view the sets as columns of a matrix , with one row for each element in the universe . the element if element is present in the set that the th column represents . let be a random-permutation of the rows of ; denote by the column that results from applying to the th column . finally , let be the index of the first row in which the column has a . we then prove that for any two columns , (248) figure 19.9 : two sets and ; their jaccard-coefficient is . consider two columns as shown in figure 19.9 . the ordered pairs of entries of and partition the rows into four types : those with 0 's in both of these columns , those with a 0 in and a 1 in , those with a 1 in and a 0 in , and finally those with 1 's in both of these columns . indeed , the first four rows of figure 19.9 exemplify all of these four types of rows . denote by the number of rows with 0 's in both columns , the second , the third and the fourth . then , (249) 249 249 end proof . thus , our test for the jaccard-coefficient of the shingle sets is probabilistic : we compare the computed values from different documents . if a pair coincides , we have candidate near-duplicates . repeat the process independently for 200 random permutations (a choice suggested in the literature) . call the set of the 200 resulting values of the sketch of . we can then estimate the jaccard-coefficient for any pair of documents to be ; if this exceeds a preset threshold , we declare that and are similar . how can we quickly compute for all pairs ? indeed , how do we represent all pairs of documents that are similar , without incurring a blowup that is quadratic in the number of documents ? first , we use fingerprints to remove all but one copy of identical documents . we may also remove common html tags and integers from the shingle computation , to eliminate shingles that occur very commonly in documents without telling us anything about duplication . next we use a union-find algorithm to create clusters that contain documents that are similar . to do this , we must accomplish a crucial step : going from the set of sketches to the set of pairs such that and are similar . to this end , we compute the number of shingles in common for any pair of documents whose sketches have any members in common . we begin with the list sorted by pairs . for each , we can now generate all pairs for which is present in both their sketches . from these we can compute , for each pair with non-zero sketch overlap , a count of the number of values they have in common . by applying a preset threshold , we know which pairs have heavily overlapping sketches . for instance , if the threshold were 80 % , we would need the count to be at least 160 for any . as we identify such pairs , we run the union-find to group documents into near-duplicate `` syntactic clusters '' . this is essentially a variant of the single-link-clustering algorithm introduced in section 17.2 (page) . one final trick cuts down the space needed in the computation of for pairs , which in principle could still demand space quadratic in the number of documents . to remove from consideration those pairs whose sketches have few shingles in common , we preprocess the sketch for each document as follows : sort the in the sketch , then shingle this sorted sequence to generate a set of super-shingles for each document . if two documents have a super-shingle in common , we proceed to compute the precise value of . this again is a heuristic but can be highly effective in cutting down the number of pairs for which we accumulate the sketch overlap counts . exercises . web-search-engines a and b each crawl a random subset of the same size-of-the-web . some of the pages crawled are duplicates - exact textual copies of each other at different urls . assume that duplicates are distributed uniformly amongst the pages crawled by a and b. further , assume that a duplicate is a page that has exactly two copies - no pages have more than two copies . a indexes pages without duplicate-elimination whereas b indexes only one copy of each duplicate page . the two random subsets have the same size before duplicate-elimination . if , 45 % of a 's indexed urls are present in b 's index , while 50 % of b 's indexed urls are present in a 's index , what fraction of the-web consists of pages that do not have a duplicate ? instead of using the process depicted in figure 19.8 , consider instead the following process for estimating the jaccard-coefficient of the overlap between two sets and . we pick a random subset of the elements of the universe from which and are drawn ; this corresponds to picking a random subset of the rows of the-matrix in the proof . we exhaustively compute the jaccard-coefficient of these random subsets . why is this estimate an unbiased estimator of the jaccard-coefficient for and ? explain why this estimator would be very difficult to use in practice .
6.3.3 modeling-natural-language text is composed of symbols from a finite-alphabet . we can divide the symbols in two disjoint subsets : symbols that separate words and symbols that belong to words . it is well known that symbols are not uniformly distributed . if we consider just letters (a to z) , we observe that vowels are usually more frequent than most consonants . for example , in english , the letter v has the highest frequency . a simple model to generate text is the binomial-model in it , each symbol is generated with a certain probability . however , natural-language has a dependency on previous symbols . for example , in english , a letter ki ' can not appear after a letter v and vowels or certain consonants have a higher probability 146 text and multimedia languages and properties of occurring . therefore , the probability of a symbol depends on previous symbols . we can use a finite-context or markovian-model to reflect this dependency . the model can consider one , two , or more letters to generate the next symbol . if we use k letters , we say that it is a fc-order model (so the binomial-model is considered a 0-order model) . we can use these models taking words as symbols . for example , text generated by a 5-order model using the distribution of words in the bible might make sense (that is , it can be grammatically correct) , but will be different from the original . more complex-models include finite-state models (which define regular-languages) , and grammar models (which define context free and other languages) . however , finding the right grammar for natural-language is still a difficult open problem . the next issue is how the different words are distributed inside each document . an approximate model is zipf 's law [847 , 310] , which attempts to capture the distribution of the frequencies (that is , number of occurrences) of the words in the text . the rule states that the frequency of the i-ih most frequent word is l/ie times that of the most frequent word . this implies that in a text of n words with a vocabulary of v words , the z-th most frequent word appears n / {ie hy {9)) times , where hy {9) is the harmonic number of order 9 of v , defined as so that the sum of all frequencies is n . the left side of figure 6.2 illustrates the distribution of frequencies considering that the words are arranged in decreasing order of their frequencies . the value of 6 depends on the text . in the most simple formulation , 0 = 1 , and therefore hy {9) = o (logn) . however , this simplified version is very inexact , and the case 9 gt ; 1 (more precisely , between 1.5 and 2.0) fits better the real-data [26] . this case is very different , since the distribution is much more skewed , and hy {6) = 0 (1) . experimental-data suggests that a better model is fc / (c-h #) * where c is an additional parameter and k is such that all frequencies add to n . this is called a mandelbrot distribution [561] . since the distribution of words is very skewed (that is , there are a few hundred words which take up 50 % of the text) , words that are too frequent , such as stopwords , can be disregarded . a stopword is a word which does not carry meaning in natural-language and therefore can be ignored (that is , made not searchable) , such as 4a , ' ` the , ' ` by , ' etc. . fortunately the most frequent words are stopwords and therefore , half of the words appearing in a text do not need to be considered . this allows us , for instance , to significantly reduce the space-overhead of indices for natural-language-texts . for example , the most frequent words in the trec-2 collection (see chapter 3 for details on this reference collection and others) are `` the ,1 fcof , ' ` and ,1 `` a ,1 `` to1 and in1 (see also chapter 7) . another issue is the distribution of words in the documents of a collection . a simple model is to consider that each word appears the same number of times in every document . however , this is not true in practice . a better model is text 147 words text size figure 6.2 distribution of sorted word-frequencies (left) and size of the vocabulary (right) . to consider a negative-binomial distribution , which says that the fraction of documents containing a word k times is a + k-l k - a-k where p and a are parameters that depend on the word and the document collection . for example , for the brown-corpus [276] and the word * said \ we have p = 9.24 and a = 0.42 [171] . the latter reference gives other models derived from a poisson-distribution . the next issue is the number of distinct words in a document . this set of words is referred to as the document vocabulary . to predict the growth of the vocabulary-size in natural-language-text , we use the so-called heaps ' law [352] . this is a very precise law which states that the vocabulary of a text of size n words is of size v = kn√¶ = 0 {np) , where k and / 3 depend on the particular text . the right side of figure 6.2 illustrates how the vocabulary-size varies with the text size . k is normally between 10 and 100 , and f3 is a positive value less than one . some experiments [26 , 42] on the trec-2 collection show that the most common values for / 3 are between 0.4 and 0.6 . hence , the vocabulary of a text grows sublinearly with the text size , in a proportion close to its square-root . notice that the set of different words of a language is fixed by a constant (for example , the number of different english words is finite) . however , the limit is so high that it is much more accurate to assume that the size of the vocabulary is o (n) instead of o (l) , although the number should stabilize for huge enough texts . on the other hand , many authors argue that the number keeps growing anyway because of typing or spelling errors . heaps ' law also applies to collections of documents because , as the total text size grows , the predictions of the model become more accurate . furthermore , this model is also valid for the world-wide-web (see chapter 13) . the last issue is the average length of words . this relates the text size in 148 text and multimedia languages and properties words with the text size in bytes (without accounting for punctuation and other extra symbols) . for example , in the different subcollections of the trec-2 collection , the average word length is very close to 5 letters , and the range of variation of this average in each subcollection is small (from 4.8 to 5.3 letters) . if we remove the stopwords , the average length of a word increases to a number between 6 and 7 (letters) . if we take only the words of the vocabulary , the average length is higher (about 8 or 9) . this defines the total space needed for the vocabulary . heaps ' law implies that the length of the words in the vocabulary increases logarithmically with the text size and thus , that longer and longer words should appear as the text grows . however , in practice , the average length of the words in the overall text is constant because shorter words are common enough (e.g. stop-words) . this balance between short and long words , such that the average word length remains constant , has been noticed many times in different contexts , and can also be explained by a finite-state model in which : (a) the space character has probability close to 0.2 ; (b) the space character can not appear twice subsequently ; and (c) there are 26 letters [561] . this simple model is consistent with zipf 's and heaps ' laws . the models presented in this section are used in chapters 8 and 13 , in particular zipf 's and heaps ' laws .
3.1 introduction before the final implementation of an information-retrieval-system , an evaluation of the system is usually carried out . the type of evaluation to be considered depends on the objectives of the retrieval-system . clearly , any software-system has to provide the functionality it was conceived for . thus , the first type of evaluation which should be considered is a functional-analysis in which the specified system functionalities are tested one by one . such an analysis should also include an error-analysis phase in which , instead of looking for functionalities , one behaves erratically trying to make the system fail . it is a simple procedure which can be quite useful for catching programming-errors . given that the system has passed the functional-analysis phase , one should proceed to evaluate the performance of the system . the most common measures of system-performance are time and space . the shorter the response-time , the smaller the space used , the better the system is considered to be . there is an inherent tradeoff between space-complexity and time-complexity which frequently allows trading one for the other . in chapter 8 we discuss this issue in detail . in a system designed for providing data-retrieval , the response-time and the space required are usually the metrics of most interest and the ones normally adopted for evaluating the system . in this case , we look for the performance of the indexing-structures (which are in place to accelerate the search) , the interaction with the operating-system , the delays in communication-channels , and the overheads introduced by the many software-layers which are usually present . we refer to such a form of evaluation simply as performance-evaluation . in a system designed for providing information-retrieval , other metrics , besides time and space , are also of interest . in fact , since the user-query request is inherently vague , the retrieved documents are not exact answers and have to be ranked according to their relevance to the query . such relevance-ranking introduces a component which is not present in data-retrieval systems and which plays a central role in information-retrieval . thus , information-retrieval-systems require the evaluation of how precise is the answer set . this type of evaluation is referred to as retrieval-performance evaluation . 74 retrieval-evaluation in this chapter , we discuss retrieval-performance evaluation for information-retrieval-systems . such an evaluation is usually based on a test-reference-collection and on an evaluation-measure . the test-reference-collection consists of a collection of documents , a set of example information requests , and a set of relevant documents (provided by specialists) for each example information request . given a retrieval-strategy 5 , the evaluation-measure quantifies (for each example information request) the similarity between the set of documents retrieved by s and the set of relevant documents provided by the specialists . this provides an estimation of the goodness of the retrieval-strategy s . in our discussion , we first cover the two most used retrieval-evaluation measures : recall and precision . we also cover alternative evaluation-measures such as the e measure , the harmonic mean , satisfaction , frustration , etc. . following that , we cover four test reference collections namely , tipster/trec , cacm , cisi , and cystic-fibrosis .
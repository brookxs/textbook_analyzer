crawling seed set 4 5 url-frontier 19 this seemingly simple recursive traversal of the-web graph is complicated by the many demands on a practical web-crawling system : the crawler has to be distributed , scalable , efficient , polite , robust and extensible while fetching pages of high quality . we examine the effects of each of these issues . our treatment follows the design of the mercator crawler that has formed the basis of a number of research and commercial crawlers . as a reference-point , fetching a billion pages (a small fraction of the static web at present) in a month-long crawl requires fetching several hundred pages each second . we will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate . before proceeding to this detailed description , we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy : only one connection should be open to any given host at a time . a waiting time of a few seconds should occur between successive requests to a host . politeness restrictions detailed in section 20.2.1 should be obeyed . subsections crawler architecture distributing the crawler dns resolution the url-frontier
2.1.9 multimedia queries the user-interface becomes far more complex with the introduction of the availability of multimedia items . all of the previous discussions still apply for search of the textual portions of a multimedia-database . but in addition , the user has to be able to specify search-terms for the other modalities . the current systems only focus on specification of still-images as another search criteria . the still-image could be used to search images that are part of an item . they also could be used to locate a specific scene in a video product . as described later , in the video modality , scene changes are extracted to represent changes in the information-presentation . the scene changes are represented as a series of images . additionally , where there is static text in the video , the current technology allows for ocring the text (e.g. , in the latest release of the virage system) . the ability to search for audio as a match makes less sense as a user specification . to adequately perform the search you would have to simulate the audio segment and then look for a match . instead audio sources are converted to searchable text via audio-transcription . this allows queries to be applied to the text . but , like optical character reading (ocr) output , the transcribed audio will contain many errors 38 chapter 2 (accuracies of 85-90 % are the best that can be achieved from news broadcasts , conversational-speech is in the range of 60 %) . thus the search-algorithms must allow for errors in the data . the errors are very different compared to ocr . ocr-errors will usually create a text string that is not a valid word . in automatic-speech-recognition (asr) , all errors are other valid words since asr selects entries only from a dictionary of words . audio also allows the user to search on specific speakers , since speaker-identification is relatively accurate against audio sources . the correlation between different parts of a query against different modalities is usually based upon time or location . the most common example would be on time . for example if a video news program has been indexed , the user could have access to the scene changes , the transcribed audio , the closed captioning and the index-terms that a user has assigned while displaying the video . the query could be `` find where bill clinton is discussing cuban refugees and there is a picture of a boat '' . all of the separate tracks of information are correlated on a time basis . the system would return those locations where bill clinton is identified as the speaker (user the audio-track and speaker-identification) , where in any of the text-streams (ocred text from the video , transcribed audio , closed captioning , or index-terms) there is discussion of refugees and cuba , and finally during that time segment there is at least one scene-change that includes a boat .
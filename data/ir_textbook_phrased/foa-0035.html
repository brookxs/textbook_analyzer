3.1 microscopic semantics and the statistics of communication in the last chapter , we described the foa process linguistically , in terms of words that occur in documents , morphological-features of these words , structures organizing the sentences of documents , etc. . we now want to treat all of these words - which have meaning to their authors and to us reading them - as a meaningless stream of data : word after word after word . (imagine it coming from some seti radio telescope , eavesdropping on the communication of some other planet !) we will now seek patterns and trends common to this data , using the same sorts of statistical tricks that physicists typically use on their data-streams . what 60 weighting and matching against indices 61 table 3.1 english letter frequencies letter frequency letter frequency e .120 f .024 t .085 m .024 a .077 w .022 i .076 y .022 n .067 p .020 o .067 b .017 s .067 g .017 r .059 v .012 h .050 k .007 d .042 q .005 l .042 j .004 u .037 x .004 c .032 z .002 can we learn from looking at the statistics of our data-stream , treating text as meaningless and attempting ; to infer a new notion of meaning from those statistics ? but now let 's narrow our focus , all the way down to the bits and characters used to represent the corpus as , for example , a file on a physical-device , like a hard-disk . imagine that you are an archaeologist trying to study some civilization that left this evidence behind . how might you interpret this modern rosetta stone ? let 's ignore those issues relating to basic ascii encoding . that is , suppose we have special knowledge of a character-set . then the frequency of these characters ' occurrences would already give us a great deal of information . anyone who has studied simple cipher techniques (or played scrabble) knows that a table of most frequently used letters (cf. table 3.1 [welsh , 1988]) can be used to break simple codes . in this chapter we will move another level above characters . we will consider morphological transformations we can perform on character sequences that help us to identify root words . we will briefly mention phrases by which multiple words can be joined into simple phrasal units . at each level we will ask very similar questions : what is our unit-of-analysis ; i.e. , what are we counting ? then , what does the distribution of frequency occurrences across this level of features tell us about the pattern of their use ? what can we tell about the meaning of these features , based on such statistics [francis and kucera , 1982] ? 62 finding out about in fact , many influential thinkers have looked at such patterns among symbols . going back to some of the most ancient writings suggests that statistical analyses of the original hebrew characters and their positions within the two-dimensional array of the page reveals new `` codes '' [witztum et al , 1994 ; drosnin , 1997] . donald knuth , one of computer-science 's most renowned theoreticians , has analyzed an apparently random verse (chapter 3 , verse 16) from 59 of the bible 's books and used these verses as the basis of stratified-sampling of the approximately 30,000 biblical verses [knuth , 1990] . he found , for example , that the 3:16 verses were particularly rich in occurrences of yhwh , the ancient hebrew name for god . personally , knuth considered this analysis the source for `` historical and spiritual insights , '' as part of a bible study class he led . but speaking scientifically , how can we find meaning in text , and how are such attempts to be distinguished from the kinds of `` ouija board '' numerology criticized by menaud in the quotation opening this chapter ?
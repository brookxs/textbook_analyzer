classification with more than two classes classification for classes that are not mutually exclusive is called any-of , multilabel , or multivalue classification . in this case , a document can belong to several classes simultaneously , or to a single class , or to none of the classes . a decision on one class leaves all options open for the others . it is sometimes said that the classes are independent of each other , but this is misleading since the classes are rarely statistically independent in the sense defined on page 13.5.2 . in terms of the formal-definition of the classification-problem in equation 112 (page 112) , we learn different classifiers in any-of classification , each returning either or : . solving an any-of classification-task with linear-classifiers is straightforward : build a classifier for each class , where the training-set consists of the set of documents in the class (positive labels) and its complement (negative labels) . given the test document , apply each classifier separately . the decision of one classifier has no influence on the decisions of the other classifiers . the second type of classification with more than two classes is one-of classification . here , the classes are mutually exclusive . each document must belong to exactly one of the classes . one-of classification is also called multinomial , polytomous , multiclass , or single-label classification . formally , there is a single classification function in one-of classification whose range is , i.e. , . knn is a (nonlinear) one-of classifier . true one-of problems are less common in text-classification than any-of problems . with classes like uk , china , poultry , or coffee , a document can be relevant to many topics simultaneously - as when the prime minister of the uk visits china to talk about the coffee and poultry trade . nevertheless , we will often make a one-of assumption , as we did in figure 14.1 , even if classes are not really mutually exclusive . for the classification-problem of identifying the language of a document , the one-of assumption is a good approximation as most text is written in only one language . in such cases , imposing a one-of constraint can increase the classifier 's effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated . figure 14.12 : hyperplanes do not divide space into disjoint regions . 14.12 build a classifier for each class , where the training-set consists of the set of documents in the class (positive labels) and its complement (negative labels) . given the test document , apply each classifier separately . assign the document to the class with the maximum score , the maximum-confidence value , or the maximum probability . assigned class money-fx trade interest wheat corn grain true class money-fx 95 0 10 0 0 0 trade 1 1 90 0 1 0 interest 13 0 0 0 0 0 wheat 0 0 1 34 3 7 corn 1 0 2 13 26 5 grain 0 0 2 14 5 10 a confusion-matrix for reuters-21578 . for example , 14 documents from grain were incorrectly assigned to wheat . adapted from picca et al. (2006) . an important tool for analyzing the performance of a classifier for classes is the confusion-matrix . the confusion-matrix shows for each pair of classes , how many documents from were incorrectly assigned to . in table 14.5 , the classifier manages to distinguish the three financial classes money-fx , trade , and interest from the three agricultural classes wheat , corn , and grain , but makes many errors within these two groups . the confusion-matrix can help pinpoint opportunities for improving the accuracy of the system . for example , to address the second largest error in table 14.5 (14 in the row grain) , one could attempt to introduce features that distinguish wheat documents from grain documents . exercises . create a training-set of 300 documents , 100 each from three different languages (e.g. , english , french , spanish) . create a test-set by the same procedure , but also add 100 documents from a fourth language . train (i) a one-of classifier (ii) an any-of classifier on this training-set and evaluate it on the test-set . (iii) are there any interesting differences in how the two classifiers behave on this task ?
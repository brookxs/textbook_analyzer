73 learning which documents to route arguably , the first use of relevance-feedback information in an adaptive ir context came out of the smart group led by gerald salton . as was discussed in section 4.2.2 , salton 's vector-space-model lends itself to a representation of documents and queries that suggest ways of producing better matches , as shown in figure 7.4 . 264 finding out about figure 7.4 document modifications due to relevance-feedback rocchio 's characterization of the task is best described as routing : we imagine that a stream of all the world 's news is available to some institution (for example , a corporation) , and many members of this institution (employees) are expected to build long-standing queries characterizing their interests . these queries are passed against the incoming stream of documents , and those matching a particular employee 's query are routed to him or her [schutze et al. , 1995a] . from this employee 's point-of-view , he or she will be receiving a stream of documents , all of which are of interest to him or her . if he or she identifies those documents that are not relevant , the resulting corpus can be viewed as a training-set and used to adjust the filter . this is a concrete application of the binary classification technology . brauen , in particular , considered `` document-vector modifications , '' resulting in `` dynamic '' document spaces [brauen , 1969] . documents marked as relevant can be moved closer to the query that successfully retrieved them in several ways . first and most obviously , features shared by query and document can have their weight increased . features in the document but not in the query can have their weight decreased , and terms in the query but not in the document can be added to the document 's representation with small initial weights . from the perspective of modern machine-learning , brauen 's method would be considered a type of gradient-descent learning-algorithm : adaptive-information-retrieval 265 the disparity between document and query creates a gradient along which small changes are made . one difference , however , is that brauen imagined a batch-learning scenario , with the entire set of labeled documents available simultaneously . online-learning algorithms , on the other hand , make small , incremental adaptive-changes in immediate response to every learning instance (document) . each individual step of learning (e.g. , weight update in neural-networks) must be very small , in order to guarantee convergence toward the globally optimal value . the size of the small-change is controlled by 77 , typically known as the learning-rate . stochastic-approximation theory suggests that this constant should be relatively small for online-learning so that the weights are small enough to allow convergence [white , 1989] . online-learning is generally preferred to avoid time-delay and data-collection complications . idealizing our learning-task to produce a perfect match (i.e. , the dot-product of query and document is 1.0) on relevant documents and no match on irrelevant documents , we can treat this behavior as the target for our error-correction learning . let r stand for this boolean relevant/not relevant classification of each document . then (as discussed further in section 7.4) , we can hope to have available a training-set t of documents that have been labeled a priori as r ^ = 0 or r = 1gt ; specifying whether they are considered relevant . with such evidence as to what documents we do and do n't consider relevant , we can define precisely how well we have learned . a typical error measure or loss-function can be defined as the squared difference between the actual vector match and the target rd : , . , r wj n \ 2 a c \ gradient runs in error = ^ (d-q-rd) 2 (7.5) * oth directions det
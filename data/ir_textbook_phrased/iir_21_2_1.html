markov-chains discrete-time stochastic-process : states a markov-chain is characterized by an transition-probability matrix each of whose entries is in the interval ; the entries in each row of add up to 1 . the markov chain can be in one of the states at any given time-step ; then , the entry tells us the probability that the state at the next time-step is , conditioned on the current-state being . each entry is known as a transition-probability and depends only on the current-state ; this is known as the markov property . thus , by the markov property , (251) (252) 252 stochastic matrix principal left eigenvector in a markov-chain , the probability distribution of next states for a markov-chain depends only on the current-state , and not on how the markov chain arrived at the current-state . figure 21.2 shows a simple markov-chain with three states . from the middle state a , we proceed with (equal) probabilities of 0.5 to either b or c. from either b or c , we proceed with probability 1 to a . the transition-probability matrix of this markov-chain is then (253) figure 21.2 : a simple markov-chain with three states ; the numbers on the links indicate the transition-probabilities . a markov-chain 's probability-distribution over its states may be viewed as a probability vector : a vector all of whose entries are in the interval , and the entries add up to 1 . an - dimensional probability vector each of whose components corresponds to one of the states of a markov-chain can be viewed as a probability-distribution over its states . for our simple markov-chain of figure 21.2 , the probability vector would have 3 components that sum to 1 . we can view a random surfer on the web-graph as a markov-chain , with one state for each web-page , and each transition-probability representing the probability of moving from one web-page to another . the teleport operation contributes to these transition-probabilities . the adjacency-matrix of the-web graph is defined as follows : if there is a hyperlink from page to page , then , otherwise . we can readily derive the transition-probability matrix for our markov-chain from the-matrix : if a row of has no 1 's , then replace each element by 1/n . for all other rows proceed as follows . divide each 1 in by the number of 1 's in its row . thus , if there is a row with three 1 's , then each of them is replaced by . multiply the resulting matrix by . add to every entry of the resulting matrix , to obtain . we can depict the probability distribution of the surfer 's position at any time by a probability vector . at the surfer may begin at a state whose corresponding entry in is 1 while all others are zero . by definition , the surfer 's distribution at is given by the probability vector ; at by , and so on . we will detail this process in section 21.2.2 . we can thus compute the surfer 's distribution over the states at any time , given only the initial distribution and the transition-probability matrix . if a markov-chain is allowed to run for many time steps , each state is visited at a (different) frequency that depends on the structure of the markov-chain . in our running analogy , the surfer visits certain web-pages (say , popular news home-pages) more often than other pages . we now make this intuition precise , establishing conditions under which such the visit frequency converges to fixed , steady-state quantity . following this , we set the pagerank of each node to this steady-state visit frequency and show how it can be computed . subsections definition :
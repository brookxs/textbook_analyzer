146 chapter 6 6.2.2.1 complete term-relation method in the complete term-relation method , the similarity between every term pair is calculated as a basis for determining the clusters . the easiest way to understand this approach is to consider the vector-model . the vector-model is represented by a matrix where the rows are individual items and the columns are the unique-words (processing tokens) in the items . the values in the-matrix represent how strongly that particular word represents concepts in the item . figure 6.2 provides an example of a database with 5 items and 8 terms . to determine the relationship between terms , a similarity-measure is required . the measure calculates the similarity between two terms . in chapter 7 a number of similarity-measures are presented . the similarity-measure is not critical terml term2 term3 term4 term5 term6 term7 term8 item 1 0 4 0 0 0 2 1 3 item 2 3 1 4 3 1 2 0 1 item 3 3 0 0 0 3 0 3 0 item 4 0 1 0 3 0 0 2 0 items 2 2 2 3 1 4 0 2 figure 6.2 vector example in understanding the methodology so the following simple measure is used : sim (term1 , termj) = e (termk , j) (termkj) where `` k '' is summed across the set of all items . in effect the formula takes the two columns of the two terms being analyzed , multiplying and accumulating the values in each row . the results can be paced in a resultant `` m '' by `` m '' matrix , called a term-term matrix (salton-83) , where `` m '' is the number of columns (terms) in the original matrix . this simple formula is reflexive so that the-matrix that is generated is symmetric . other similarity formulas could produce a non-symmetric matrix . using the data in figure 6.2 , the term-term matrix produced is shown in figure 6.3 . there are no values on the diagonal since that represents the autocorrelation of a word to itself . the next step is to select a threshold that determines if two terms are considered similar enough to each other to be in the same class . in this example , the threshold-value of so is used . thus two terms are considered similar if the similarity value between them is 10 or greater . this produces a new binary-matrix called the term-relationship matrix (figure 6.4) that defines which terms are similar . a one in the matrix indicates that the terms specified by the column and the row are similar enough to be in the same class . term 7 demonstrates that a term may exist on its own with no other similar terms document and term-clustering 147 identified . in any of the clustering processes described below this term will always migrate to a class by itself . the final step in creating clusters is to determine when two objects (words) are in the same cluster . there are many different algorithms available . the following algorithms are the most common : cliques , single link , stars and connected-components . terml term2 term3 term4 term5 term6 term 7 term8 term 1 7 16 15 14 14 9 7 term 2 7 8 12 3 18 6 17 term 3 16 8 18 6 16 0 8 term 4 15 12 18 6 18 6 9 term 5 14 3 6 6 6 9 3 term 6 14 18 16 18 6 2 16 term 7 9 6 0 6 9 2 3 term 8 7 17 8 9 3 16 3 figure 6.3 term-term matrix terml term2 term3 term4 terms term6 term7 term8 term 1 0 1 1 1 1 0 0 term 2 0 0 1 0 1 0 1 term 3 1 0 1 0 1 0 0 term 4 1 1 1 0 1 0 0 terms 1 0 0 0 0 0 0 term 6 1 1 1 1 0 0 1 term 7 0 0 0 0 0 0 0 term 8 0 1 0 0 0 1 0 figure 6.4 term-relationship matrix cliques require all items in a cluster to be within the threshold of all other items . the methodology to create the clusters using cliques is : 0 . let i = 1 1 . select term , and place it in a new class 2 . start with term ^ where r = k = i + 1 3 . validate if termk is within the threshold of all terms within the current class 4 . ifnotjetk = k + 1 5 . if k gt ; m (number of words) 148 chapter 6 then r = r + 1 if r = m then go to 6 else k = r create a new class with termj in it goto 3 else go to 3 6 . if current class only has term ; in it and there are other classes with term , in them then delete current class else i = i + 1 7 . ifi = m + 1 then go to 8 else go to 1 8 . eliminate any classes that duplicate or are subsets of other classes . applying the algorithm to figure 6.4 , the following classes are created : class 1 (term 1 , term 3 , term 4 , term 6) class 2 (term 1 , term 5) class 3 (term 2 , term 4 , term 6) class 4 (term 2 , term 6 , term 8) class 5 (term 7) notice that term 1 and term 6 are in more than one class . a characteristic of this approach is that terms can be found in multiple classes . in single link-clustering the strong constraint that every term in a class is similar to every other term is relaxed . the rule to generate single link-clusters is that any term that is similar to any term in the cluster can be added to the cluster . it is impossible for a term to be in two different clusters . this in effect partitions the set of terms into the clusters . the algorithm is : 1 . select a term that is not in a class and place it in a new class 2 . place in that class all other terms that are related to it 3 . for each term entered into the class , perform step 2 4 . when no new terms can be identified in step 2 , go to step 1 . applying the algorithm for creating clusters using single link to the term-relationship matrix , figure 6.4 , the following classes are created : class 1 (term 1 , term 3 , term 4 , term 5 , term 6 , term 2 , term8) class 2 (term 7) there are many other conditions that can be placed on the selection of terms to be clustered . the star technique selects a term and then places in the class all terms that are related to that term (i.e. , in effect a star with the selected document and term-clustering 149 term as the core) . terms not yet in classes are selected as new seeds until all terms are assigned to a class . there are many different classes that can be created using the star technique . if we always choose as the starting point for a class the lowest numbered term not already in a class , using figure 6.4 , the following classes are created : class 1 (term 1 , term 3 , term 4 , term 5 , term 6) class 2 (term 2 , term 4 , term 8 , term6) class 3 (term 7) this technique allows terms to be in multiple clusters (e.g. , term 4) . this could be eliminated by expanding the constraints to exclude any term that has already been selected for a previous cluster the string technique starts with a term and includes in the class one additional term that is similar to the term selected and not already in a class . the new term is then used as the new node and the process is repeated until no new terms can be added because the term being analyzed does not have another term related to it or the terms related to it are already in the class . a new class is started with any term not currently in any existing class . using the additional guidelines to select the lowest number term similar to the current term and not to select any term already in an existing class produces the following classes : {t5) figure 6Âª5 network diagram of term similarities 150 chapter 6 class 1 (term 1 , term 3 , term 4 , term 2 , term 6 , term 8) class 2 (term 5) class 3 (term 7) a technique to understand these different algorithms for generating classes is based upon a network diagram of the terms . each term is considered a node and arcs between the nodes indicate terms that are similar . a network diagram for figure 6.4 is given in figure 6 . 5 . to determine cliques , sub-networks are identified where all of the items are connected by arcs . from this diagram it is obvious that term 7 (t7) is in a class by itself and term 5 (t5) is in a class with term 1 (tl) . other common structures to look for are triangles and four sided polygons with diagonals . to find all classes for an item , it is necessary to find all subnetworks , where each subnetwork has the maximum number of nodes , that the term is contained . for term 1 (tl) , it is the subnetwork tl , t3 , t4 , and t6 . term 2 (t2) has two subnetworks : t2 , t4 , t6 and the subnetwork t2 , t6 , t8 . the network diagram provides a simple visual tool when there are a small number of nodes to identify classes using any of the other techniques . the clique technique produces classes that have the strongest relationships between all of the words in the class . this suggests that the class is more likely to be describing a particular concept . the clique algorithm produces more classes than the other techniques because the requirement for all terms to be similar to all other terms will reduce the number of terms in a class . this will require more classes to include all the terms . the single link technique partitions the terms into classes . it produces the fewest number of classes and the weakest relationship between terms (salton-72 , jones-71 , saiton-75) . it is possible using the single link algorithm that two terms that have a similarity value of zero will be in the same class . classes will not be associated with a concept but cover a diversity of concepts . the other techniques lie between these two extremes . the selection of the technique is also governed by the density of the term-relationship matrix and objectives of the thesaurus . when the term-relationship matrix is sparse (i.e. , contains a few number of ones) , then the constraint dependencies between terms need to be relaxed such as in single link to create classes with a reasonable number of items . if the-matrix is dense (i.e. , lots of ones implying relationships between many terms) , then the tighter constraints of the clique are needed so the number of items in a class does not become too large . cliques provide the highest precision when the statistical thesaurus is used for query-term expansion . the single link algorithm maximizes recall but can cause selection of many non-relevant items . the single link-assignment process has the least overhead in assignment of terms to classes , requiring o (rt) comparisons (croft-77)
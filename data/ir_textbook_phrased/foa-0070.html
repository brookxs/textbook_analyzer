4.33 traditional evaluation-methodologies before surveying all of the ways in which evaluation mighthe performed , it is worthwhile to sketch how it has typically been done in the past [cleverdon and mills , 1963] . in the beginning , computers were slow and had very limited disk space and even more limited memories ; initial test-corpora needed to be small , too . one benefit of these small corpora was that it allowed at least the possibility of having a set of test queries compared exhaustively against every document in the corpus . 120 finding out about the source of these test queries , and the assessment of their relevance , varied in early experiments . for example , in the cranfield experiments [lancaster , 1968] , 1400 documents in metallurgy were searched according to 221 queries generated by some of the documents ' authors . in salton 's experiments with the adi corpus , 82 papers presented at a 1963 american documentation institute meeting were searched against 35 queries and evaluated by students and `` staff experts '' associated with salton 's lab [salton and lesk , 1968] . lancaster 's construction of the medigital libraryars collection was similar [lancaster , 1969] . as computers have increased in capacity , reasonable evaluation has required much larger corpora . the text-retrieval-conference (trec) , begun in 1992 and still held annually , has set a new standard for search-engine-evaluation [harman , 1995] . the trec methodology is notable in several respects . first , it avoids exhaustive assessment of all documents by using the pooling method , a proposal for the construction of `` ideal '' test-collections that predates trec by decades [sparck jones and van rijsbergen , 1976] . the basic idea is to use each search-engine independently and then `` pool '' their results to form a set of those documents that have at least this recommendation of potential relevance . all search-engines retrieve ranked-lists of k potentially relevant documents , and the union of these retrieved sets is presented to human judges for relevance-assessment . in the case of trec , k = 100 and the human assessors were retired security analysts , like those that work at the national-security agency (nsa) watching the world 's communications . because only documents retrieved by one of the systems being tested are evaluated there remains the possibility that relevant documents remain undiscovered , and we might worry that our evaluations will change as new systems retrieve new documents and these are evaluated . recent analysis seems to suggest that , at least in the case of the trec-corpus , evaluations are in fact quite stable [voorhees , 1998] . an important consequence of this methodological convenience is that unassessed documents are assumed to be irrelevant this creates an unfortunate dependence on the retrieval-methods used to nominate documents , which we can expect to be most pronounced when the methods are similar to one another . for example , if the alternative retrieved sets are the result of manipulating single parameters of the same basic assessing the retrieval 121 retrieval-procedure , the resulting assessments may have overlap with , and hence be useless for comparison of , methods producing significantly different retrieval sets . for the trec collection , this problem was handled by drawing the top 200 documents from a wide range of 25 methods , which had little overlap [harman , 1995] . vogt [vogt and cottrell , 1998] has explored how similarities and differences between retrieval-methods can be similarly exploited as part of combined , hybrid retrieval-systems (cf. section 7.5.4) . it is also possible to sample a small subset of a corpus , submit the entire sample to review by the human expert , and extrapolate from the number of relevant documents found to an expected number across the entire corpus . one famous example of this methodology is blair and maron 's assessment of ibm 's stairs retrieval-system [blair and maron , 1985] of the early 1980s . this evaluation studied the real-world use of stairs by a legal firm as part of a litigation support task : 40,000 memos , design documents , etc. were to be searched with respect to 51 different queries . the lawyers themselves then agreed to evaluate the documents5 relevance . as they reported : to find the unretrieved relevant documents we developed sample frames consisting of subsets of unretrieved databases that we believed to be rich in relevant documents ___ random-samples were taken from these subsets , and the samples were examined by the lawyers in a blind-evaluation ; the lawyers were not aware they were evaluating sample sets rather than retrieved sets they had personally generated . the total number of relevant documents that existed in these subsets could then be estimated . we sampled from subsets of the database rather than the entire database because , for most queries , the percentage of relevant documents in the database was less than 2 % , making it almost impossible to have both manageable sample sizes and a high level of confidence in the resulting recall-estimates . of course , no extrapolation to the entire database could be made from these recall calculations . nonetheless , the estimation of the number of relevant unretrieved documents in the subsets did give us a maximum value for recall for each request . [blair and maron , 1985 , pp. 291-293] 122 finding out about high-recall retrieval ^ ï * ï '' * '' ' retrieved / / high-precision retrieval ** '' ^ ^ ó - v i \ corpus figure 4.9 relevant versus retrieved sets killing the messenger this is a difficult methodology , but it allows some of the best estimates of recall available . and the news was not good : on average , retrievals captured only 20 percent of relevant documents ^ in short , methodologies for valid search-engine evaluations require much more sophistication and care than is generally appreciated . careful experimental-design [tague-sutcliffe , 1992] , statistical-analysis [hull , 1993] , and presentation [keen , 1992] are all critical .
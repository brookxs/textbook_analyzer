1.5.2 computer-assisted indexing the field of library-science has studied the manual process of constructing effective indices for a very long time . this standard becomes a useful comparison against which our best automatic techniques can be compared , but it also demonstrates how difficult comparison will be . there are data , for example , that suggest that the capacity of one person (e.g. , theindexer) to anticipatethe words used by another person (e.g. , asecond indexer or the query of a subsequent user) is severely limited [furnas et al , 1987] ; we are all quite idiosyncratic in this regard . the lack of interindexer consistency among humans must make us humble in our expectations for automated techniques . but manual and automatic-indexing need not be viewed as competing alternatives . in economic terms , if we had sufficient resources , we could hire enough highly trained catalogers to carefully read every document in a corpus and index each of them . if we could n't afford this very expensive option , we would have to be satisfied with the best index our automatic-system could construct . but if we have enough resources overview 29 to hire one or two human indexers , what tools might we give them that would make the most effective use of their time ? we seek methods that leveragethe editorial resource , in the sense that this manual effort does not grow as the corpus does . how might editors and librarians guide an automatic-indexing process ? what information should this computation provide that would allow intelligent human readers the assurance of a high-quality indexing function ? chapter 7 will discuss ways that editors can train machine-learning systems , and a number of analyses that are of interest to editors will be mentioned , especially in chapter 6 .
4.4.2 n-gram data-structure as shown in figure 4.7 , an n-gram is a data-structure that ignores words and treats the input as a continuous-data , optionally limiting its processing by interword symbols . the data-structure consists of fixed length overlapping symbol segments that define the searchable processing tokens . these tokens have logical linkages to all the items in which the tokens are found . inversion lists , document-vectors (described in chapter 5) and other proprietary data-structures are used to store the linkage data-structure and are used in the search-process . in some cases just the least frequently occurring n-gram is kept as part of a first pass search-process (yochum-85) . examples of these implementations are found in chapter 5 . the choice of the fixed length word fragment size has been studied in many contexts . yochum and d'amore investigated the impacts of different values for `` n/1 fatah comlekoglu (comlekoglu-90) investigated n-gram data-structures using an inverted-file system for n = 2 to n = 26 . trigrams (n-grams of length 3) were determined to be the optimal length , trading-off information versus size of data-structure . the aquaintance system uses longer n-grams. , ignoring word chapter 4 boundaries . the advantage of n-grams is that they place a finite limit on the number of searchable tokens . maxsegn = (a ,) n the maximum number of unique n-grams that can be generated , maxseg , can be calculated as a function of n which is the length of the n-grams , and x which is the number of processable symbols from the alphabet (i.e. , non-interword symbols) . although there is a savings in the number of unique processing tokens and implementation-techniques allow for fast processing on minimally sized machines , false hits can occur under some architectures . for example , a system that uses trigrams and does not include interword symbols or the character position of the n-gram in an item finds an item containing `` retain detail '' when searching for `` retail '' (i.e. , all of the trigrams associated with `` retail '' are created in the processing of `` retain detail '') . inclusion of interword symbols would not have helped in this example . inclusion of character position of the n-gram would have discovered that the n-grams `` ret , '' `` eta , '' `` tai , '' `` ail '' that define `` retail '' are not all consecutively starting within one character of each other . the longer the n-gram , the less likely this type-error is to occur because of more information in the word fragment . but the longer the n-gram , the more it provides the same result as full word-data structures since most words are included within a single n-gram . another disadvantage of n-grams is the increased size of inversion lists (or other data-structures) that store the linkage data-structure . in effect , use of n-grams expands the number of processing tokens by a significant factor . the average word in the english-language is between six and seven characters in length . use of trigrams increases the number of processing tokens by a factor of five (see figure 4.7) if interword symbols are not included . thus the inversion lists increase by a factor of five . because of the processing token bounds of n-gram data-structures , optimized performance techniques can be applied in mapping items to an n-gram searchable structure and in query-processing . there is no semantic-meaning in a particular n-gram since it is a fragment of processing token and may not represent a concept . thus n-grams are a poor representation of concepts and their relationships . but the juxtaposition of n-grams can be used to equate to standard word indexing , achieving the same levels of recall and within 85 per cent precision levels with a significant improvement in performance (adams-92) . vector-representations of the n-grams from an item can be used to calculate the similarity between items .
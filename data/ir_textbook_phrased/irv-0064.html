selecting the best dependence-trees our problem now is to find a probability-function of the form pt (x) on a set of documents which is the best approximation to the true joint-probability function p (x) , and of course a better approximation than the one afforded by making assumption a1 * . the set on which the approximation is defined can be arbitrary , it might be the entire collection , the relevant documents (w1) , or the non-relevant documents (w2) . for the moment i shall leave the set unspecified , all three are important . however , when constructing a decision-rule similar to d4 we shall have to approximate p (x/w1) and p (x/w2) . the goodness of the approximation is measured by a well known function (see , for example , kullback [12]) ; if p (x) and pa (x) are two discrete probability distributions then * that this is indeed the case is shown by ku and kullback [11] . is a measure of the extent to which pa (x) approximates p (x) . in terms of this function we want to find a distribution of tree dependence pt (x) such that i (p , pt) is a minimum . or to put it differently to find the dependence tree among all dependence-trees which will make i (p , pt) as small as possible . if the extent to which two index-terms i and j deviate from independence is measured by the expected mutual-information measure (emim) (see chapter 3 , p 41) . then the best approximation pt (x) , in the sense of minimising i (p , pt) , is given by the maximum-spanning-tree (mst) (see chapter 3 , p. 56) on the variables x1 , x2 , ... , xn . the spanning-tree is derived from the graph whose nodes are the index-terms 1,2 , ... , n , and whose edges are weighted with i (xi , xj) . the mst is simply the tree spanning the nodes for which the total weight is a maximum . this is a highly condensed statement of how the dependence tree is arrived at , unfortunately a fuller statement would be rather technical . a detailed proof of the optimisation procedure can be found in chow and liu [13] . here we are mainly interested in the application of the tree structure . one way of looking at the mst is that it incorporates the most significant of the dependences between the variables subject to the global-constraint that the sum of them should be a maximum . for example , in figure 6.1 the links between the variables (nodes , x1 , ... , x6) have been put in just because the sum i (x1 , x2) + i (x2 , x3) + i (x2 , x4) + i (x2 , x5) + i (x5/x6) is a maximum . any other sum will be less than or equal to this sum . note that it does not mean that any individual weight associated with an edge in the tree will be greater than one not in the tree , although this will mostly be the case . once the dependence tree has been found the approximating distribution can be written down immediately in the form a2 . from this i can derive a discriminant function just as i did in the independent case . ti = prob (xi = 1/xj (i) = 1) ri = prob (xi = 1/xj (i) = 0) and r1 = prob (x1 = 1) p (xi / xj (i)) = [ti [xi] (1 - ti) [1] [- xi]] [xj (i) [-rsb- ri [xi] (1 - ri) [1] [- xi]] [1] [- xj (i)] then this is a non-linear weighting-function which will simplify to the one derived from a1 when the variables are assumed to be independent , that is , when ti = ri . the constant has the same interpretation in terms of prior-probabilities and loss-function . the complete decision function is of course g (x) = log p (x/w1) - log p (x/w2) which now involves the calculation (or estimation) of twice as many parameters as in the linear case . it is only the sum involving xj (i) which make this weighting-function different from the linear one , and it is this part which enables a retrieval-strategy to take into account the fact that xi depends on xj (i) . when using the weighting-function a document containing xj (i) , or both xi and xj (i) , will receive a contribution from that part of the weighting-function . it is easier to see how g (x) combines different weights for different terms if one looks at the weights contributed to g (x) for a given document x for different settings of a pair of variables xi , xj (i) . when xi = 1 and xj (i) = 0 the weight contributed is and similarly for the other three settings of xi and xj (i) . this shows how simple the non-linear weighting-function really is . for example , given a document in which i occurs but j (i) does not , then the weight contributed to g (x) is based on the ratio of two probabilities . the first is the probability of occurrence of i in the set of relevant documents given that j (i) does not occur , the second is the analogous probability computed on the non-relevant documents . on the basis of this ratio we decide how much evidence there is for assigning x to the relevant or non-relevant documents . it is important to remember at this point that the evidence for making the assignment is usually based on an estimate of the pair of probabilities .
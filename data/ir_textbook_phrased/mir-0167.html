9.2.2 mimd architectures mimd architectures offer a great deal of flexibility in how parallelism is defined and exploited to solve a problem . the simplest way in which a retrieval-system can exploit a mimd computer is through the use of multitasking . each of the processors in the parallel-computer runs a separate , independent search-engine . the search-engines do not cooperate to process individual queries , but they may share code libraries and data cached by the file-system or loaded into shared-memory . the submission of user queries to the search-engines is managed by a broker , which accepts search requests from the end-users and distributes the requests among the available search-engines . this is depicted in figure 9.1 . as more processors are added to the system , more search-engines may be run and more search requests may be processed in parallel , increasing the throughput of the system . note , however , that the response-time of individual queries remains unchanged . in spite of the simplicity of this approach , care must be taken to properly balance the hardware-resources on the system . in particular , as the number of processors grows , so must the number of disks and i/o channels . unless the entire retrieval index fits in main-memory , the search processes running on the different processors will perform i/o and compete for disk access . a bottleneck at the disk will be disastrous for performance and could eliminate the throughput gains anticipated from the addition of more processors . in addition to adding more disks to the computer , the system-administrator must properly distribute the index data over the disks . disk contention will remain as long as two search processes need to access index data stored on the same disk . at one extreme , replicating the entire index on each disk eliminates disk contention at the cost of increased storage requirements and update-complexity . alternatively , the system-administrator may partition-and-replicate index data across the disks according to profile-information ; heavily accessed data is replicated while less frequently accessed data is distributed randomly . yet another approach is to install a disk-array , or raid [165] , and let the operating-system handle partitioning the index . disk-arrays can provide low-latency and high-throughput disk access by striping files across many disks . 234 parallel and distributed ir user-query ^ user-query broker result result figure 9.1 parallel multitasking on a mimd machine . to move beyond multitasking and improve query response-time , the computation required to evaluate a single query must be partitioned into subtasks and distributed among the multiple processors , as shown in figure 9.2 . in this configuration the broker and search processes run in parallel on separate processors as before , but now they all cooperate to evaluate the same query . high level processing in this system proceeds as follows . the broker accepts a query from the end-user and distributes it among the search processes . each of the search processes then evaluates a portion of the query and transmits an intermediate-result back to the broker . finally , the broker combines the intermediate results into a final result for presentation to the end-user . since ir computation is typically characterized by a small amount of processing per datum applied to a large amount of data , how to partition the computation boils down to a question of how to partition the data . figure 9.3 presents a high level view of the data processed by typical search-algorithms (see chapter 8) . each row represents a document , djlt ; and each column represents an indexing item , kt . here , k {may be a term , phrase , concept , or a more abstract indexing item such as a dimension in an lsi vector or a bit in a document-signature . the entries in the matrix , wlnj , are (possibly binary) weights , indicating if and to what degree indexing item i is assigned to document j . the indexing item weights associated with a particular document form a vector , d3 = (u ` i , j. ... wtj) - during search , a query is also represented as a vector of indexing item weights , q = {iv \ ^ ___ tr ^ 9) , and the search-algorithm scores each document by applying a matching-function f (dj , lt ;[) = sim (dj , q) . this high level data-representation reveals two possible methods for partitioning the data . the first method , document-partitioning , slices the data-matrix horizontally , dividing the documents among the subtasks . the x documents in the collection are distributed across the p processors in the system . parallel ir 235 subquery / results user-query search-process result search-process search-process figure 9.2 partitioned parallel-processing on a mimd machine . indexing items k \ k2 ... ki ... kt d d \ 2 d2 u w \ i w2 1 ï ï ï wi 1 ïï ï iv x 2 ^ 2 2 ï ï ï ^ -72 2 ï ï ï wt 1 y ^ t 2 m , e ^ * ? n ^ 1 7 y ^ 2 j ï ' ï ^ i-ij '' * * t '' '' '' s ´ n figure 9.3 basic data elements processed by a search-algorithm . creating p subcollections of approximately n/p documents each . during query-processing , each parallel-process (one for each processor) evaluates the query on the subcollection of n/p documents assigned to it , and the results from each of the subcollections are combined into a final result list . the second method , term partitioning , slices the data-matrix vertically , dividing the indexing items among the p processors such that the evaluation-procedure for each document is spread over multiple processors in the system . below we consider both of these partitioning schemes for each of the three main index-structures . inverted-files we first discuss inverted-files for systems that employ document-partitioning . following that , we cover systems that employ term partitioning . tiiere are two approaches to document-partitioning in systems that use inverted-files , namely , logical document partitioning and physical document-partitioning . 236 parallel and distributed ir dictionary inverted list term / term / po pi pz p3 figure 9.4 extended dictionary-entry for document-partitioning . logical document partitioning in this case , the data-partitioning is done logically using essentially the same basic underlying inverted-file index as in the original sequential-algorithm (see chapter 8) . the inverted-file is extended to give each parallel-process (one for each processor) direct-access to that portion of the index related to the processor 's subcollection of documents . each term dictionary-entry is extended to include p pointers into the corresponding inverted list , where the j-th pointer indexes the block of document entries in the inverted list associated with the subcollection in the j-th processor . this is shown in figure 9.4 , where the dictionary-entry for term i contains four pointers into term fs inverted list , one for each parallel-process (p = 4) . when a query is submitted to the system , the broker (from figure 9.2) first ensures that the necessary term dictionary and inverted-file entries are loaded into shared-memory , where all of the parallel processes can access a single shared copy . the broker then initiates p parallel processes to evaluate the query . each process executes the same document scoring algorithm on its document subcollection , using the extended dictionary to access the appropriate entries in the inverted-file . since all of the index operations during query-processing are read-only , there is no lock-contention among the processes for access to the shared term dictionary and inverted-file . the search processes record document scores in a single shared array of document score accumulators and notify the broker when they have completed . updates to the accumulator array do not produce lock-contention either since the subcollections scored by the different search processes are mutually exclusive . after all of the search processes have finished , the broker sorts the array of document score accumulators and produces the final ranked list of documents . parallel ir 237 at inverted-file construction time , the indexing process for logically partitioned documents can exploit the parallel processors using a variant of the indexing-scheme described by brown [123] (see chapter 8) . first , the indexer partitions the documents among the processors . next , it assigns document identifiers such that all identifiers in partition i are less than all identifiers in partition i + 1 . the indexer then runs a separate indexing process on each processor in parallel each indexing process generates a batch of inverted-lists , sorted by indexing item . after all of the batches have been generated , a merge step is performed to create the final inverted-file . since the inverted-lists in each batch are sorted the same way , a binary heap-based priority-queue is used to assemble the inverted list components from each batch that correspond to the current indexing item . the components are concatenated in partition number order to produce a final inverted list and a dictionary-entry for the indexing item is created that includes the additional indexing pointers shown in figure 9.4 . physical document-partitioning in this second approach to document-partitioning , the documents are physically partitioned into separate , self-contained subcollections , one for each parallel-processor . each subcollection has its own inverted-file and the search processes share nothing during query-evaluation . when a query is submitted to the system , the broker distributes the query to all of the parallel-search processes . each parallel-search process evaluates the query on its portion of the document collection , producing a local , intermediate hit-list . the broker then collects the intermediate hit-lists from all of the parallel-search processes and merges them into a final hit-list . the p intermediate hit-lists can be merged efficiently using a binary heap-based priority-queue [188] . a priority-queue of n elements has the property that element i is greater than elements 2i and 2i 4-1 , where i ranges from 1 to n . a priority-queue is not fully sorted , but the maximal element is always immediately available (i.e. , in 6 (1) time) and can be extracted in o (logn) time . inserting an element into a priority-queue can be done in o (logra) time as well . to merge the intermediate hit-lists , a priority-queue of p elements is created with the first entry from each intermediate hit-list inserted into the queue in o (plog p) time . to generate the final (and global) hit-list with the top k retrieved documents (in a global-ranking) , k elements are extracted from the priority-queue . as each element is extracted from the priority-queue , the intermediate hit-list from which the element was originally drawn inserts a new element into the priority-queue . the p intermediate hit-lists can be merged into a final hit-list of a ^ elements in o ((p ^ k) log p) time . the merge procedure just described assumes that the parallel-search processes produce globally consistent document scores , i.e. , document scores that can be merged directly . depending on the ranking-algorithm in use , each parallel-search process may require global term statistics in order to produce globally consistent document scores . there are two basic approaches to collect information on global term statistics . the first approach is to compute global term statistics at indexing time and store these statistics with each of the subcollec238 parallel and distributed ir tions . the second approach is for the query-processing to proceed in two phases . during the first phase , the broker collects subcollection term statistics from each of the search processes and combines them into global term statistics . during the second phase , the broker distributes the query and global term statistics to the search processes and query-evaluation proceeds as before . the first solution offers better query-processing performance at the expense of more complex indexing , while the second solution allows subcollections to be built and maintained independently at the expense of doubling communication-costs during query-evaluation . to build the inverted-files for physically partitioned documents , each processor creates , in parallel , its own complete index corresponding to its document partition . if global collection statistics are stored in the separate term dictionaries , then a merge step must be performed that accumulates the global statistics for all of the partitions and distributes them to each of the partition dictionaries . logical document partitioning requires less communication than physical document-partitioning with similar parallelization , and so is likely to provide better overall performance . physical document-partitioning , on the other hand , offers more flexibility (e.g. , document partitions may be searched individually) and conversion of an existing ir-system into a parallel ir-system is simpler using physical document-partitioning . for either document-partitioning scheme , threads provide a convenient programming-paradigm for creating the search processes , controlling their operation , and communicating between them . threads are natively supported in some modern programming-languages (e.g. , java [491]) and well supported in a standard way in others (e.g. , posix threads in c/c + +) . thread packages allow programmers to develop parallel-programs using high level abstractions of concurrent-execution , communication , and synchronization . the compiler and runtime-system then map these abstractions to efficient operating-system services and shared-memory operations . term partitioning when term partitioning is used with an inverted file-based system , a single inverted-file is created for the document collection (using the parallel construction-technique described above for logical document partitioning) and the inverted-lists are spread across the processors . during query-evaluation , the query is decomposed into indexing items and each indexing item is sent to the processor that holds the corresponding inverted list . the processors create hit-lists with partial document scores and return them to the broker . the broker then combines the hit-lists according to the semantics of the query . for boolean-queries , the hit-lists are unioned , intersected , or subtracted as appropriate . for ranked free-text queries , the hit-lists contain term scores that must be combined according to the semantics of the ranking formula . in comparison , document-partitioning affords simpler inverted-index construction and maintenance than term partitioning . their relative-performance during query-processing was shown by jeorig and omieiinski [404] to depend on term distributions . assuming each processor has its own i/o channel and disks , when term distributions in the documents and the queries are more skewed . parallel ir 239 document-partitioning performs better . when terms are uniformly distributed in user queries , term partitioning performs better . for instance , using trec-data , ribeiro-neto and barbosa [673 , 57] have shown that term partitioning might be twice as fast with long-queries and 5-10 times faster with very short (web-like) queries . suffix-arrays we can apply document-partitioning to suffix-arrays in a straightforward fashion . as with physical document-partitioning for inverted-files , the document collection is divided among the p processors and each partition is treated as an independent , self-contained collection . the system can then apply the suffix-array construction techniques described in chapter 8 to each of the partitions , with the enhancement that all of the partitions are indexed concurrently . during search , the broker broadcasts the query to all of the search processes , collects the intermediate results , and merges the intermediate results into a final hit-list . if all of the documents will be kept in a single collection , we can still exploit the parallel processors to reduce indexing time . an interesting property of the suffix-array construction algorithm for large texts (described in chapter 8) is that each of the merges of partial indices is independent . therefore all of the o ((n/m) 2) merges may be run in parallel on separate processors . after all merges are complete , the counters for each partial index must be accumulated and the final index merge may be performed . term partitioning for a suffix-array amounts to distributing a single suffix-array over multiple processors such that each processor is responsible for a lexicographical interval of the array . during query-processing , the broker distributes the query to the processors that contain the relevant portions of the suffix-array and merges the results . note that when searching the suffix-array , all of the processors require access to the entire text . on a single parallel-computer with shared-memory (e.g. , an smp system) , this is not a problem since the text may be cached in shared-memory . this may be a problem , however , if shared-memory is not available and communication-costs are high , as is the case in a distributed-system (e.g. , a network-of-workstations) . signature-files to implement document-partitioning in a system that uses signature-files , the documents are divided among the processors as before and each processor generates signatures for its document partition . at query time , the broker generates a signature for the query and distributes it to all of the parallel processors . each processor evaluates the query signature locally as if its document partition was a separate , self-contained collection . then the results axe sent to the broker , which combines them into a final hit-list for the user . for boolean-queries , the final result is simply a union of the results returned from each processor . for 240 parallel and distributed ir ranked-queries , the ranked hit-lists are merged as described above for inverted-file implementations . to apply term partitioning in a signature file-based system , we would have to use a bit-sliced signature-file [627] and partition the bit slices across the processors . the amount of sequential work required to merge the intermediate results from each of the processors and eliminate false drops , however , severely limits the speedup 5 available with this organization . accordingly , this organization is not recommended .
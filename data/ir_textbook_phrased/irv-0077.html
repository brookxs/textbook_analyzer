much effort and research has gone into solving the problem of evaluation of information-retrieval-systems . however , it is probably fair to say that most people active in the field of information-storage-and-retrieval still feel that the problem is far from solved . one may get an idea of the extent of the effort by looking at the numerous survey articles that have been published on the topic (see the regular chapter in the annual review on evaluation) . nevertheless , new approaches to evaluation are constantly being published (e.g. cooper [1] ; jardin and van rijsbergen [2] ; heine [3]) . in a book of this nature it will be impossible to cover all work to date about evaluation . instead i shall attempt to explicate the conventional , most commonly used method of evaluation , followed by a survey of the more promising attempts to improve on the older methods of evaluation . to put the problem of evaluation in perspective let me pose three questions : (1) why evaluate ? (2) what to evaluate ? (3) how to evaluate ? the answers to these questions pretty well cover the whole field of evaluation . there is much controversy about each and although i do not wish to add to the controversy i shall attempt an answer to each one in turn . the answer to the first question is mainly a social and economic one . the social part is fairly intangible , but mainly relates to the desire to put a measure on the benefits (or disadvantages) to be got from information-retrieval-systems . i use ` benefit ' here in a much wider sense than just the benefit accruing due to acquisition of relevant documents . for example , what benefit will users obtain (or what harm will be done) by replacing the traditional sources-of-information by a fully automatic and interactive-retrieval system ? studies to gauge this are going on but results are hard to interpret . for some kinds of retrieval-systems the benefit may be more easily measured than for others (compare statute or case-law-retrieval with document-retrieval) . the economic answer amounts to a statement of how much it is going to cost you to use one of these systems , and coupled with this is the question ` is it worth it ? ' . even a simple statement of cost is difficult to make . the computer costs may be easy to estimate , but the costs in terms of personal effort are much harder to ascertain . then whether it is worth it or not depends on the individual user . it should be apparent now that in evaluating an information-retrieval-system we are mainly concerned with providing data so that users can make a decision as to (1) whether they want such a system (social question) and (2) whether it will be worth it . furthermore , these methods of evaluation are used in a comparative way to measure whether certain changes will lead to an improvement in performance . in other words , when a claim is made for say a particular search-strategy , the yardstick of evaluation can be applied to determine whether the claim is a valid one . the second question (what to evaluate ?) boils down to what can we measure that will reflect the ability of the system to satisfy the user . since this book is mainly concerned with automatic document retrieval-systems i shall answer it in this context . in fact , as early as 1966 , cleverdon gave an answer to this . he listed six main measurable quantities : (1) the coverage of the collection , that is , the extent to which the system includes relevant matter ; (2) the time lag , that is , the average interval between the time the search request is made and the time an answer is given ; (3) the form of presentation of the output ; (4) the effort involved on the part of the user in obtaining answers to his search requests ; (5) the recall of the system , that is , the proportion of relevant material actually retrieved in answer to a search request ; (6) the precision of the system , that is , the proportion of retrieved material that is actually relevant . it is claimed that (1) - (4) are readily assessed . it is recall and precision which attempt to measure what is now known as the effectiveness of the retrieval-system . in other words it is a measure of the ability of the system to retrieve relevant documents while at the same time holding back non-relevant one . it is assumed that the more effective the system the more it will satisfy the user . it is also assumed that precision-and-recall are sufficient for the measurement of effectiveness . there has been much debate in the past as to whether precision-and-recall are in fact the appropriate quantities to use as measures-of-effectiveness . a popular alternative has been recall and fall-out (the proportion of non-relevant documents retrieved) . however , all the alternatives still require the determination of relevance in some way . the relationship between the various measures and their dependence on relevance will be made more explicit later . later in the chapter a theory of evaluation is presented based on precision-and-recall . the advantages of basing it on precision-and-recall are that they are : (1) the most commonly used pair ; (2) fairly well understood quantities . the final question (how to evaluate ?) has a large technical answer . in fact , most of the remainder of this chapter may be said to be concerned with this . it is interesting to note that the technique of measuring retrieval-effectiveness has been largely influenced by the particular retrieval-strategy adopted and the form of its output . for example , when the output is a ranking of documents an obvious parameter such as rank position is immediately available for control . using the rank position as cut-off , a series of precision recall values could then be calculated , one part for each cut-off value . the results could then be summarised in the form of a set of points joined by a smooth curve . the path along the curve would then have the immediate interpretation of varying effectiveness with the cut-off value . unfortunately , the kind of question this form of evaluation does not answer is , for example , how many queries did better than average and how many did worse ? nevertheless , we shall need to spend more time explaining this approach to the measurement of effectiveness since it is the most common approach and needs to be understood . before proceeding to the technical details relating to the measurement of effectiveness it is as well to examine more closely the concept of relevance which underlies it .
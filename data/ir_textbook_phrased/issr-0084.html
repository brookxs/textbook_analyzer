5.5 hypertext linkages a new class of information-representation , described in chapter 4 as the hypertext data structure , is evolving on the internet . hypertext data structures must be generated manually although user-interface-tools may simplify the process . very little research has been done on the information-retrieval aspects of hypertext linkages and automatic mechanisms to use the information of item pointers in creating additional search structures . in effect , hypertext linkages are creating an additional-information retrieval dimension . traditional items can be viewed as two dimensional constructs . the text of the items is one dimension representing the automatic-indexing 133 information in the items . imbedded references are a logical second dimension that has had minimal use in information-search techniques . the major use of the citations has been in trying to determine the concepts within an item and clustering items (salton-83) . hypertext , with its linkages to additional electronic items , can be viewed as networking between items that extends the contents . to understand the total subject of an item it is necessary to follow these additional-information concept paths . the imbedding of the linkage allows the user to go immediately to the linked item for additional-information . the issue is how to use this additional dimension to locate relevant-information . the easiest approach is to do nothing and let the user follow these paths to view items . but this is avoiding one of the challenges in information-systems on creating techniques to assist the user in finding relevant-information . looking at the-internet at the current time there are three classes of mechanisms to help find information : manually generated indexes , automatically generated indexes and web-crawlers (intelligent-agents) . yahoo (http://www.yahoo.com) is an example of the first case where information-sources (home-pages) are indexed manually into a hyperlinked hierarchy . the user can navigate through the hierarchy by expanding the hyperlink on a particular topic to see the more detailed subtopics . at some point the user starts to see the end items . lycos (http://www.lycos.com) and altavista (http://www.altavista.digital.com) automatically go out to other internet sites and return the text at the sites for automatic-indexing . lycos returns home-pages from each site for automatic-indexing while altavista indexes all of the text at a site . none of these approaches use the linkages in items to enhance their indexing . webcrawlers (e.g. , webcrawler , opentext , pathfinder) and intelligent-agents (coriolis groups ' netseeker√¥) are tools that allow a user to define items of interest and they automatically go to various sites on the internet-searching for the desired information . they are better described as a search-tool than an indexing tool that a priori analyzes items to assist in finding them via a search . what is needed is an index algorithm for items that looks at the hypertext linkages as an extension of the concepts being presented in the item where the link exists . some links that are for references to multi-media imbedded objects would not be part of the indexing process . the universal reference locator (url) hypertext-links can map to another item or to a specific location within an item . the current concept is defined by the information within proximity of the location of the link . the concepts in the linked item , or with a stronger weight the concepts in the proximity of the location included in the link , need to be included in the index of the current item . if the current item is discussing the financial state of louisiana and a hyperlink is included to a discussion on crop damage due to draughts in the southern states , the index should allow for a t4hif on a search statement including `` droughts in louisiana . '' one approach is to view the hyperlink as an extension of the text of the item in another dimension . the index values of the hyperlieked item has a reduced weighted value from contiguous iqxi biased by the type of linkage . the weight of processing tokens appears : 134 chapters weighty = (a * weighty + p * weightk , ,) * (y * linki , k) where weighty is the weight associated with processing token `` j '' in item `` i '' and processing token `` 1 '' in item `` k '' that are related via a hyperlink . link ^ is the weight associated with strength of the link . it could be a one-level link that is weak or strong , or it could be a multilevel transitive link , a , (3 and y are weighting/normalization factors . the values could be stored in an expanded index-structure or calculated dynamically if only the hyperlink relationships between items are available . taking another perspective , the system could automatically generate hyperlinks between items . attempts have been made to achieve this capability , but they suffer from working with static versus dynamic growing databases or ignoring the efficiency needed for an operational environment (allan-95 , furuta-89 , rearick-91) . kellog and subhas have proposed a new solution based upon document-segmentation and clustering (kellog-96) . they link at both the document and document sub-part level using the cover-coefficient based incremental-clustering method (c2icm) to generate links between the document (document sub-parts) pairs for each cluster . (can-95) . the automatic-link-generation phase is performed in parallel with the clustering phase . item pairs in the same cluster are candidates for hyperlinking (link-similarity) if they have a similarity above a given threshold . the process is completed in two phases . in the first phase the document seeds and an estimate of the number of clusters is calculated . in the second phase the items are clustered and the links are created . rather than storing the link-information within the item or storing a persistent link id within the item and the link-information externally , they store all of the link-information externally . they create html items on-demand . when analyzing links missed by their algorithm , three common problems were discovered : misspellings or multiple word representations (e.g. , cabinet maker and cabinetmaker) parser problems with document-segmentation caused by punctuation errors (lines were treated as paragraphs and sentences) problems occurred when the definition of sobparts (smaller sentences) of items was attempted a significant portion of errors came from parsing rather than algorithmic problems . this technique has maximum effectiveness for referential links which naturally have higher similarity-measures .
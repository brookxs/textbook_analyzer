86 finding out about virtual-spaces sparse vector-spaces 3.4 vector-space one of life 's most satisfying pleasures is going to a good library and browsing in an area-of-interest . after negotiating the library 's organization and finding which floor and shelves are associated with the call numbers of your topic , you are physically surrounded by books and books , all of interest to you . some are reassuring old friends , already known to you ; others are new books by familiar authors , and (best of all !) some are brand-new titles by unknowns . this system works because human catalogers have proven themselves able to reliably and consistently identify the (primary !) topic of a book according to conventional systems of subject-headings like the library-of-congress subject-headings or the dewey decimal system . our goal is to abstract away from this very friendly notion of physical-space in the library to a similar but generalized notion of semantic-space in which documents about the same topic remain close together . but rather than allowing ourselves to be restricted by the physical realities of three-dimensional-space and the fact that books can only be shelved in a single place in a library , we will consider abstract spaces of thousands of dimensions . ^ we can make concrete progress toward these lofty goals beginning with the index matrix relating each document in a corpus to all of its keywords . a very natural and influential interpretation of this matrix (due to gerry salton [salton et al. , 1975 ; salton and mcgill , 1983]) is to imagine each and every keyword of the vocabulary as a separate dimension of a vector-space . in other words , the dimensionality of the vector-space is the size of our vocabulary . each document can be represented as a vector within such a space . figure 3.7 shows a very simplified (binary) index matrix , and a cartoon of its corresponding vector-representation . estimates of the vocabulary-size of a native speaker of a language approach 50,000 words ; if you are articulate , your speaking and reading vocabularies might be 100,000 or more words . assuming that we have a modest 106 document-corpus , this matrix is something like 106 x 105 . that 's a big matrix , even by modern supercomputing standards . '' ^ in addition to the vectors representing all documents , another vector corresponds to a query . because documents and queries exist within a common vector-space , we naturally characterize how we 'd like our u2 : i docl / 1 0 1 doc2 doc3 ... o \ 0 1 1 0 0 1 weighting and matching against indices 87 kw3 d3 docn \ 1 1 0 ... 0 q .100 ... kwl figure 3.7 vector-space-retrieval system to work - just as we go to a physical location in the library to be near books about a topic , we seek those documents that are close to our query-vector . this is a useful characterization of what we 'd like our retrieval-system to accomplish , but it is still far from a specification of an algorithm for accomplishing it . for example , it seems to require that the query-vector be compared against each and every document , something we hope to avoid . ^ an even more important issue to be resolved before the vector-space-model can be useful is being specific about just what it means for a document and query to be close to one another . as will be discussed in section 5.2.2 , there are many plausible measures of proximity within a vector-space . for the time being , we will assume the use of the inner-product of query and document vectors as our metric : sim (q , d) = q Ã¯ d (3.23) people have difficulty imagining spaces with more than the three physical dimensions of experience , so it is no wonder that abstract spaces of 105 dimensions are difficult to conceptualize . sketches like figure 3.7 do the best they can to convey ideas in the three-dimensions we appreciate , but it is critically important that we not let intuitions based on such small-dimensional experiences bias our understanding of the large-dimensional spaces actually being represented and searched . implementation hack 88 finding out about theres a quicker way to compute average similarity
7.4.2 basic concepts there are two general approaches to text-compression : statistical and dictionary based . statistical-methods rely on generating good probability-estimates (of appearance in the text) for each symbol . the more accurate the estimates are , the better the compression obtained . a symbol here is usually a character , a text word , or a fixed number of characters . the set of all possible symbols in the text is called the alphabet the task of estimating the probability on each next symbol is called modeling . a model is essentially a collection of probability-distributions , one for each context in which a symbol can be coded . once these probabilities are available the symbols are converted into binary digits , a process called coding . in practice , both the encoder-and-decoder use the same model . the decoder interprets the output of the encoder (with reference to the same model) to find out the original symbol . there are two well known statistical coding strategies : huffman-coding and arithmetic-coding . the idea of huffman-coding is to assign a fixed-length bit encoding to each different symbol of the text . compression is achieved by assigning a smaller number of bits to symbols with higher probabilities of appearance . huffman-coding was first proposed in the early 1950s and was the most important compression-method until the late 1970s , when arithmetic-coding made higher compression rates possible . arithmetic-coding computes the code incrementally , one symbol at a time , as opposed to the huffman-coding scheme in which each different symbol is pre-encoded using a fixed-length number of bits . the incremental nature does not allow decoding a string which starts in the middle of a compressed file . to decode a symbol in the middle of a file compressed with arithmetic-coding , it is necessary to decode the whole text from the very beginning until the desired word is reached . this characteristic makes arithmetic-coding inadequate for use in an ir environment . dictionary methods substitute a sequence of symbols by a pointer to a previous occurrence of that sequence . the pointer representations are references to entries in a dictionary composed of a list of symbols (often called phrases) that are expected to occur frequently . pointers to the dictionary-entries are 176 text operations chosen so that they need less space than the phrase they replace , thus obtaining compression . the distinction between modeling and coding does not exist in dictionary methods and there are no explicit probabilities associated to phrases . the most well known dictionary methods are represented by a family of methods , known as the ziv-lempel family . character-based huffman methods are typically able to compress english texts to approximately five bits per character (usually , each uncompressed character takes 7-8 bits to be represented) . more recently , a word-based huffman method has been proposed as a better alternative for natural-language-texts . this method is able to reduce english texts to just over two bits per character . as we will see later on , word-based huffman-coding achieves compression rates close to the entropy and allows random-access to intermediate points in the compressed text . ziv-lempel methods are able to reduce english texts to fewer than four bits per character . methods based on arithmetic-coding can also compress english texts to just over two bits per character . however , the price paid is slower compression and decompression , and the impossibility of randomly accessing intermediate points in the compressed text . before proceeding , let us present an important definition which will be useful from now on . definition compression-ratio is the size of the compressed file as a fraction of the uncompressed file .
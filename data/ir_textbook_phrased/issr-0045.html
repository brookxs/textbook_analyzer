3.3.3 multimedia-indexing indexing associated with multimedia differs from the previous discussions of indexing . the automated-indexing takes place in multiple passes of the information versus just a direct conversion to the indexing-structure . the first pass in most cases is a conversion from the analog input mode into a-digital structure . then algorithms are applied to the digital structure to extract the unit of processing of the different modalities that will be used to represent the item . in an abstract sense this could be considered the location of a processing token in the modality . this unit will then undergo the final processing that will extract the searchable features that represent the unit . indexing video or images can be accomplished at the raw data level (e.g. , the aggregation of raw pixels) , the feature level distinguishing primitive attributes such as color and luminance , and at the semantic level where meaningful objects are recognized (e.g. , an airplane in the image/video frame) . an example is processing of video . the system (e.g. , virage) will periodically collect a frame of video input for processing . it might compare that frame to the last frame captured to determine the differences between the frames . if the difference is below a threshold it will discard the frame . for a frame requiring processing , it will define a vector that represents the different features associated with that frame . each dimension of the vector represents a different feature level aspect of the frame . the vector then becomes the unit of processing in the search-system . this is similar to processing an image . semantic level indexing requires pattern-recognition of objects within the images . examples can be found cataloging and indexing 65 in mits photobook (pentland-94) , ibm 's qbic (niblack-93) and the multimedia datablade from informix/virage (bach-96 .) if you consider an analog audio input , the system will convert the audio to digital format and determine the phonemes associated with the utterances . the phonemes will be used as input to a hidden-markov search model (see chapter 4 and chapter 10) , that will determine with a confidence level the words that were spoken . a single phoneme can be divided into four states for the-markov-model . it is the textual words assocaited with the audio that becomes the searchable structure . in addition to storing the extracted index searchable data , a multimedia item needs to also store some mechanism to correlate the different modalities during search . there are two main mechanisms that are used , positional and temporal . positional is used when the modalities are interspersed in a linear sequential composition . for example a document that has images or audio inserted , can be considered a linear structure and the only relationship between the modalities will be the juxtaposition of each modality . this would allow for a query that would specify location of an image of a boat within one paragraph of `` cuba and refugees '' . the second mechanism is based upon time because the modalities are executing concurrently . the typical video source off television is inherently a multimedia source . it contains video , audio , and potentially closed captioning . also the creation of multimedia-presentations are becoming more common using the synchronized-multimedia-integration-language (smil) . it is a mark-up-language designed to support multimedia-presentations that integrate text (e.g. , from slides or free running text) with audio , images and video . in both of these examples , time is the mechanism that is used to synchronize the different modalities . thus the indexing must include a time-offset parameter versus a physical displacement . it also suggests that the proximity to increase precision will be based upon time concurrency (or ranges) versus physical-proximity .
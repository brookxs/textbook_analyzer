2.5.3 vector-model the vector-model [697 , 695] recognizes that the use of binary weights is too limiting and proposes a framework in which partial-matching is possible . this is accomplished by assigning non-binary weights to index-terms in queries and in documents . these term-weights are ultimately used to compute the degree of similarity between each document stored in the system and the user-query . by sorting the retrieved documents in decreasing order of this degree of similarity , the vector-model takes into consideration documents which match the query terms only partially . the main resultant effect is that the ranked document answer set is a lot more precise (in the sense that it better matches the user-information need) than the document answer set retrieved by the boolean-model . definition for the vector-model , the weight w {j associated with a pair ($ , d3) is positive and non-binary . further , the index-terms in the query are also weighted . let wi ^ q be the weight associated with the pair [ki , q] , where w ^ q gt ; 0 . then , the query-vector q is defined as q = (wiyq , w2 , q , . . gt ; lt ; , wtiq) where t is the total number of index-terms in the system . as before , the vector for a document therefore , a document dj and a user-query q are represented as t-dimensional vectors as shown in figure 2.4 . the vector-model proposes to evaluate the degree of similarity of the document dj with regard to the query q as the correlation between the vectors dj and q . this correlation can be quantified , for instance , by the cosine of the angle between these two vectors . that is , - ti \ dj9q sirn (dj , q) = -- f贸贸 \ dj \ x \ q \ 28 modeling figure 2.4 the cosine of 0 is adopted as sim (d3 , q) . where \ dj \ and \ q \ are the norms of the document and query-vectors . the factor \ q \ does not affect the ranking (i.e. , the ordering of the documents) because it is the same for all documents . the factor \ d3 \ provides a normalization in the space of the documents . since wzj gt ; 0 and w ^ q gt ; 0 , sim (q , dj) varies from 0 to +1 . thus , instead of attempting to predict whether a document is relevant or not , the vector-model ranks the documents according to their degree of similarity to the query . a document might be retrieved even if it matches the query only partially . for instance , one can establish a threshold on sim (d3 , q) and retrieve the documents with a degree of similarity above that threshold . but to compute rankings we need first to specify how index term weights are obtained . index term weights can be calculated in many different ways . the work by salton and mcgill [698] reviews various term-weighting techniques . here , we do not discuss them in detail . instead , we concentrate on elucidating the main idea behind the most effective term-weighting techniques . this idea is related to the basic principles which support clustering-techniques , as follows . given a collection c of objects and a vague description of a set a , the goal of a simple clustering-algorithm might be to separate the collection c of objects into two sets : a first one which is composed of objects related to the set a and a second one which is composed of objects not related to the set a. vague description here means that we do not have complete information for deciding precisely which objects are and wthich are not in the set a. for instance , one might be looking for a set .4 of cars which have a price comparable to that of a lexus 400 . since it is not clear what the term comparable means exactly , there is not a precise (and unique) description of the set a . more sophisticated clustering-algorithms might attempt to separate the objects of a collection into various clusters (or classes) according to their properties . for instance , patients of a doctor specializing in cancer could be classified into five classes : terminal , advanced , metastasis , diagnosed , and healthy . again , the possible class descriptions might be imprecise (and not unique) and the problem is one of deciding to which of these classes a new patient should be assigned . in what follows , however , wre only discuss the simpler version of the clustering problem (i.e. , the one which considers only two classes) because all that is required is a decision on which documents are predicted to be relevant and which ones are predicted to be not relevant (with regard to a given user-query) . classic information-retrieval 29 to view the ir problem as one of clustering , we refer to the early work of salton . we think of the documents as a collection c of objects and think of the user-query as a (vague) specification of a set a of objects . in this scenario , the ir problem can be reduced to the problem of determining which documents are in the set a and which ones are not (i.e. , the ir problem can be viewed as a clustering problem) . in a clustering problem , two main issues have to be resolved . first , one needs to determine what are the features which better describe the objects in the set a. second , one needs to determine what are the features which better distinguish the objects in the set a from the remaining objects in the collection c . the first set-of-features provides for quantification of intra-cluster similarity , while the second set-of-features provides for quantification of inter-cluster dissimilarity . the most successful clustering-algorithms try to balance these two effects . in the vector-model , intra-clustering similarity is quantified by measuring the raw frequency of a term ki inside a document d3 . such term-frequency is usually referred to as the tf factor and provides one measure of how well that term describes the document contents (i.e. , intra-document characterization) . furthermore , inter-cluster dissimilarity is quantified by measuring the inverse of the frequency of a term ki among the documents in the collection . this factor is usually referred to as the inverse-document-frequency or the idf factor . the motivation for usage of an idf factor is that terms which appear in many documents are not very useful for distinguishing a relevant-document from a non-relevant one . as with good clustering-algorithms , the most effective term-weighting schemes for ir try to balance these two effects . definition let n be the total number of documents in the system and n2 be the number of documents in which the index term ki appears . let freqij be the raw frequency of term ki in the document dj (i.e. , the number of times the term k {is mentioned in the text of the document dj) . then , the normalized frequency fi , j of term ki in document dj is given by maxi freqij where the maximum is computed over all terms which are mentioned in the text of the document d3 . if the term kt does not appear in the document d3 then fzj = 0 . further , let idfi , inverse-document-frequency for kt , be given by idf , = log - (2.2) the best known term-weighting schemes use weights which are given by u-1j = / ijxlog贸 (2.3) `` i 30 modeling or by a variation of this formula . such term-weighting strategies are called tf-idf schemes . several variations of the above expression for the weight wij are described in an interesting paper by salton and buckley which appeared in 1988 [696] . however , in general , the above expression should provide a good weighting-scheme for many collections . for the query-term weights , salton and buckley suggest : log贸 (2.4) maxi freqi ^ where freq % a is the raw frequency of the term ki in the text of the information request q . the main advantages of the vector-model are : (1) its term-weighting-scheme improves retrieval-performance ; (2) its partial-matching strategy allows retrieval of documents that approximate the query conditions ; and (3) its cosine ranking formula sorts the documents according to their degree of similarity to the query . theoretically , the vector-model has the disadvantage that index-terms are assumed to be mutually independent (equation 2.3 does not account for index term dependencies) . however , in practice , consideration of term dependencies might be a disadvantage . due to the locality of many term dependencies , their indiscriminate application to all the documents in the collection might in fact hurt the overall performance . despite its simplicity , the vector-model is a resilient ranking strategy with general collections . it yields ranked answer sets which are difficult to improve upon without query-expansion or relevance-feedback (see chapter 5) within the framework of the vector-model . a large variety of alternative ranking methods have been compared to the vector-model but the consensus seems to be that , in general , the vector-model is either superior or almost as good as the known alternatives . furthermore , it is simple and fast . for these reasons , the vector-model is a popular retrieval-model nowadays .
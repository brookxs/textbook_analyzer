13.4.5 crawling the-web in this section we discuss how to crawl the-web , as there are several techniques . the simplest is to start with a set of urls and from there extract other urls which are followed recursively in a breadth-first or depth-first fashion . for that reason , search-engines allow users to submit top web-sites that will be added to the url set . a variation is to start with a set of populars urls , because we can expect that they have information frequently requested . both cases work well for one crawler , but it is difficult to coordinate several crawlers to avoid visiting the same page more than once . another technique is to partition the-web using country codes or internet names , and assign one or more robots to each partition , and explore each partition exhaustively . considering how the-web is traversed , the index of a search-engine can be thought of as analogous to the stars in an sky . what we see has never existed , as the light has traveled different distances to reach our eye . similarly , web-pages referenced in an index were also explored at different dates and they may not exist any more . nevertheless , when we retrieve a page , we obtain its actual content . how fresh are the-web pages referenced in an index ? the pages will be from one day to two months old . for that reason , most search-engines show in the answer the date when the page was indexed . the percentage of invalid links stored in search-engines vary from 2 to 9 % . user submitted pages are usually crawled after a few days or weeks . starting there , some engines traverse the whole web-site , while others select just a sample of pages or pages up to a certain depth . non-submitted pages will wait from weeks up to a couple of months to be detected . there are some engines that learn the change frequency of a page and visit it accordingly [175] . they may also crawl more frequently popular pages (for example , pages having many links pointing to them) . overall , the current fastest crawlers are able to traverse up to 10 million web-pages per day . the order in which the urls are traversed is important . as already mentioned , the links in a web-page can be traversed breadth first or depth first . using a breadth first policy , we first look at all the pages linked by the current page , and so on . this matches well web-sites that are structured by related topics . on the other hand , the coverage will be wide but shallow and a web-server can be bombarded with many rapid requests . in the depth first case , we follow the first link of a page and we do the same on that page until we can not go deeper , returning recursively . this provides a narrow but deep traversal only recently , some research on this problem has appeared [168] , showing that good ordering schemes can make a difference if crawling better pages first (using the pagerank scheme mentioned above) . search-engines 383 due to the fact that robots can overwhelm a server with rapid requests and can use significant internet bandwidth (in particular the whole bandwidth of small domains can be saturated) , a set of guidelines for robot-behavior has been developed [457] . for this purpose , a special file is placed at the root of every web-server indicating the restrictions at that site , in particular the pages that should not be indexed . crawlers can also have problems with html pages that use frames (a mechanism to divide a page in two or more parts) or image-maps (hyperlinks associated to images) . in addition , dynamically generated pages can not be indexed as well as password protected pages .
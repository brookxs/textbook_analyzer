the bernoulli model there are two different ways we can set up an nb classifier . the model we introduced in the previous section is the multinomial-model . it generates one term from the vocabulary in each position of the document , where we assume a generative-model that will be discussed in more detail in section 13.4 (see also page 12.1.1) . an alternative to the multinomial-model is the multivariate bernoulli model or bernoulli model . it is equivalent to the binary independence-model of section 11.3 (page) , which generates an indicator for each term of the vocabulary , either indicating presence of the term in the document or indicating absence . figure 13.3 presents training and testing algorithms for the bernoulli model . the bernoulli model has the same time-complexity as the multinomial-model . the different generation models imply different estimation strategies and different classification-rules . the bernoulli model estimates as the fraction of documents of class that contain term (figure 13.3 , trainbernoullinb , line 8) . in contrast , the multinomial-model estimates as the fraction of tokens or fraction of positions in documents of class that contain term (equation 119) . when classifying a test document , the bernoulli model uses binary occurrence information , ignoring the number of occurrences , whereas the multinomial-model keeps track of multiple occurrences . as a result , the bernoulli model typically makes many mistakes when classifying long documents . for example , it may assign an entire book to the class china because of a single occurrence of the term china . the models also differ in how nonoccurring terms are used in classification . they do not affect the classification decision in the multinomial-model ; but in the bernoulli model the probability of nonoccurrence is factored in when computing (figure 13.3 , applybernoullinb , line 7) . this is because only the bernoulli nb model models absence of terms explicitly . worked example . applying the bernoulli model to the example in table 13.1 , we have the same estimates for the priors as before : , . the conditional-probabilities are : the denominators are and because there are three documents in and one document in and because the constant in equation 119 is 2 - there are two cases to consider for each term , occurrence and nonoccurrence . the scores of the test document for the two classes are end worked example .
2.3.2 noise words from the earliest days of ir (e.g. , luhn 's seminal work [luhn , 1957]) , two related facts have been obvious : first , a relatively small number of words account for a very significant fraction of all text 's bulk . words like it , amd , and to can be found in virtually every sentence . second , these noise words make very poor index-terms . users are unlikely to ask for documents about to , and it is hard to imagine a document about be . ^ due then to both their frequency and their lack of indexing consequence , we will build the capability of ignoring noise words into our lexical-analyzer . as will be discussed extensively in chapter 3 , noise words are often imagined to be the most frequently occurring words in a corpus . one problem with defining noise words in this way is that it requires a frequency-analysis of the corpus prior to lexical-analysis . it is possible to use frequency analyses from other corpora , assuming that the distribution of noise words is relatively constant across corpora , but such an extrapolation is not always warranted . worse , the most frequently used words often include those that might make very good keywords . fox notes that the words time , war , home , life , water , and world are among the 200 most frequently used words in general english literature [fox , 1992 , p. 113] . instead , we will define noise words extensionally , in terms of a finite list or negative dictionary . the list we use , stof.wrd , was derived by fox from an analysis of the brown-corpus [fox , 1990] . the relationship between these noise words and those words most critical to syntactic-analysis of natural-language-sentences is striking . note that the same tokens that are thrown away as noise because they have no meaning are precisely those function-words that are most important to the syntactic-analysis of well-formed sentences . this is the first , but not the last , suggestion of a fundamental complementarity between foa 's concern with semantics and computational-linguistics ' concern with syntax . but sometimes we care about noise words !
7.4.2 training a classifier the parameters © controlling the classifier could come from many places , but of course here we are concerned with learning them . in terms of the training-set t : t ={ (dhci)} (7.14) we seek the parameters © with the highest probability of having produced t. depending on the model we employ , how we decompose 0 into its constituent parameters 0 {will differ . one piece of this is easy to estimate : the prior-probability of the class pr (c) is how frequently one classification is observed in t relative to the others . using a `` twiddle '' hat to distinguish estimates 0 of the probabilities from their true values 0 : s - = w \ lt ; 7 ' 15) estimating 0ck is more complicated . the fact that both the multi variate bernoulli and the multinomial models of document-generation involve the product of the keywords ' 0ck should make it obvious that our cumulative estimate will be very sensitive to any one of these values ; consider , for example , what happens if even one of these terms is zero ! within the bayesian-framework , ^ '' these statistical sensitivities are ad ~ probabilists ' dressed by providing priors for the underlying word-events of document religious wars generation . * for simplicity , we assume that the documents1 lengths are independent of the classes , i.e. , that knowing a document 's length tells us little about which class it should be in .
measures of association some classification-methods are based on a binary relationship between objects . on the basis of this relationship a classification-method can construct a system of clusters . the relationship is described variously as ` similarity ' , ` association ' and ` dissimilarity ' . ignoring dissimilarity for the moment as it will be defined mathematically later , the other two terms mean much the same except that ` association ' will be reserved for the similarity between objects characterised by discrete-state attributes . the measure of similarity is designed to quantify the likeness between objects so that if one assumes it is possible to group objects in such a way that an object in a group is more like the other members of the group than it is like any object outside the group , then a cluster method enables such a group-structure to be discovered . informally speaking , a measure of association increases as the number or proportion of shared attribute states increases . numerous coefficients of association have been described in the literature , see for example goodman and kruskal [11 , 12] , kuhns [13] , cormack [14] and sneath and sokal [15] . several authors have pointed out that the difference in retrieval-performance achieved by different measures of association is insignificant , providing that these are appropriately normalised . intuitively , one would expect this since most measures incorporate the same information . lerman [16] has investigated the mathematical relationship between many of the measures and has shown that many are monotone with respect to each other . it follows that a cluster method depending only on the rank-ordering of the association values would given identical clusterings for all these measures . there are five commonly used measures of association in information-retrieval . since in information-retrieval documents and requests are most commonly represented by term or keyword lists , i shall simplify matters by assuming that an object is represented by a set of keywords and that the counting measure | . | gives the size of the set . we can easily generalise to the case where the keywords have been weighted , by simply choosing an appropriate measure (in the measure-theoretic sense) . the simplest of all association measures is | x [[intersection]] y | simple matching coefficient which is the number of shared index-terms . this coefficient does not take into account the sizes of x and y . the following coefficients which have been used in document-retrieval take into account the information provided by the sizes of x and y . these may all be considered to be normalised versions of the simple matching coefficient . failure to normalise leads to counter intuitive results as the following example shows : then | x1 | = 1 | y1 | = 1 | x1 [[intersection]] y2 | = 1 = gt ; s1 = 1s2 = 1 | x2 | = 10 | y2 | = 10 | x2 [[intersection]] y2 | = 1 = gt ; s1 = 1s2 = 1/10 s1 (x1 , y1) = s1 (x2 , y2) which is clearly absurd since x1 and y1 are identical representatives whereas x2 and y2 are radically different . the normalisation for s2 , scales it between) and 1 , maximum-similarity being indicated by 1 . doyle [17] hinted at the importance of normalisation in an amusing way : ` one would regard the postulate `` all documents are created equal '' as being a reasonable foundation for a library description . therefore one would like to count either documents or things which pertain to documents , such as index tags , being careful of course to deal with the same number of index tags for each document . obviously , if one decides to describe the library by counting the word tokens of the text as `` of equal interest '' one will find that documents contribute to the description in proportion to their size , and the postulate `` big documents are more important than little documents '' is at odds with `` all documents are created equal '' ' . i now return to the promised mathematical definition of dissimilarity . the reasons for preferring the ` dissimilarity ' point-of-view are mainly technical and will not be elaborated here . interested readers can consult jardine and sibson [2] on the subject , only note that any dissimilarity function can be transformed into a similarity-function by a simple transformation of the form s = (1 + d) [-1] but the reverse is not always true . if p is the set of objects to be clustered , a pairwise dissimilarity coefficient d is a function from p x p to the non-negative real numbers . d , in general , satisfies the following conditions : d1 d (x , y) gt ; = 0 for all x , y [[propersubset]] p d2 d (x , x) = 0 for all x [[propersubset]] p d3 d (x , y) = d (y , x) for all x , y [[propersubset]] p informally , a dissimilarity coefficient is a kind of ` distance ' function . in fact , many of the dissimilarity coefficients satisfy the triangle-inequality : d4 d (x , y) lt ; = d (x , z) + d (y , z) which may be recognised as the theorem from euclidean geometry which states that the sum of the lengths of two sides of a triangle is always greater than the length of the third side . an example of a dissimilarity coefficient satisfying d1 - d4 is where (x [[delta]] y) = (x [[union]] y) - (x [[intersection]] y) is the symmetric different of sets x and y . it is simply related to dice 's coefficient by and is monotone with respect to jaccard 's coefficient subtracted from 1 . to complete the picture , i shall express this last dc in a different form . instead of representing each document by a set of keywords , we represent it by a binary string where the absence or presence of the ith keyword is indicated by a zero or one in the ith position respectively . in that case where summation is over the total number of different keywords in the document-collection . salton considered document representatives as binary vectors embedded in an n-dimensional euclidean-space , where n is the total number of index-terms . can then be interpreted as the cosine of the angular separation of the two binary vectors x and y . this readily generalises to the case where x and y are arbitrary real vectors (i.e. weighted keyword lists) in which case we write where (x , y) is the inner-product and | | . | | the length of a vector . if the space is euclidean then for x = (x1 , ... , xn) and y = (y1 , ... , yn) we get some authors have attempted to base a measure of association on a probabilistic-model [18] . they measure the association between two objects by the extent to which their distributions deviate from stochastic independence . this way of measuring association will be of particular importance when in chapter 5 i discuss how the association between index-terms is to be used to improve retrieval-effectiveness . there i use the expected mutual-information measure to measure association . for two discrete probability distributions p (xi) and p (xj) it can be defined as follows : when xi and xj are independent p (xi) p (xj) = p (xi , xj) and so i (xi , xj) = 0 . also i (xixj) = 0 . also i (xixj) = i (xjxi) which shows that it is symmetric . it also has the nice property of being invariant under one-to-one transformations of the co-ordinates . other interesting properties of this measure may be found in osteyee and good [19] . rajski [20] shows how i (xixj) may be simply transformed into a distance-function on discrete probability distributions . i (xixj) is often interpreted as a measure of the statistical-information contained in xi about xj (or vice versa) . when we apply this function to measure the association between two index-terms , say i and j , then xi and xj are binary variables . thus p (xi = 1) will be the probability of occurrence of the term i and similarly p (xi = 0) will be the probability of its non-occurrence . the extent to which two index-terms i and j are associated is then measured by i (xixj) which measures the extent to which their distributions deviate from stochastic independence . a function very similar to the expected mutual-information measure was suggested by jardine and sibson [2] specifically to measure dissimilarity between two classes of objects . for example , we may be able to discriminate two classes on the basis of their probability-distributions over a simple two-point space {1 , 0} . thus let p1 (1) , p1 (0) and p2 (1) , p2 (0) be the probability distributions associated with class i and ii respectively . now on the basis of the difference between them we measure the dissimilarity between i and ii by what jardine and sibson call the information radius , which is here u and v are positive weights adding to unit . this function is readly generalised to multi-state , or indeed continuous distribution . it is also easy to shown that under some interpretation the expected mutual-information measure is a special case of the information radius . this fact will be of some importance in chapter 6 . to see it we write p1 (.) and p2 (.) as two conditional distributions p (. / w1) and p (. / w2) . if we now interpret u = p (. / w1) and v = p (. / w2) , that is the prior-probability of the conditioning variable in p (. / wi) , then on substituting into the expression for the information radius and using the identities . p (x) = p (x/w1) p (w1) + p (x/w2) p (w2) x = 0 , 1 p (x/wi) = p (x/wi) p (x) i = 1 , 2 we recover the expected mutual-information measure i (x , wi) .
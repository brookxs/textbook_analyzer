11 information-system evaluation 11.1 introduction to information-system evaluation 11.2 measures used in system evaluations 11.3 measurement example - treoresults 11.4 summary interest in the evaluation-techniques for information-retrieval-systems has significantly increased with the commercial use of information-retrieval technologies in the everyday-life of the millions of users of the-internet . until 1993 the evaluations were done primarily by academicians using a few small , well known corpora of test documents or even smaller test-databases created within academia . the evaluations focused primarily on the effectiveness of search-algorithms . the creation of the annual text-retrieval evaluation conference (trec) sponsored by the defense advanced research projects agency (darpa) and the national institute of standards and technology (nist) changed the standard process of evaluating information-systems . conferences have been held every year , starting from 1992 , usually in the fall months . the conference provides a standard database consisting of gigabytes of test-data , search statements and the expected results from the searches to academic-researchers and commercial companies for testing of their systems . this has placed a standard baseline into comparisons of algorithms . although there is now a standard database , there is still debate on the accuracy and utility of the results from use of the test-corpus . section 11.2 introduces the measures that are available for evaluating information-systems . the techniques are compared stressing their utility from an academic as well as a commercial perspective . section 11.3 gives examples of results from major comparisons of information-systems and algorithms .
2.5.1 basic algorithm we now assume that : ï prior technology has successfully broken our stream of characters , our large-corpus , into a set of documents ; ï within each document we have identified individual tokens ; and ï noise word tokens have been identified . then the basic flow of what we will call the postdoc function operates as follows (see algorithm 2.1) . 2 www.apple.com/slierlock/ 3 ftp : / / ftp.os.cornell.eciu / pub/smart / 4 www.mcis.iiiiit.e4u.au/nig/welcome.iitml : lt ; www.seargtitools.eom/tools/toois.litm.l * www.glue.umcl edu/dlrg/filter / software , html '' http://www.cse.urad.edu/-rik/foa/ 52 finding out about algorithm 2.1 basic algorithm for every doc in corpus while (token = getnonnoisetoken) if (stemp) token = stem (token) save posting (token , doc) in tree for every token in tree accumulate ndoc (token) , totfreq (token) sortp g postings (token) descending docfreq (p) order write tokengt ; ndoc , totfreq , postings for every document in the corpus we will iterate through a loop until weve exhausted every token in that document . so let 's call getnonnoisetoken a routine that repeatedly builds tokens from the document 's stream , does whatever character assessments are required , checks it against a negative dictionary , and returns a token . if stemming is to be applied , well stem the word at this point . then we will save a posting for that token 's occurrence in that document . a posting is simply a correspondence between a particular word and a particular document , representing the occurrence of that word in that document . * that is , we have a document in front of us and it contains a set of tokens . we are now going to build a representation for each token that tells all of the documents in which ones it occurs . for each keyword we will maintain the token itself as the key used for subsequent access and the head of a linked-list of all postings , each containing the document number and the number of occurrences of the keyword in that document . a sketch of these data-structures is shown in figure 2.4 . after going through every document in the corpus in this fashion , we have a large-collection of postings . here we recommend splay trees as an appropriate data-structure for these keywords and their postings . in the c implementation shown in algorithm 2.2 , the installtermo implementation function inserts a new posting into the terms tree j details * well discuss either data we might also keep with the posting later ; c # . section 2.5.2 . extracting lexical-features 53 kw_inv token totdoc head `` aardvarck '' 20 65 totfreq i c posting locno freq next ^ ª o figure 2.4 basic postings data-structures algorithm 2.2 postdoc.c details void postdoc (int docno , file * docf , long int bpos , long int epos , char * proxy) {gettermstring (proxypos , noise , maxtermsize , newterm) ; getterm (docf , noise , maxtermsize , newterm) ; installterm (newterm , docno , terms) ; during the processing of each document , it will prove important to know how many keywords are extracted from it . this will be known as the documents length , denoted lengths this quantity is important when normalizing documents of different lengths . one way to implement this computation is to maintain a small separate file doclend.d containing only this one number for each document . when the set of documents has been exhausted , we need to write out this inverted-representation to a file for subsequent processing . for every token in the splay-tree (typically the traversal will be in lexicographic-order) , we will organize all its postings . first , we count the number of occurrences of the keyword across all the documents in the corpus ; we will call this variable totfreqk . a second , less obvious statistic we will maintain is how many documents contain this keyword ; this variable will be called docfreqi . if there is exactly one occurrence of a 54 finding out about kw token totdoc wgt head `` aardvarck '' 20 65 0.634 totfreq fpost freq dochd next o o figure 2.5 refined postings data-structures keyword in each document , then these two numbers will be the same . but typically there are multiple occurrences of the same keyword in a single-document and totfreq gt ; docfreqk . both variables will be important to us in determining appropriate weights for the index relation (cf. chapter 3) . after going through all of the documents and accumulating for each these two statistics , we must sort the postings in decreasing frequency order . the reason for this wo n't be apparent until we discuss the matching-algorithms (cf. section 3.5) , but it turns out to be important that documents that use a keyword most often are at the beginning of the list . once the documents ' postings have been sorted into descending order of frequency , it is likely that several of the documents in this list will have the same frequency , and we can exploit this fact to compress their representation . figure 2.5 shows the posting list broken into a list of fpost sublists , one for unique frequency count .
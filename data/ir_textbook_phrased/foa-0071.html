4.3.4 basic measures figure 4.9 shows the relationship between relevant (rel) and retrieved (retr) sets as a venn-diagram , against the backdrop of the universe 17 of the rest of the documents of the corpus . obviously , our focus should be on those documents that are in the intersection of rel and retr and on making this intersection as large as possible . informally , we will be most happy with a rel set when it best overlaps with the retr set , and therefore we seek evaluation-measures that reflect this . the basic relations between the sizes of these sets can also be captured in the contingency-table of table 4.1 . we know we want the intersection of the rel and retr sets to be large , but large relative to what ?! as mentioned in chapter 1 , if we are most focused on the rel set and use it as our standard of comparison , assessing the retrieval 123 table 4.1 contingency-table not relevant relevant retrieved ret a rel ret lt ; - ª rel nref not retrieved wt * - * rel ` ret lt ; - gt ; llel nnre ^ nrelc nnreld ndolt ; f a nret is the number of retrieved documents . b nnret is the number of documents not retrieved . c nrel is the number of relevant documents . d nnrel is the number of irrelevant documents . e ndoc is the total number of documents . we 'd like to know what fraction of these we 've retrieved . this ratio is called recall : anticipating the probabilistic-analysis of section 5.5 , we can think of recall as (an estimate of) the conditional-probability that a document will be retrieved , given that it is relevant : pr {ret | rel) . conversely , if we instead focus on the ret set , we are most interested in what fraction of these are relevant ; this ratio is called precision : ^ . . \ retd rel \ fa n precision = - ó ; ----- : ó - (4.3) \ ret \ similarly , this is the probability that a document will be relevant , given that it is retrieved : pr (rel \ ret) . a closely related but less common measure is called fallout , where we (perversely !) focus on the irrelevant documents and the fraction of them retrieved : ` ret n rel i fallout = ----- == ----- l - (4.4) \ ret the close relationship between these three measures can be defined precisely , if the generality g of the query (cf. section 4.3.7) is known : recall ï g , , precision = --------------------------------------- (4.5) recall - g + fallout - (i - g) 124 finding out about this is pr (ret \ rel) . these two measures , recall and precision , have remained the bedrock of search-engine-evaluation since they were first introduced by kent in 1955 [kent et al , 1955 ; saracevic , 1975] . by far the most common measures of search-engine-performance are just the pair of measures , precision-and-recall . ideally , of course , we 'd like a system that has both high precision and high recall : only relevant documents and all of them . but real-world , practical systems must select documents based on features that are only statistically useful indicators of relevance ; we can never be sure . in this case efforts made to improve recall must retrieve more documents , and it is likely that precision will suffer as a consequence . the best we can hope for is some balance . in some applications it is nevertheless desirable to evaluate ir-system performance according to a single measure rather than the twosingle dimensional recall/precision criteria . ^ we will return to this topic in dimensions for section 4.3.8 . simple minds
automatic-indexing 125 5.3.1 index phrase-generation the goal of indexing is to represent the semantic concepts of an item in the information-system to support finding relevant-information . single words have conceptual context , but frequently they are too general to help the user find the desired information . term phrases allow additional specification and focusing of the concept to provide better precision and reduce the user 's overhead of retrieving non-relevant items . having the modifier `` grass '' or `` magnetic '' associated with the term `` field '' clearly disambiguates between very different concepts . one of the earliest statistical-approaches to determining term phrases proposed by salton was use of a cohesion factor between terms (salton-83) : cohesion ^ = size-factor * (pair-freqk , h / totfk * totfh) where size-factor is a normalization-factor based upon the size of the vocabulary and pair-freqkj , is the total frequency of co-occurrence of the pair teraik ternih in the item collection . co-occurrence may be defined in terms of adjacency , word proximity , sentence proximity , etc. . this initial algorithm has been modified in the smart system to be based on the following guidelines (buckley-95) : any pair of adjacent non-stop words is a potential phrase any pair must exist in 25 or more items phrase-weighting uses a modified version of the smart system single term algorithm normalization is achieved by dividing by the length of the single-term subvector . natural-language-processing can reduce errors in determining phrases by determining inter-item dependencies and using that information to create the term phrases used in the indexing process . statistical-approaches tend to focus on two term phrases . a major advantage of natural-language approaches is their ability to produce multiple-term phrases to denote a single concept . if a phrase such as `` industrious intelligent students '' was used often , a statistical approach would create phrases such as `` industrious intelligent '' and `` intelligent student . '' a natural-language approach would create phrases such as `` industrious student , '' `` intelligent student '' and `` industrious intelligent student . '' the first step in a natural-language determination of phrases is a lexical-analysis of the input . in its simplest form this is a part-of-speech tagger that , for example , identifies noun-phrases by recognizing adjectives and nouns . precise part 126 chapter5 of speech taggers exist that are accurate to the 99 per cent range . additionally , proper-noun-identification tools exist that allow for accurate identification of names , locations and organizations since these values should be indexed as phrases and not undergo stemming . greater gains come from identifying syntactic and semantic level dependencies creating a hierarchy of semantic-concepts . for example , `` nuclear reactor fusion '' could produce term phrases of `` nuclear reactor '' and `` nuclear fusion . '' in the ideal case all variations of a phrase would be reduced to a single canonical-form that represents the semantics for a phrase . thus , where possible the phrase-detection process should output a normalized-form . for example , `` blind venetian '' and `` venetian who is blind '' should map to the same phrase . this not only increases the precision of searches , but also increases the frequency of occurrence of the common phrase . this , in turn , improves the likelihood that the frequency of occurrence of the common phrase is above the threshold required to index the phrase . once the phrase is indexed , it is available for search , thus participating in an item 's selection for a search and the rank associated with an item in the hit file . one solution to finding a common form is to transform the phrases into a operator-argument form or a header-modifier form . there is always a category of semantic phrases that comes from inferring concepts from an item that is non-determinable . this comes from the natural ambiguity inherent in languages that is discussed in chapter 1 . a good example of application of natural-language to phrase creation is in the natural-language-information-retrieval-system at new york university developed in collaboration with ge corporate-research and development (carballo-95) . the text of the item is processed by a fast syntactical process and extracted phrases are added to the index in addition to the single word terms . statistical-analysis is used to determine similarity links between phrases and identification of subphrases . once the phrases are statistically noted as similar , a filtering process categorizes the link onto a semantic-relationship (generality , specialization , antonymy , complementation , synonymy , etc.) . the tagged-text parser (ttp) , based upon the linguistic string grammar (sager-81) , produces a regularized parse-tree representation of each sentence reflecting the predicate-argument-structure (strzalkowski-93) . the tagged-text parser contains over 400 grammar production-rules . some examples of the part-of-speech tagger identification are given in figure 5.8 . class examples determiners a , the singular nouns paper , notation , structure , language plural nouns operations , data , processes preposition in , by , of , for adjective high , concurrent present tense verb presents , associates present participal multiprogramming 5.8 part-of-speech-tags automatic-indexing 127 the ttp parse trees are header-modifier pairs where the header is the main concept and the modifiers are the additional descriptors that form the concept and eliminate ambiguities . figure 5.9 gives an example of a regularized parse-tree structure generated for the independent clause : the former soviet president has been a local hero ever since a russian tank invaded wisconsin | assert perf [have] verb [be] subject np noun [president] t_pos [the] adj [former] adj [soviet] object np noun [hero] t __ pos [a] adj [local] adv [ever] sub_ord [since] verb [invade] subject np nounftank] t __ pos [a] adj [russian] object np noun [wisconsin] figure 5.9 ttp parse-tree this structure allows for identification of potential term phrases usually based upon noun-identification . to determine if a header-modifier pair warrants indexing , strzalkowski calculates a value for informational contribution (ic) for each element in the pair . higher values of ic indicate a potentially stronger semantic-relationship between terms . the basis behind the ic formula is a conditional-probability between the terms . the formula for ic between two terms (x , y) is ; 128 chapter 5 where fxy is the frequency of (x , y) in the database , nx is the number of pairs in which `` x '' occurs at the same position as in (x , y) and d (x) is the dispersion parameter which is the number of distinct words with which x is paired . when ic = 1 , x occurs only with y (fx , y = nx and dx = 1) . nominal compounds are the source of many inaccurate identifications in creating header-modifier pairs . use of statistical-information on frequency of occurrence of phrases can eliminate some combinations that occur infrequently and are not meaningful . the next challenge is to assign weights to term phrases . the most popular term-weighting-scheme uses term frequencies and inverse document frequencies with normalization based upon item length to calculate weights assigned to terms (see section 5.2.2.2) . term phrases have lower frequency occurrences than the individual terms . using natural-language-processing , the focus is on semantic-relationships versus frequency relationships . thus weighting-schemes such as inverse-document-frequency require adjustments so that the weights are not overly diminished by the potential lower frequency of the phrases . for example , the weighting-scheme used in the new york university system uses the following formula for weighting phrases : weight (phraseo = (ci * log (termf) + c2 * oc (n , i)) * idf where a (n , i) is 1 for ilt ; n and 0 otherwise and ci and c2 are normalizing factors . the n assumes the phrases are sorted by idf value and allows the top `` n '' highest idf (inverse-document-frequency) scores to have a greater effect on the overall weight than other terms .
2.2 interdocument parsing the first step is to break the corpus - an arbitrary `` pile of text '' - into individually retrievable documents . this demands that we be specific about the format of the corpus and that we decide how it is to be divided into individual documents . for all operating-systems we will consider , this problem can be defined more precisely in terms of paths , directories , files , and positions within files . for any application in which the corpus can be described by the path to its root , these tools will translate directories , files , and documents-within-files into a homogeneous corpus . of course , there are some situations (e.g. , when documents are maintained within a database) that can not be captured in these terms , but these primitives do allow a wide range of corpora to be specified . our model will assume that many documents may be contained within a single file and that each document occupies a contiguous region within the file . extracting lexical-features 41 issues concerning structure within a single-document are closely related to assumptions we may or may not be able to make about the length of the documents in question . our assumptions about how long a typical document is will recur throughout this book . it is obvious , for example , that different document browsers are necessary if we need to browse through an entire book rather than look at a single paragraph . less obvious is that the fundamental weighting algorithms used by our indexing-techniques will depend very sensitively on the number of tokens contained in each document . in this textbook we will focus primarily on two particular test-corpora , ai theses (ait) and email ; these are discussed in more detail in section 2.4 . each of these has natural notions of the individual document : in the case of the ait it is the thesis 's abstract , and for email it is the entire message . in both cases , more refined notions of document (the individual paragraphs within the abstract or within the email-message) are possible . with these assumptions , we can define our corpus simply with two files : one specifying full path-information for each file , and a second specifying where within these files each message resides . a large portion of the task of navigating a directory full of files and visiting each of them can be accomplished using the dirent . ^ this utility allows the recursive descent through all directories from a specified root , visiting every file contained therein . in many cases , the files we will be indexing have a great deal of syntactic structural-information above and beyond the meaningful-text itself . for example , our email will often contain a great deal of mail header information , as (loosely .) specified in rfc822j many text-formatting languages , for example , tjx , xml , and html , now produce documents with a well-defined syntax . if , for example , the documents are written in html , we do n't want to index pseudo-words like lt ; h1gt ; . in many of these situations , filters exist that can extract just the meaningful-text from surrounding header or format information ; detex1 is an example of a useful filter for removing ejtex andt ^ x markup . use of such utilities spares us the task of parsing this elaborate structure , but it also means that more elaborate solutions for maintaining the difference between the document 's index and the document 's presentation must be addressed . what is dirent ? what is rfc8s2 ? 1 www . cs.purdue.edu/trinkle/detex/lndex.html 42 finding out about advisor committee university department proxy text this is a sample of some text . it might be from an email-message , a dissertation , or anything else . we simply assume it is a string of characters about a paragraph in length . figure 2.1 parsing email and ait to common specifications the basic data elements to be parsed from our two examples , email and ait , are shown in figure 2.1 .
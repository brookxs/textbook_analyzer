5.1 classes of automatic-indexing automatic-indexing is the process of analyzing an item to extract the information to be permanently kept in an index . this process is associated with 106 chapters the generation of the searchable data-structures associated with an item . figure 1.5 data-flow in an information-processing system is reproduced here as figure 5.1 to show where the indexing process is in the overall processing of an item . the figure is expanded to show where the search-process relates to the indexing process . the left side of the figure including identify processing tokens , apply stop lists , characterize tokens , apply stemming and create searchable data-structure is all part of the indexing process . all systems go through an initial stage of zoning (described in section 1.3.1) and identifying the processing tokens used to create the index . some systems automatically divide the document up into fixed length passages or localities , which become the item unit that is indexed (kretser-99 .) filters , such as stop lists and stemming algorithms , are frequently applied to reduce the number of tokens to be processed . the next step depends upon the search-strategy of a particular system . search-strategies can be classified as statistical , natural-language , and concept . an index is the data-structure created to support the search-strategy . statistical strategies cover the broadest range of indexing-techniques and are the most prevalent in commercial systems . the basis for a statistical approach is-use of frequency of occurrence of events . the events usually are related to occurrences of processing tokens (words/phrases) within documents and within the database . the words/phrases are the domain of searchable values . the statistics that are applied to the event data are probabilistic , bayesian , vector-space , neural-net . the static approach stores a single statistic , such as how often each word occurs in an item , that is used in generating relevance-scores after a standard boolean search . probabilistic indexing stores the information that are used in calculating a probability that a particular item satisfies (i.e. , is relevant to) a particular query . bayesian and vector approaches store information used in generating a relative confidence level of an item 's relevance to a query . it can be argued that the bayesian-approach is probabilistic , but to date the developers of this approach are more focused on a good relative relevance value than producing and absolute probability . neural-networks are dynamic learning structures that are discussed under concept-indexing where they are used to determine concept classes . natural-language approaches perform the similar processing token-identification as in statistical-techniques , but then additionally perform varying levels of natural-language-parsing of the item . this parsing disambiguates the context of the processing tokens and generalizes to more abstract-concepts within an item (e.g. , present , past , future actions) . this additional-information is stored within the index to be used to enhance the search precision . concept-indexing uses the words within an item to correlate to concepts discussed in the item . this is a generalization of the specific words to values used to index the item . when generating the concept classes automatically , there may not be a name applicable to the concept but just a statistical significance . automatic-indexing 107 standardize input logical subsetting (zoning) identify processing tokens \ r apply stoplists (stop algorithms) characterize tokens apply stemming lt ; create searchable data-structure search-results update document file query display user command figure 5.1 data-flow in information-processing system 108 chapter 5 finally , a special class of indexing can be defined by creation of hypertext linkages . these linkages provide virtual threads of concepts between items versus directly defining the concept within an item . each technique has its own strengths and weaknesses . current evaluations from trec conferences (see chapter 11) show that to maximize location of relevant items , applying several different algorithms to the same corpus provides the optimum results , but the storage and processing overhead is significant .
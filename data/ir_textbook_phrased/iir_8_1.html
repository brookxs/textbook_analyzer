information-retrieval-system evaluation to measure ad-hoc-information-retrieval effectiveness in the standard way , we need a test-collection consisting of three things : a document-collection a test-suite of information-needs , expressible as queries a set of relevance-judgments , standardly a binary assessment of either relevant or nonrelevant for each query-document pair . relevant nonrelevant gold-standard ground-truth relevance is assessed relative to an , not a query . for example , an information-need might be : information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine . wine and red and white and heart and attack and effective 8.5.1 many systems contain various weights (often known as parameters) that can be adjusted to tune system-performance . it is wrong to report results on a test-collection which were obtained by tuning these parameters to maximize performance on that collection . that is because such tuning overstates the expected performance of the system , because the weights will be set to maximize performance on one particular set of queries rather than for a random-sample of queries . in such cases , the correct procedure is to have one or more development test-collections , and to tune the parameters on the development test-collection . the tester then runs the system with those weights on the test-collection and reports the results on that collection as an unbiased estimate of performance .
cluster cardinality in k-means 16.2 a naive approach would be to select the optimal value of according to the objective-function , namely the value of that minimizes rss . defining as the minimal rss of all clusterings with clusters , we observe that is a monotonically decreasing function in (exercise 16.7) , which reaches its minimum 0 for where is the number of documents . we would end up with each document being in its own cluster . clearly , this is not an optimal clustering . a heuristic-method that gets around this problem is to estimate as follows . we first perform (e.g. ,) clusterings with clusters (each with a different initialization) and compute the rss of each . then we take the minimum of the rss values . we denote this minimum by . now we can inspect the values as increases and find the `` knee '' in the curve - the point where successive decreases in become noticeably smaller . there are two such points in figure 16.8 , one at , where the gradient flattens slightly , and a clearer flattening at . this is typical : there is seldom a single best number-of-clusters . we still need to employ an external constraint to choose from a number of possible values of (4 and 9 in this case) . a second type of criterion for cluster cardinality imposes a penalty for each new cluster - where conceptually we start with a single cluster containing all documents and then search for the optimal number of clusters by successively incrementing by one . to determine the cluster cardinality in this way , we create a generalized objective-function that combines two elements : distortion , a measure of how much documents deviate from the prototype of their clusters (e.g. , rss for - means) ; and a measure of model-complexity . we interpret a clustering here as a model of the data . model-complexity in clustering is usually the number of clusters or a function thereof . for - means , we then get this selection criterion for : (195) the obvious difficulty with equation 195 is that we need to determine . unless this is easier than determining directly , then we are back to square one . in some cases , we can choose values of that have worked well for similar data-sets in the past . for example , if we periodically cluster news stories from a newswire , there is likely to be a fixed value of that gives us the right in each successive clustering . in this application , we would not be able to determine based on past experience since changes . a theoretical justification for equation 195 is the akaike information-criterion or aic , an information-theoretic measure that trades off distortion against model-complexity . the general form of aic is : (196) for - means , the aic can be stated as follows : (197) 197 195 to derive equation 197 from equation 196 observe that in - means since each element of the centroids is a parameter that can be varied independently ; and that (modulo a constant) if we view the model underlying - means as a gaussian-mixture with hard assignment , uniform cluster priors and identical spherical covariance-matrices (see exercise 16.7) . the derivation of aic is based on a number of assumptions , e.g. , that the data are . these assumptions are only approximately true for data-sets in information-retrieval . as a consequence , the aic can rarely be applied without modification in text-clustering . in figure 16.8 , the dimensionality of the vector-space is 50,000 . thus , dominates the smaller rss-based term (, not shown in the figure) and the minimum of the expression is reached for . but as we know , (corresponding to the four classes china , germany , russia and sports) is a better choice than . in practice , equation 195 is often more useful than equation 197 - with the caveat that we need to come up with an estimate for . exercises . why are documents that do not use the same term for the concept car likely to end up in the same cluster in - means clustering ? two of the possible termination-conditions for - means were (1) assignment does not change , (2) centroids do not change (page 16.4) . do these two conditions imply each other ?
evaluation of clustering typical objective-functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low inter-cluster similarity (documents from different clusters are dissimilar) . this is an internal criterion for the quality of a clustering . but good scores on an internal criterion do not necessarily translate into good effectiveness in an application . an alternative to internal criteria is direct evaluation in the application of interest . for search-result-clustering , we may want to measure the time it takes users to find an answer with different clustering-algorithms . this is the most direct evaluation , but it is expensive , especially if large user-studies are necessary . as a surrogate for user judgments , we can use a set of classes in an evaluation-benchmark or gold-standard (see section 8.5 , page 8.5 , and section 13.6 , page 13.6) . the gold-standard is ideally produced by human judges with a good level of inter-judge agreement (see chapter 8 , page 8.1) . we can then compute an external criterion that evaluates how well the clustering matches the gold-standard classes . for example , we may want to say that the optimal clustering of the search-results for jaguar in figure 16.2 consists of three classes corresponding to the three senses car , animal , and operating-system . in this type of evaluation , we only use the partition provided by the gold-standard , not the class labels . this section introduces four external-criteria of clustering-quality . purity is a simple and transparent evaluation-measure . normalized mutual-information can be information-theoretically interpreted . the rand index penalizes both false-positive and false-negative decisions during clustering . the f-measure in addition supports differential-weighting of these two types of errors . to compute purity , each cluster is assigned to the class which is most frequent in the cluster , and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by . formally : (182) 182 we present an example of how to compute purity in figure 16.4 . bad clusterings have purity values close to 0 , a perfect clustering has a purity of 1 . purity is compared with the other three measures discussed in this chapter in table 16.2 . table 16.2 : the four external evaluation-measures applied to the clustering in figure 16.4 . purity nmi ri lower-bound 0.0 0.0 0.0 0.0 maximum 1 1 1 1 value for figure 16.4 0.71 0.36 0.68 0.46 high purity is easy to achieve when the number of clusters is large - in particular , purity is 1 if each document gets its own cluster . thus , we can not use purity to trade-off the quality of the clustering against the number of clusters . a measure that allows us to make this tradeoff is normalized mutual-information or nmi : (183) 13 13.5.1 (184) (185) 185 184 is entropy as defined in chapter 5 (page 5.3.2) : (186) (187) in equation 184 measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are . the minimum of is 0 if the clustering is random with respect to class-membership . in that case , knowing that a document is in a particular cluster does not give us any new information about what its class might be . maximum mutual-information is reached for a clustering that perfectly recreates the classes - but also if clusters in are further subdivided into smaller clusters (exercise 16.7) . in particular , a clustering with one-document clusters has maximum mi . so mi has the same problem as purity : it does not penalize large cardinalities and thus does not formalize our bias that , other things being equal , fewer clusters are better . the normalization by the denominator in equation 183 fixes this problem since entropy tends to increase with the number of clusters . for example , reaches its maximum for , which ensures that nmi is low for . because nmi is normalized , we can use it to compare clusterings with different numbers of clusters . the particular form of the denominator is chosen because is a tight-upper-bound on (exercise 16.7) . thus , nmi is always a number between 0 and 1 . an alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions , one for each of the pairs of documents in the collection . we want to assign two documents to the same cluster if-and-only-if they are similar . a true-positive (tp) decision assigns two similar documents to the same cluster , a true negative (tn) decision assigns two dissimilar documents to different clusters . there are two types of errors we can commit . a (fp) decision assigns two dissimilar documents to the same cluster . a (fn) decision assigns two similar documents to different clusters . the rand index (-rrb- measures the percentage of decisions that are correct . that is , it is simply accuracy (section 8.3 , page 8.3) . as an example , we compute ri for figure 16.4 . we first compute . the three clusters contain 6 , 6 , and 5 points , respectively , so the total number of `` positives '' or pairs of documents that are in the same cluster is : (188) (189) and are computed similarly , resulting in the following contingency-table : same cluster different clusters same class different classes the rand index gives equal weight to false-positives and false-negatives . separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster . we can use the f-measure measuresperf to penalize false-negatives more strongly than false-positives by selecting a value , thus giving more weight to recall . exercises . replace every point in figure 16.4 with two identical copies of in the same class . (i) is it less difficult , equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in figure 16.4 ? (ii) compute purity , nmi , ri , and for the clustering with 34 points . which measures increase and which stay the same after doubling the number of points ? (iii) given your assessment in (i) and the results in (ii) , which measures are best suited to compare the quality of the two clusterings ?
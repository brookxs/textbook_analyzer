9.1.1 parallel-computing parallel-computing is the simultaneous application of multiple processors to solve a single problem , where each processor works on a different part of the problem . with parallel-computing , the overall time required to solve the problem can be reduced to the amount of time required by the longest running part . as long as the problem can be further decomposed into more parts that will run in parallel , we can add more processors to the system , reduce the time required to solve the problem , and scale up to larger problems . processors can be combined in a variety of ways to form parallel-architectures . flynn [259] has defined a commonly used taxonomy of parallel-architectures based on the number of the instruction and data-streams in the architecture . the taxonomy includes four classes : 誰 sisd single instruction stream , single data-stream 誰 simd single instruction stream , multiple data stream 誰 misd multiple instruction stream , single data-stream 誰 mimd multiple instruction stream , multiple data stream . the sisd class includes the traditional von neumann [134] computer running sequential-programs , e.g. , uniprocessor personal-computers . simd-computers consist of at processors operating on n data-streams , with each processor executing the same instruction at the same time . machines in this class are often massively-parallel computers with many relatively simple processors , a communication-network between the processors , and a control unit that supervises the synchronous-operation of the processors , e.g. , the thinking-machines cm-2 . the processors may use shared-memory , or each processor may have its own local-memory . sequential-programs require significant modification to make effective use of a simd-architecture , and not all problems lend themselves to a simd implementation . misd computers use n processors operating on a single data-stream in shared-memory . each processor executes its own instruct ion stream , such that multiple operations are performed simultaneously on the same data item . misd architectures are relatively rare . systolic-arrays are the best known example . mimd is the most general and most popular class of parallel-architectures . a mimd computer contains n processors , a '' instruction streams , and ar data-streams . the processors are similar to those used in a sisd computer ; each introduction 231 processor has its own control unit , processing unit , and local memory.f mimd systems usually include shared-memory or a communication-network that connects the processors to each other . the processors can work on separate , unrelated tasks , or they can cooperate to solve a single task , providing a great deal of flexibility . mimd systems with a high degree of processor interaction are called tightly-coupled , while systems with a low degree of processor interaction are loosely coupled . examples of mimd systems include multiprocessor pc servers , symmetric multiprocessors (smps) such as the sun hpc server , and scalable parallel processors such as the ibm-sp2 . although mimd typically refers to a single , self-contained parallel-computer using two or more of the same kind of processor , mimd also characterizes distributed-computing architectures . in distributed-computing , multiple-computers connected by a local or wide-area-network cooperate to solve a single problem . even though the coupling between processors is very loose in a distributed-computing-environment , the basic components of the mimd-architecture remain . each computer contains a processor , control unit , and local-memory , and the local or wide-area-network forms the communication-network between the processors . the main difference between a mimd parallel-computer and a distributed-computing-environment is the cost of interprocessor-communication , which is considerably higher in a distributed-computing-environment . as such , distributed-programs are usually coarse grained , while programs running on a single parallel-computer tend to be finer grained . granularity refers to the amount of computation relative to the amount of communication performed by the program . coarse-grained programs perform large amounts of computation relative to communication ; fine-grained programs perform large amounts of communication relative to computation . of course , an application may use different levels of granularity at different times to solve a given problem .
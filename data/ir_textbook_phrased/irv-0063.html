the index-terms are not independent although it may be mathematically convenient to assume that the index-terms are independent it by no means follows that it is realistic to do so . the objection to independence is not new , in 1964 h. h. williams [9] expressed it this way : ` the assumption of independence of words in a document is usually made as a matter of mathematical convenience . without the assumption , many of the subsequent mathematical relations could not be expressed . with it , many of the conclusions should be accepted with extreme caution . ' it is only because the mathematics become rather intractable if dependence is assumed that people are quick to assume independence . but , ` dependence is the norm rather than the contrary ' to quote the famous probability theorist de finetti [10] . therefore the correct procedure is to assume dependence and allow the analysis to simplify to the independent case should the latter be true . when speaking of dependence here we mean stochastic-dependence ; it is not intended as logical dependence although this may imply stochastic-dependence . for ir data , stochastic-dependence is simply measured by a correlation function or in some other equivalent way . the assumption of dependence could be crucial when we are trying to estimate p (relevance/document) in terms of p (x/wi) since the accuracy with which this latter probability is estimated will no doubt affect the retrieval-performance . so our immediate task is to make use of dependence (correlation) between index-terms to improve our estimate of p (x/wi) on which our decision-rule rests . in general the dependence can be arbitrarily complex as the following identity illustrates , p (x) = p (x1) p (x2/x1) p (x3/x1 , x2) ... p (xn/x1 , x2 , ... , xn - 1) therefore , to capture all dependence data we would need to condition each variable in turn on a steadily increasing set of other variables . although in principle this may be possible , it is likely to be computationally inefficient , and impossible in some instances where there is insufficient data to calculate the high order dependencies . instead we adopt a method of approximation to estimate p (x) which captures the significant dependence information . intuitively this may be described as one which looks at each factor in the above expansion and selects from the conditioning variables one particular variable which accounts for most of the dependence relation . in other words we seek a product approximation of the form where (m1 , m2 , ... , mn) is a permutation of the integers 1,2 , ... , n and j (.) is a function-mapping i into integers less than i , and p (xi/xm0) is p (xi) . an example for a six component-vector x = (x1 , ... , x6) might be pt (x) = p (x1) p (x2/x1) p (x3/x2) p (x4/x2) p (x5/x2) p (x6/x5) notice how similar the a2 assumption is to the independence-assumption a1 , the only difference being that in a2 each factor has a conditioning variable associated with it . in the example the permutation (m1 , m2 , ... , m6) is (1,2 , ... , 6) which is just the natural order , of course the reason for writing the expansion for pt (x) the way i did in a2 is to show that a permutation of (1,2 , ... , 6) must be sought that gives a good approximation . once this permutation has been found the variables could be relabelled so as to have the natural order again . the permutation and the function j (.) together define a dependence tree and the corresponding pt (x) is called a probability-distribution of (first-order) tree dependence . the tree corresponding to our six variable example is shown in figure 6.1 . the tree shows which variable appears either side of the conditioning stroke in p (. / .) . although i have chosen to write the function pt (x) the way i did with xi as the unconditioned variable , and hence the root of the tree , and all others consistently conditioned each on its parent node , in fact any one of the nodes of the tree could be singled out as the root as long as the conditioning is done consistently with respect to the new root-node . (in figure 6.1 the ` direction ' of conditioning is marked by the direction associated with an edge .) the resulting pt (x) will be the same as can easily be shown by using the fact that applying this to the link between the root-node x1 and its immediate descendant x2 in the example will shift the root to x2 and change the expansion to pt (x1 , x2 , ... x6) = p (x2) p (x1) / x2) p (x3/x2) p (x4/x2) p (x5/x2) p (x6/x5) of course , to satisfy the rule about relabelling we would exchange the names ' 1 ' and ' 2 ' . all expansions transformed in this way are equivalent in terms of goodness of approximation to p (x) . it is therefore the tree which represents the class of equivalent expansions . clearly there are a large number of possible dependence-trees , the approximation problem we have is to find the best one ; which amounts to finding the best permutation and mapping j (.) . in what follows i shall assume that the relabelling has been done and that xmi = xi .
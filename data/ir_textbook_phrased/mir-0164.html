9.1.2 performance-measures when we employ parallel-computing , we usually want to know what sort of performance-improvement we are obtaining over a comparable sequential program running on a uniprocessor . a number of metrics are available to measure the performance of a parallel-algorithm . one such measure is the speedup obtained with the parallel-algorithm relative to the best available sequential-algorithm for solving the same problem , defined as : __ running-time of best available sequential-algorithm running-time of parallel-algorithm i the processors used in a mimd system may be identical to those used in sisd systems , or they may provide additional functionality , such as hardware cache-coherence for shared-memory . 232 parallel and distributed ir ideally , when running a parallel-algorithm on n processors , we would obtain perfect speedup , or s = n . in practice , perfect speedup is unattainable either because the problem can not be decomposed into n equal subtasks , the parallel-architecture imposes control overheads (e.g. , scheduling or synchronization) , or the problem contains an inherently sequential component . amdahl 's law [18] states that the maximal speedup obtainable for a given problem is related to / , the fraction of the problem that must be computed sequentially . the relationship is given by : another measure of parallel-algorithm performance is efficiency , given by : where s is speedup and n is the number of processors . ideal efficiency occurs when 0 = 1 and no processor is ever idle or performs unnecessary work . as with perfect speedup , ideal efficiency is unattainable in practice . ultimately , the performance-improvement of a parallel-program over a sequential program should be viewed in terms of the reduction in real-time required to complete the processing task combined with the additional monetary cost associated with the parallel hardware required to run the parallel-program . this gives the best overall picture of parallel-program-performance and cost-effectiveness .
implication of classification-methods it is fairly difficult to talk about the implementation of an automatic-classification method without at the same time referring to the file-structure representing it inside the computer . nevertheless , there are a few remarks of importance which can be made . just as in many other computational-problems , it is possible to trade core storage and computation-time . in experimental ir , computation-time is likely to be at a premium and a classification-process can usually be speeded up by using extra storage . one important decision to be made in any retrieval-system concerns the organisation of storage . usually part of the file-structure will be kept in fast store and the rest on backing store . in experimental ir we are interested in a flexible system and getting experiments done quickly . therefore , frequently much or all of a classification structure is kept in fast store although this would never be done in an operational system where the document collections are so much bigger . another good example of the difference in approach between experimental and operational implementations of a classification is in the permanence of the cluster representatives . in experiments we often want to vary the cluster representatives at search-time . in fact , we require that each cluster representative can be quickly specified and implemented at search-time . of course , were we to design an operational classification , the cluster representatives would be constructed once and for all at cluster time . probably one of the most important features of a classification implementation is that it should be able to deal with a changing and growing document-collection . adding documents to the classification should not be too difficult . for instance , it should not be necessary to take the document classification ` off the air ' for lengthy periods to update it . so , we expect the classification to be designed in such a way that a new batch of documents can be readily inserted without reclassifying the entire set of both old and new documents . although many classification algorithms claim this feature , the claim is almost invariably not met . because of the heuristic nature of many of the algorithms , the updated classification is not the same as it would have been if the increased set had been classified from scratch . in addition , many of the updating strategies mess up the classification to such an extent that it becomes necessary to throw away the classification after a series of updates and reclassify completely . these comments tend to apply to the n log n classification-methods . unfortunately , they are usually recommended over the n [2] methods for two reasons . firstly , because n log n is considerably less than n [2] , and secondly because the time increases only as log n for the n log n methods but as n for the n [2] methods . on the face of it these are powerful arguments . however , i think they mislead . if we assume that the n log n methods can not be updated without reclassifying each time and that the n [2] methods can (for example , single-link) , then the correct comparison is between where n1 lt ; n2 lt ; ... lt ; nt = n , and t is the number of updates . in the limit when n is a continuous variable and the sum becomes an integral we are better off with n [2] . in the discrete case the comparison depends rather on the size of the updates ni - ni - 1 . so unless we can design an n log n dependence as extra documents are added , we may as well stick with the n [2] methods which satisfy the soundness conditions and preserve n [2] dependence during updating . in any case , if one is willing to forego some of the theoretical adequacy conditions then it is possible to modify the n [2] methods to ` break the n [2] barrier ' . one method is to sample from the document collection and construct a core clustering using an n [2] method on the sample of the documents . the remainder of the documents can then be fitted into the core clustering by a very fast assignment strategy , similar to a search-strategy which has log n dependence . a second method is to initially do a coarse clustering of the document collection and then apply the finer classification-method of the n [2] kind to each cluster in turn . so , if there are n documents and we divide into k coarse clusters by a method that has order n time dependence (e.g. rieber and marathe 's method) then the total cluster time will be of order n + [[sigma]] (n/k) [2] which will be less than n [2] . another comment to be made about n log n methods is that although they have this time dependence in theory , examination of a number of the algorithms implementing them shows that they actually have an n [2] dependence (e.g. rocchio 's algorithm) . furthermore , most n log n methods have only been tested on single-level classifications and it is doubtful whether they would be able to preserve their n log n dependence if they were used to generate hierarchic classifications (senko [54]) . in experiments where we are often dealing with only a few thousand documents , we may find that the proportionality constant in the n log n method is so large that the actual time taken for clustering is greater than that for an n [2] method . croft [55] recently found this when he compared the efficiency of snob (boulton and wallace [56]) , an n log n cluster method , with single-link . in fact , it is possible to implement single-link in such a way that the generation of the similarity values is overlapped in real-time with the cluster generation-process . the implementation of classification algorithms for use in ir is by necessity different from implementations in other fields such as for example numerical taxonomy . the major differences arise from differences in the scale and in the use to which a classification structure is to be put . in the case of scale , the size of the problem in ir is invariably such that for cluster methods based on similarity matrices it becomes impossible to store the entire similarity-matrix , let alone allow random-access to its elements . if we are to have a reasonably useful cluster method based on similarity matrices we must be able to generate the similarity-matrix in small sections , use each section to update the classification structure immediately after it has been generated and then throw it away . the importance of this fact was recognised by needham [57] . van rijsbergen [48] has described an implementation of single-link which satisfies this requirement . when a classification is to be used in ir , it affects the design of the algorithm to the extent that a classification will be represented by a file-structure which is (1) easily updated ; (2) easily searched ; and (3) reasonably compact . only (3) needs some further comment . it is inevitable that parts of the storage used to contain a classification will become redundant during an updating phase . this being so it is of some importance to be able to reuse this storage , and if the redundant storage becomes excessive to be able to process the file-structure in such a way that it will subsequently reside in one contiguous part of core . this ` compactness ' is particularly important during experiments in which the file-structure is read into core before being accessed .
8.1.1 www crawling one important way in which web-search-engines extend beyond the notions of foa presented here concerns the crawlers that feed them . in all of our discussions , the corpus was imagined to be a static object . for www-search engines , the underlying set of documents that are to be indexed and made available to users is constantly changing . further , the task of quickly , reliably , and exhaustively visiting all www-linked pages is a fundamental task in and of itself . one good , accessible example of 2 aslgeeves.com / conclusions and future-directions 295 0.4 8gt ; gt ; o o ÃŸgt ; s 0.3 0.2 0.1 hotbot altavista northern excite infoseek lycos light figure 8.1 crawler coverage . from [lowrence and giles , 1998] . reproduced with permission of american association for the advancement of science crawler code is provided by the libwww robot ,3 part of the www consortium (w3c) libwww distribution . a perl-based crawler interface4 has also been developed by gisle aas ; paralleluseragent ,5 developed by marc langheinrich , is another perl alternative . naive www users often seem to have the tacit belief that every web-crawler is aware of (i.e. , has indexed) every document on the web . more sophisticated users know that there is a certain lag time between the posting of a new page and its inclusion in the-web search-engine 's index . but the fundamental omissions by most search-engine crawlers are still underappreciated . the most concrete data in this respect is due to a recent experiment done by lawrence and giles [lawrence and giles , 1998] , shown in figure 8.1 . using a statistical extrapolation from the mismatch of documents found by one of the six most important search-engines suggests that at that time the-web contained approximately 320 million pages . of this total , even the best search-engine was able to capture only about a third of those documents . the ecology of these various search-engines and their co-evolutionary technological responses to one another create an extremely dynamic situation . danny sullivan edits an excellent newsletter , search-engine watch ,6 that does nothing but track changes in the volatile 3 www.w5.org/robot/ 4 www.mnpro.no/lwp/ a ' http : / / www.inf . ethz.ch / - laxigpieta/barauelua / 296 finding out about marketplace of search-engines and portals . the search-engine business and supporting-technologies can be expected to continue to foment for some time to come . in asymptote , however , current notions of search-engines will go extinct for two basic reasons : their methods do not scale to the-internet , and they only get in the way . search-engines do n't scale scalability is a major issue limiting the effectiveness of search-engines . the factors contributing to the problem are the large size of the www , its rapid growth , and its highly dynamic nature . in order to keep indexes up to date , crawlers periodically revisit every indexed document to see what has been changed , moved , or deleted . heuristics are used to estimate how frequently a document is changed and needs to be revisited , but the accuracy of such statistics is highly volatile . moreover , crawlers attempt to find newly added documents either exhaustively or based on user-supplied urls . yet lawrence and giles have shown that the coverage achieved by search-engines is at best around 33 percent , and that coverage is anticorrelated with currency - the more complete an index , the staler the links [lawrence and giles , 1998] . more importantly , such disappointing performance comes at high costs in terms of the load imposed on the net [eichmann , 1994] . this becomes an important reason for investigating search agents for the www like those described in section 7.6 . online agents do not have a scale problem because they search through the current environment and therefore do not run into stale-information . on the other hand , they are less efficient than search-engines because they can not amortize the cost of a search over many queries . disintermediation section 8.2.1 will discuss foa as a particular type of `` language game . '' in brief , the foa language game is played by three players : the text 's author , its readers , and the search-engine . authors have something to say and an audience they are trying to say it to . they attempt to characterize their content to intermediates (book publishers , journal editors , www-search engines) in ways that capture `` markets '' for what they have to say . readers have an information-need and some ideas about where to look for writings that might satisfy it . these readers sometimes (and now conclusions and future-directions 297 much more often than in the past) characterize their information-need to intermediates (librarians , paralegals , www-search engines) in hopes of being shown documents likely to be relevant to their information-needs . the second fundamental flaw of current search-engines , then , is that they are and will forever be only mediators ; they neither produce content nor consume it directly . the search-engine is caught in the middle of the other two players . it must somehow make the correspondence between the languages used by writers and readers . if it plays its part of the foa language game well , it reliably connects readers with writers . said another way , search-engines are simply noise in the channel between author and reader . if they are doing their jobs effectively , they should disappear as transparent background to facilitate easy communication of rich messages . the difficulty browsing users currently experience as they attempt to foa documents on the www makes it clear just how far current search-engines are from this ideal . traditionally , authors have made conventional assumptions about how their readers would find them . they would sell their book to a publisher , and part of this economic relationship involved the publisher putting its distribution-channels at the services of the author . for magazine and newspaper reporters , as well as for fiction authors , periodical publications provided a regular audience for a magazine of contributions . textbook authors would favor publishers with extensive connections with educational institutions . scientists would submit articles to peer-review under the supervision of editors for professional-societies . in every case , multiple levels of mediation between the author and the reader are assumed by the author . even if the www were only a new technological substrate on which all of these conventional activities occurred , we might expect the level of confusion now present as search-engines cross everyone 's wires . but it seems likely that the change is even more fundamental : the number of content-producers (writers) is rapidly approaching the number of content-consumers (readers) ! never before has the machinery of producing and distributing media been as widely available as it is today . our collective expectations as to just what documents are clt ; out there , '' not to mention the care and authority with which they have been authored , are in terrific flux . 298 finding out about authors trying to be heard through this cacophony must fundamentally rethink their assumptions of how their content will be published . the most obvious examples of this are author-created keyword meta-tags . a wide range of meta-tags are now in use - ranging from ones that carry intellectual-property information to ones that carry `` decency '' ratings ; the html standard in fact allows an open-ended set of such tags to support any number of additional attributes of the document . two meta-tags , however , are especially important from the perspective of foa . the keywords meta-tag is designed to contain (the author 's recommendation for) content descriptors , and the description meta-tag to provide a proxy string . both provide explicit mechanisms for authors to convey additional meaning in their writings , beyond words that happen to be in the text of the document itself . they have the additional advantage of being free of any morphological and weighting heuristics used by a particular search-engine . of course , this additional expressive-power on the part of authors also makes it at least possible for them (or their web masters) to attempt to spoof search-engines with meta-tags designed simply to draw users to the page . like much of the law concerning the www , exactly what constitutes `` good faith '' use of meta-tags is a matter of great debate (a recent example is playboy v. terri welles7) . whether in good faith or not , attempts by authors to express themselves clearly are currently compromised by the refusal of search-engines to publicly commit to some basic standards of crawling and indexing behavior . their wide variety in operation , compounded by opaque descriptions of how each works , currently makes articulate expression by just how do an author impossible . ^ it is no wonder that searching users become alta-vista , confused . hotbot , ... work ?!
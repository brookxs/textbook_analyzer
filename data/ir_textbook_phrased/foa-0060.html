4 ________ assessing the retrieval we 've come a long way since chapter 1 , where we first sketched the full range of activities we might consider foa . as chapter 2 considered the various ways of breaking text into indexable features , and chapter 3 explained the various ways of weighting combinations of these features to identify the best matches to a query , i hope you have been aware of how many alternatives have been mentioned ! that is , rarely has there been a single method that can be proven to be better than all others . ir has traditionally been driven by empirical demonstrations , and the range of commercial competitors now trying to provide the `` best '' search of the www makes it likely this performance-orientation will continue . but whether we are search engineers , scientists objectively assessing one particular technique , or consumers of www-search engine technology interested in buying and using the best , a solid basis of performance-assessment is criticalseveral perspectives on assessment are possible . in the first chapter foa was viewed as a personal activity , adopting the users1 points of view . section 4.1 will continue in this theme , considering how users assess the results of their retrievals and how they can express their opinions using relevance-feedback (relevance-feedback) . oddy is credited with first identifying this important stream of data , naturally provided by users as a part of their foa browsing [oddy , 1977 ; belkin et al , 1982] . 105 106 finding out about but in this book we are also concerned with foa from the ir-system builder 's point-of-view . ideally , we would like to construct a search-engine that robustly finds the `` right '' documents for each query and for each user . the second section of this chapter discusses performance-measures of statistical-properties that are reliable across large numbers of users and their highly variable queries . the key to these measures is having some insight into which documents should have been retrieved , typically because some idealized omniscient expert has determined (within a specially constructed experimental situation) that certain documents `` should '' have been retrieved . alternatively the relevance-feedback of many users can be combined to form a consensual opinion of relevance , as described in section 4.4 . a concrete notion of relevance would seem a fundamental precondition for understanding either an individual 's relevance-feedback or how this can be used to assess a search-engine . but in this respect , information-retrieval generally , and relevance-feedback in particular , is like many other academic areas of study (including artificial-intelligence and genetics) in that the lack of a fully satisfactory definition of the core concept (information , intelligence , genes , and so on) has not entirely stopped progress . that is , a great deal can be done by operationalizing relevance-feedback to be simply those relevance-assessment behaviors produced as part of an foa dialog . this operational simplification will hold us until fundamental issues of language-and-communication are again addressed in section 8.2.1 .
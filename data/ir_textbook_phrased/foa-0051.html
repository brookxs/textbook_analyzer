3.4.1 keyword discrimination we can immediately use this vector-space for something useful , as the source of yet another approach to the question of appropriate keyword weightings . recall that in figure 3.7 our initial assumption was that each and every keyword was to be used as a dimension of the vector-space . now we ask : what would happen if we removed one of these keywords ? the first step is to extend the measure sim (q , d) of documentquery similarity to measure interdocument similarities sim {di , dj) as well . then , for an arbitrary measure of document-document similarity (e.g. , the inner-product measure mentioned earlier) , we consider all pairs of documents and then the average similarity across all t sim {di , dj) = similarity among documents d * = centroid ; average document 1 ndoc2 * r โข y] sim (dhdj) (3.24) (3.25) (3.26) recall that our goal is to devise a representation of documents that makes it easy for queries to discriminate among them . because each keyword corresponds to a dimension , removing one results in a compression of the space into k รณ 1 dimensions , and we can expect that the representation of each document will change at least slightly . for example , removing a dimension along which the documents varied significantly means that vectors that were far apart in the ic-dimensional space are now much closer together . this observation can be used to ask how useful each potential keyword is . if it is discriminating , removing it will result in a significant compression of the documents ' vectors ; if removing it changes very little , the keyword is less helpful using the average similarity as our measure of how close together the documents are , and asking this question for each and every keyword * we arrive at yet another measure of keyword discrimination : = sim when terntk removed disci ; h sinii ~ ~ sim w ' u = fkd * disc * (3.27) (3.28) (3.29)
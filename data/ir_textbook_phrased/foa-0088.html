5.2.1 a simple example imagine that we 've collected data on the height and weight of everyone in a classroom of n students . if these are plotted , the result would be 154 finding out about beyond the puny three-dimensions of human existence size weight figure 5.3 weight and height data-reduction something like figure 5.3 . notice the correlation around an axis we might call something like size . students vary most along this dimension ; it captures most of the information about their distribution . it is possible to capture a major source of variation across the height/weight sample because , just as with our keywords , the two quantities are correlated . in this section we analyze similar statistical correlations among the keywords and documents contained in the much larger vector-space-model first mentioned in section 3.4 . recall that in the vector-space-model , the index relation placing d = ndoc vectors corresponding to the corpus documents within the space 5ft v , where v = nkw (for vocabulary-size) , is defined by its keyword vocabulary . here we describe this in the terms of linear-algebra , * where / = index is a d x v element matrix , t attempts to reduce this large dimensional space into something smaller are called dimensionality-reduction . there are two reasons we might be interested in reducing dimensions . the first is probably more obvious : it 's a very unwieldy representation of documents ' content . individual documents will have many zeros , corresponding to the many words in the corpus v not present in an individual document ; the vector-space matrix is very sparse . dimensionality-reduction is a search for a representation that is denser , more compressed . another reason might be to exploit what has become known as latent-semantic relationships among these keywords . when we make each term in our vocabulary a dimension , we are effectively assuming they are orthogonal to one another ; we expect their effects to be independent . * in this language , single-letter identifiers are simplest , but that would make the index relation / . unfortunately , the letter / already plays a useful role in linear-algebra , as the identity matrix ; hence , / = index . for similar reasons , within this section , we will use v = nkwmd d = ndoc mathematical foundations 155 but many features of foa suggest that index-terms are highly dependent , highly correlated with one another . if that 's the case , we can exploit that correlation by capturing only those axes of maximal variation and throwing away the rest .
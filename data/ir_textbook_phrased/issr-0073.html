5.2.2.1 simple term-frequency algorithm in both the unweighted and weighted approaches , an automatic-indexing process implements an algorithm to determine the weight to be assigned to a processing token for a particular item . in a statistical system , the data that are potentially available for calculating a weight are the frequency of occurrence of the processing token in an existing item (i.e. , term-frequency - tf) , the frequency of occurrence of the processing token in the existing database (i.e. , total frequency totf) and the number of unique items in the database that contain the processing token (i.e. , item frequency - if , frequently labeled in other publications as document-frequency - df) . as discussed in chapter 3 , the premises by luhn and later brookstein that the resolving power of content-bearing words is directly proportional to the frequency of occurrence of the word in the item is used as the basis for most automatic-weighting techniques . weighting techniques usually are based upon positive weight values . 114 chapters the simplest approach is to have the weight equal to the term-frequency . this approach emphasizes the use of a particular processing token within an item . thus if the word `` computer '' occurs 15 times within an item it has a weight of 15 . the simplicity of this technique encounters problems of normalization between items and use of the processing token within the database . the longer an item is , the more often a processing token may occur within the item . use of the absolute value biases weights toward longer items , where a term is more likely to occur with a higher frequency . thus , one normalization typically used in weighting algorithms compensates for the number of words in an item . an example of this normalization in calculating term-frequency is the algorithm used in the smart system at cornell (buckley-96) . the term-frequency weighting-formula used in trec 4 was : ______________ (1 + logftfwl + log (average (tf)) (1 - slope) * pivot + slope * number of unique terms where slope was set at .2 and the pivot was set to the average number of unique terms occurring in the collection (singhal-95) . in addition to compensating for document-length , they also want the formula to be insensitive to anomalies introduced by stemming or misspellings . although initially conceived of as too simple , recent experiments by the smart system using the large-databases in trec demonstrated that use of the simpler algorithm with proper normalization factors is far more efficient in processing queries and return hits similar to more complex algorithms . there are many approaches to account for different document lengths when determining the value of term-frequency to use (e.g. , an items that is only 50 words may have a much smaller term-frequency then and item that is 1000 words on the same topic) . in the first technique , the term-frequency for each word is divided by the maximum frequency of the word in any item . this normalizes the term-frequency values to a value between zero and one . this technique is called maximum term-frequency . the problem with this technique is that the maximum term-frequency can be so large that it decreases the value of term-frequency in short items to too small a value and loses significance . another option is to use logaritmetic term-frequency . in this technique the log of the term-frequency plus a constant is used to replace the term-frequency . the log function will perform the normalization when the term frequencies vary significantly due to size of documents . along this line the cosine function used as a similarity-measure (see chapter 7) can be used to normalize values in a document . this is accomplished by treating the index of a document as a vector and divide the weights of all terms by the length of the vector . this will normalize to a vector of maximum length one . this uses all of the data in a particular item to perform the normalization and will not be distorted by any particular term . the problem occurs when there are multiple topics within an item . the cosine technique will normalize all values based upon the total length of the vector that automatic-indexing 115 represents all of topics . if a particular topic is important but briefly discussed , its normalized value could significantly reduce its overall importance in comparison to another document that only discusses the topic . another approach recognizes that the normalization-process may be over penalizing long documents (singhal-95) . singhal did experiments that showed longer documents in general are more likely to be relevant to topics then short documents . yet normalization was making all documents appear to be the same length . to compensate , a correction factor was defined that is based upon document-length that maps the cosine function into an adjusted normalization function . the function determines the document length crossover-point for longer documents where the probability of relevance equals the probability of retrieval , (given a query set) . this value called the `` pivot point '' is used to apply an adjustment to the normalization-process . the theory is based upon straight lines so it is a matter of determining slope of the lines . new normalization = (slope) * (old normalization) + k k is generated by the rotation of the pivot point to generate the new line and the old normalization = the new normalization at that point . the slope for all higher values will be different . substituting pivot for both old and new value in the above formula we can solve for k at that point . then using the resulting formula for k and substituting in the above formula produces the following formula : pivoted function = slope) * (old normalization) + (1.0 - slope) * (pivot) slope and pivot are constants for any document/query set . another problem is that the cosine function favors short documents over long documents and also favors documents with a large number of terms . this favoring is increased by using the pivot technique . if log (tf) is used instead of the normal frequency then tf is not a significant factor , in documents with large number of terms the cosine factor is approximated by the square-root of the number of terms . this suggests that using the ratio of the logs of term frequencies would work best for longer items in the calculations : (1 + log (tf)) / (i + log (average (tf)) this leads to the final algorithm that weights each term by the above formula divided by the pivoted normalization : ((1 + log (tf)) / (l + log (average (tf)) / (slope) (no . unique terms) + (l-slope) * (pivot) singhal demonstrated the above formula works better against trec-data then tf/iviax (tf) or vector length-normalization . the effect of a document with a high term-frequency is reduced by the normalization function by dividing the tf by the average tf and by use of the log function . the use of pivot normalization 16 chapter 5 adjusts for the bias towards shorter documents increasing the weights of longer documents .
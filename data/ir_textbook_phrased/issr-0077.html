5.2.2.5 problems with weighting-schemes often weighting-schemes use information that is based upon processing token distributions across the database . the two weighting-schemes , inverse-document-frequency and signal , use total frequency and item frequency factors which makes them dependent upon distributions of processing tokens within the database . information databases tend to be dynamic with new items always being added and to a lesser degree old items being changed or deleted . thus these factors are changing dynamically . there are a number of approaches to compensate for the constant changing values . a. ignore the variances and calculate weights based upon current values , with the factors changing over time . periodically rebuild the complete-search database . b. use a fixed value while monitoring changes in the factors . when the changes reach a certain threshold , start using the new value and update all existing vectors with the new value . c. store the invariant variables (e.g. , term-frequency within an item) and at search-time calculate the latest weights for processing tokens in items needed for search-terms . in the first approach the assumption minimizes the system overhead of maintaining currency on changing values , with the effect that term-weights for the same term vary from item to item as the aggregate variables used in calculating the weights based upon changes in the database vary over time . periodically the database and all term-weights are recalculated based upon the most recent updates to the database . for large-databases in the millions of items , the overhead of automatic-indexing 121 rebuilding the database can be significant . in the second approach , there is a recognition that for the most frequently occurring items , the aggregate values are large . as such , minor changes in the values have negligible effect on the final weight calculation . thus , on a term basis , updates to the aggregate values are only made when sufficient changes not using the current value will have an effect on the final weights and the search/ranking process . this process also distributes the update process over time by only updating a subset of terms at any instance in time . the third approach is the most accurate . the weighted values in the database only matter when they are being used to determine items to return from a query or the rank-order to return the items . this has more overhead in that database vector term-weights must be calculated dynamically for every query-term . if the system is using an inverted-file search structure , this overhead is very minor . an interesting side-effect of maintaining currency in the database for term-weights is that the same query over time returns a different ordering of items . a new word in the database undergoes significant changes in its weight structure from initial introduction until its frequency in the database reaches a level where small changes do not have significant impact on changes in weight values . another issue is the desire to partition an information database based upon time . the value of many sources-of-information vary exponentially based upon the age of an item (older items have less value) . this leads to physically partitioning the database by time (e.g. , starting a new database each year) , allowing the user to specify the time period to search . there are issues then of how to address the aggregate variables that are different for the same processing token in each database and how to merge the results from the different databases into a single hit file . the best environment would allow a user to run a query against multiple different time periods and different databases that potentially use different weighting algorithms , and have the system integrate the results into a single ranked hit file . this issue is discussed in chapter 7 .
hierarchical-agglomerative-clustering agglomerate hierarchical-agglomerative-clustering hac 17.6 a dendrogram of a single-link-clustering of 30 documents from reuters-rcv1 . two possible cuts of the dendrogram are shown : at 0.4 into 24 clusters and at 0.1 into 12 clusters . before looking at specific similarity-measures used in hac in sections 17.2 -17.4 , we first introduce a method for depicting hierarchical clusterings graphically , discuss a few key properties of hacs and present a simple algorithm for computing an hac . an hac clustering is typically visualized as a dendrogram as shown in figure 17.1 . each merge is represented by a horizontal line . the y-coordinate of the horizontal line is the similarity of the two clusters that were merged , where documents are viewed as singleton clusters . we call this similarity the combination similarity of the merged cluster . for example , the combination similarity of the cluster consisting of lloyd 's ceo questioned and lloyd 's chief / u.s. grilling in figure 17.1 is . we define the combination similarity of a singleton cluster as its document 's self-similarity (which is 1.0 for cosine-similarity) . by moving up from the bottom layer to the top node , a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering . for example , we see that the two documents entitled war hero colin powell were merged first in figure 17.1 and that the last merge added ag trade reform to a cluster consisting of the other 29 documents . a fundamental assumption in hac is that the merge-operation is monotonic . monotonic means that if are the combination similarities of the successive merges of an hac , then holds . a non-monotonic hierarchical-clustering contains at least one inversion and contradicts the fundamental assumption that we chose the best merge available at each step . we will see an example of an inversion in figure 17.12 . hierarchical-clustering does not require a prespecified number-of-clusters . however , in some applications we want a partition of disjoint clusters just as in flat clustering . in those cases , the hierarchy needs to be cut at some point . a number of criteria can be used to determine the cutting point : cut at a prespecified level of similarity . for example , we cut the dendrogram at 0.4 if we want clusters with a minimum combination similarity of 0.4 . in figure 17.1 , cutting the diagram at yields 24 clusters (grouping only documents with high similarity together) and cutting it at yields 12 clusters (one large financial news cluster and 11 smaller clusters) . cut the dendrogram where the gap between two successive combination similarities is largest . such large gaps arguably indicate `` natural '' clusterings . adding one more cluster decreases the quality of the clustering significantly , so cutting before this steep decrease occurs is desirable . this strategy is analogous to looking for the knee in the - means graph in figure 16.8 (page 16.8) . apply equation 195 (page 16.4.1) : where refers to the cut of the hierarchy that results in clusters , rss is the residual sum-of-squares and is a penalty for each additional cluster . instead of rss , another measure of distortion can be used . as in flat clustering , we can also prespecify the number of clusters and select the cutting point that produces clusters . figure 17.2 : a simple , but inefficient hac algorithm . a simple , naive hac algorithm is shown in figure 17.2 . we first compute the similarity-matrix . the algorithm then executes steps of merging the currently most similar clusters . in each iteration , the two most similar clusters are merged and the rows and columns of the merged cluster in are updated.the clustering is stored as a list of merges in . indicates which clusters are still available to be merged . the function sim computes the similarity of cluster with the merge of clusters and . for some hac algorithms , sim is simply a function of and , for example , the maximum of these two values for single-link . we will now refine this algorithm for the different similarity-measures of single-link and complete-link-clustering (section 17.2) and group-average and centroid-clustering (and 17.4) . the merge criteria of these four variants of hac are shown in figure 17.3 .
3.3.6 informative signals versus noise words we begin with a weighting algorithm derived from information-theory . information-theory has proven itself to be an extraordinarily useful model of many different situations in which some message must be communicated across a noisy-channel and our goal is to devise an encoding for messages that is most robust in the face of this noise . in our case , we must imagine that the `` messages '' describe the content of documents in our corpus . on this account , the amount of information we get about this content from a word is inversely proportional to its probability of occurrence . in other words , the least informative word in our corpus is the one that occurs approximately uniformly across the corpus . for example , the word the occurs at about the same frequency across every document in the collection ; its probability of occurrence in any one document is almost uniform . we gain the least information about the document 's contents from observing it .1 '' what is salton and mcgill [salton and mcgill , 1983] , following dennis `` information '' ? [dennis , 1967] , use shannon 's classic binary logarithm to measure the amount of information conveyed by each word 's occurrence in bits and noise to be the absence of information : pk = pr (keyword k occurs) (3.14) infok = - log pk (3.15) noisek = - log (l/p *) (3.16) note that our evidence about the probability of a keyword occurring comes from statistics of how frequently it occurs . we must compare how frequently a keyword occurs in a particular document , relative to how frequently it occurs throughout the entire collection . we can calculate the expected noise associated with a keyword across the corpus , and from this we can infer its remaining signal . signal then becomes another measure we can use to weight the frequency of occurrence of the keyword 84 finding out about fid informative word noise word liliiliii figure 3.6 hypothetical word distributions document : (noisek) = (pjfclog (l/pjt) gt ; = signalk = log fk - noisek = fkd * signalk a po % fd (3.17) (3.18) (3.19) two hypothetical distributions , for a noise word and a useful index term , are shown in figure 3.6 . a noise word is equally likely to occur anywhere ; its distribution is nearly uniform . on the other hand , if all of the occurrences of a keyword are localized in a few documents (conveniently clustered together in the cartoon of figure 3.6) and mostly zero everywhere else , this is an informative word . you 've learned something about the document 's content when you see it . 3.3.7 inverse-document-frequency up to this point , we 've been concerned only with the total number of times a word occurs across the entire corpus . karen sparck jones has observed that , from a discrimination point-of-view , what we 'd really like to know is the number of documents containing a keyword . this thinking underlies the inverse-document-frequency (idf) weighting : the basis for idf weighting is the observation that people tend to express their information-needs using rather broadly defined , frequently occurring terms , whereas it is the more specific * i.e. , weighting and matching against indices 85 low-frequency terms that are likely to be of particular importance in identifying relevant material . this is because the number of documents relevant to a query is generally small , and thus any frequently occurring terms must necessarily occur in many irrelevant documents ; infrequently occurring terms have a greater probability of occurring in relevant documents - and should thus be considered as being of greater potential when searching a database . [sparck jones and willett , 1997 , p. 307] rather than looking at the raw occurrence frequencies , we will aggregate occurrences within any document and consider only the number of documents in which a keyword occurs . idf proposes , again using a `` statistical interpretation of term-specificity '' [sparck jones , 1972] , that the value of a keyword varies inversely with the log of the number of documents in which it occurs : (ugt ; kd = fkd * (log ^ p + l) (3.20) where dk is as defined in equation 3.12 . the formula in equation 3.20 is still not fully specified in that the count dk must be normalized with respect to a constant norm . we could normalize with respect to the total number of documents in the corpus [sparck jones , 1972 ; croft and harper , 1979] ; another possibility is to normalize against the maximum document-frequency (i.e. , the most documents any keyword appears in) [sparck jones 1979a ; sparck jones , 1979b] : xt indoc or / aoi \ norm = \ _ (3.21) [argmaxkdk today the most common form of idf weighting is that used by robertson and sparck jones [robertson and sparck jones , 1976] , which normalizes with respect to the number of documents not containing a keyword (ndoc รณ dk) and adds a constant of 0.5 to both numerator and denominator to moderate extreme-values : / (ndoc - djt) + 03 \ = fkd * ^ log รณ ^ ^ 5 ----- j (3.22)
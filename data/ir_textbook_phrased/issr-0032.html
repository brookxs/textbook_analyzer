2.3.4 multimedia once a list of potential items that satisfy the query are discovered , the techniques for displaying them when they are multimedia introduces new challenges . the discussions under ranking , above , suggest that typically hits are listed in the likely order of importance with one hit per line . the rationale is to present the most information possible to the user on the largest number of item to allow the user to select the items to be retrieved . to display more aggregate data , textual-interfaces sometimes allow for clustering of the hits and then use of graphical display to show a higher level view of the information . neither of these techniques lend themselves well when the information is multimodal . the textual aspect of the multimedia can be used to apply all of the techniques described above . but using the example in 2.1.9 above , how does the system also include the image of the boat that was part of the query . typically a `` thumbnail '' of the image is displayed with the hit item . but this has the disadvantage of using more than one line per hit and reducing the number of hits that a user can select from on a single screen . if the source is audio , then other problems associated with the human linear processing of audio becomes major issues . in the textual domain , users can visually scan text very fast and via peripheral-processing can maintain the context of what they are reading . if the output against audio searches was audio hit files , the user processing rate associated with listening drops dramatically . this is another reason why the transcribed audio (even if it is errorful) becomes a critical augmentation in users reviewing audio files . thus in addition to listening to the audio , the user can visually be following the transcribed text . this provides a mechanism for the user to percieve the context and additionally provides a quick 44 chapter 2 scanning option to look-ahead at upcoming information to be used in conjunction with the audio-processing of the original source . in tests recently performed (may 2000 in paper to be published) it was shown that the combination of errorful transcribed audio used in conjunction with the having the original audio also available reduced the processing-time of items by 50 per cent from audio only (this was the initial testing , experience would drop it even further) . the transcribed text could also be used as a navigation technique through the audio (whittaker-99 .) they appropriately label this new paradigm what you see is (almost) what you hear (wysiawyh) . they also noted that even though the transcribed text could be used as an index into future retrieval of the audio source , most users also needed the ability to annotate the transcriptions (note-taking) to allow them to include other audio information such as in tonal information provided by the original speech . the second area of complexity comes from any attempt at ranking such an output . in the textual only domain , there has been significant research into algorithms and heuristics associated with weighting algorithms for textual items . but when the hit is composed of different modalities , (e.g. , text , image , audio) , how did you create an aggregate weight of each `` hit region '' . little research has gone into the derivation of the weight assigned to an item that combines the weights assigned to how well each modality satisfied its portion of the query .
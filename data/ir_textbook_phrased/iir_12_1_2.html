types of language-models how do we build probabilities over sequences of terms ? we can always use the chain-rule from equation 56 to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events : (94) unigram-language-model (95) bigram language models (96) speech-recognition spelling-correction machine-translation sparseness 13.2 bias-variance-tradeoff 11 11.4.2
130 chapter 5 5.4 concept-indexing natural-language-processing starts with a basis of the terms within an item and extends the information kept on an item to phrases and higher level concepts such as the relationships between concepts . in the dr-link system , terms within an item are replaced by an associated subject code . use of subject codes or some other controlled-vocabulary is one way to map from specific terms to more general terms . often the controlled-vocabulary is defined by an organization to be representative of the concepts they consider important representations of their data . concept-indexing takes the abstraction a level further . its goal is to gain the implementation advantages of an index term system but use concepts instead of terms as the basis for the index , producing a reduced dimension vector-space . rather than a priori defining a set of concepts that the terms in an item are mapped to , concept-indexing can start with a number of unlabeled concept classes and let the information in the items define the concepts classes created . the process of automatic creation of concept classes is similar to the automatic-generation of thesaurus classes described in chapter 6 . the process of mapping from a specific term to a concept that the term represents is complex because a term may represent multiple different concepts to different degrees . a term such as `` automobile '' could be associated with concepts such as `` vehicle , '' `` transportation , '' `` mechanical device , '' `` fuel , '' and `` environment . '' the term `` automobile '' is strongly related to `` vehicle , '' lesser to `` transportation '' and much lesser the other terms . thus a term in an item needs to be represented by many concept codes with different weights for a particular item . an example of applying a concept approach is the convectis system from hnc software inc. (caid-93 , carleton-95) . the basis behind the generation of the concept approach is a neural-network-model (waltz-85) . context-vector representation and its application to textual items is described by gallant (gallant91a , gallant-91b) . if a vector approach is envisioned , then there is a finite number of concepts that provide coverage over all of the significant concepts required to index a database of items . the goal of the indexing is to allow the user to find required information , minimizing the reviewing of items that are non-relevant . in an ideal environment there would be enough vectors to account for all possible concepts and thus they would be orthogonal in an `` n '' dimensional vector-space-model . it is difficult to find a set of concepts that are orthogonal with no aspects in common . additionally , implementation trade offs naturally limit the number of concept classes that are practical . these limitations increase the number of classes to which a processing token is mapped . the convectis system uses neural-network algorithms and terms in a similar context (proximity) of other terms as a basis for determining which terms are related and defining a particular concept . a term can have different weights associated with different concepts as described . the definition of a similar context is typically defined by the number of non-stop words separating the terms . the automatic-indexing 131 farther apart terms are , the less coupled the terms are associated within a particular concept class . existing terms already have a mapping to concept classes . new terms can be mapped to existing classes by applying the context rules to the classes that terms near the new term are mapped . special rules must be applied to create a new concept class . example 5.9 demonstrates how the process would work for the term `` automobile . '' term : automobile weights for associated concepts : vehicle .65 transportation .60 environment .35 fuel .33 mechanical device . 15 vector-representation automobile : (.65 , ... , .60 , ... , .35 , .33 , ... , .15) figure 5.10 concept-vector for automobile using the concept-representation of a particular term , phrases and complete items can be represented as a weighted-average of the concept-vectors of the terms in them . the algorithms associated with vectors (e.g. , inverse-document-frequency) can be used to perform the merging of concepts . another example of this process is latent-semantic-indexing (lsi) . its assumption is that there is an underlying or `` latent '' structure represented by interrelationships between words (deerwester-90 , dempster-77 , dumais-95 , gildea-99 , hofmann-99) . the index contains representations of the `` latent semantics '' of the item . like convectis , the large term-document-matrix is decomposed into a small set (e.g. , 100-300) of orthogonal factors which use linear-combinations of the factors (concepts) to approximate the original matrix . latent-semantic-indexing uses singular-value-decomposition to model the associative relationships between terms similar to eigenvector decomposition and factor-analysis (see cullum-85) . any rectangular matrix can be decomposed into the product of three matrices . let x be a mxn matrix such that : where to and do have orthogonal columns and are m x r and r x n matrices , so is anrxr diagonal matrix and r is the rank of matrix x . this is the singular value 132 chapter 5 decomposition ofx the k largest singular-values of so are kept along with their corresponding columns in to and do matrices , the resulting matrix : is the unique matrix of rank k that is closest in least-squares sense to x . the-matrix x , containing the first k independent linear components of the original x represents the major associations with noise eliminated . if you consider x to be the term-document-matrix (e.g. , all possible terms being represented by columns and each item being represented by a row) , then truncated singular-value-decomposition can be applied to reduce the dimmensionality caused by all terms to a significantly smaller dimensionality that is an approximation of the original x : where u} ... uk and v1 ... vk are left and right singular vectors and svj ... svk are singualr values . a threshold is used against the full sv diagnonal matrix to determine the cutoff on values to be used for query and document representation (i.e. , the dimensionality-reduction) . hofmann has modified the standard lsi approach using addional formalism via probabilistic-latent-semantic-analysis (hofmann-99) . with so much reduction in the number of words , closeness is determined by patterns of word-usage versus specific co-locations of terms . this has the effect of a thesaurus in equating many terms to the same concept . both terms and documents (as collections of terms) can be represented as weighted vectors in the k dimensional space . the selection of k is critical to the success of this procedure . if k is too small , then there is not enough discrimination between vectors and too many false hits are returned on a search . if k is too large , the value of latent-semantic-indexing is lost and the system equates to a standard vector-model .
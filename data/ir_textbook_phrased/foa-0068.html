43.1 underlying assumptions as with all scientific-models , our attempts to evaluate the performance of a search-engine rests on a number of assumptions . many of these involve the user and simplifying assumptions about how users assess the relevance of documents . assessing the retrieval 117 1 . real foa versus laboratory retrieval . from the foa perspective , users retrieve documents as part of an extended search-process . they do this often , and because they need the information for something important to them . if we are to collect useful statistics about foa , we must either capture large numbers of such users `` in the act '' (i.e. , in the process of a real , information-seeking activity) or attempt to create an artificial laboratory setting . the former is much more desirable but makes strong requirements concerning a desirable corpus , a population of users , and access to their retrieval data . so , typically , we must work in a lab . the first big assumption , then , is that our lab setting is similar to real life ; i.e. , `` guinea pig '' users will have reactions that mirror real ones . * 2 . intersubject reliability . even if we assume we have a typical user and that this user is engaged in an activity that at least mirrors the natural foa process , we have to believe that this user will assess relevance the same as everyone else ! but clearly , the educational background of each user , the amount of time he or she has to devote to the foa process relative to the rest of the task , and similar factors will make one user 's reaction differ from another 's . for example , there is some evidence that stylistic variations also impact perceived relevance [karlgren , 1996] . the consensual relevance statistic (cf. section 4.3.2) is one mechanism for aggregating across multiple users . this becomes a concern with intersubject reliability . if we intend to make changes to document representations based on one user 's relevance-feedback opinions , we would like to believe that there is at least some consistency between this user 's opinion and those of others . this is a critical-area for further research , but some encouraging , preliminary results are available . for example , users of a multilingual-retrieval system that presents some documents in their first language (`` mother tongue '') and others in foreign languages they read less well , seem to be able to provide consistent relevance-feedback data even for documents in their second , weaker language [sheridan and ballerini , 1996] . 3 . independence of interdocument relevance-assessments . finally , notice that the atomic element of data-collection for relevance-assessments is typically a (query ; , documentj) pair : documentj is relevant to query ^ implicitly , this assumes that the relevance of a document can be assessed independently of assessments of other documents . again , this is a very questionable assumption . uwith web-search-engines do n't we have access to enormous numbers of users searching the same corpus ? '' [sg] 118 finding out about recall also that often the proxy on which the user 's relevance-assessment depends is distinct from the document itself . the user sees only the proxy , a small-sample of the document in question , for example , its title , first paragraph , or bibliographic-citation . while we must typically take user reaction to the proxy as an opinion about the whole document , this inference depends critically on how informative the proxy is . cryptic titles and very condensed citation formats can make these judgments suspect . and of course the user 's ultimate assessment of whether a document is relevant , after having read it , remains a paradox .
9.3.4 query-processing query-processing in a distributed-ir system proceeds as follows : (1) select collections to search . (2) distribute query to selected collections . (3) evaluate query at distributed-collections in parallel . (4) combine results from distributed-collections into final result . as described in the previous section , step 1 may be eliminated if the query is always broadcast to every document-collection in the system . otherwise , one of the previously described selection-algorithms is used and the query is distributed to the selected collections . each of the participating search servers then evaluates the query on the selected collections using its own local-search algorithm . finally , the results are merged . at this point we have covered everything except how to merge the results . there are a number of scenarios . first , if the query is boolean and the search servers return boolean result sets , all of the sets are simply unioned to create the final result-set . if the query involves free-text ranking , a number of techniques are available ranging from simple/naive to complex/accurate . the simplest approach is to combine the ranked hit-lists using round-robin interleaving . this is likely to produce poor quality results since hits from irrelevant collections are given status equal to that of hits from highly relevant collections . an improvement on this process is to merge the hit-lists based on relevance score . as with the parallel-process described for document-partitioning in section 9.2.2 , unless proper global term statistics are used to compute the document scores , we may get incorrect results . if documents are randomly distributed such that global term statistics are consistent across all of the distributed-collections , the merging based on relevance score is sufficient to maintain retrieval-effectiveness . if , however , the distributed document-collections are 254 parallel and distributed ir semantically partitioned or maintained by independent parties , then reranking must be performed . callan [139] proposes reranking documents by weighting document scores based on their collection similarity computed during the source-selection step . the weight for a collection is computed as w = 1 + | c \ - (s รณ s) / s , where | c | is the number of collections searched , s is the collection 's score , and s is the mean of the collection scores . the most accurate technique for merging ranked hit-lists is to use accurate global term statistics . this can be accomplished in one of a variety of ways . first , if the collections have been indexed for source-selection , that index will contain global term statistics across all of the distributed-collections . the broker can include these statistics in the query when it distributes the query to the remote search servers . the servers can then account for these statistics in their processing and produce relevance-scores that can be merged directly . if a collection-index is unavailable , query-distribution can proceed in two rounds of communication . in the first round , the broker distributes the query and gathers collection statistics from each of the search servers . these statistics are combined by the broker and distributed back to the search servers in the second round . finally , the search-protocol can require that search servers return global query-term statistics and per-document query-term statistics [317 , 441] . the broker is then free to rerank every document using the query-term statistics and a ranking-algorithm of its choice . the end result is a hit-list that contains documents from the distributed-collections ranked in the same order as if all of the documents had been indexed in a single collection .
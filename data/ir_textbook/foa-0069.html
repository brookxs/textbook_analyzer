 43.2 Consensual Relevance  In most search engine evaluation, the assumption has been that a single expert can be trusted to provide reliable relevance assessments. Whether any one, "omniscient" individual is capable of providing reliable data about the appropriate set of documents to be retrieved remains a foundational issue within IR. For example, a number of papers in a recent special issue ofthe Journal ofthe American Society for Information Systems devoted to relevance advocated a move toward a more "user-centered, situational" view of relevance [Froehlich, 1994].  Our attention to the opinions of individual users suggests the possibility of combining evidence from multiple human judges. Rather than having relevance be a Boolean determination made by a single expert, we will consider "relevance" to be a consensual, central tendency ofthe searching users' opinions. The relevance assessments of individual users and the resulting central tendency of relevance is suggested by Figure 4.8. Two features of this definition are significant. First, consensual relevance posits a "consumers'" perspective on what will count as IR system success. A document's relevance to a query is not going to be determined by an expert in the topical area, but by the users who are doing the searching. If they find it relevant, it's relevant, whether or not some domain expert thinks the document "should" have been retrieved.  Second, consensual relevance becomes a statistical, aggregate property of multiple users' reactions rather than a discrete feature elicited from any one individual. By making relevance a statistical measure, our confidence in the relevance of a document (with respect to a query) ASSESSING THE RETRIEVAL       119  Consensually relevant  ivant  respect to User,.  FIGURE 4.8 Consensual Relevance  increases as more relevance assessment data are collected. This reliance on statistical stability creates a strong link between IR and machine learning (cf. Chapter 7). Allen's investigation into idiosyncratic cognitive styles of browsing users [Allen, 1992] and Wilbur's assessment of the reliability of relevance feedback across users [Wilbur, 1998] provide a more textured view of how multiple relevance assessments can be compared and combined.  It seems, however, that our move from omniscient to consensual relevance has only made the problem of evaluation that much more difficult. Test corpora must be large enough to provide robust tests for retrieval methods, and multiple queries are necessary to evaluate the overall performance of a search engine. Getting even a single person's opinion about the relevance of a document to a particular query is hard, and we are now interested in getting many! However, software like RAVE (cf. Section 4.4) allows an IR experimenter to effectively collect large numbers of relevance assessments for an arbitrary document corpus.   
 4.3.4 Basic Measures  Figure 4.9 shows the relationship between relevant (Rel) and retrieved (Retr) sets as a Venn diagram, against the backdrop of the universe 17 of the rest of the documents of the corpus. Obviously, our focus should be on those documents that are in the intersection of Rel and Retr and on making this intersection as large as possible. Informally, we will be most happy with a Rel set when it best overlaps with the Retr set, and therefore we seek evaluation measures that reflect this. The basic relations between the sizes of these sets can also be captured in the contingency table of Table 4.1.  We know we want the intersection of the Rel and Retr sets to be large, but large relative to what?! As mentioned in Chapter 1, if we are most focused on the Rel set and use it as our standard of comparison, ASSESSING THE RETRIEVAL       123  TABLE 4.1 Contingency Table  Not Relevant        Relevant  Retrieved           Ret A Rel     Ret lt;-ª Rel      NRef  Not retrieved     Wt *-* Rel     'Ret lt;-gt; llel     NNRe^ NRelc           NNReld        NDolt;f  a NRet is the number of retrieved documents. b NNRet is the number of documents not retrieved. c NRel is the number of relevant documents. d NNRel is the number of irrelevant documents. e NDoc is the total number of documents.  we'd like to know what fraction of these we've retrieved. This ratio is called recall:    Anticipating the probabilistic analysis of Section 5.5, we can think of Recall as (an estimate of) the conditional probability that a document will be retrieved, given that it is relevant: Pr{Ret | Rel).  Conversely, if we instead focus on the Ret set, we are most interested in what fraction of these are relevant; this ratio is called precision:  ^     . .          \RetD Rel\                            fA   N  Precision = -ó;-----:ó-                           (4.3)  \Ret\  Similarly, this is the probability that a document will be relevant, given that it is retrieved: Pr(Rel \Ret). A closely related but less common measure is called fallout, where we (perversely!) focus on the irrelevant documents and the fraction of them retrieved:  'Ret n Rel I Fallout = -----==-----l-                               (4.4)  \Ret  The close relationship between these three measures can be defined  precisely, if the generality G of the query (cf. Section 4.3.7) is known:  Recall ï G                                ,     ,  Precision =---------------------------------------               (4.5)  Recall- G + Fallout- (I - G) 124      FINDING OUT ABOUT  This is Pr(Ret\Rel). These two measures, Recall and Precision, have remained the bedrock of search engine evaluation since they were first introduced by Kent in 1955 [Kent et al, 1955; Saracevic, 1975]. By far the most common measures of search engine performance are just the pair of measures, Precision and Recall.  Ideally, of course, we'd like a system that has both high precision and high recall: only relevant documents and all of them. But real-world, practical systems must select documents based on features that are only statistically useful indicators of relevance; we can never be sure. In this case efforts made to improve recall must retrieve more documents, and it is likely that precision will suffer as a consequence. The best we can hope for is some balance.  In some applications it is nevertheless desirable to evaluate IR system performance according to a single measure rather than the twoSingle                 dimensional Recall/Precision criteria.^ We will return to this topic in dimensions for      Section 4.3.8. simple minds   
 4.3.10 Other Measures  The performance measures already listed are by far the most common ways in which search engines are evaluated in the literature. Several others, however, have been important in the past and may again prove useful in some situations.  Expected Search Length  The ordered list of relevance assessments described in Section 4.3.5 also recommends another, holistic evaluation of the entire retrieval's behavior; this method is known as expected search length (ESL). ESL considers the length of a "path" as users walk down the ordered hitlist, measuring how many irrelevant documents were seen on this path before each relevant document; "expected" refers to the average length of each path ending in a relevant document. Cooper initially proposed this model to measure the work a search engine saves, in comparison to searching the entire collection at random [Cooper, 1968].  Given that a search engine retrieves documents in hitlist order, ESL also requires a criterion by which the users' wandering paths are stopped. Van Rijsbergen, p. 160 discusses a number of predicates that 138       FINDING OUT ABOUT  might be used to terminate the search: some fixed number of relevant documents, some fraction of all relevant documents, etc. Because the generality of queries can vary considerably, it is desirable to terminate the ESL after some fixed fraction E of relevant documents has been retrieved.  For this same reason, it makes sense to normalize ESL with respect to the number of relevant documents we might expect to retrieve if we were retrieving at random. If we use NRet for the number of retrieved documents (i.e., those satisfying the predicate mentioned earlier), we can estimate the expected random search length RandSL as:  NRet- (NDoc- Rel)  óknó        (4-"gt;  Then the expected search length reduction factor  RandSL ~ ESL  ESLRF- -----óó-----                        (4.20)  RandSL  captures the amount a real search method improves over the random case.  Operating Characteristic Curves  Swets [Swets, 1963] enumerated a number of abstract desiderata (quoted by van Rijsbergen, p. 155) that we might wish for any assessment measure. According to these, IR's standard Re/Pre plot leaves much to be desired, in particular because this two-dimensional assessment makes direct comparison impossible. Swets therefore recommended an analysis from the perspective of signal detection, based on several key assumptions:  ASSUMPTION 1 There is a "relevant" signal we wish to distinguish from background noise. We can consider the worst case to be comparison against an "irrelevant" signal with both signals imposed over the data collection. We can imagine that this signal is generated by the presence or absence of some keywords. ASSESSING THE RETRIEVAL       139  FIGURE 4.17 Distinguishing between Overlapping Distributions  ASSUMPTION 2   These two signals are to be discriminated according to only a single dimension.  ASSUMPTION 3   These signals are both distributed normally across the corpus.  In this idealized case, we get a picture similar to Figure 4.17. Then, because our corpus has been ordered by the ranking, the goal becomes to select a value RankT that best separates these two modal distributions.  Using a simple retrieval rule that retrieves a document if its value is above the threshold RankTgt; wherever we place this threshold we are bound to make two types of errors. There will be some Rel documents that fall below our threshold (shaded in Figure 4.17) and some irrelevant documents that fall above it (cross-hatched). Following signal detection theory we can call the first set "FALSEó " errors and the second "FALSE+" errors. (These are often called Type 1 and Type 2 errors, respectively.) Note that the ratio of the right tail of the Rel curve (the area not cross-hatched in in Figure 4.17) to the total area under the Rel curve corresponds exactly to the Recall measure defined earlier (Equation 4.2), while the ratio of the right tail of the NRel curve (cross-hatched in Figure 4.17) to the total area under the NRel curve corresponds exactly to Fallout (Equation 4.4). 140       FINDING OUT ABOUT  0.8  0.6  0.4  0.2  Fraction Rel gt; %  Fraction NRel gt; x  0.2                 0.4                 0.6                 0.8  FIGURE 4.18 Operating Characteristic Curve  The parametric curve defined by the percentage of Rel versus NRel documents retrieved as r is varied is called the operating characteristic curve. Obviously, if these two distributions are identical, this curve will be exactly a diagonal line, from (0,0) to (1,1). If the mean value of the Rel distribution is greater than that of the NRel the operating characteristic curve is moved closer to the upper-left corner, as shown in Figure 4.18.  While Swets (and subsequently others [Robertson, 1969;Bookstein and Kraft, 1977]) then considered fairly elaborate tests to discriminate the relative performance of retrieval systems with respect to such curves, it is fair to say that the 1979 assessment of van Rijsbergen, p. 154 still stands:  ... although the Swets model is theoretically attractive and links IR measurements to ready-made and well-developed statistical theory, it has not found general acceptance amongst workers in the field. ASSESSING THE RETRIEVAL       141  Optimal selection of Rankr depends on specification of the costs (losses) of making FALSE+ or FALSEó errors. For example, if you are an overworked and underpaid law clerk and you read an irrelevant document (FALSE+), youVe wasted precision attention, but that's all; if you miss a reference you should have found (FALSEó) the cost might be huge. But if you're a partying undergraduate with one more term paper between you and summer vacation, your assessments might be quite different. Section 5.5.6 gives an example of how explicit models of these various costs can be incorporated within a Bayesian decision-making framework.   
 3.4.1 Keyword Discrimination  We can immediately use this vector space for something useful, as the source of yet another approach to the question of appropriate keyword weightings. Recall that in Figure 3.7 our initial assumption was that each and every keyword was to be used as a dimension of the vector space. Now we ask: What would happen if we removed one of these keywords? The first step is to extend the measure Sim(q, d) of documentquery similarity to measure interdocument similarities Sim{di, dj) as well. Then, for an arbitrary measure of document-document similarity (e.g., the inner product measure mentioned earlier), we consider all pairs of documents and then the average similarity across all             t  Sim{di, dj) = Similarity among documents D* = Centroid; average document  1  NDOC2 *r•  Y]Sim(dhdj)  (3.24) (3.25)  (3.26)  Recall that our goal is to devise a representation of documents that makes it easy for queries to discriminate among them. Because each keyword corresponds to a dimension, removing one results in a compression of the space into K ó 1 dimensions, and we can expect that the representation of each document will change at least slightly. For example, removing a dimension along which the documents varied significantly means that vectors that were far apart in the iC-dimensional space are now much closer together.  This observation can be used to ask how useful each potential keyword is. If it is discriminating, removing it will result in a significant compression of the documents' vectors; if removing it changes very little, the keyword is less helpful Using the average similarity as our measure of how close together the documents are, and asking this question for each and every keyword* we arrive at yet another measure of keyword discrimination:  = Sim when terntk removed  Disci; h Sinii ~~ Sim w'u = fkd * Disc*  (3.27) (3.28) (3.29)  
 5.2.1 A Simple Example  Imagine that we've collected data on the HEIGHT and WEIGHT of everyone in a classroom of N students. If these are plotted, the result would be 154       FINDING OUT ABOUT  Beyond the puny three dimensions of  human existence  Size  Weight FIGURE 5.3 Weight and Height Data Reduction  something like Figure 5.3. Notice the correlation around an axis we might call something like SIZE. Students vary most along this dimension; it captures most of the information about their distribution. It is possible to capture a major source of variation across the HEIGHT/WEIGHT sample because, just as with our keywords, the two quantities are correlated.  In this section we analyze similar statistical correlations among the keywords and documents contained in the much larger vector space model first mentioned in Section 3.4. Recall that in the vector space model, the Index relation placing D = NDoc vectors corresponding to the corpus documents within the space 5ft v, where V =NKw (for vocabulary size), is defined by its keyword vocabulary.  Here we describe this in the terms of linear algebra,* where / = Index is a D x V element matrix, t  Attempts to reduce this large dimensional space into something smaller are called dimensionality reduction. There are two reasons we might be interested in reducing dimensions. The first is probably more obvious: It's a very unwieldy representation of documents' content. Individual documents will have many zeros, corresponding to the many words in the corpus V not present in an individual document; the vector space matrix is very sparse. Dimensionality reduction is a search for a representation that is denser, more compressed.  Another reason might be to exploit what has become known as latent semantic relationships among these keywords. When we make each term in our vocabulary a dimension, we are effectively assuming they are orthogonal to one another; we expect their effects to be independent.  * In this language, single-letter identifiers are simplest, but that would make the Index relation /. Unfortunately, the letter / already plays a useful role in linear algebra, as the identity matrix; hence, / = Index. For similar reasons, within this section, we will use V = NKwmd D=NDoc MATHEMATICAL FOUNDATIONS       155  But many features of FOA suggest that index terms are highly dependent, highly correlated with one another. If that's the case, we can exploit that correlation by capturing only those axes of maximal variation and throwing away the rest.   
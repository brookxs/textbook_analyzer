 1.3.1 Item Normalization  The first step in any integrated system is to normalize the incoming items  to a standard format.   In addition to translating multiple external formats that  might be received into a single consistent data structure that can be manipulated by the functional processes,  item normalization provides logical restructuring of the  item. Additional operations during item normalization are needed to create a searchable data structure: Identification of processing tokens (e.g., words), characterization of the tokens, and stemming (e.g., removing word endings) of the  tokens. The original item or any of its logical subdivisions Is available for the user to display. The processing tokens and their characterization are used to define the searchable text from the total received text. Figure 1.5 shows the normalization  process.  Standardizing the Input takes the different external formats of Input data and performs the translation to the formats acceptable to the system. A system may have a single format for all Items or allow multiple formats. One example of standardization could be translation of foreign languages Into Unicode. Every language has a different internal binary encoding for the characters in the language. One standard encoding that covers English, French, Spanish, etc. is ISO-Latin.   The are other Internal encodings for other language groups  such as Introduction to Information Retrieval Systems  11  Russian (e.g, KOI-7, KOI-8), Japanese, Arabic, etc.    Unicode is an evolving international standard based upon 16 bits (two bytes) that will be able to represent  ITEM INPUT  ITEM NORMALIZATION  SELECTIVE  DISSEMINATION OF  INFORMATION  (MAIL)  DOCUMENT FILE CREATION  AUTOMATIC FILE BUILD (AFB)  CO  o O  PUBLIC INDEXING         PRIVATE INDEXING       Figure 1.4 Total Information Retrieval System 12  Chapter 1  STANDARDIZE INPUT  LOGICAL  SUBSETTING  (ZONING)  IDENTIFY  PROCESSING  TOKENS  UPDATE DOCUMENT FILE  APPLY STOPLISTS (STOP ALGORITHMS)  CHARACTERIZE TOKENS  APPLY STEMMING  CREATE  SEARCHABLE DATA STRUCTURE  Figure 1.5 The Text Normalization Process  all languages. Unicode based upon UTF-8, using multiple 8-bit bytes, is becoming the practical Unicode standard. Having all of the languages encoded into a single format allows for a single browser to display the languages and potentially a single search system to search them. Of course such a search engine would have to have the capability of understanding the linguistic model for all the languages to allow for correct tokenization (e.g., word boundaries, stemming, word stop lists, etc.) of each language. Introduction to Information Retrieval Systems                                              13  Multi-media adds an extra dimension to the normalization process. In addition to normalizing the textual input, the multi-media input also needs to be standardized. There are a lot of options to the standards being applied to the normalization. If the input is video the likely digital standards will be either MPEG-2, MPEG-1, AVI or Real Media. MPEG (Motion Picture Expert Group) standards are the most universal standards for higher quality video where Real Media is the most common standard for lower quality video being used on the Internet. Audio standards are typically WAV or Real Media (Real Audio). Images vary from JPEG to BMP. In all of the cases for multi-media, the input analog source is encoded into a digital format. To index the modal different encodings of the same input may be required (see Section 1.3.5 below). But the importance of using an encoding standard for the source that allows easy access by browsers is greater for multi-media then text that already is handled by all interfaces.  The next process is to parse the item into logical sub-divisions that have meaning to the user. This process, called "Zoning," is visible to the user and used to increase the precision of a search and optimize the display. A typical item is sub-divided into zones, which may overlap and can be hierarchical, such as Title, Author, Abstract, Main Text, Conclusion, and References. The term "Zone" was selected over field because of the variable length nature of the data identified and because it is a logical sub-division of the total item, whereas the term "fields" has a connotation of independence. There may be other source-specific zones such as "Country" and "Keyword." The zoning information is passed to the processing token identification operation to store the information, allowing searches to be restricted to a specific zone. For example, if the user is interested in articles discussing "Einstein" then the search should not include the Bibliography, which could include references to articles written by "Einstein." Zoning differs for multi-media based upon the source structure. For a news broadcast, zones may be defined as each news story in the input. For speeches or other programs, there could be different semantic boundaries that make sense from the user's perspective.  Once a search is complete, the user wants to efficiently review the results to locate the needed information. A major limitation to the user is the size of the display screen which constrains the number of items that are visible for review. To optimize the number of items reviewed per display screen, the user wants to display the minimum data required from each item to allow determination of the possible relevance of that item. Quite often the user will only display zones such as the Title or Title and Abstract. This allows multiple items to be displayed per screen. The user can expand those items of potential interest to see the complete text.  Once the standardization and zoning has been completed, information (i.e., words) that are used in the search process need to be identified in the item. The term processing token is used because a "word" is not the most efficient unit on which to base search structures. The first step in identification of a processing token consists of determining a word. Systems determine words by dividing input symbols into three classes: valid word symbols, inter-word symbols, and special processing symbols.   A word   is defined as a contiguous set of   word symbols 14                                                                                                Chapter  bounded by inter-word symbols. In many systems inter-word symbols are nonsearchable and should be carefully selected. Examples of word symbols are alphabetic characters and numbers. Examples of possible inter-word symbols are blanks, periods and semicolons. The exact definition of an inter-word symbol is dependent upon the aspects of the language domain of the items to be processed by the system. For example, an apostrophe may be of little importance if only used for the possessive case in English, but might be critical to represent foreign names in the database. Based upon the required accuracy of searches and language characteristics, a trade off is made on the selection of inter-word symbols. Finally there are some symbols that may require special processing. A hyphen can be used many ways, often left to the taste and judgment of the writer (Bernstein-84). At the end of a line it is used to indicate the continuation of a word. In other places it links independent words to avoid absurdity, such as in the case of "small business men." To avoid interpreting this as short males that run businesses, it would properly be hyphenated "small-business men." Thus when a hyphen (or other special symbol) is detected a set of rules are executed to determine what action is to be taken generating one or more processing tokens.  Next, a Stop List/Algorithm is applied to the list of potential processing tokens. The objective of the Stop function is to save system resources by eliminating from the set of searchable processing tokens those that have little value to the system. Given the significant increase in available cheap memory, storage and processing power, the need to apply the Stop function to processing tokens is decreasing. Nevertheless, Stop Lists are commonly found in most systems and consist of words (processing tokens) whose frequency and/or semantic use make them of no value as a searchable token. For example, any word found in almost every item would have no discrimination value during a search. Parts of speech, such as articles (e.g., "the"), have no search value and are not a useful part of a user's query. By eliminating these frequently occurring words the system saves the processing and storage resources required to incorporate them as part of the searchable data structure. Stop Algorithms go after the other class of words, those found very infrequently.  Ziph (Ziph-49) postulated that, looking at the frequency of occurrence of the unique words across a corpus of items, the majority of unique words are found to occur a few times. The rank-frequency law of Ziph is:  Frequency * Rank = constant  where Frequency is the number of times a word occurs and rank is the rank order  of the word. The law was later derived analytically using probability and information theory (Fairthorne-69). Table 1.1 shows the distribution of words in the first TREC test database (Harman-93), a database with over one billion characters and 500,000 items. In Table LI, WSJ is Wall Street Journal (1986-89), AP is AP Newswire (1989), ZIFF - Information from Computer Select disks, FR Federal Register (1989), and DOE - Short abstracts from Department of Energy. Introduction to Information Retrieval Systems  15  The highly precise nature of the words only found once or twice in the database reduce the probability of their being in the vocabulary of the user and the terms are almost never included in searches. Eliminating these words saves on storage and access structure (e.g., dictionary - see Chapter 4) complexities. The best technique to eliminate the majority of these words is via a Stop algorithm versus trying to list them individually. Examples of Stop algorithms are:  Stop all numbers greater than "999999"  (this was selected to allow dates to be searchable)  Stop any processing token that has numbers and characters intermixed  The algorithms are typically source specific, usually eliminating unique item numbers that are frequently found in systems and have no search value.  In some systems (e.g., INQUIRE DBMS), inter-word symbols and Stop words are not included in the optimized search structure (e.g., inverted file structure, see Chapter 4) but are processed via a scanning of potential hit documents after inverted file search reduces the list of possible relevant items. Other systems never allow interword symbols to be searched.  Source WSJ AP ZIFF FR DOE  Size in Mbytes 295 266 251 258 190  Median number terms/record 182 353 181 313 82  Average number terms/record 329 375 412 1017 89  Number Unique Terms 156,298 197,608 173,501 126,258 186,225  Number         of Terms Occurring Once 64,656 89,627 85,992 58,677 95,782  Average number terms occurrences gt; 1 199 174 165 106 159  Table LI Distribution of words in TREC Database (from TREC-1 Conference Proceedings, Harmon-93)  The next step in finalizing on processing tokens is identification of any specific word characteristics. The characteristic is used in systems to assist in disambiguation of a particular word.   Morphological analysis of the processing 16                                                                                                Chapter 1  token's part of speech is included here. Thus, for a word such as "plane," the system understands that it could mean "level or flat" as an adjective, "aircraft or facet" as a noun, or "the act of smoothing or evening" as a verb. Other characteristics may classify a token as a member of a higher class of tokens such as "European Country" or "Financial Institution." Another example of characterization is if upper case should be preserved. In most systems upper/lower case is not preserved to avoid the system having to expand a term to cover the case where it is the first word in a sentence. But, for proper names, acronyms and organizations, the upper case represents a completely different use of the processing token versus it being found in the text. "Pleasant Grant" should be recognized as a person's name versus a "pleasant grant" that provides funding. Other characterizations that are typically treated separately from text are numbers and dates.  Once the potential processing token has been identified and characterized, most systems apply stemming algorithms to normalize the token to a standard semantic representation. The decision to perform stemming is a trade off between precision of a search (i.e., finding exactly what the query specifies) versus standardization to reduce system overhead in expanding a search term to similar token representations with a potential increase in recall. For example, the system must keep singular, plural, past tense, possessive, etc. as separate searchable tokens and potentially expand a term at search time to all its possible representations, or just keep the stem of the word, eliminating endings. The amount of stemming that is applied can lead to retrieval of many non-relevant items. The major stemming algorithms used at this time are described in Chapter 4. Some systems such as RetrievalWare, that use a large dictionary/thesaurus, looks up words in the existing dictionary to determine the stemmed version in lieu of applying a sophisticated algorithm.  Once the processing tokens have been finalized, based upon the stemming algorithm, they are used as updates to the searchable data structure. The searchable data structure is the internal representation (i.e., not visible to the user) of items that the user query searches. This structure contains the semantic concepts that represent the items in the database and limits what a user can find as a result of their search. When the text is associated with video or audio multi-media, the relative time from the start of the item for each occurrence of the processing token is needed to provide the correlation between the text and the multi-media source. Chapter 4 Introduces the internal data structures that are used to store the searchable data structure for textual items and Chapter 5 provides the algorithms for creating the data to be stored based upon the identified processing tokens.   
 86      FINDING OUT ABOUT  Virtual spaces  Sparse vector spaces  3.4 Vector Space  One of life's most satisfying pleasures is going to a good library and browsing in an area of interest. After negotiating the library's organization and finding which floor and shelves are associated with the call numbers of your topic, you are physically surrounded by books and books, all of interest to you. Some are reassuring old friends, already known to you; others are new books by familiar authors, and (best of all!) some are brand-new titles by unknowns.  This system works because human catalogers have proven themselves able to reliably and consistently identify the (primary!) topic of a book according to conventional systems of subject headings like the Library of Congress Subject Headings or the Dewey Decimal system.  Our goal is to abstract away from this very friendly notion of physical space in the library to a similar but generalized notion of semantic space in which documents about the same topic remain close together. But rather than allowing ourselves to be restricted by the physical realities of three-dimensional space and the fact that books can only be shelved in a single place in a library, we will consider abstract spaces of thousands of dimensions.^  We can make concrete progress toward these lofty goals beginning with the Index matrix relating each document in a corpus to all of its keywords. A very natural and influential interpretation of this matrix (due to Gerry Salton [Salton et al., 1975; Salton and McGill, 1983]) is to imagine each and every keyword of the vocabulary as a separate dimension of a vector space. In other words, the dimensionality of the vector space is the size of our vocabulary. Each document can be represented as a vector within such a space. Figure 3.7 shows a very simplified (binary) Index matrix, and a cartoon of its corresponding vector representation.  Estimates of the vocabulary size of a native speaker of a language approach 50,000 words; if you are articulate, your speaking and reading vocabularies might be 100,000 or more words. Assuming that we have a modest 106 document corpus, this matrix is something like 106 x 105. That's a big matrix, even by modern supercomputing standards."^  In addition to the vectors representing all documents, another vector corresponds to a query. Because documents and queries exist within a common vector space, we naturally characterize how we'd like our U2    :   I  docl    / 1 0   1  doc2  doc3  ...   o\  0 1   1 0 0   1  WEIGHTING AND MATCHING AGAINST INDICES       87  kw3  d3  docn   \ 1  1   0      ...    0  Q    .100    ...  kwl  FIGURE 3.7 Vector Space  retrieval system to work - just as we go to a physical location in the library to be near books about a topic, we seek those documents that are close to our query vector. This is a useful characterization of what we'd like our retrieval system to accomplish, but it is still far from a specification of an algorithm for accomplishing it. For example, it seems to require that the query vector be compared against each and every document, something we hope to avoid. ^  An even more important issue to be resolved before the vector space model can be useful is being specific about just what it means for a document and query to be close to one another. As will be discussed in Section 5.2.2, there are many plausible measures of proximity within a vector space. For the time being, we will assume the use of the inner product of query and document vectors as our metric:  Sim(q, d) = q Ã¯ d  (3.23)  People have difficulty imagining spaces with more than the three physical dimensions of experience, so it is no wonder that abstract  spaces of 105 dimensions are difficult to conceptualize. Sketches like Figure 3.7 do the best they can to convey ideas in the three dimensions we appreciate, but it is critically important that we not let intuitions based on such small-dimensional experiences bias our understanding of the large-dimensional spaces actually being represented and searched.  Implementation hack 88      FINDING OUT ABOUT  Theres a quicker way to compute average  similarity   
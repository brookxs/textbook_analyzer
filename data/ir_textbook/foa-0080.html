 4.4.2 RAVePlan  A second challenge is to find the desired density or redundancy of sample points. That is, for each document that we believe may be relevant to a query, how many subjects should evaluate it? The answer will vary depending on such factors as the number of participants, their expertise, their motivation to produce quality judgments, how long each will spend rating documents, etc. A higher density means that fewer documents will be evaluated, but also that the intersubject cumulative assessment is likely to be more statistically stable. This can be especially important when relevance feedback is to be used to train the system.  The trade-off between the most important of these factors is captured in the following formula:  x =  NR STQ  (4.21) ASSESSING THE RETRIEVAL       143  where  x = number of documents to be evaluated for each query N = number of subjects  R = expected subject efficiency (votes/user/time) 5 = desired density (votes/document) T = time spent by subjects Q = number of queries to be evaluated  Note that this formula ignores the overlap between queries that occurs when users see a document that may be relevant to two or more of the queries in their list. Care must be taken, therefore, to minimize expected overlap between the topical areas of the queries. We have also found that the assessment densities constructed using this formula are unfortunately uneven. The main source of these is variability in JR, the rate at which subjects are able to produce relevance assessments.  EAVePlan takes as input a list of Q query specifications, a list of N subject logins, the desired density S, and the number of documents R* T that should be allocated to each subject. The query specifications indicate which queries can go in which fields and which queries should not be shown together. This allows us to limit possible interactions between queries about similar topics.^                                                                  A better  RAVePlan outputs two files. The plan file, which is an input to the RAVePlan RAVE interactive application, lists each subject ID along with the queries and document numbers that have been selected for that subject. The assignments file is a list of document-query pairs, which tells us which query we expect the document to be relevant to. RAVeCompile uses this file after data collection is complete to generate true and false-positive measures.   
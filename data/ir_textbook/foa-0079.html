 4.4.1 RAVeUnion  It would be most useful if, for every query, the relevance of every document could be assessed. However, the collection of this many assessments, 142       FINDING OUT ABOUT  More elaborate ways of  merging ranked lists  for a corpus large enough to provide a real retrieval test, quickly becomes much too expensive. But if the evaluation goal is relaxed to being the relative comparison of one retrieval system to one or more alternative systems, assessments can be constrained to only those documents retrieved by one of the systems.  We therefore follow the pooling procedure used by many other evaluators, viz., using the proposed retrieval methods themselves as procedures for identifying which documents are worth assessing.  The first step in constructing a RAVE experiment is to combine the ranked retrieval lists of the two or more retrieval methods, creating a single list of documents ordered according to how interested we are in having them assessed by a subject.  RAVeTJnion produces the most straightforward "zipper" merge of the lists, beginning with the most highly ranked and alternating one.^ The output of RAVeUnion is a file of (query, document) pairs along with a field that indicates if the pair was uniquely suggested by only one of the methods. This last information can be used to compare the average relevance scores of documents suggested by one method alone to those retrieved by more than one.   
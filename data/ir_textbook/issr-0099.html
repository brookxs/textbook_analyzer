 7.2.1 Similarity Measures  A variety of different similarity measures can be used to calculate the similarity between the item and the search statement. A characteristic of a similarity formula is that the results of the formula increase as the items become more similar. The value is zero if the items are totally dissimilar. An example of a simple "sum of the products" similarity measure from the examples in Chapter 6 to determine the similarity between documents for clustering purposes is:  SIM(Itemi, Itemj) = I (Termigt;k) (Termjtk)  This formula uses the summation of the product of the various terms of two items when treating the index as a vector. If Itertij is replaced with Queryj then the same formula generates the similarity between every Item and Query,. The problem with this simple measure is in the normalization needed to account for variances in the length of items. Additional normalization is also used to have the final results come between zero and +1 (some formulas use the range รณ1 to +1).  One of the originators of the theory behind statistical indexing and similarity functions was Robertson and Spark Jones (Robertson-76). Their model suggests that knowledge of terms in relevant items retrieved from a query should adjust the weights of those terms in the weighting process. They used the number of relevant documents versus the number of non-relevant documents in the database and the number of relevant documents having a specific query term versus the number of non-relevant documents having that term to devise four formulas for weighting. This assumption of the availability of relevance information in the weighting process was later relaxed by Croft and Harper (Croft-79). Croft expanded this original concept, taking into account the frequency of occurrence of terms within an item producing the following similarity formula (Croft-83):  SIM(DOQ, QUERYJ = J (C + IDF.) * /, j)  where C is a constant used in tuning, IDFj is the inverse document frequency for term "i" in the collection and  /j = K + (K- l)TFl/maxfreqJ User Search Techniques                                                                             169  where K is a tuning constant, TFjtJ is the frequency of term! "i" item j and maxfrecjj is the maximum frequency of any term in item "j." The best values for K seemed to range between 0.3 and 0.5.  Another early similarity formula was used by Salton in the SMART system (Saiton-83). Salton treated the index and the search query as ndimensional vectors (see Chapter 5). To determine the "weight" an item has with respect to the search statement, the Cosine formula is used to calculate the distance between the vector for the item and the vector for the query:  (DOC,,k * QTERMj,k) SIM(DOCi, QUERYj) =  __________________________________  1 k=\                            k=\  where DOQk is the kth term in the weighted vector for Item "i" and QTERM^ Is the kth term in query "j." The Cosine formula calculates the Cosine of the angle between the two vectors. As the Cosine approaches "1," the two vectors become coincident (i.e., the term and the query represent the same concept). If the two are totally unrelated, then they will be orthogonal and the value of the Cosine is "0." What is not taken into account is the length of the vectors. For example, if the following vectors are in a three dimensional (three term) system:  Item = (4 ,8,0)  Query 1 = (1, 2,0)  Query 2 = (3, 6,0)  then the Cosine value is identical for both queries even though Query 2 has significantly higher weights in the terms in common. To improve the formula, Salton and Buckley (Salton-88) changed the term factors in the query to:  QTERMlJt = (0.5 + (0.5 TFi.k/maxfreqk)) * IDFj  where TFj?k is the frequency of term "i" in query "k," maxfreqk is the maximum  frequency of any term in query "k* and IDF; is the inverse document frequency for term "i" (see Chapter 5 for the formula). In the most recent evolution of the formula, the IDF factor has been dropped (Buckley-96).  Two other commonly used measures are the Jaccard and the Dice similarity measures (Rijsbergen-79). Both change the normalizing factor in the denominator to account for different characteristics of the data. The denominator in the Cosine formula is invariant to the number of terms in common and produces very small numbers when the vectors are large and the number of common 170                                                                                               Chapter 7  elements is small. In the Jaccard similarity measure, the denominator becomes dependent upon the number of terms in common. As the common elements increase, the similarity value quickly decreases, but is always in the range -1 to +1. The Jaccard formula is :  (DOClik * QTERMhk)  k=l  SIM(DOC,, QUERYj) =   ------------------------------------------------- k=\                    k=\                           k=\  The Dice measure simplifies the denominator from the Jaccard measure and introduces a factor of 2 in the numerator. The normalization in the Dice formula is also invariant to the number of terms in common.  2*   2.   (DOClik * QTERM,k)  k=l  SIM(DOC,, QUERYj) =                ______________________________  *=1                                                   *=I  Figure 7.2 shows how the normalizing denominator results vary with the commonality of terms. For the Dice value, the numerator factor of 2 is divided into  the denominator. Notice that as long as the vector values are same, independent of their order, the Cosine and Dice normalization factors do not change. Also notice  that when there are a number of terms in common between the query and the document, that the Jaccard formula can produce a negative normalization factor.  It might appear that similarity measures only apply to statistical systems where the formulas directly apply to the stored indexes.  In the implementation of  Natural Language systems, also weighted values come from statistical data in conjunction with the natural language processing stored as indexes. Similarity algorithms are applied to these values in a similar fashion to statistical systems. But in addition to the similarity measures, constructs are used at the discourse level to perform additional filtering of the items.  Use of a similarity algorithm returns the complete data base as search results. Many of the items have a similarity close or equal to zero (or minimum value the similarity measure produces). For this reason, thresholds are usually associated with the search process. The threshold defines the items in the resultant Hit file from the query. Thresholds are either a value that the similarity measure must equal or exceed or a number that limits the number of items in the Hit file. A User Search Techniques                                                                             171  QUERY = (2, 2, 0, 0, 4) DOC1 = (0,2,6,4,0) DOC2    = (2, 6, 0, 0, 4)  Cosine                      Jaccard                     Dice  DOC1                       36.66                        16                             20  DOC2                      36.66                        -12                           20  Figure 7.2 Normalizing Factors for Similarity Measures  default is always the case where the similarity is greater than zero. Figure 7.3 illustrates the threshold process. The simple "sum of the products" similarity formula is used to calculate similarity between the query and each document. If no threshold is specified, all three documents are considered hits. If a threshold of 4 is selected, then only DOC1 is returned.  Vector:                American, geography, lake, Mexico, painter, oil,  reserve, subject  DOC1                 geography o/Mexico suggests oil reserves are available  vector (0, 1,0,2,0,3, 1,0)  DOC2                 American geography has lakes available everywhere  vector (1,3,2,0,0,0,0,0)  DOC3                 painters suggest Mexico lakes as subjects  vector (0,0, 1,3,3,0,0,2)  QUERY              oil reserves in Mexico  vector (0, 0, 0, 1,0, 1, 1,0)  SIM (Q, DOC!) = 6, SIM (Q, DOC2) = 0, SIM(Q, DOC3) = 3 Figure 7.3 Query Threshold Process 172  Chapter 7  Figure 7.4 Item Cluster Hierarchy  One special area of concern arises from search of clusters of terms that are stored in a hierarchical scheme (see Chapter 6). The items are stored in clusters that are represented by the centroid for each cluster. Figure 7.4 shows a cluster representation of an item space. In Figure 7.4, each letter at the leaf (bottom nodes) represent an item (i.e., K, L, M, N, D, E, F, G, H, P, Q, R, J). The letters at the higher nodes (A, C, B, I) represent the centroid of their immediate children nodes. The hierarchy is used in search by performing a top-down process. The query is compared to the centroids "A" and "B." If the results of the similarity measure are above the threshold, the query is then applied to the nodes' children. If not, then that part of the tree is pruned and not searched. This continues until the actual leaf nodes that are not pruned are compared. The problem comes from the nature of a centroid which is an average of a collection of items (in Physics, the center of gravity). The risk is that the average may not be similar enough to the query for continued search, but specific items used to calculate the centroid may be close enough to satisfy the search. The risks of missing items and thus reducing recall increases as the standard deviation increases. Use of centroids reduces the similarity computations but could cause a decrease in recall. It should have no effect on precision since that is based upon the similarity calculations at the leaf (item) level.  In Figure 7.5 the filled circle represents the query and the filled boxes represent the centroids for the three clusters represented by the ovals. In this case, the query may only be similar enough to the end two circles for additional analysis. But there are specific items in the right cluster that are much closer to the query than the cluster centroid and could satisfy the query. These items cannot be returned because when their centroid is eliminated they are no longer considered.  As part of investigating improved techniques to present Hits to users, Hearst and Pederscn from XEROX Palo Alto Research Center (PARC) are User Search Techniques  173   V V      V\  AX  / VV vvv        \ รฏ       ! / vvv v\   i         1 /fvvK /   \     VV VV    VV    V  V    VV  V VV VVV / I     VV     V/\ vvv   I  Figure 7.5 Centroid Comparisons  pursuing topical clustering as an alternative to similarity search ranking (Hearst96). In their experiments they applied the clustering to the entire corpora. Although the clustering conveyed some of the content and structure of the corpora, it was shown to be less effective in retrieval than a standard similarity query (Pirolli-96). Constraining the search to the hierarchy retrieved fewer relevant items than a similarity query that focused the results on an indexed logical subset of the corpus.   
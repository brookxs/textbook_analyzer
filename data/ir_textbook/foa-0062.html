 4.1.1 Cognitive Assumptions  "Garbage in, garbage out" is one of the first insights every software developer learns, and FOA is no exception. The primary source of data considered by traditional IR methods, and the focus of Chapters 2 and 3 of this text, are the documents of the corpus, particularly the keywords they contain. (Chapter 6 will consider the use of other document attributes.) A fundamental feature of the broader FOA view is that browsing users provide an equally important source of data concerning  Portions previously published with John Hatton [Belew and Hatton, 1996]. ASSESSING THE RETRIEVAL       107  what keywords mean and what documents are about. It is therefore appropriate to begin by characterizing who these users we will be watching are.  We begin with one important cognitive assumption we must make about our users: How thorough an FOA search do they wish to perform? Is this an important search to which the users are willing to dedicate a great deal of time and attention, or will a quick, cursory answer suffice? For example, Chapter 1 mentioned how much less thorough the typical undergraduate (doing some quick research before submitting a term paper) is than the Ph.D. candidate (who wants to ensure his or her proposed dissertation topic is new). The typical WWW searcher seems satisfied with only a few useful leads, but the professional searcher (a lawyer looking for any case that might help, a doctor looking for any science that might heal a patient) will search diligently if there is even a small chance of finding another relevant document. This kind of variability can be observed not only across different classes of users but even  across the same user at different times.f                                                   In for a fact,  stay for a lesson  Prototypic Retrievals  From the perspective of cognitive psychology, the task facing users who are asked to produce relevance feedback (relevance feedback) can best be described as one of object recognition, in the tradition of Rosch and others [Rosch and Mervis, 1975; Rosch, 1977]. The object to be recognized is an internally represented prototypic document satisfying the users' information need. In this case, the prototype corresponds to the model the subjects maintain of ideally relevant documents. As users consider an actual retrieved document's relevance, they evaluate how well it matches the prototype. Barry and others have suggested the many and varied features over which prototypes can be defined [Barry, 1994]. Only a small number of these may be revealed by any one of the users' queries, of course.  Here we will simply assume that users are capable of grading the quality of this match. Users might be asked to score the quality of relevance match according to a five-point scale like that shown in Figure 4.1. Users can qualify the middle Relevant response either by weakening it (Possibly Relevant) or strengthening it (Critically Relevant). Such distinctions are often made in experimental settings (e.g., in the use of the STAIRS 108      FINDING OUT ABOUT  Not                Possibly    Relevant   Critically  i  -e (No response)  FIGURE 4.1 Relevance Scale  retrieval system by lawyers [Blair and Maron, 1985]) and relate to the different purposes for FOA that different users may have. To make these distinctions concrete, we might imagine "Critically Relevant" to apply only to those documents that must be read even for an undergrad term paper, while "Possibly Relevant" would be much more broadly applied to those a Ph.D. student needs as part of his or her literature review.  For now, however, we will simplify the types of relevance feedback to allow users to reply with only a single grade of "Relevant" (©) or "Not Relevant" (0). These two assessments require overt action on the part of subjects; "No response" (#) is the default relevance feedback assessment for documents not receiving any other responses. Again, this frees users from the much more cognitively demanding task of exhaustively assessing every retrieved document Those documents that "jump out" at users as particularly good or especially bad - examples of the prototype they seek provide the most informative relevance feedback. Figure 4.1 also introduces a color-code convention in the electronic version: © will be used to indicate positive relevance feedback and 0 to indicate negative relevance feedback.  relevance feedback is Nonmetric  As we move from a cognitive understanding of the users' tasks to statistical analyses of their behaviors, we must understand one important feature of the relevance feedback data stream: relevance feedback is nonmetric data. That is, while users find it easy and natural to critique retrieved documents with ©, ©, and #, they would find it much more difficult to reliably assign numeric quantities reflecting something like the relative applicability of each retrieval.  Think for a moment just why this is hard, by imagining your reactions to a typical retrieval Is the first document to be rated 10 or 6743? If you rate the first document as 10, the second as 6ª and the third as 2, then you must ensure that the third document is exactly as much less relevant ASSESSING THE RETRIEVAL       109  FIGURE 4.2 relevance feedback Labeling of the Retr Set  than the second as the second is from the first. Trying to keep all Rel(di) assessments consistent in the metric sense, for many retrieved documents or any other set, makes people crazy.  This is not only a property of relevance assessments. A large literature on psychological assessment [Kruskal, 1977a; Kruskal, 1977b; Shepard et al, 1972] has demonstrated that human subjects can quite easily and reliably sort objects into "piles," and that they like one pile better than another. Yet the same people find it more difficult to quantify how much they like each object, let alone make these quantitative assessments consistent with one another. Reliable estimates of both cognitive qualities would be necessary if we are to have a true preference metric*  Rather than assuming that users can provide a separate score for each retrieved document, we will treat this as an ordered nonmetric scale of increasing preference:  #lt; @  (4.1)  Each of these assumptions about what "relevance" is and how it can be measured is a matter of considerable debate [Wilson, 1973; Froehlich, 1994] and is likely to be the topic of much future work. It is also interesting to note that in our later attempts at a comprehensive model of how and why humans use language, "relevance" again plays a central role (cf. Section 8.2.2).   
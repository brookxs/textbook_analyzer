 7.4 Classification  The classification task is ... classic), and a wide range of technologies for accomplishing it have been developed [Duda and Hart, 1973; Carlin and Louis, 1996]. Even in the relatively recent context of text-based domains, many techniques have been applied [Lewis and Hayes, 1994]. Here we will focus primarily on extending the probabilistic approach of Section 5.5, but we'll consider other alternatives in Section 7.5. Andrew McCallum and colleagues have developed a software suite called RAINBOW3 that is very useful for experiments into text classification.  We begin by assuming that we have been given a set of classes C = {c\, ci,... cq} and have been asked to produce a function that classifies a new document into one of these classes. McCallum gives a good overview of the Bayesian approach applied to text:  This approach assumes that the text data was generated by a parametric model, and uses training data to calculate the Bayesoptimal estimates of the model parameters. Then, equipped with these estimates, it classifies new documents using Bayes' rule to turn the generative model around and calculate the posterior probability that a class would have generated the test document in question. [McCallum and Nigam, 1998, p. 42, emphasis added]  The major features of the training regime are shown in Figure 7.5. During the first phase, a correspondence has been established manually between each example document d,- and some classification c\ in the training set T:  T = {(di, Ci)}                                   (7.7)  11 www.csxmu.edij/mccalliim/bow/rainbow/ 268      FINDING OUT ABOUT  Classifier 0  Automatic assignment to classes  FIGURE 7.5 Training a Classifier  (In Figure 7.5, the classifications are imagined to be part of a hierarchical classification system, as will be discussed in detail in Section 7.5.5.) These data are used somehow (for now it's okay to think of it as magic) to tune the set of parameters 0 specifying a particular classifier. The second and dominant phase is then to use this classifier to automatically assign documents to classes in an analogous manner to those manually classified in the training set.  We seek the posterior probability of a particular class, given the evidence provided by a new document, Pr{c | d). The second step is then, given these probabilities for all classes, to return the one that maximizes this likelihood. Bayes' rule is typically invoked in such situations to "invert" this conditional dependency:  Pr(c\) =  Pr(d\c)Pr(c) J2Pr(d\c)Pr(c)  (7.8)  The likelihood Pr(d | c) can be more easily estimated if we assume classes are represented as mixtures of the documents' features. We can think of hypothetical documents (that we imagine belong to the class) as generated by some model, the parameters 0 of which we hope to discover. Assuming that a document is generated by first picking a particular class and then using its parameters to select features,* the likelihood of  * More general models are also possible [McCallum and Nigam, 1998]. ADAPTIVE INFORMATION RETRIEVAL      269  a document being selected can be computed by considering the prior probability of each class Pr(c |©) and its distribution Pr(d | c; 6):  Pr(d\) = J2 Pr(c\@) ï Pr(d \c; 0)                (7.9)  These measures allow us to precisely state how well a learned model captures regularities found in the training set: The model is doing a good job if it applies the same classifications as observed in the training data. But this criterion also highlights the dependence of any learning method on the training data T used to construct it, which cannot be overemphasized. Independent of the range of hypotheses considered, and of the learning methods used to build the best possible model, our ability to inductively generalize from the training data to new examples (e.g., unclassified documents) depends entirely on how typical the training data are. Within computational learning theory this dependence is captured by the assumption that training data and subsequent trials are drawn from the same distribution [Valiant, 1984; Kearns and Vazirani, 1994].  In practice, performance of a classifier on the training set is bound to be an optimistic overestimate of how well it can generalize to previously unseen data. The most common way to guard against such overestimates is to artificially hold out some of the training set as a separate test set. Training proceeds as before on all the training data but not on the test set, and then the system is evaluated on the test set, which provides a more reasonable estimate as to how the system will fare. More sophisticated cross-validation procedures begin by partitioning the available training data into k subsets. Iteratively, each partition is used as the holdout set while the remaining ^p- balance of T is used for training. The average performance across all k tests is then used as a more statistically reliable estimate of true performance. The statistical validity derived by crossvalidation and related techniques becomes especially important when the total amount of training data is small; given the time and effort required to produce manual classifications, this is often the case.   
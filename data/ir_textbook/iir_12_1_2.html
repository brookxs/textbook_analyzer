    Types of language models How do we build probabilities over sequences of terms? We can always use the chain rule from EquationÂ 56 to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events: (94)   unigram language model  (95)   bigram language models  (96)   speech recognition  spelling correction  machine translation  sparseness 13.2  bias-variance tradeoff 11 11.4.2    
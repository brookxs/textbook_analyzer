 Feedback  The word feedback is normally used to describe the mechanism by which a system can improve its performance on a task by taking account of past performance. In other words a simple input-output system feeds back the information from the output so that this may be used to improve the performance on the next input. The notion of feedback is well established in biological and automatic control systems. It has been popularised by Norbert Wiener in his book Cybernetics. In information retrieval it has been used with considerable effect.  Consider now a retrieval strategy that has been implemented by means of a matching function M. Furthermore, let us suppose that both the query Q and document representatives D are t-dimensional vectors with real components where t is the number of index terms. Because it is my purpose to explain feedback I will consider its applications to a serial search only.  It is the aim of every retrieval strategy to retrieve the relevant documents A and withhold the non-relevant documents `A. Unfortunately relevance is defined with respect to the user's semantic interpretation of his query. From the point of view of the retrieval system his formulation of it may not be ideal. An ideal formulation would be one which retrieved only the relevant documents. In the case of a serial search the system will retrieve all D for which M(Q,D) gt; T and not retrieve any D for which M(Q,D) lt;= T, where T is a specified threshold. It so happens that in the case where M is the cosine correlation function, i.e.  the decision procedure  M(Q,D) - T gt; 0  corresponds to a linear discriminant function used to linearly separate two sets A and `A in R[t]. Nilsson[14] has discussed in great detail how functions such as this may be 'trained' by modifying the weights qi to discriminate correctly between two categories. Let us suppose for the moment that A and `A are known in advance, then the correct query formulation Q0 would be one for which  M(Q0,D) gt; T whenever D [[propersubset]] A  and  M(Q0,D) lt;= T whenever D [[propersubset]] `[[Alpha]]  The interesting thing is that starting with any Q we can adjust it iteratively using feedback information so that it will converge to Q0. There is a theorem (Nilsson[14], page 81) which states that providing Q0 exists there is an iterative procedure which will ensure that Q will converge to Q0 in a finite number of steps.  The iterative procedure is called the fixed-increment error correction procedure.  It goes as follows:  Qi = Qi-1 + cD if M(Qi-1, D) - T lt;= 0  and D [[propersubset]] A  Qi = Qi-1 - cD if M(Qi-1, D) - T gt; 0  and D [[propersubset]] `A  and no change made to Qi-1 if it diagnoses correctly. c is the correction increment, its value is arbitrary and is therefore usually set to unit. In practice it may be necessary to cycle through the set of documents several times before the correct set of weights are achieved, namely those which will separate A and `A linearly (this is always providing a solution exists).  The situation in actual retrieval is not as simple. We do not know the sets A and `A in advance, in fact A is the set we hope to retrieve. However, given a query formulation Q and the documents retrieved by it we can ask the user to tell the system which of the documents retrieved were relevant and which were not. The system can then automatically modify Q so that at least it will be able to diagnose correctly those documents that the user has seen. The assumption is that this will improve retrieval on the next run by virtue of the fact that its performance is better on a sample.  Once again this is not the whole story. It is often difficult to fix the threshold T in advance so that instead documents are ranked in decreasing matching value on output. It is now more difficult to define what is meant by an ideal query formulation. Rocchio[15] in his thesis defined the optimal query Q0 as one which maximised:  If M is taken to be the cosine function (Q, D) /||Q || ||D || then it is easy to show that [[Phi]] is maximised by  where c is an arbitrary proportionality constant.  If the summations instead of being over A and `A are now made over A [[intersection]] Bi and `A [[intersection]] Bi where Bi is the set of retrieved documents on the ith iteration, then we have a query formulation which is optimal for Bi a subset of the document collection. By analogy to the linear classifier used before, we now add this vector to the query formulation on the ith step to get:  where wi and w2 are weighting coefficients. Salton[2] in fact used a slightly modified version. The most important difference being that there is an option to generate Qi+1 from Qi, or Q, the original query. The effect of all these adjustments may be summarised by saying that the query is automatically modified so that index terms in relevant retrieved documents are given more weight (promoted) and index terms in non-relevant documents are given less weight (demoted).  Experiments have shown that relevance feedback can be very effective. Unfortunately the extent of the effectiveness is rather difficult to gauge, since it is rather difficult to separate the contribution to increased retrieval effectiveness produced when individual documents move up in rank from the contribution produced when new documents are retrieved. The latter of course is what the user most cares about.  Finally, a few comments about the technique of relevance feedback in general. It appears to me that its implementation on an operational basis may be more problematic. It is not clear how users are to assess the relevance, or non-relevance of a document from such scanty evidence as citations. In an operational system it is easy to arrange for abstracts to be output but it is likely that a user will need to browse through the retrieved documents themselves to determine their relevance after which he is probably in a much better position to restate his query himself.   
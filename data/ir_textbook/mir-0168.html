 9.2.3    SSMD Architectures SIMD architectures lend themselves to a more restricted domain of problems than MIMD architectures. As such, SIMD computers are less common than MIMD computers. Perhaps the best known example of the SIMD architecture is the Thinking Machines CM-2, which has been used to support both signature file- and inverted file-based information retrieval algorithms. Each processing element in the CM-2 has a 1 bit arithmetic logic unit (ALU) and a small amount of local memory. The processing elements execute local and non-local parallel instructions. A local parallel instruction causes each processing element to perform the same operation in unison on data stored in the element's local memory. A non-local parallel instruction involves communication between the processing elements and includes operations such as summing the components of a vector or finding a global maximum. The CM-2 uses a separate front-end host computer to provide an interface to the back-end parallel processing elements. The front-end controls the loading and unloading of data in the back-end and executes serial program instructions, such as condition and iteration statements. Parallel macro instructions are sent from the front-end to a back-end microcontroller, which controls the simultaneous execution of the instruction on a set of back-end processing elements. The CM-2 provides a layer of abstraction over the back-end processors, called virtual processors. One or more virtual processors map to a single physical processor. Programs express their processing needs in terms of virtual processors, and the hardware maps virtual processor operations onto physical processors. A physical processor must sequentially perform the operations for each of its virtual processors. The ratio of virtual to physical processors is called the virtual processing ratio, VP. As VP increases, an approximately linear increase in running time occurs. Signature Files The most natural application of a SIMD computer in IR is to support signature files. Recall from Chapter 8 the basic search process for signature files. First, the search system constructs a signature for the query terms. Next, the system compares the query signature with the signature of every document in the collection and marks documents with matching signatures as potentially relevant. PARALLEL IR        241 probe_doc   (P_bit Doc_sig[],   char *term) { int       i; P__int Doc.match; Doc_match = 1; for   (i ´ 0;   i  lt; numjiashes;   i++)   i Doc.match = Doc_sig[hash  (i,   term)]; } return Doc_match; Figure 9.5    probe_doc. Finally, the system scans the full text of potentially relevant documents to eliminate false drops, ranks the matching documents, and returns the hit-list to the user. If the probability of false drops is acceptably low, the full text scan may be eliminated. Also, if the system is processing Boolean queries, it may need to generate more than one signature for the query and combine the intermediate results of each signature according to the operators used in the query. Stanfill [741] shows how this procedure can be adapted to the CM-2 (or any similar SIMD machine). The core of the procedure is the subroutine shown in Figure 9.54 This routine probes the document signature Doc.sig for the given query word term by applying each of the signature hash functions to term and ANDing together the corresponding bits in Doc.sig. The result of the AND operation is stored in Doc_match. If Doc_match is 1, term is present in Doc.sig; if Doc_match is 0, term is absent. Both Doc_sig and Doc_match are parallel variables, such that each virtual processor operates in parallel on its own copy of the variables. By loading the entire signature file onto the back-end virtual processors, all of the document signatures can be searched in parallel. This procedure must be enhanced under the following condition. If the number of words in a document |d| exceeds the number of words W that can be inserted into a document signature, then the document must be segmented into I^l/IF segments and represented by |d|/W signatures. In this case, the probe_doc routine is applied to all signatures for a document and an OR is taken over the individual signature results to obtain the final result for the document. If the false drop probability warrants scanning the full text of the documents. only those segments with matching signatures need be scanned. As soon as a qualifying segment is found, the entire document is marked as a match for the query. % The algorithms shown in this chapter are presented using a C-like pseudo-code.   Parallel data type names begin with a capital kP\ 242        PARALLEL AND DISTRIBUTED IR bool_search r	'PJbit Doc_sig[] , bquery_t query) switch (query.op) { case AND return	(bool_search (Doc_sig, query,argl)  bool_search (Doc_sig, query.arg2)); case OR: return	(bool_search (Doc_sig, query.argl) I! bool_search (Doc_sig, query.arg2)); case NOT return	(!bool_search (Doc_sig, query.argl)); case WORD: return gt; }	(probe_doc (Doc.sig, query.argl)); Figure 9.6    bool_search. A general Boolean retrieval system can be implemented on top of probe_doc with the recursive procedure shown in Figure 9.6. Here bquery.t is a recursive data type that contains two arguments and an operator. If the operator is NOT or WORD, then the second argument in the bquery_t is empty. The final return value is stored in a parallel Boolean variable, which indicates for each document whether or not that document satisfies the Boolean query. Again, if the probability of false drops associated with the signature scheme is acceptably low, the set of matching documents may be returned immediately. Otherwise, the system must perform further processing on the text of each matching document to eliminate false drops. If weights are available for the query terms, it is possible to build a ranking retrieval system on top of the parallel signature file search process. Query term weights could be supplied by the end-user when the query is created, or they could be assigned by the system using a collection statistic such as idf (see Chapter 2). The algorithm in Figure 9.7 shows how to use probe_doc to build a ranking system. In rank_search, the wqueryjt data type contains an array of query terms and an array of weights associated with those terms. First, all documents that contain the current term are identified with probe_doc. Next, the score for each of those documents is updated by adding the weight associated with the current query term (the where clause tests a parallel variable expression and activates only those processors that satisfy the expression). After all query terms have been processed, the parallel variable Doc.score contains the rank scores for all of the documents. The final step in the processing of a weighted query is to rank the scored documents by sorting and returning the top k hits. This can be accomplished in PARALLEL IR        243 rank_search (P_bit Doc_sig[], wquery_t query) { int    i; P_float Doc.score; P_bool Doc_match; Doc_score = 0; for  (i = 0;   i lt; query.num_terms;   i++)   { Docjaatch = probe_doc   (Doc_sig,  query.terms[i]): where  (Docjmatch)   { Doc_score += query.weights[i]; return  (Doc_score); gt; Figure 9.7    rank.search. a number of ways. One possibility is to use the global ranking routine provided by the CM-2, which takes a parallel variable and returns 0 for the largest value, 1 for the next largest value, etc. Applying this routine to Doc_score yields the ranked documents directly. If the number of hits returned is much less than the number of documents in the collection (k ´ JV), the global ranking function performs more work than necessary. An alternative is for the retrieval system to use the global maximum routine in an iterative process of identification and extraction. During each iteration, the system applies the global maximum routine to Doc_score to identify the current top ranked document. The document is added to the hit-list and its score in Doc_score is set to ó 1. After k iterations, the top k hits will have been entered on the hit-list. The techniques just described assume that the entire signature file fits in main memory. If this is not the case, additional steps must be taken to process the entire document collection. A straightforward approach is to process the collection in batches. A batch consists of as many document signatures as will fit in main memory at one time. Each batch is read into memory and scored using one of the above algorithms. The intermediate results are saved in an array of document scores. After all batches have been processed, the array of document scores is ranked and the final hit-list is generated. In general, processing the collection in batches performs poorly due to the I/O required to read in each batch. The performance penalty imposed by the I/O can be reduced by processing multiple queries on each batch, such that the I/O costs are amortized over multiple queries. This helps query processing throughput, but does nothing to improve query processing response time. An alternative to processing in batches is to use a parallel hit-sliced sig-nature file, proposed by Panagopoulos and Faloutsos [627] (see Chapter 8). 244        PARALLEL AND DISTRIBUTED IR docs 0 110 11 10 0 10 0 1110 10 0 10 0 0 0 rfoc5     110    0    0     1 Figure 9.8    Document signatures. Figure 9.8 shows a matrix representation of the signatures for a small document collection (N = 5). In a traditional signature file, each row of the matrix, or document signature, is stored contiguously. In a bit-sliced signature file, each column of the matrix, or bit-slice, is stored contiguously. A bit-slice is a vertical slice through the matrix, such that bit-slice i contains the i-th bit from every document signature. With this organization, the retrieval system can load just those bit-slices required by the query terms in question. Note that the file offset of bit-slice i (starting with 0) is z*iV bits, and the length of each bit-slice is iV bits. When using a bit-sliced signature file, each virtual processor is still responsible for scoring a single document. A virtual processor's local memory is used to store the bits from each bit-slice that correspond to the processor's document. A bit-slice, therefore, is distributed across the virtual processors with one bit at each processor. The set of bits across the virtual processors that corresponds to a single bit-slice is called a frame. The total number of frames is F = M/N, where M is the size of memory in bits available for storing bit-slices. When F lt; W (W is the number of bit-slices in the file), the system employs a frame replacement policy to determine which bit-slices must be resident to process the query. The frame replacement policy may simply fetch all of the bit-slices that correspond to the query terms, or it may analyze the query and identify a subset of bit-slices that, when evaluated, still provides an acceptably low false drop probability. To search the bit-sliced signature file, we must make a few modifications to our basic query processing procedures. First, the frame replacement routine must be run at the start of processing a query to insure that the required bit-slices are resident. Second, the signature hash functions must be updated to return a frame index rather than a signature bit index. The frame index is the index of the frame that contains the bit-slice corresponding to the previously computed signature bit index. Finally, the parallel bit array, Doc_sig, passed into probe^doc is replaced with the parallel bit array Frames, which provides eacii virtual processor access to its frames. Panagopoulos and Faloutsos [627] analyze the performance of the parallel bit-sliced signature file and show that query response times of under 2 seconds can be achieved on a 128 Gb database on the CM-2. Although this technique addresses the issue of query response time on large document collections, it defeats one of the often claimed advantages of the signature file organization, namely, that indexing new documents is straightforward. In a traditional signature file PARALLEL IR        245 organization, new document signatures may simply be appended to the signature file. With a bit-sliced signature file, the signature file must be inverted, resulting in update costs similar to that of an inverted file. inverted Files While the adaptation of signature file techniques to SIMD architectures is rather natural, inverted files are somewhat awkward to implement on SIMD machines. Nevertheless, Stanfill et al. [744, 740] have proposed two adaptations of inverted files for the CM-2. Recall from Chapter 8 the structure of an inverted list. In its simplest form, an inverted list contains a posting for each document in which the associated term appears. A posting is a tuple of the form (^, dj), where kz is a term identifier and dj is a document identifier. Depending on the retrieval model, postings may additionally contain weights or positional information. If positional information is stored, then a posting is created for each occurrence of k7 in dj. The first parallel inverted file implementation for the CM-2 uses two data structures to store the inverted file: a postings table and an index. The postings table contains the document identifiers from the postings and the index maps terms to their corresponding entries in the postings table. Before the postings are loaded into these structures, they are sorted by term identifier. The document identifiers are then loaded into the postings table in this sorted order, filling in a series of rows of length P, where P is the number of processors in use. The postings table is treated as a parallel array, where the array subscript selects a particular row, and each row is spread across the P processors. For each term, the index stores the locations of the first and last entries in the postings table for the set of document identifiers associated with the term. Figure 9.9 shows a small document collection, the raw postings, and the resulting postings table and index. For example, to find the documents that contain the term "piggy/ we look up 'piggy' in the index and determine that the postings table entries from row 1, position 3, to row 2, position 1, contain the corresponding document identifiers, or 0, 1, and 2. At search time these data structures are used to rank documents as follows. First, the retrieval system loads the postings table onto the back-end processors. Next, the system iterates over the query terms. For each query term, an index lookup returns the range of postings table entries that must be processed. The search system then iterates over the rows included in this range. For each row, the processors that contain entries for the current term are activated and the associated document identifiers are used to update the scores of the corresponding documents. Document scores are built up in accumulators (called mailboxes by Stanfill), which are allocated in a parallel array similar to the postings table. To update the accumulator for a particular document, we must determine the accumulator's row and position within the row. For convenience, well assume that this information (rather than document identifiers) is stored in the postings table. Furthermore, 246        PARALLEL AND DISTRIBUTED IR Documents This   little  piggy went to market. This  little  piggy stayed home. This  little   piggy had roast beef. Postings beef	2 had	2 home	1 little	0 little	1 little	2 market	0 piggy	0 piggy	1 piggy	2 roast	2 stayed	1 this	0 this	1 this	2 to	0 went	0 Index Term	First		Last Row	Pos.	Row	Pos. beef	0	0	0	0 had	0	1	0	1 home	0	2	0	2 little	0	3	1	1 market	1	2	1	2 piggy	1	3	2	1 roast	2	2	2	2 stayed	2	3	2	3 this	3	0	3	2 to	3	3	3	3 went	4	0	4	0 Postings			Table 2	2	1	0 1	2	0	0 1	2	2	1 0	1	2	0 0 Figure 9.9    Parallel inverted file. we'll assume that weights have been associated with each posting and stored in the postings table. The complete algorithm for scoring a weighted term is showTn in Figure 9.10. The score_term routine assumes that the index lookup for the query term has been done and the results were stored in term. The routine iterates over each row of postings associated with the term and determines which positions to process within the current row. Position is a parallel integer constant where the first instance contains 0, the second instance contains 1, etc., and the last instance contains N-PROCS ó 1. It is used in the where clause to activate the appropriate processors based on the positions of interest in the current row. The left-indexing performed on Doc.score at the end of the routine provides access to a particular instance of the parallel variable. This operation is significant because it involves communication between the processors. Posting weights must be shipped from the processor containing the posting to the processor containing the accumulator for the corresponding document. After the system has processed all of the query terms with score_term, it ranks the documents based on their scores and returns the top k documents. It is expensive to send posting weights to accumulators on different processors. To address this problem, Stanfill [740] proposed the partitioned postings PARALLEL IR        247 score_term  (P_float Doc_score[],  P_posting PostingG, term_t term) { int         i; int         first_pos; int         last_pos; P_int      Doc_row; P_int      Doc_pos; P.float Weight; for   (i = term.first_row;   i  lt;= term.last_row; first_pos =  (i === term.first_row ? term.first_pos   :   0); last_pos =  (i == term.last_row ? term.last_pos   :   N_PR0CS  -1); where   (Position gt;= first_pos  Position lt;= last.pos)   -C Doc_row = Posting[i].row; Doc_pos = Posting[i].pos; Weight = term.weight  * Posting[i].weight; [Doc_pos]Doc_score[Doc_row]   += Weight; Figure 9.10    score_term. file, which eliminates the communication required in the previous algorithm by storing the postings and accumulator for a given document on the same processor. There are two tricks to accomplishing this. First, as the postings are loaded into the postings table, rather than working left to right across the rows and filling each row before starting with the next one, the postings are added to the column that corresponds to the processor where the associated document will be scored. This ensures that all of the postings associated with a document will be loaded onto the same processor as the document's accumulator. Figure 9.11 (a) shows how the postings from Figure 9.9 would be loaded into a table for two processors, with documents 0 and 1 assigned to processor 0 and document 2 assigned to processor 1. Figure 9.11(a) also demonstrates a problem with this scheme. The postings for the term this are skewed and no longer span consecutive rows. To handle this situation, we apply the second trick of the partitioned postings file, which is to segment the postings such that every term in segment i is lexicographically less than or equal to every term in segment i 4-1. This is shown in Figure 9.11(b) using segments of three rows. Note how some segments may need to be padded with blank space in order to satisfy the partitioning constraints. 248        PARALLEL AND DISTRIBUTED IR home	1	beef	2 little	0	had	2 little	1	little	2 market	0	piggy	2 piggy	0	roast	2 piggy	1	this	2 stayed	1 this	0 this	1 to	0 went	0 home 1 little 0 little         1	beef 2 had 2 little      2 market     0 piggy o piggy     i	piggy    2 roast      2 stayed 1 this 0 this          1	this        2 to 0 went         0 (a) (b) Figure 9.11    Skewed and partitioned postings. Index First	Last Terni	Partition	Partition	Tag beef	0	0	0 had	0	0	1 home	0	0	2 little	0	0	3 market	1	1	0 piggy	1	1	1 | roast	1	1	2 stayed	2	2	0 this		2	1 to	3	3	0 went	3	3	1 Postings			Table 2	1	0	0 3	0	1	0 3	1	3	0 0	0	1	0 1	0	2	0 1	1 0	1	1	0 1	0 1	1 0	0 1	0 Figure 9.12   Partitioned postings file. The postings table and index undergo a few more modifications before reaching their final form, shown in Figure 9.12. First, term identifiers in the postings are replaced by term tags. The system assigns tags to terms such that no two terms in the same partition share the same tag. Second, document identifiers in the postings are replaced by document row numbers, where the row number Identifies which row contains the accumulator for the document. Since the accumulator is at the same position (i.e., processor) as the posting, the row number is sufficient to identify the document. Finally, the index is modified to record the .starting partition, ending partition, and tag for each term. DISTRIBUTED IR        249 ppf_score_term (P_float Doc_score [] , P_posting Posting [], term_t term) / \ int    i; P_int  Doc_row; P.float Weight; for (i - term.first_part	* N_R0WS; i lt; (term.last_part	+ 1) * N_R0WS; i++) { where (Posting[i].tag	== term.tag) { Doc_row = Posting [i]	.row; Weight = term.weight	* Posting[i].weight; Doc_score[Doc_row] + } } }	= Weight; Figure 9.13    ppf_s core-term. The modified term scoring algorithm is shown in Figure 9.13. Here NJLQWS is the number of rows per partition. The algorithm iterates over the rows of postings that span the term's partitions and activates the processors with matching postings. Each active processor extracts the document row from the posting, calculates the term weight, and updates the document's score. After all query terms have been processed, the system ranks the documents and returns the top k. Stanfill [740] shows that the partitioned postings file imposes a space overhead of approximately 1/3 the original text (of which 10-20% is wasted partition padding) and can support sub 2-second query response times on a terabyte of text using a 64K processor CM-2.  
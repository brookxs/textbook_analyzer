 5.2.7 "Latent Semantic" Claims  Within the IR community, SVD was first applied to the Index matrix by Deerwester et al. [Deerwester et al., 1990; Dumais, 1991] and was called latent semantic indexing (LSI). The "latent semantic" claim derives from  the authors* belief that the reduced dimension representation of documents in fact reveals semantic correlations among index terms. Further, they argue that evidence collected across entire corpora transcend individually "fallible" document instances. That is, while one document's  www. netlib. org. s vdpack 160      FINDING OUT ABOUT  author might use the word CAR and another the synonym AUTO, the correlation of both of these with other terms like HIGHWAY, GASOLINE, and DRIVING will result in an abstracted document feature/dimension on which queries using either keyword, CAR or AUTO, will project equivalently. "Synonymous" retrieval has been accomplished!  Landauer and Dumais have recently extended this algebraic manipulation of the Index relation into an ambitious model of human memory [Landauer and Dumais, 1997]. Much of psychology is concerned with the problem of how children, those most powerful learning agents, are able to learn so much from such a "poverty of the stimulus." That is, by many forms of analysis the stimuli driving learning do not by themselves contain sufficient information to induce the elaborate conceptual structures children demonstrate.  Applying these ideas to textual corpora, Landauer and Dumais "trained" an LSI model with presentation of paragraph after paragraph drawn from more than 30,000 encyclopedia articles. Using retrieval on a standardized synonym test as their performance measure, the emerging eigenvector representation (compressed to 300 dimensions) showed a rate of improvement comparable to that of schoolchildren! Because this performance required "indirect inference" like that supported by LSI eigenvectors and beyond what could be accomplished on the basis of simple word cooccurrence alone, Landauer and Dumais suggested that  A new                LSI provides an important model of human memory. 1"  argument for  [Sjome domains of knowledge contain vast numbers of weak correlations that... can greatly amplify learning by a process of  inference___[A] substantial portion of the information needed  ... can be inferred from the contextual statistics of usage alone. [Landauer and Dumais, 1997]  At the very least, LSI demonstrates how traditional associative memory models [James, 1893; Hebb, 1949; Baddeley, 1976; Anderson and Kline,  1979] can be extended to exploit higher-order correlations.  The earlier work trying to connect a small number of factor-analytic,  "semantically" meaningful dimensions [Koll, 1979; Borko and Bernick, 1963] is also interesting in this respect. Jones and Furnas have also investigated how well cosine/inner product reflects human similarity judgments [Jones and Furnas, 1987]. In any case, a cognitive interpretation of these issues promises to remain an active area of investigation. MATHEMATICAL FOUNDATIONS       161  We now consider how relevance feedback assessments might also be used to provide document-similarity information, and how they can be used to reduce the dimensionality of documents' representations.   
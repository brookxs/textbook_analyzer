automatic-image-annotation using tag-related random-search over visual neighbors in this paper , we propose a novel image auto-annotation model using tag-related random-search over range-constrained visual neighbors of the to-be-annotated image . the proposed model , termed as tagsearcher , observes that the annotating performances of many previous visual-neighbor-based models are generally sensitive to the quantity setting of visual neighbors , and the probabilities for visual neighbors to be selected is better to be tag-dependent , meaning that each candidate tag can have its own trustworthy part of visual neighbors for score-prediction . and thus tagsearcher uses a constrained range rather than an identical and fixed number of visual neighbors for auto-annotation . by performing a novel tag-related random-search process over the graphical-model made up of range-constrained visual neighbors , tagsearcher can find the trustworthy part for each candidate tag , and further utilize both visual similarities and tag correlations for score-prediction . with the range constraint for visual neighbors and the tag-related random-search process , tagsearcher can not only achieve satisfactory annotating performances , but also reduce the performance sensitivity . experiments conducted on benchmark corel5k well demonstrate its rationality and effectiveness .
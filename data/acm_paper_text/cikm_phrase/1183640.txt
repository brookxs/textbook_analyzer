finding highly correlated pairs efficiently with powerful pruning we consider the problem of finding highly correlated pairs in a large-data-set . that is , given a threshold not too small , we wish to report all the pairs of items (or binary attributes) whose (pearson) correlation coefficients are greater than the threshold . correlation-analysis is an important step in many statistical and knowledge-discovery tasks . normally , the number of highly correlated pairs is quite small compared to the total number of pairs . identifying highly correlated pairs in a naive way by computing the correlation coefficients for all the pairs is wasteful . with massive-data-sets , where the total number of pairs may exceed the main-memory capacity , the computational cost of the naive method is prohibitive . in their kdd '04 paper [15] , hui xiong et al . address this problem by proposing the taper algorithm . the algorithm goes through the data-set in two passes . it uses the first pass to generate a set of candidate pairs whose correlation coefficients are then computed directly in the second pass . the efficiency of the algorithm depends greatly on the selectivity (pruning-power) of its candidate-generating stage.in this work , we adopt the general-framework of the taper algorithm but propose a different candidate-generation method . for a pair of items , taper 's candidate-generation method considers only the frequencies (supports) of individual items . our method also considers the frequency (support) of the pair but does not explicitly count this frequency (support) . we give a simple randomized-algorithm whose false-negative probability is negligible . the space and time complexities of generating the candidate-set in our algorithm are asymptotically the same as taper 's . we conduct experiments on synthesized and real-data . the results show that our algorithm produces a greatly reduced candidate-set - one that can be several orders of magnitude smaller than that generated by taper . because of this , our algorithm uses much less memory and can be faster . the former is critical for dealing with massive-data .
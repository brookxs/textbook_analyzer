improving binary-classification on text problems using differential word-features we describe an efficient technique to weigh word-based features in binary-classification tasks and show that it significantly improves classification-accuracy on a range of problems . the most common text-classification approach uses a document 's ngrams (words and short phrases) as its features and assigns feature values equal to their frequency or tfidf score relative to the training-corpus . our approach uses values computed as the product of an ngram 's document-frequency and the difference of its inverse document frequencies in the positive and negative training-sets . while this technique is remarkably easy to implement , it gives a statistically significant improvement over the standard bag-of-words approaches using support-vector-machines on a range of classification-tasks . our results show that our technique is robust and broadly applicable . we provide an analysis of why the approach works and how it can generalize to other domains and problems .
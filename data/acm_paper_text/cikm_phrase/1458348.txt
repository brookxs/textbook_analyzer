suppressing outliers in pairwise-preference ranking many of the recently proposed algorithms for learning feature-based ranking-functions are based on the pairwise-preference framework , in which instead of taking documents in isolation , document pairs are used as instances in the learning-process . one disadvantage of this process is that a noisy relevance-judgment on a single-document can lead to a large number of mis-labeled document pairs . this can jeopardize robustness and deteriorate overall ranking-performance . in this paper we study the effects of outlying pairs in rank learning with pairwise preferences and introduce a new meta-learning algorithm capable of suppressing these undesirable effects . this algorithm works as a second optimization step in which any linear baseline ranker can be used as input . experiments on eight different ranking datasets show that this optimization step produces statistically significant performance gains over state-of-the-art methods .
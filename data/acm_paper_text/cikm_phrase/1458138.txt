tinylex : static n-gram index-pruning with perfect recall inverted-indexes using sequences of characters (n-grams) as terms provide an error-resilient and language-independent way to query for arbitrary substrings and perform approximate-matching in a text , but present a number of practical-problems : they have a very-large number of terms , they exhibit pathologically expensive worst-case query times on certain natural inputs , and they can not cope with very short query strings . in word-based indexes , static index-pruning has been successful in reducing index-size while maintaining precision , at the expense of recall . taking advantage of the unique inclusion structure of n-gram terms of different lengths , we show that the lexicon size of an n-gram index can be reduced by 7 to 15 times without any loss of recall , and without any increase in either index-size or query time . because the lexicon is typically stored in main-memory , this substantially reduces the memory required for queries . simultaneously , our construction is also the first overlapping n-gram index to place tunable worst-case bounds on false-positives and to permit efficient queries on strings of any length . using this construction , we also demonstrate the first feasible n-gram index using words rather than characters as units , and its-applications to phrase-searching .
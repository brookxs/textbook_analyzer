are click-through-data adequate for learning web-search-rankings ? learning-to-rank algorithms , which can automatically adapt ranking-functions in web-search , require a large volume of training-data . a traditional way of generating-training-examples is to employ human experts to judge the relevance of documents . unfortunately , it is difficult , time-consuming and costly . in this paper , we study the problem of exploiting click-through-data for learning web-search-rankings that can be collected at much lower cost . we extract pairwise relevance preferences from a large-scale aggregated click-through dataset , compare these preferences with explicit human-judgments , and use them as training-examples to learn ranking-functions . we find click-through-data are useful and effective in learning ranking-functions . a straightforward use of aggregated click-through-data can outperform human-judgments . we demonstrate that the strategies are only slightly affected by fraudulent clicks . we also reveal that the pairs which are very reliable , e.g. , the pairs consisting of documents with large click-frequency differences , are not sufficient for learning .
designing clustering-based web-crawling policies for search-engine crawlers the world-wide-web is growing and changing at an astonishing rate . web-information-systems such as search-engines have to keep up with the growth and change of the-web . due to resource-constraints , search-engines usually have difficulties keeping the local database completely synchronized with the-web . in this paper , we study how tomake good use of the limited system resource and detect as many changes as possible . towards this goal , a crawler for the-web search-engine should be able to predict the change behavior of the webpages . we propose applying clustering-based sampling approach . specifically , we first group all the local webpages into different clusters such that each cluster contains webpages with similar change pattern . we then sample webpages from each cluster to estimate the change frequency of all the webpages in that cluster . finally , we let the crawler re-visit the cluster containing webpages with higher change frequency with a higher probability . to evaluate the performance of an incremental-crawler for a web-search-engine , we measure both the freshness and the quality of the query results provided by the search-engine . we run extensive experiments on a real web-data set of about 300,000 distinct urls distributed among 210 websites . the results demonstrate that our clustering-algorithm effectively clusters the pages with similar change-patterns , and our solution significantly outperforms the existing methods in that it can detect more changed webpages and improve the quality of the user-experience for those who query the search-engine .
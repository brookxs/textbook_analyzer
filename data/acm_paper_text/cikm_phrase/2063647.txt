partial-duplicate-detection for large book collections a framework is presented for discovering partial duplicates in large-collections of scanned-books with optical-character-recognition (ocr) errors . each book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book . these words are referred to as `` unique-words '' and they constitute a small percentage of all the words in a typical book . along with the order information the set of unique-words provides a compact-representation which is highly descriptive of the content and the flow-of-ideas in the book . by aligning the sequence of unique-words from two books using the longest-common-subsequence (lcs) one can discover whether two books are duplicates . experiments on several datasets show that dupniq is more accurate than traditional methods for duplicate-detection such as shingling and is fast . on a collection of 100k scanned english books dupniq detects partial duplicates in 30 min using 350 cores and has precision 0.996 and recall 0.833 compared to shingling with precision 0.992 and recall 0.720 . the technique works on other languages as well and is demonstrated for a french dataset .
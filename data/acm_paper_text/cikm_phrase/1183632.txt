evaluation by comparing result sets in context familiar evaluation-methodologies for information-retrieval (ir) are not well suited to the task of comparing systems in many real settings . these systems and evaluation-methods must support contextual , interactive-retrieval over changing , heterogeneous-data collections , including private and confidential information.we have implemented a comparison tool which can be inserted into the natural ir process . it provides a familiar search-interface , presents a small number of result sets in side-by-side panels , elicits searcher judgments , and logs interaction events . the tool permits study of real information-needs as they occur , uses the documents actually available at the time of the search , and records judgments taking into account the instantaneous needs of the searcher.we have validated our proposed evaluation-approach and explored potential biases by comparing different whole-of-web search facilities using a web-based version of the tool . in four experiments , one with supplied queries in the laboratory and three with real queries in the workplace , subjects showed no discernable left-right bias and were able to reliably distinguish between high - and low-quality result sets . we found that judgments were strongly predicted by simple implicit measures.following validation we undertook a case-study comparing two leading whole-of-web search-engines . the approach is now being used in several ongoing investigations .
on caption bias in interleaving experiments information-retrieval-evaluation most often involves manually assessing the relevance of particular query-document pairs . in cases where this is difficult (such as personalized-search) , interleaved-comparison methods are becoming increasingly common . these methods compare pairs of ranking-functions based on user-clicks on search-results , thus better reflecting true user-preferences . however , by depending on clicks , there is a potential for bias . for example , users have been previously shown to be more likely to click on results with attractive titles and snippets . an interleaving evaluation where one ranker tends to generate results that attract more clicks (without being more relevant) may thus be biased . we present an approach for detecting and compensating for this type of bias in interleaving evaluations . introducing a new model of caption bias , we propose features that model-bias based on (1) per-document effects , and (2) the (pairwise) relationships between a document and surrounding documents . we show that our model can effectively capture click-behavior , with best results achieved by a model that combines both per-document and pairwise-features . applying this model to re-weight observed user-clicks , we find a small overall effect on real interleaving comparisons , but also identify a case where initially detected preferences vanish after caption bias re-weighting is applied . our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias .
semantic convolution-kernels over dependency-trees : smoothed partial tree-kernel in recent years , natural-language-processing-techniques have been used more and more in ir . among other syntactic and semantic parsing are effective methods for the design of complex applications like for example question-answering and sentiment-analysis . unfortunately , extracting feature representations suitable for machine-learning-algorithms from linguistic-structures is typically difficult . in this paper , we describe one of the most advanced piece of technology for automatic engineering of syntactic and semantic patterns . this method merges together convolution dependency-tree kernels with lexical similarities . it can efficiently and effectively measure the similarity between dependency-structures , whose lexical nodes are in part or completely different . its use in powerful algorithm such as support-vector-machines (svms) allows for fast design of accurate automatic systems . we report some experiments on question-classification , which show an unprecedented result , e.g. 41 \ % of error-reduction of the former state-of-the-art , along with the analysis of the nice properties of the approach .
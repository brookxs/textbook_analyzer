collaborative dual-plsa : mining distinction and commonality across multiple domains for text-classification the distribution-difference among multiple data domains has been considered for the cross-domain text-classification problem . in this study , we show two new observations along this line . first , the data-distribution difference may come from the fact that different domains use different key-words to express the same concept . second , the association between this conceptual feature and the document class may be stable across domains . these two issues are actually the distinction and commonality across data domains . inspired by the above observations , we propose a generative statistical-model , named collaborative dual-plsa (cd-plsa) , to simultaneously capture both the domain distinction and commonality among multiple domains . different from probabilistic-latent-semantic-analysis (plsa) with only one latent-variable , the proposed model has two latent-factors y and z , corresponding to word concept and document-class respectively . the shared commonality intertwines with the distinctions over multiple domains , and is also used as the-bridge for knowledge-transformation . we exploit an expectation-maximization (em) algorithm to learn this model , and also propose its distributed version to handle the situation where the data domains are geographically separated from each other . finally , we conduct extensive experiments over hundreds of classification-tasks with multiple-source domains and multiple target domains to validate the superiority of the proposed cd-plsa model over existing state-of-the-art methods of supervised and transfer-learning . in particular , we show that cd-plsa is more tolerant of distribution differences .
comparing metrics across trec and ntcir : the robustness to system bias test-collections are growing larger , and relevance data constructed through pooling are suspected of becoming more and more incomplete and biased . several studies have used evaluation-metrics specifically designed to handle this problem , but most of them have only examined the metrics under incomplete but unbiased conditions , using random-samples of the original relevance data . this paper examines nine metrics in a more realistic setting , by reducing the number of pooled systems . even though previous work has shown that metrics based on a condensed list , obtained by removing all unjudged documents from the original ranked list , are effective for handling very incomplete but unbiased relevance data , we show that these results do not hold in the presence of system bias . in our experiments using trec and ntcir data , we first show that condensed-list metrics overestimate new systems while traditional metrics underestimate them , and that the overestimation tends to be larger than the underestimation . we then show that , when relevance data is heavily biased towards a single team or a few teams , the condensed-list versions of average-precision (ap) , q-measure (q) and normalised-discounted-cumulative-gain (ndcg) , which we call ap ' , q ' and ndcg ' , are not necessarily superior to the original metrics in terms of discriminative power , i.e. , the overall ability to detect pairwise-statistical-significance . nevertheless , even under system bias , ap ' and q ' are generally more discriminative than bpref and the condensed-list version of rank-biased precision (rbp) , which we call rbp ' .
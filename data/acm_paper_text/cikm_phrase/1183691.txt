efficient model-selection for regularized linear-discriminant-analysis classical linear-discriminant-analysis (lda) is not applicable for small-sample size problems due to the singularity of the scatter-matrices involved . regularized-lda (rlda) provides a simple strategy to overcome the singularity problem by applying a regularization term , which is commonly estimated via cross-validation from a set of candidates . however , cross-validation may be computationally prohibitive when the candidate-set is large . an efficient-algorithm for rlda is presented that computes the optimal transformation of rlda for a large set of parameter candidates , with approximately the same cost as running rlda a small number of times . thus it facilitates efficient model-selection for rlda.an intrinsic relationship between rlda and uncorrelated lda (ulda) , which was recently proposed for dimension-reduction and classification is presented . more specifically , rlda is shown to approach ulda when the regularization value tends to zero . that is , rlda without any regularization is equivalent to ulda . it can be further shown that ulda maps all data points from the same class to a common point , under a mild condition which has been shown to hold for many high-dimensional datasets . this leads to the overfitting problem in ulda , which has been observed in several applications . thetheoretical analysis presented provides further justification for the use of regularization in rlda . extensive experiments confirm the claimed theoretical estimate of efficiency . experiments also show that , for a properly chosen regularization-parameter , rlda performs favorably in classification , in comparison with ulda , as well as other existing lda-based algorithms and support-vector-machines (svm) .
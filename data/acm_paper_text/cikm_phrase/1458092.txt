how does clickthrough-data reflect retrieval quality ? automatically judging the quality of retrieval functions based on observable user-behavior holds promise for making retrieval-evaluation faster , cheaper , and more user-centered . however , the relationship between observable user-behavior and retrieval quality is not yet fully understood . we present a sequence of studies investigating this relationship for an operational search-engine on the arxiv.org e-print archive . we find that none of the eight absolute usage metrics we explore (e.g. , number of clicks , frequency of query-reformulations , abandonment) reliably reflect retrieval quality for the sample sizes we consider . however , we find that paired experiment designs adapted from sensory analysis produce accurate and reliable statements about the relative quality of two retrieval functions . in particular , we investigate two paired-comparison tests that analyze clickthrough-data from an interleaved presentation of ranking pairs , and we find that both give accurate and consistent results . we conclude that both paired-comparison tests give substantially more accurate and sensitive evaluation results than absolute usage metrics in our domain .
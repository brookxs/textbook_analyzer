learning similarity measures in non-orthogonal-space many machine-learning and data-mining-algorithms crucially rely on the similarity-metrics . the cosine-similarity , which calculates the inner-product of two normalized feature-vectors , is one of the most commonly used similarity-measures . however , in many practical tasks such as text-categorization and document-clustering , the cosine-similarity is calculated under the assumption that the input space is an orthogonal space which usually could not be satisfied due to <i> synonymy </i> and <i> polysemy </i> . various algorithms such as latent-semantic-indexing (lsi) were used to solve this problem by projecting the original data into an orthogonal space . however lsi also suffered from the high computational cost and data-sparseness . these shortcomings led to increases in computation-time and storage requirements for large-scale realistic data . in this paper , we propose a novel and effective similarity-metric in the non-orthogonal input space . the basic idea of our proposed metric is that the similarity of features should affect the similarity of objects , and vice versa . a novel iterative-algorithm for computing non-orthogonal-space similarity-measures is then proposed . experimental-results on a synthetic-data set , a real msn search click-thru logs , and 20ng dataset show that our algorithm outperforms the traditional cosine-similarity and is superior to lsi .
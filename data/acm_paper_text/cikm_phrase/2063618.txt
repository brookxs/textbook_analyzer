a probabilistic-method for inferring preferences from clicks evaluating rankers using implicit-feedback , such as clicks on documents in a result list , is an increasingly popular alternative to traditional evaluation-methods based on explicit relevance judgments . previous work has shown that so-called interleaved-comparison methods can utilize click-data to detect small differences between rankers and can be applied to learn ranking-functions online . in this paper , we analyze three existing interleaved-comparison methods and find that they are all either biased or insensitive to some differences between rankers . to address these problems , we present a new method based on a probabilistic interleaving process . we derive an unbiased estimator of comparison outcomes and show how marginalizing over possible comparison outcomes given the observed click-data can make this estimator even more effective . we validate our approach using a recently developed simulation-framework based on a learning-to-rank dataset and a model of click-behavior . our experiments confirm the results of our analysis and show that our method is both more accurate and more robust to noise than existing methods .
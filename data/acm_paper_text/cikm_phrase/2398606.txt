scaling multiple-source entity-resolution using statistically efficient transfer-learning we consider a serious , previously-unexplored challenge facing almost all approaches to scaling up entity-resolution (er) to multiple data sources : the prohibitive cost of labeling training-data for supervised-learning of similarity-scores for each pair of sources . while there exists a rich literature describing almost all aspects of pairwise er , this new challenge is arising now due to the unprecedented ability to acquire and store data from online-sources , interest in features driven by er such as enriched search verticals , and the uniqueness of noisy and missing-data characteristics for each source . we show on real-world and synthetic-data that for state-of-the-art techniques , the reality of heterogeneous-sources means that the number of labeled-training-data must scale quadratically in the number of sources , just to maintain constant precision/recall . we address this challenge with a brand new transfer-learning algorithm which requires far less training-data (or equivalently , achieves superior accuracy with the same data) and is trained using fast convex-optimization . the intuition behind our approach is to adaptively share structure learned about one scoring problem with all other scoring problems sharing a data source in common . we demonstrate that our theoretically-motivated approach improves upon existing techniques for multi-source er .
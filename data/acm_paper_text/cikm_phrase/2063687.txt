focusing on novelty : a crawling strategy to build diverse language-models word-prediction performed by language-models has an important role in many tasks as e.g. word-sense-disambiguation , speech-recognition , hand-writing recognition , query-spelling and query-segmentation . recent research has exploited the textual content of the-web to create language-models . in this paper , we propose a new focused-crawling strategy to collect web-pages that focuses on novelty in order to create diverse language-models . in each crawling cycle , the crawler tries to ll the gaps present in the current language-model built from previous cycles , by avoiding visiting pages whose vocabulary is already well represented in the model . it relies on an information-theoretic measure to identify these gaps and then learns link-patterns to pages in these regions in order to guide its visitation policy . to handle constantly evolving domains , a key feature of our crawler approach is its ability to adjust its focus as the crawl progresses . we evaluate our approach in two different scenarios in which our solution can be useful . first , we demonstrate that our approach produces more effective language-models than the ones created by a baseline crawler in the context of a speech-recognition-task of broadcast-news . in fact , in some cases , our crawler was able to obtain similar results to the baseline by crawling only 12.5 \ % of the pages collected by the latter . secondly , since in the news domain avoiding well-represented content might lead to novelty , i.e. up-to-date pages , we show that our diversity-based crawler can also be helpful to guide the crawler for the most recent content in the news . the results show that our approach was able to obtain on average 50 \ % more up-to-date pages than the baseline crawler .
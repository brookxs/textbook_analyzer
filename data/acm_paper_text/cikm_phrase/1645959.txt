an empirical-study on using hidden-markov-model for search-interface segmentation this paper describes a hidden-markov-model (hmm) based approach to perform search-interface segmentation . automatic-processing of an-interface is a must to access the invisible contents of deep-web . this entails automatic-segmentation , i.e. , the task of grouping related components of an-interface together . while it is easy for a human to discern the logical relationships among interface-components , machine-processing of an interface is difficult . in this paper , we propose an approach to segmentation that leverages the probabilistic nature of the interface-design process . the design process involves choosing components based on the underlying database-query requirements , and organizing them into suitable patterns . we simulate this process by creating an `` artificial designer '' in the form of a 2-layered hmm . the learned hmm acquires the implicit design-knowledge required for segmentation . we empirically study the effectiveness of the approach across several representative domains of deep-web . in terms of segmentation-accuracy , the hmm-based-approach outperforms an existing state-of-the-art approach by at least 10 \ % in most cases . furthermore , our cross-domain investigation shows that a single hmm trained on data having varied and frequent design-patterns can accurately segment interfaces from multiple domains .
combining labeled and unlabeled-data with word-class distribution learning we describe a novel simple and highly scalable semi-supervised method called word-class distribution learning (wcdl) , and apply it task of information-extraction (ie) by utilizing unlabeled sentences to improve supervised-classification methods . wcdl iteratively builds class label distributions for each word in the dictionary by averaging predicted labels over all cases in the unlabeled-corpus , and re-training a base classifier adding these distributions as word-features . in contrast , traditional self-training or co-training methods self-labeled examples (rather than features) which can degrade performance due to incestuous learning bias . wcdl exhibits robust behavior , and has no difficult parameters to tune . we applied our method on german and english name entity-recognition (ner) tasks . wcdl shows improvements over self-training , multi-task semi-supervision or supervision alone , in particular yielding a state-of-the art 75.72 f1 score on the german ner task .
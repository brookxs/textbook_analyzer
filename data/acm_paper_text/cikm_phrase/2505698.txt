evaluating aggregated-search using interleaving a result page of a modern web-search-engine is often much more complicated than a simple list of `` ten blue links . '' in particular , a search-engine may combine results from different sources (e.g. , web , news , and images) , and display these as grouped results to provide a better user-experience . such a system is called an aggregated or federated-search-system . because search-engines evolve over time , their results need to be constantly evaluated . however , one of the most efficient and widely used evaluation-methods , interleaving , can not be directly applied to aggregated-search systems , as it ignores the need to group results originating from the same source (vertical results) . we propose an interleaving algorithm that allows comparisons of search-engine result pages containing grouped vertical documents . we compare our algorithm to existing interleaving algorithms and other evaluation-methods (such as a/b-testing) , both on real-life click-log data and in simulation-experiments . we find that our algorithm allows us to perform unbiased and accurate interleaved comparisons that are comparable to conventional evaluation-techniques . we also show that our interleaving algorithm produces a ranking that does not substantially alter the user-experience , while being sensitive to changes in both the vertical result block and the non-vertical document rankings . all this makes our proposed interleaving algorithm an essential tool for comparing ir systems with complex aggregated pages .
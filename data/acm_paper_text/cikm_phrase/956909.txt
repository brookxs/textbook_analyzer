text-classification from positive and unlabeled documents most existing studies of text-classification assume that the training-data are completely labeled . in reality , however , many information-retrieval problems can be more accurately described as learning a binary-classifier from a set of incompletely labeled examples , where we typically have a small number of labeled positive examples and a very-large number of unlabeled-examples . in this paper , we study such a problem of performing text-classification without labeled negative data tc-won) . in this paper , we explore an efficient extension of the standard support-vector-machine (svm) approach , called svmc (support-vector mapping convergence) [17] for the tc-won tasks . our analyses show that when the positive training-data is not too under-sampled , svmc significantly outperforms other methods because svmc basically exploits the natural `` gap '' between positive and negative documents in the feature-space , which eventually corresponds to improving the generalization performance . in the text domain there are likely to exist many gaps in the feature-space because a document is usually mapped to a sparse and high-dimensional feature-space . however , as the number of positive training-data decreases , the boundary of svmc starts overfitting at some point and end up generating very poor results.this is because when the positive training-data is too few , the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature-space and thus ends up fitting tightly around the few positive training-data .
heterogeneous-cross-domain-ranking in latent-space traditional ranking mainly focuses on one type of data source , and effective modeling still relies on a sufficiently large number of labeled or supervised examples . however , in many real-world-applications , in particular with the rapid growth of the-web 2.0 , ranking over multiple interrelated (heterogeneous) domains becomes a common situation , where in some domains we may have a large amount of training-data while in some other domains we can only collect very little . one important question is : `` if there is not sufficient supervision in the domain of interest , how could one borrow labeled information from a related but heterogenous domain to build an accurate model ? '' . this paper explores such an approach by bridging two heterogeneous-domains via the latent-space . we propose a regularized framework to simultaneously minimize two loss-functions corresponding to two related but different information-sources , by mapping each domain onto a `` shared latent-space '' , capturing similar and transferable oncepts . we solve this problem by optimizing the convex upper-bound of the non-continuous loss-function and derive its generalization-bound . experimental-results on three different genres of data-sets demonstrate the effectiveness of the proposed approach .
using historical click-data to increase interleaving sensitivity interleaving is an online-evaluation method to compare two alternative ranking-functions based on the users ' implicit-feedback . in an interleaving experiment , the results from two ranking-functions are merged in a single result list and presented to the users . the users ' click-feedback on the merged result list is analysed to derive preferences over the ranking-functions . an important property of interleaving methods is their sensitivity , i.e. their ability to reliably derive the comparison outcome with a relatively small amount of user-behaviour data . this allows testing of changes in the search-engine ranking-functions frequently and , as a result , rapid iterations in developing search-quality improvements can be achieved . in this paper we propose a novel approach to further improve interleaving sensitivity by using pre-experimental user-behaviour data . in particular , the click history is used to train a click-model , which is then used to predict which interleaved result pages are likely to contribute to the experiment outcome . the probabilities of presenting these interleaved result pages to the users are then optimised , such that the sensitivity of interleaving is maximised . in order to evaluate the proposed approach , we re-use data from six actual interleaving experiments , previously performed by a commercial-search-engine . our results demonstrate that the proposed approach outperforms a state-of-the-art baseline , achieving up to a median of 48 % reduction in the number of impressions for the same level of confidence .
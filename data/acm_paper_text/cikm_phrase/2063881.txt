a novel framework of training hidden-markov-support-vector-machines from lightly-annotated data natural-language-understanding (nlu) aims to map sentences to their semantic mean representations . statistical-approaches to nlu normally require fully-annotated training-data where each sentence is paired with its word-level semantic-annotations . in this paper , we propose a novel learning-framework which trains the hidden-markov-support-vector-machines (hm-svms) without the use of expensive fully-annotated data . in particular , our learning-approach takes as input a training-set of sentences labeled with abstract semantic-annotations encoding underlying embedded structural relations and automatically induces derivation rules that map sentences to their semantic-meaning representations . the proposed approach has been tested on the darpa-communicator data and achieved 93.18 \ % in f-measure , which outperforms the previously proposed approaches of training the hidden vector state-model or conditional-random-fields from unaligned data , with a relative-error-reduction rate of 43.3 \ % and 10.6 \ % being achieved .
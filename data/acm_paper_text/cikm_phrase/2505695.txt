building a large-scale corpus for evaluating event-detection on twitter despite the popularity of twitter for research , there are very few publicly available corpora , and those which are available are either too small or unsuitable for tasks such as event-detection . this is partially due to a number of issues associated with the creation of twitter corpora , including restrictions on the distribution of the tweets and the difficultly of creating relevance-judgements at such a large-scale . the difficulty of creating relevance-judgements for the task of event-detection is further hampered by ambiguity in the definition of event . in this paper , we propose a methodology for the creation of an event-detection corpus . specifically , we first create a new corpus that covers a period of 4 weeks and contains over 120 million tweets , which we make available for research . we then propose a definition of event which fits the characteristics of twitter , and using this definition , we generate a set of relevance-judgements aimed specifically at the task of event-detection . to do so , we make use of existing state-of-the-art event-detection approaches and wikipedia to generate a set of candidate events with associated tweets . we then use crowdsourcing to gather relevance-judgements , and discuss the quality of results , including how we ensured integrity and prevented spam . as a result of this process , along with our twitter corpus , we release relevance-judgements containing over 150,000 tweets , covering more than 500 events , which can be used for the evaluation of event-detection approaches .
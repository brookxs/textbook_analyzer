fast supervised feature-extraction by term discrimination-information pooling dimensionality-reduction (dr) through feature-extraction (fe) is desirable for efficient and effective processing of text-documents . many of the techniques for text fe produce features that are not readily interpretable and require super-linear computation-time . in this paper , we present a fast supervised dr/fe technique , named fedip , that is motivated by the notion of relatedness of terms to topics or contexts . this relatedness is quantified by using the discrimination information provided by a term for a topic in a labeled document-collection . features are constructed by pooling the discrimination information of highly related terms for each topic . fedip 's time-complexity is linear in the size of the vocabulary and document-collection . fedip is evaluated for document-classification with svm and naive-bayes-classifiers on six text-data sets . the results show that fedip produces low-dimension feature spaces that yield higher classification-accuracy when compared with lda and lsi . fedip is also found to be significantly faster than the other techniques on our evaluation-data sets .
completing wikipedia 's hyperlink-structure through dimensionality-reduction wikipedia is the largest monolithic repository of human-knowledge . in addition to its sheer size , it represents a new encyclopedic paradigm by interconnecting articles through hyperlinks . however , since these links are created by human authors , links one would expect to see are often missing . the goal of this work is to detect such gaps automatically . in this paper , we propose a novel method for augmenting the structure of hyperlinked document-collections such as wikipedia . it does not require the extraction of any manually defined features from the article to be augmented . instead , it is based on principal-component-analysis , a well-founded mathematical generalization technique , and predicts new links purely based on the statistical structure of the graph formed by the existing links . our method does not rely on the textual content of articles ; we are exploiting only hyperlinks . a user-evaluation of our technique shows that it improves the quality of top link suggestions over the-state-of-the-art and that the best predicted links are significantly more valuable than the ` average ' link already present in wikipedia . beyond link-prediction , our algorithm can potentially be used to point out topics an article misses to cover and to cluster articles semantically .
question-answering in trec traditional text-retrieval-systems return a ranked list of documents in response to a user 's request . while a ranked list of documents can be an appropriate response for the user , frequently it is not . usually it would be better for the system to provide the answer itself instead of requiring the user to search for the answer in a set of documents . the text-retrieval-conference (trec) is sponsoring a question-answering `` track '' to foster research on the problem of retrieving answers rather than document lists.trec is a workshop series sponsored by the national institute of standards and technology and the u.s. department-of-defense [7] . the purpose of the conference series is to encourage research on text-retrieval for realistic applications by providing large test collections , uniform-scoring procedures , and a forum for organizations interested in comparing results . the conference has focused primarily on the traditional ir problem of retrieving a ranked list of documents in response to a statement of information-need , but has also included other tasks , called tracks , that focus on new areas or particularly difficult aspects of information-retrieval . a question-answering track was introduced in trec-8 1999 . the track has generated wide-spread interest in the qa problem [2 , 3 , 4] , and has documented significant improvements in question-answering-system effectiveness in its two-year history.this paper provides a brief summary of the findings of the trec question-answering track to date and discusses the-future directions of the track . the paper is extracted from a fuller description of the track given in `` the trec question-answering track '' [8] . complete details about the trec question-answering track can be found in the trec proceedings .
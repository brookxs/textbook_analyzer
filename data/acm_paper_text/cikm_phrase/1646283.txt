url-normalization for de-duplication of web-pages presence of duplicate-documents in the world-wide-web adversely affects crawling , indexing and relevance , which are the core building-blocks of web-search . in this paper , we present a set of techniques to mine rules from urls and utilize these learnt rules for de-duplication using just url strings without fetching the content explicitly . our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract specific rules from urls belonging to each cluster . preserving each mined rules for de-duplication is not efficient due to the large number of specific rules . we present a machine-learning technique to generalize the set of rules , which reduces the resource footprint to be usable at web-scale . the rule-extraction techniques are robust against web-site specific url conventions . we demonstrate the effectiveness of our techniques through experimental-evaluation .
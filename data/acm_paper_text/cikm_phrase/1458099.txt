transfer-learning from multiple-source domains via consensus-regularization recent years have witnessed an increased interest in transfer-learning . despite the vast amount of research performed in this field , there are remaining challenges in applying the knowledge learnt from multiple-source domains to a target-domain . first , data from multiple-source domains can be semantically related , but have different distributions . it is not clear how to exploit the distribution differences among multiple-source domains to boost the learning-performance in a target-domain . second , many real-world-applications demand this transfer-learning to be performed in a distributed manner . to meet these challenges , we propose a consensus-regularization framework for transfer-learning from multiple-source domains to a target-domain . in this framework , a local classifier is trained by considering both local data available in a source-domain and the prediction consensus with the classifiers from other source domains . in addition , the training-algorithm can be implemented in a distributed manner , in which all the source-domains are treated as slave nodes and the target-domain is used as the master node . to combine the training results from multiple-source domains , it only needs share some statistical-data rather than the full contents of their labeled-data . this can modestly relieve the privacy-concerns and avoid the need to upload all data to a central location . finally , our experimental-results show the effectiveness of our consensus-regularization learning .
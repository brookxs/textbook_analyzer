smoothing dcg for learning-to-rank : a novel approach using smoothed hinge functions discounted-cumulative-gain (dcg) is widely used for evaluating ranking-functions . it is therefore natural to learn a ranking-function that directly optimizes dcg . however , dcg is non-smooth , rendering gradient-based-optimization algorithms inapplicable . to remedy this , smoothed versions of dcg have been proposed but with only partial success . in this paper , we first present analysis that shows it is ineffective using the gradient of the smoothed dcg to drive the optimization-algorithm . we then propose a novel approach , shf-sdcg , for smoothing dcg by using smoothed hinge functions (shf) . it has the advantage of seamlessly transition from driving the optimization mimicking pairwise learning when the ranking-function does not fit the data well , to driving the optimization using dcg when the ranking-function becomes more accurate . shf-sdcg is then extended to reg-shf-sdcg , an algorithm which gradually transits from pointwise and pairwise to listwise learning . finally experimental-results are provided to validate the effectiveness of shf-sdcg and reg-shf-sdcg .
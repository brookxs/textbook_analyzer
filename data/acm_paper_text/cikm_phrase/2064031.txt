large-scale array analytics : taming the data tsunami never before in history mankind has collected data at the rates we face today . alone in 2002 , an estimated 403 petabyte of data has been acquired , equivalent to all printed information ever created before . earth orbiting satellites , as well as ground , airborne , and underwater sensors , space observatories scan their environment at unprecedented resolutions , giving rise to `` big science '' . the same holds for the life-sciences where genomic-data , high-resolution scans , and other modalities are collected in steadily increasing streams . social-network-analysis , olap , and stock exchange trading represent further examples , the latter involving real-time correlation of thousands of ticker time-series resulting in terabytes of data to be analysed per single run . summarized under large-scale analytics we are witnessing an exploding demand for flexible access to massive volumes of scientific and business data sets . arguably a large class of these massive-data is represented by multi-dimensional-arrays . consequently , large arrays pose new challenges to data-modelling , querying , optimization , and maintenance -- in short : we need large-scale array analytics . this tutorial introduces to the topic from a database perspective . aspects addressed include modelling , query-languages , query-optimization and parallelization , and storage-management . high emphasis will be devoted to applications in `` big science '' , particularly geo , space , and life-sciences ; real-life use-cases will be presented and discussed which stem from our 15 years of experience with the open-source rasdaman array dbms and our work on geo raster service standardization . we will highlight requirements , achievements , open-research issues , and avenues for future-research . discussion will make use of real-life examples , many of which internet connected participants can replay hands-on .
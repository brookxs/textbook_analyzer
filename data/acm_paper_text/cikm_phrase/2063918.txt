transfer active-learning active-learning traditionally assumes that labeled and unlabeled-samples are subject to the same distributions and the goal of an active-learner is to label the most informative unlabeled-samples . in reality , situations may exist that we may not have unlabeled-samples from the same domain as the labeled samples (i.e. target-domain) , whereas samples from auxiliary domains might be available . under such situations , an interesting question is whether an active-learner can actively label samples from auxiliary domains to benefit the target-domain . in this paper , we propose a transfer active-learning method , namely transfer active svm (tracsvm) , which uses a limited number of target instances to iteratively discover and label informative auxiliary instances . tracsvm employs an extended sigmoid-function as instance weight-updating approach to adjust the models for prediction of (newly arrived) target data . experimental-results on real-world-data sets demonstrate that tracsvm obtains better efficiency and prediction-accuracy than its peers .
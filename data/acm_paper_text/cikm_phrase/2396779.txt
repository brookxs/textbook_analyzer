an analysis of systematic judging errors in information-retrieval test-collections are powerful mechanisms for the evaluation and optimization of information-retrieval-systems . however , there is reported evidence that experiment outcomes can be affected by changes to the judging guidelines or changes in the judge population . this paper examines such effects in a web-search setting , comparing the judgments of four groups of judges : nist web track judges , untrained crowd workers and two groups of trained judges of a commercial-search-engine . our goal is to identify systematic judging errors by comparing the labels contributed by the different groups , working under the same or different judging guidelines . in particular , we focus on detecting systematic differences in judging depending on specific characteristics of the queries and urls . for example , we ask whether a given population of judges , working under a given set of judging guidelines , are more likely to consistently overrate wikipedia pages than another group judging under the same instructions . our approach is to identify judging errors with respect to a consensus set , a judged gold set and a set of user-clicks . we further demonstrate how such biases can affect the training of retrieval-systems .
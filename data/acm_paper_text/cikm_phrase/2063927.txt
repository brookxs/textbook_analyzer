learning kernels with upper-bounds of leave-one-out-error we propose a new leaning method for multiple-kernel-learning (mkl) based on the upper-bounds of the leave-one-out-error that is an almost unbiased estimate of the expected generalization-error . specifically , we first present two new formulations for mkl by minimizing the upper-bounds of the leave-one-out-error . then , we compute the derivatives of these bounds and design an efficient iterative-algorithm for solving these formulations . experimental-results show that the proposed method gives better accuracy results than that of both svm with the uniform combination of basis kernels and other state-of-art kernel-learning approaches .
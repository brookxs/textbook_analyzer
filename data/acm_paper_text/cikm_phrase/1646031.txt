improvements that do n't add up : ad-hoc-retrieval results since 1998 the existence and use of standard test-collections in information-retrieval experimentation allows results to be compared between research groups and over time . such comparisons , however , are rarely made . most researchers only report results from their own experiments , a practice that allows lack of overall improvement to go unnoticed . in this paper , we analyze results achieved on the trec-ad-hoc , web , terabyte , and robust collections as reported in sigir (1998 -- 2008) and cikm (2004 -- 2008) . dozens of individual published experiments report effectiveness improvements , and often claim statistical-significance . however , there is little evidence of improvement in ad-hoc-retrieval technology over the past decade . baselines are generally weak , often being below the median original trec system . and in only a handful of experiments is the score of the best trec automatic run exceeded . given this finding , we question the value of achieving even a statistically significant result over a weak baseline . we propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress , or at least prevent the lack of it from going unnoticed . we describe an online-database of retrieval runs that facilitates such a practice .
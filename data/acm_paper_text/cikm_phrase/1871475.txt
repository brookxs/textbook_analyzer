decomposing background topics from keywords by principal-component-pursuit low-dimensional topic-models have been proven very useful for modeling a large-corpus of documents that share a relatively small number of topics . dimensionality-reduction tools such as principal-component-analysis or latent-semantic-indexing (lsi) have been widely adopted for document-modeling , analysis , and retrieval . in this paper , we contend that a more pertinent model for a document-corpus as the combination of an (approximately) low-dimensional topic-model for the corpus and a sparse model for the keywords of individual documents . for such a joint topic-document model , lsi or pca is no longer appropriate to analyze the corpus data . we hence introduce a powerful new tool called principal-component-pursuit that can effectively decompose the low-dimensional and the sparse components of such corpus data . we give empirical-results on data synthesized with a latent-dirichlet-allocation (lda) mode to validate the new model . we then show that for real document data-analysis , the new tool significantly reduces the perplexity and improves retrieval-performance compared to classical baselines .
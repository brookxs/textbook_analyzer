can irrelevant-data help semi-supervised-learning , why and how ? previous semi-supervised-learning (ssl) techniques usually assume unlabeled-data are relevant to the target task . that is , they follow the same distribution as the targeted labeled-data . in this paper , we address a different and very difficult scenario in ssl , where the unlabeled-data may be a mixture of data relevant or irrelevant to the target binary-classification-task . in our framework , we do not require explicitly prior-knowledge on the relatedness of the unlabeled-data to the target data . in order to alleviate the effect of the irrelevant unlabeled-data and utilize the implicit-knowledge among all available data , we develop a novel maximum-margin classifier , named the tri-class support-vector-machine (3c-svm) , to seek an inductive rule to separate the target binary-classification-task well while finding out the irrelevant-data by-product . to attain this goal , we introduce a new min loss-function , which can relieve the impact of the irrelevant-data while relying more on the labeled-data and the relevant unlabeled-data . this loss-function can therefore achieve the maximum-entropy-principle . the 3c-svm can then generalize standard svms , semi-supervised svms , and svms learned from the universum as its special cases . we further analyze the property of 3c-svm on why the irrelevant-data can help to improve the model-performance . for implementation , we make relaxation and approximate the objective by the convex-concave procedure , which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic-programming problems . empirical-results are reported to demonstrate the advantages of our 3c-svm model .
approximate-tensor-decomposition within a tensor-relational algebraic framework in this paper , we first introduce a tensor-based relational data-model and define algebraic operations on this model . we note that , while in traditional relational algebraic systems the join operation tends to be the costliest operation of all , in the tensor-relational framework presented here , tensor-decomposition becomes the computationally costliest operation . therefore , we consider optimization of tensor-decomposition operations within a relational algebraic framework . this leads to a highly efficient , effective , and easy-to-parallelize join-by-decomposition approach and a corresponding kl-divergence_based optimization-strategy . experimental-results provide evidence that minimizing kl-divergence within the proposed join-by-decomposition helps approximate the conventional join-then-decompose scheme well , without the associated time and space costs .
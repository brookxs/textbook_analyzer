real-time memory efficient data-redundancy-removal algorithm data-intensive-computing has become a central theme in research-community and industry . there is an ever growing need to process and analyze massive amounts of data from diverse sources such as telecom call data records , telescope imagery , online-transaction records , web-pages , stock-markets , medical-records (monitoring critical health conditions of patients) , climate warning-systems , etc. . removing redundancy in the data is an important problem as it helps in resource and compute efficiency for downstream-processing of the massive (1 billion to 10 billion records) datasets . in application-domains such as ir , stock-markets , telecom and others , there is a strong need for real-time-data redundancy-removal (referred to as drr) of enormous amounts of data flowing at the rate of 1 gb/s or more . real-time scalable data redundancy-removal on massive datasets is a challenging problem . we present the design of a novel parallel data redundancy-removal algorithm for both in-memory and disk-based execution . we also develop queueing theoretic analysis to optimize the throughput of our parallel-algorithm on multi-core-architectures . for 500 million records , our parallel-algorithm can perform complete de-duplication in 255 s , on 16 core intel xeon 5570 architecture , with in-memory execution . this gives a throughput of 2 m records/s . for 6 billion records , our parallel-algorithm can perform complete de-duplication in less than 4.5 hours , using 6 cores of intel xeon 5570 , with disk-based execution . this gives a throughput of around 370 k records/s . to the best of our knowledge , this is the highest real-time throughput for data-redundancy-removal on such massive datasets . we also demonstrate the scalability of our algorithm with increasing number of cores and data .
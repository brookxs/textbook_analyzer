effective measures for inter-document-similarity while supervised learning-to-rank algorithms have largely supplanted unsupervised query-document similarity-measures for search , the exploration of query-document measures by many researchers over many years produced insights that might be exploited in other domains . for example , the bm25 measure substantially and consistently outperforms cosine across many tested environments , and potentially provides retrieval-effectiveness approaching that of the best learning-to-rank methods over equivalent features sets . other measures based on language-modeling and divergence-from-randomness can outperform bm25 in some circumstances . despite this evidence , cosine remains the prevalent method for determining inter-document-similarity for clustering and other applications . however , recent research demonstrates that bm25 terms weights can significantly improve clustering . in this work , we extend that result , presenting and evaluating novel inter-document-similarity measures based on bm25 , language-modeling , and divergence-from-randomness . in our first experiment we analyze the accuracy of nearest neighborhoods when using our measures . in our second experiment , we analyze using clustering-algorithms in conjunction with our measures . our novel symmetric bm25 and language-modeling similarity-measures outperform alternative measures in both experiments . this outcome strongly recommends the adoption of these measures , replacing cosine-similarity in future work .
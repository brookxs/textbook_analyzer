a diversity measure leveraging domain-specific auxiliary-information this article deals with the notion of reduction in uncertainty when the probability mass is distributed over similar values than dissimilar values . shannon 's entropy is a frequently used information-theoretic measure of the uncertainty associated with random variables , but it depends solely on the set of values the probability mass-function assumes , and does not take into consideration whether the mass is distributed among extreme-values or not . a similarity structure , possibly obtained through domain-knowledge , on the values assumed by the random-variable may reduce the associated uncertainty . more the similarity , less the uncertainty . a novel measure named similarity adjusted entropy (or sim-adjusted entropy for short) , that generalizes shannon 's entropy , is then proposed to capture the effects of this similarity structure . sim-adjusted entropy provides a mechanism for incorporating the domain-expertise into an entropy_based framework for solving various data-mining tasks . applications highlighted in this manuscript include clustering of categorical-data and measuring audience-diversity . experiments performed on yahoo!-answers data-set demonstrate the ability of the proposed method to obtain more cohesive clusters . another set of experiments confirm the utility of the proposed measure for measuring audience-diversity .
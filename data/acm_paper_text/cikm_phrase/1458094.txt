achieving both high precision and high recall in near-duplicate-detection to find near-duplicate-documents , fingerprint-based paradigms such as broder 's shingling and charikar 's simhash algorithms have been recognized as effective approaches and are considered the state-of-the-art . nevertheless , we see two aspects of these approaches which may be improved . first , high score under these algorithms ' similarity-measurement implies high probability of similarity between documents , which is different from high similarity of the documents . but how similar two documents are is what we really need to know . second , there has to be a tradeoff between hash-code length and hash-code multiplicity in fingerprint paradigms , which makes it hard to maintain a satisfactory recall level while improving precision . in this paper our contributions are two-folded . first , we propose a framework for implementing the longest-common-subsequence (lcs) as a similarity-measurement in reasonable computing time , which leads to both high precision and recall . second , we present an algorithm to get a trustable partition from the lcs to reduce the negative impact from templates used in web-page-design . a comprehensive experiment was conducted to evaluate our method in terms of its effectiveness , efficiency , and-quality of result . more specifically , the method has been successfully used to partition a set of 430 million web-pages into 68 million subsets of similar pages , which demonstrates its effectiveness . for quality , we compared our method with simhash and a cosine-based method through a sampling process (cosine is compared to lcs as an alternative similarity-measurement) . the result showed that our algorithm reached an overall precision of 0.95 while simhash was 0.71 and cosine was 0.82 . at the same time our method obtains 1.86 times as much recall as simhash and 1.56 times as much recall as cosine . comparison experiment was also done for documents in the same web-sites . for that , our algorithm , simhash and cosine find almost the same number of true-positives at a precision of 0.91 , 0.50 and 0.63 respectively . in terms of efficiency , our algorithm takes 118 hours to process the whole archive of 430 million topic-type pages on a cluster of six linux boxes , at the same time the processing-time of simhash and cosine is 94 hours and 68 hours respectively . when considering the need of word-segmentation for languages such as chinese , the processing-time of cosine should be multiplied and in our experiment it is 602 hours .
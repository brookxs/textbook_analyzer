improving bag-of-visual-words model with spatial-temporal correlation for video-retrieval most of the state-of-art approaches to query-by-example (qbe) video-retrieval are based on the bag-of-visual-words (bovw) representation of visual-content . it , however , ignores the spatial-temporal-information , which is important for similarity-measurement between videos . direct incorporation of such information into the video-data representation for a large-scale-data set is computationally expensive in terms of storage and similarity-measurement . it is also static regardless of the change of discriminative power of visual-words for different queries . to tackle these limitations , in this paper , we propose to discover spatial-temporal correlations (stc) imposed by the query example to improve the bovw model for video-retrieval . the stc , in terms of spatial proximity and relative-motion coherence between different visual-words , is crucial to identify the discriminative power of the visual-words . we develop a novel technique to emphasize the most discriminative visual-words for similarity-measurement , and incorporate this stc-based approach into the standard inverted-index architecture . our approach is evaluated on the trecvid2002 and cc_web_video datasets for two typical qbe video-retrieval tasks respectively . the experimental-results demonstrate that it substantially improves the bovw model as well as a state-of-the-art-method that also utilizes spatial-temporal-information for qbe video-retrieval .
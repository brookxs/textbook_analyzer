modeling hidden topics on document manifold topic-modeling has been a key problem for document-analysis . one of the canonical approaches for topic-modeling is probabilistic-latent-semantic-indexing , which maximizes the joint-probability of documents and terms in the corpus . the major disadvantage of plsi is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size-of-the-corpus , which leads to serious problems with overfitting . latent-dirichlet-allocation (lda) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random-variable . both of these two methods discover the hidden topics in the euclidean-space . however , there is no convincing evidence that the document space is euclidean , or flat . therefore , it is more natural and reasonable to assume that the document space is a manifold , either linear or nonlinear . in this paper , we consider the problem of topic-modeling on intrinsic document manifold . specifically , we propose a novel algorithm called laplacian probabilistic-latent-semantic-indexing (lapplsi) for topic-modeling . lapplsi models the document space as a submanifold embedded in the ambient space and directly performs the topic-modeling on this document manifold in question . we compare the proposed lapplsi approach with plsi and lda on three text-data sets . experimental-results show that lapplsi provides better representation in the sense of semantic-structure .
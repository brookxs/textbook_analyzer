bagboo : a scalable hybrid bagging-the-boosting model in this paper , we introduce a novel machine-learning-approach for regression based on the idea of combining bagging and boosting that we call bagboo . our bagboo model borrows its high accuracy potential from . friedman 's gradient-boosting [2] , and high efficiency and scalability through parallelism from breiman 's bagging [1] . we run empirical evaluations on large-scale web-ranking data , and demonstrate that bagboo is not only showing superior relevance than standalone bagging or boosting , but also outperforms most previously published results on these data-sets . we also emphasize that bagboo is intrinsically scalable and parallelizable , allowing us to train order of half a million trees on 200 nodes in 2 hours cpu-time and beat all of the competitors in the internet mathematics relevance competition sponsored by yandex and be one of the top algorithms in both tracks of yahoo icml-2010 challenge . we conclude the paper by stating that while impressive experimental-evaluation results are presented here in the context of regression-trees , the hybrid bagboo model is applicable to other domains , such as classification , and base training models .
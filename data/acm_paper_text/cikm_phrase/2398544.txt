discovering logical knowledge for deep question-answering most open-domain-question-answering systems achieve better performances with large-corpora , such as web , by taking advantage of information-redundancy . however , explicit answers are not always mentioned in the corpus , many answers are implicitly contained and can only be deducted by inference . in this paper , we propose an approach to discover logical knowledge for deep question-answering , which automatically extracts knowledge in an unsupervised , domain-independent manner from background texts and reasons out implicit answers for the questions . firstly , we use semantic-role-labeling to transform natural-language expressions to predicates in first-order-logic . then we use association-analysis to uncover the implicit relations among these predicates and build propositions for inference . since our knowledge is drawn from different sources , we use markov-logic to merge multiple knowledge-bases without resolving their inconsistencies . our experiments show that these propositions can improve the performance of question-answering significantly .
toward interactive-training and evaluation machine-learning often relies on costly labeled-data , and this impedes its application to new classification and information-extraction problems . this has motivated the development of methods for leveraging abundant prior-knowledge about these problems , including methods for lightly-supervised-learning using model expectation constraints . building on this work , we envision an interactive-training paradigm in which practitioners perform evaluation , analyze errors , and provide and refine expectation constraints in a closed loop . in this paper , we focus on several key subproblems in this paradigm that can be cast as selecting a representative sample of the unlabeled-data for the practitioner to inspect . to address these problems , we propose stratified-sampling methods that use model expectations as a proxy for latent output variables . in classification and sequence-labeling experiments , these sampling strategies reduce accuracy-evaluation effort by as much as 53 \ % , provide more reliable estimates of $ f_1 $ for rare labels , and aid in the specification and refinement of constraints .
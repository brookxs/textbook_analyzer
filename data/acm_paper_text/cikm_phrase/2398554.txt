map to humans and reduce error : crowdsourcing for deduplication applied to digital-libraries detecting duplicate entities , usually by examining metadata , has been the focus of much recent work . several methods try to identify duplicate entities , while focusing either on accuracy or on efficiency and speed - with still no perfect solution . we propose a combined layered approach for duplicate-detection with the main advantage of using crowdsourcing as a training and feedback-mechanism . by using active-learning techniques on human provided examples , we fine tune our algorithm toward better duplicate-detection accuracy . we keep the training cost low by gathering training-data on-demand for borderline cases or for inconclusive assessments . we apply our simple and powerful methods to an online publication-search system : first , we perform a coarse duplicate-detection relying on publication signatures in real-time . then , a second automatic step compares duplicate candidates and increases accuracy while adjusting based on both feedback from our online users and from crowdsourcing platforms . our approach shows an improvement of 14 % over the untrained setting and is at only 4 % difference to the human assessors in accuracy .
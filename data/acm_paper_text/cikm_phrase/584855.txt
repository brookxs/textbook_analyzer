capturing term dependencies using a language-model based on sentence trees we describe a new probabilistic sentence tree language-modeling-approach that captures term-dependency patterns in topic-detection-and-tracking 's (tdt) story-link-detection task . new features of the approach include modeling the syntactic-structure of sentences in documents by a sentence-bin approach and a computationally efficient-algorithm for capturing the most significant sentence-level term dependencies using a maximum-spanning-tree approach , similar to van rijsbergen 's modeling of document-level term dependencies.the new model is a good discriminator of on-topic and off-topic story pairs providing evidence that sentence-level term dependencies contain significant information about relevance . although runs on a subset of the tdt2 corpus show that the model is outperformed by the unigram-language-model , a mixture of the unigram and the sentence tree-models is shown to improve on the best performance especially in the regions of low false alarms .
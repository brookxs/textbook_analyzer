from sbow to dcot marginalized encoders for text-representation in text-mining , information-retrieval , and machine-learning , text-documents are commonly represented through variants of sparse bag-of-words (sbow) vectors (e.g. tf-idf [1]) . although simple and intuitive , sbow style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy . especially when labeled-data is limited (e.g. in document-classification) , or the text-documents are short (e.g. emails or abstracts) , many features are rarely observed within the training-corpus . this leads to overfitting and reduced generalization accuracy . in this paper we propose dense cohort of terms (dcot) , an unsupervised-algorithm to learn improved sbow document features . dcot explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled-corpus . with this approach , dcot learns to reconstruct frequent words from co-occurring infrequent words and maps the high-dimensional sparse sbow vectors into a low-dimensional dense representation . we show that the feature-removal can be marginalized out and that the reconstruction can be solved for in closed-form . we demonstrate empirically , on several benchmark datasets , that dcot features significantly improve the classification-accuracy across several document-classification tasks .
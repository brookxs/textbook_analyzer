on empirical tradeoffs in large-scale hierarchical-classification while multi-class-categorization of documents has been of research interest for over a decade , relatively fewer approaches have been proposed for large-scale taxonomies in which the number of classes range from hundreds of thousand as in directory mozilla to over a million in wikipedia . as a result of ever increasing number of text-documents and images from various sources , there is an immense need for automatic-classification of documents in such large hierarchies . in this paper , we analyze the tradeoffs between the important characteristics of different classifiers employed in the top-down fashion . the properties for relative comparison of these classifiers include , (i) accuracy on test instance , (ii) training-time (iii) size of the model and (iv) test-time required for prediction . our analysis is motivated by the well known error-bounds from learning-theory , which is also further reinforced by the empirical-observations on the publicly available data from the large-scale hierarchical-text-classification challenge . we show that by exploiting the data heterogenity across the large-scale hierarchies , one can build an overall classification-system which is approximately 4 times faster for prediction , 3 times faster to train , while sacrificing only 1 % point in accuracy .
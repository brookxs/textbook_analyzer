takes : a fast method to select features in the kernel-space feature-selection is an effective tool to deal with the `` curse-of-dimensionality '' . to cope with the non-separable problem , feature-selection in the kernel-space has been investigated . however , previous study can not adequately estimate the intrinsic dimensionality of the kernel-space . thus , it is difficult to accurately preserve the sketch of the kernel-space using the learned basis , and the feature-selection performance is affected . moreover , the computing load of the algorithm reaches at least cubic with the number of training-data . in this paper , we propose a fast framework to conduct feature-selection in the kernel-space . by designing a fast kernel subspace-learning method , we automatically learn the intrinsic dimensionality and construct an orthogonal basis set of kernel-space . the learned basis can accurately preserve the sketch of kernel-space . then backed by the constructed basis , we directly select features in kernel-space . the whole proposed framework has a quadratic complexity with the number of training-data , which is faster than existing kernel-methods for feature-selection . we evaluate our work under several typical datasets and find it not only preserves the sketch of the kernel-space more accurately but also achieves better classification-performance compared with many state-of-the-art methods .
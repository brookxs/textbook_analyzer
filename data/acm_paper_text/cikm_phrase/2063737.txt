probabilistic near-duplicate-detection using simhash this paper offers a novel look at using a dimensionality-reduction technique called simhash to detect similar document pairs in large-scale collections . we show that this algorithm produces interesting intermediate-data , which is normally discarded , that can be used to predict which of the bits in the final hash are more susceptible to being flipped in similar documents . this paves the way for a probabilistic-search technique in the hamming-space of simhashes that can be significantly faster and more space-efficient than the existing simhash approaches . we show that with 95 \ % recall compared to deterministic-search of prior work , our method exhibits 4-14 times faster lookup and requires 2-10 times less ram on our collection of 70m web-pages .
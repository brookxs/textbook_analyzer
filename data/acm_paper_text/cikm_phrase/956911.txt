boosting support-vector-machines for text-classification through parameter-free threshold relaxation support-vector-machine (svm) learning-algorithms focus on finding the hyperplane that maximizes the margin (the distance from the separating hyperplane to the nearest examples) since this criterion provides a good upper-bound of the generalization-error . when applied to text-classification , these learning-algorithms lead to svms with excellent precision but poor recall . various relaxation approaches have been proposed to counter this problem including : asymmetric svm-learning algorithms (soft svms with asymmetric misclassification-costs) ; uneven margin_based learning ; and thresholding . a review of these approaches is presented here . in addition , in this paper , we describe a new threshold relaxation algorithm . this approach builds on previous thresholding work based upon the beta-gamma algorithm . the proposed thresholding strategy is parameter free , relying on a process of retrofitting and cross-validation to set algorithm-parameters empirically , whereas our previous approach required the specification of two parameters (beta and gamma) . the proposed approach is more efficient , does not require the specification of any parameters , and similarly to the parameter-based approach , boosts the performance of baseline svms by at least 20 \ % for standard information-retrieval-measures .
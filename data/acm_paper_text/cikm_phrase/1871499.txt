learning-to-rank relevant and novel documents through user-feedback we consider the problem of learning-to-rank relevant and novel documents so as to directly maximize a performance-metric called expected global utility (egu) , which has several desirable properties : (i) it measures retrieval-performance in terms of relevant as well as novel information , (ii) gives more importance to top ranks to reflect common browsing-behavior of users , as opposed to existing objective-functions based on set-coverage , (iii) accommodates different levels of tolerance towards redundancy , which is not taken into account by existing evaluation-measures , and (iv) extends naturally to the evaluation of session-based retrieval comprising multiple ranked-lists . our ground-truth is defined in terms of `` information-nuggets '' , which are obviously not known to the retrieval-system when processing a new-user query . therefore , our approach uses observable query and document features (words and named-entities) as surrogates for nuggets , whose weights are learned based on user-feedback in an iterative-search session . the ranked list is produced to maximize the weighted coverage of these surrogate nuggets . the optimization of such coverage-based metrics is known to be np-hard . therefore , we use a greedy-algorithm and show that it guarantees good performance due to the submodularity of the objective-function . our experiments on topic-detection-and-tracking data show that the proposed approach represents an efficient and effective retrieval-strategy for maximizing egu , as compared to a purely-relevance based ranking approach that uses indri , as well as a mmr-based approach for non-redundant ranking .
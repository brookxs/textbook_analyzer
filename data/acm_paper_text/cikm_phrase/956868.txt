using titles and category names from editor-driven taxonomies for automatic-evaluation evaluation of ir systems has always been difficult because of the need for manually assessed relevance-judgments . the advent of large editor-driven taxonomies on the web opens the door to a new evaluation-approach . we use the odp (open-directory-project) taxonomy to find sets of pseudo-relevant documents via one of two assumptions : 1) taxonomy entries are relevant to a given query if their editor-entered titles exactly match the query , or 2) all entries in a leaf-level taxonomy category are relevant to a given query if the category title exactly matches the query . we compare and contrast these two methodologies by evaluating six web-search-engines on a sample from an america online log of ten million web-queries , using mrr measures for the first method and precision-based measures for the second . we show that this technique is stable with respect to the query set selected and correlated with a reasonably large manual-evaluation .
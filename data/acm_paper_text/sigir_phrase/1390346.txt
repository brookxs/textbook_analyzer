score standardization for inter-collection comparison of retrieval-systems the goal of system-evaluation in information-retrieval has always been to determine which of a set of systems is superior on a given collection . the tool used to determine system-ordering is an evaluation-metric such as average-precision , which computes relative , collection-specific scores . we argue that a broader goal is achievable . in this paper we demonstrate that , by use of standardization , scores can be substantially independent of a particular collection , allowing systems to be compared even when they have been tested on different collections . compared to current methods , our techniques provide richer information about system-performance , improved clarity in outcome reporting , and greater simplicity in reviewing results from disparate sources .
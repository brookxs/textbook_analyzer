retrieval-system evaluation : automatic-evaluation versus incomplete-judgments in information-retrieval (ir) , research aiming to reduce the cost of retrieval-system evaluations has been conducted along two lines : (i) the evaluation of ir systems with reduced amounts of manual relevance-assessments , and (ii) the fully automatic evaluation of ir systems , thus foregoing the need for manual assessments altogether . the proposed methods in both areas are commonly evaluated by comparing their performance-estimates for a set of systems to a ground-truth (provided for instance by evaluating the set of systems according to mean-average-precision) . in contrast , in this poster we compare an automatic-system-evaluation approach directly to two evaluations based on incomplete manual relevance-assessments . for the particular case of trec 's million-query-track , we show that the automatic-evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments .
when will information-retrieval be good enough ? we describe a user-study that examined the relationship between the quality of an information-retrieval-system and the effectiveness of its users in performing a task . the task involves finding answer facets of questions pertaining to a collection of newswire documents over a six month period . we artificially created sets of ranked-lists at increasing levels of quality by blending the output of a state-of-the-art retrieval-system with truth data created by annotators . subjects performed the task by using these ranked-lists to guide their labeling of answer passages in the retrieved articles . we found that as system accuracy improves , subject time-on-task and error-rate decrease , and the rate of finding new correct answers increases . there is a large intermediary region in which the utility difference is not significant ; our results suggest that there is some threshold of accuracy for this task beyond which user utility improves rapidly , but more experiments are needed to examine the area around that threshold closely .
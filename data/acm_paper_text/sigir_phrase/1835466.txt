caching search-engine-results over incremental indices a web-search-engine must update its index periodically to incorporate changes to the-web . we argue in this paper that index-updates fundamentally impact the design of search-engine result caches , a performance-critical component of modern search-engines . index-updates lead to the problem of cache-invalidation : invalidating cached entries of queries whose results have changed . naive approaches , such as flushing the entire cache upon every index update , lead to poor performance and in fact , render caching futile when the frequency of updates is high . solving the invalidation problem efficiently corresponds to predicting accurately which queries will produce different results if re-evaluated , given the actual changes to the index . to obtain this property , we propose a framework for developing invalidation predictors and define metrics to evaluate invalidation schemes . we describe concrete predictors using this framework and compare them against a baseline that uses a cache-invalidation scheme based on time-to-live (ttl) . evaluation over wikipedia documents using a query-log from the yahoo! search-engine shows that selective invalidation of cached search-results can lower the number of unnecessary query evaluations by as much as 30 \ % compared to a baseline scheme , while returning results of similar freshness . in general , our predictors enable fewer unnecessary invalidations and fewer stale results compared to a ttl-only scheme for similar freshness of results .
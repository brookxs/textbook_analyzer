a case for improved evaluation of query-difficulty-prediction query-difficulty-prediction aims to identify , in advance , how well an information-retrieval-system will perform when faced with a particular search request . the current standard evaluation-methodology involves calculating a correlation-coefficient , to indicate how strongly the predicted query-difficulty is related with an actual system-performance measure , usually average-precision . we run a series of experiments based on predictors that have been shown to perform well in the literature , comparing these across different trec runs . our results demonstrate that the current evaluation-methodology is severely limited . although it can be used to demonstrate the performance of a predictor for a single system , such performance is not consistent over a variety of retrieval-systems . we conclude that published results in the query-difficulty area are generally not comparable , and recommend that prediction be evaluated against a spectrum of underlying search systems .
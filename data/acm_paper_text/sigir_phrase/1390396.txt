learning to reduce the semantic gap in web-image-retrieval and annotation we study in this paper the problem of bridging the semantic gap between low-level-image-features and high-level-semantic-concepts , which is the key hindrance in content-based-image-retrieval . piloted by the rich textual-information of web-images , the proposed framework tries to learn a new distance-measure in the visual-space , which can be used to retrieve more semantically relevant images for any unseen query image . the framework differentiates with traditional distance-metric-learning methods in the following ways . 1) a ranking-based-distance-metric-learning method is proposed for image-retrieval problem , by optimizing the leave-one-out retrieval-performance on the training-data . 2) to be scalable , millions of images together with rich textual-information have been crawled from the-web to learn the similarity-measure , and the learning-framework particularly considers the indexing problem to ensure the retrieval-efficiency . 3) to alleviate the noises in the unbalanced labels of images and fully utilize the textual-information , a latent-dirichlet-allocation based topic-level text model is introduced to define pairwise semantic-similarity between any two images . the learnt distance-measure can be directly applied to applications such as content-based-image-retrieval and search-based-image-annotation . experimental-results on the two applications in a two million web-image database show both the effectiveness and efficiency of the proposed framework .
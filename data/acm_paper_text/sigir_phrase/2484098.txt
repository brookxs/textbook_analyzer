modeling term dependencies with quantum language-models for ir traditional information-retrieval (ir) models use bag-of-words as the basic representation and assume that some form of independence holds between terms . representing term dependencies and defining a scoring-function capable of integrating such additional evidence is theoretically and practically challenging . recently , quantum-theory (qt) has been proposed as a possible , more general-framework for ir . however , only a limited number of investigations have been made and the potential of qt has not been fully explored and tested . we develop a new , generalized language-modeling-approach for ir by adopting the probabilistic-framework of qt . in particular , quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies . this naturally allows us to avoid the weight-normalization problem , which arises in the current practice by mixing scores from matching compound terms and from matching single terms . our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline .
parallelizing listnet training using spark as ever-larger training-sets for learning-to-rank are created , scalability of learning has become increasingly important to achieving continuing improvements in ranking accuracy . exploiting independence of `` summation form '' computations , we show how each iteration in listnet gradient-descent can benefit from parallel-execution . we seek to draw the attention of the ir community to use spark , a newly introduced distributed cluster-computing system , for reducing training-time of iterative-learning to rank algorithms . unlike mapreduce , spark is especially suited for iterative and interactive algorithms . our results show near linear reduction in listnet training-time using spark on amazon-ec2 clusters .
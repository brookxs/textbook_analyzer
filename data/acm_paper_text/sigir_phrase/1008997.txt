scaling ir-system-evaluation using term-relevance sets this paper describes an evaluation-method based on term-relevance sets trels that measures an ir-system 's quality by examining the content of the retrieved results rather than by looking for pre-specified relevant pages . trels consist of a list of terms believed to be relevant for a particular query as well as a list of irrelevant terms . the proposed method does not involve any document-relevance judgments , and as such is not adversely affected by changes to the underlying collection . therefore , it can better scale to very-large , dynamic collections such as the-web . moreover , this method can evaluate a system 's effectiveness on an updatable `` live '' collection , or on collections derived from different data sources . our experiments show that the proposed method is very highly correlated with official trec measures .
sequential-testing in classifier-evaluation yields biased estimates of effectiveness it is common to develop and validate classifiers through a process of repeated testing , with nested training and/or test-sets of increasing size . we demonstrate in this paper that such repeated testing leads to biased estimates of classifier effectiveness . experiments on a range of text-classification tasks under three sequential-testing frameworks show all three lead to optimistic estimates of effectiveness . we calculate empirical adjustments to unbias estimates on our data-set , and identify directions for research that could lead to general techniques for avoiding bias while reducing labeling costs .
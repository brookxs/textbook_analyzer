resource-selection for domain-specific cross-lingual-ir an under-explored question in cross-language-information-retrieval (clir) is to what degree the performance of clir methods depends on the availability of high-quality-translation resources for particular domains . to address this issue , we evaluate several competitive clir methods - with different training-corpora - on test documents in the medical-domain . our results show severe performance degradation when using a general-purpose training-corpus or a commercial machine-translation-system (systran) , versus a domain-specific training-corpus . a related unexplored question is whether we can improve clir performance by systematically analyzing training resources and optimally matching them to target collections . we start exploring this problem by suggesting a simple criterion for automatically matching training resources to target corpora . by using cosine-similarity between training and target corpora as resource weights we obtained an average of 5.6 \ % improvement over using all resources with no weights . the same metric yields 99.4 \ % of the performance obtained when an oracle chooses the optimal resource every time .
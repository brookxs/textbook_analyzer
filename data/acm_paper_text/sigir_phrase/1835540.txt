the effect of assessor-error on ir-system evaluation recent efforts in test-collection building have focused on scaling back the number of necessary relevance-judgments and then scaling up the number of search topics . since the largest source of variation in a cranfield-style experiment comes from the topics , this is a reasonable approach . however , as topic set sizes grow , and researchers look to crowdsourcing and amazon 's mechanical-turk to collect relevance-judgments , we are faced with issues of quality-control . this paper examines the robustness of the trec million-query-track methods when some assessors make significant and systematic errors . we find that while averages are robust , assessor errors can have a large effect on system rankings .
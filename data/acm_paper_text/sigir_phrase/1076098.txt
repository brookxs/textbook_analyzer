a phonotactic-semantic paradigm for automatic spoken-document-classification we demonstrate a phonotactic-semantic paradigm for spoken-document-categorization . in this framework , we define a set of acoustic-words instead of lexical words to represent acoustic activities in spoken languages . the strategy for acoustic vocabulary-selection is studied by comparing different feature-selection methods . with an appropriate acoustic vocabulary , a voice-tokenizer converts a spoken document into a text-like document of acoustic-words . thus , a spoken document can be represented by a count vector , named a bag-of-sounds vector , which characterizes a spoken document 's semantic-domain . we study two phonotactic-semantic classifiers , the support-vector-machine-classifier and the latent-semantic-analysis classifier , and their properties . the phonotactic-semantic framework constitutes a new paradigm in spoken-document-classification , as demonstrated by its success in the spoken-language-identification task . it achieves 18.2 \ % error-reduction over state-of-the-art benchmark performance on the 1996 nist language-recognition evaluation database .
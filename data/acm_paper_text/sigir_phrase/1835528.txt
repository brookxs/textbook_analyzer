multi-style-language-model for web-scale information-retrieval web-documents are typically associated with many text-streams , including the body , the title and the url that are determined by the authors , and the anchor-text or search queries used by others to refer to the documents . through a systematic large-scale analysis on their cross-entropy , we show that these text-streams appear to be composed in different language styles , and hence warrant respective language-models to properly describe their properties . we propose a language-modeling-approach to web-document retrieval in which each document is characterized by a mixture-model with components corresponding to the various text-streams associated with the document . immediate issues for such a mixture-model arise as all the text-streams are not always present for the documents , and they do not share the same lexicon , making it challenging to properly combine the statistics from the mixture components . to address these issues , we introduce an ` open-vocabulary ' smoothing-technique so that all the component language-models have the same cardinality and their scores can simply be linearly combined . to ensure that the approach can cope with web-scale applications , the model-training algorithm is designed to require no labeled-data and can be fully automated with few heuristics and no empirical parameter tunings . the evaluation on web-document ranking tasks shows that the component language-models indeed have varying degrees of capabilities as predicted by the cross-entropy analysis , and the combined mixture-model outperforms the state-of-the-art bm25f_based system .
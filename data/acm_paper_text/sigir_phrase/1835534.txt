learning more powerful test-statistics for click-based retrieval-evaluation interleaving experiments are an attractive methodology for evaluating retrieval functions through implicit-feedback . designed as a blind and unbiased test for eliciting a preference between two retrieval functions , an interleaved ranking of the results of two retrieval functions is presented to the users . it is then observed whether the users click more on results from one retrieval-function or the other . while it was shown that such interleaving experiments reliably identify the better of the two retrieval functions , the naive approach of counting all clicks equally leads to a suboptimal test . we present new methods for learning how to score different types of clicks so that the resulting test statistic optimizes the statistical-power of the experiment . this can lead to substantial savings in the amount of data required for reaching a target confidence level . our methods are evaluated on an operational search-engine over a collection of scientific articles .
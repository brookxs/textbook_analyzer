deconstructing nuggets : the stability and reliability of complex-question-answering evaluation a methodology based on `` information-nuggets '' has recently emerged as the de facto standard by which answers to complex questions are evaluated . after several implementations in the trec question-answering tracks , the community has gained a better understanding of its many characteristics . this paper focuses on one particular aspect of the evaluation : the human assignment of nuggets to answer strings , which serves as the basis of the f-score computation . as a byproduct of the trec 2006 ciqa task , identical answer strings were independently evaluated twice , which allowed us to assess the consistency of human-judgments . based on these results , we explored simulations of assessor behavior that provide a method to quantify scoring variations . understanding these variations in turn lets researchers be more confident in their comparisons of systems .
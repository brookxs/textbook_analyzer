the bag-of-repeats representation of documents n-gram representations of documents may improve over a simple bag-of-word-representation by relaxing the independence-assumption of word and introducing context . however , this comes at a cost of adding features which are non-descriptive , and increasing the dimension of the vector-space-model exponentially . we present new representations that avoid both pitfalls . they are based on sound theoretical notions of stringology , and can be computed in optimal asymptotic time with algorithms using data structures from the suffix family . while maximal repeats have been used in the past for similar tasks , we show how another equivalence-class of repeats -- largest-maximal repeats -- obtain similar or better results , with only a fraction of the features . this class acts as a minimal generative basis of all repeated substrings . we also report their use for topic-modeling , showing easier to interpret models .
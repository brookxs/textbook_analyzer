learning to name faces : a multimodal-learning scheme for search-based face-annotation automated face-annotation aims to automatically detect human faces from a photo and further name the faces with the corresponding human names . in this paper , we tackle this open problem by investigating a search-based face-annotation (sbfa) paradigm for mining large amounts of web-facial-images freely available on the www . given a query facial-image for annotation , the idea of sbfa is to first search for top-n similar facial-images from a web facial image-database and then exploit these top-ranked similar facial-images and their weak labels for naming the query facial-image . to fully mine those information , this paper proposes a novel framework of learning to name faces (l2nf) -- a unified multimodal-learning approach for search-based face-annotation , which consists of the following major components : (i) we enhance the weak labels of top-ranked similar images by exploiting the `` label smoothness '' assumption ; (ii) we construct the multimodal-representations of a facial-image by extracting different types of features ; (iii) we optimize the distance-measure for each type of features using distance-metric-learning techniques ; and finally (iv) we learn the optimal combination of multiple modalities for annotation through a learning-to-rank scheme . we conduct a set of extensive empirical-studies on two real-world facial-image databases , in which encouraging results show that the proposed algorithms significantly boost the naming accuracy of search-based face-annotation task .
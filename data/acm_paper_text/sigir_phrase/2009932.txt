bagging gradient-boosted trees for high precision , low variance ranking-models recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks . in learning-to-rank , boosted models such as rankboost and lambdamart have been shown to be among the best performing learning-methods based on evaluations on public data-sets . in this paper , we show how the combination of bagging as a variance-reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking-models . we perform thousands of parameter-tuning experiments for lambdamart to achieve a high precision boosting model . then we show that a bagged ensemble of such lambdamart boosted models results in higher accuracy ranking-models while also reducing variance as much as 50 \ % . we report our results on three public learning-to-rank data-sets using four metrics . bagged lamdbamart outperforms all previously reported results on ten of the twelve comparisons , and bagged lambdamart outperforms non-bagged lambdamart on all twelve comparisons . for example , wrapping bagging around lambdamart increases ndcg@1 from 0.4137 to 0.4200 on the mq2007 data-set ; the best prior results in the literature for this data-set is 0.4134 by rankboost .
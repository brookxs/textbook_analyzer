on the mono - and cross-language detection of text-reuse and plagiarism plagiarism , the unacknowledged reuse of text , has increased in recent years due to the large amount of texts readily available . for instance , recent studies claim that nowadays a high rate of student reports include plagiarism , making manual plagiarism-detection practically infeasible . automatic plagiarism-detection tools assist experts to analyse documents for plagiarism . nevertheless , the lack of standard collections with cases of plagiarism has prevented accurate comparing models , making differences hard to appreciate . seminal efforts on the detection of text-reuse [2] have fostered the composition of standard resources for the accurate evaluation and comparison of methods . the aim of this phd thesis is to address three of the main problems in the development of better models for automatic plagiarism-detection : (i) the adequate identification of good potential sources for a given suspicious text ; (ii) the detection of plagiarism despite modifications , such as words substitution and paraphrasing (special stress is given to cross-language plagiarism) ; and (iii) the generation of standard collections of cases of plagiarism and text-reuse in order to provide a framework for accurate-comparison of models . regarding difficulties (i) and (ii) , we have carried out preliminary experiments over the meter corpus [2] . given a suspicious document dq and a collection of potential source-documents d , the process is divided in two steps . first , a small subset of potential source-documents d * in d is retrieved . the documents d in d * are the most related to dq and , therefore , the most likely to include the source of the plagiarised fragments in it . we performed this stage on the basis of the kullback-leibler-distance , over a subsample of document 's vocabularies . afterwards , a detailed analysis is carried out comparing dq to every d in d * in order to identify potential cases of plagiarism and their source . this comparison was made on the basis of word n-grams , by considering n = {2 , 3} . these n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications [1] . the result is offered to the user to take the final decision . further experiments were done in both stages in order to compare other similarity-measures , such as the cosine-measure , the jaccard-coefficient and diverse fingerprinting and probabilistic-models . one of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism . approaching the detection of this kind of plagiarism is of high relevance , as the most of the information published is written in english , and authors in other languages may find it attractive to make use of direct translations . our experiments , carried out over parallel and a comparable-corpora , show that models of `` standard '' cross-language-information-retrieval are not enough . in fact , if the analysed source and target languages are related in some way (common linguistic ancestors or technical-vocabulary) , a simple comparison based on character-n-grams seems to be the option . however , in those cases where the relation between the implied languages is weaker , other models , such as those based on statistical-machine-translation , are necessary [3] . we plan to perform further experiments , mainly to approach the detection of cross-language plagiarism . in order to do that , we will use the corpora developed under the framework of the pan competition on plagiarism-detection (cf. pan@clef: http://pan.webis.de) . models that consider cross-language thesauri and comparison of cognates will also be applied .
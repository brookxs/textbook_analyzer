including summaries in system-evaluation in batch evaluation of retrieval-systems , performance is calculated based on predetermined relevance-judgements applied to a list of documents returned by the system for a query . this evaluation-paradigm , however , ignores the current standard operation of search systems which require the user to view summaries of documents prior to reading the documents themselves . in this paper we modify the popular ir-metrics map and p@10 to incorporate the summary reading step of the search-process , and study the effects on system rankings using trec-data . based on a user-study , we establish likely disagreements between relevance-judgements of summaries and of documents , and use these values to seed simulations of summary relevance in the trec-data . re-evaluating the runs submitted to the trec web track , we find the average correlation between system rankings and the original trec rankings is 0.8 (kendall ?) , which is lower than commonly accepted for system orderings to be considered equivalent . the system that has the highest map in trec generally remains amongst the highest map systems when summaries are taken into account , but other systems become equivalent to the top ranked system depending on the simulated summary relevance . given that system orderings alter when summaries are taken into account , the small amount of effort required to judge summaries in addition to documents (19 seconds vs 88 seconds on average in our data) should be undertaken when constructing test-collections .
compression-based document-length prior for language-models the inclusion of document-length factors has been a major topic in the development of retrieval-models . we believe that current models can be further improved by more refined estimations of the document 's scope . in this poster we present a new document-length prior that uses the size of the compressed document . this new prior is introduced in the context of language-modeling with dirichlet-smoothing . the evaluation performed on several collections shows significant improvements in effectiveness .
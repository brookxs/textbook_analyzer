learning probabilistic-models of the-web (poster session) in the world-wide-web , myriads of hyperlinks connect documents and pages to create an unprecedented , highly complex graph-structure - the-web graph . this paper presents a novel approach to learning probabilistic-models of the-web , which can be used to make reliable predictions about connectivity and information content of web-documents . the proposed method is a probabilistic dimension-reduction-technique which recasts and unites latent-semantic-analysis and kleinberg 's hubs-and-authorities algorithm in a statistical setting . this meant to be a first step towards the development of a statistical foundation for web related information-technologies . although this paper does not focus on a particular application , a variety of algorithms operating in the web/internet environment can take advantage of the presented techniques , including search-engines , web-crawlers , and information agent-systems .
evaluating and predicting answer-quality in community qa question-answering (qa) helps one go beyond traditional keywords-based querying and retrieve information in more precise form than given by a document or a list of documents . several community-based qa (cqa) services have emerged allowing information seekers pose their information-need as questions and receive answers from their fellow users . a question may receive multiple answers from multiple-users and the asker or the community can choose the best answer . while the asker can thus indicate if he was satisfied with the information he received , there is no clear way of evaluating the quality of that information . we present a study to evaluate and predict the quality of an answer in a cqa setting . we chose yahoo!-answers as such cqa service and selected a small set of questions , each with at least five answers . we asked amazon-mechanical-turk workers to rate the quality of each answer for a given question based on 13 different criteria . each answer was rated by five different workers . we then matched their assessments with the actual asker 's rating of a given answer . we show that the quality-criteria we used faithfully match with asker 's perception of a quality answer . we furthered our investigation by extracting various features from questions , answers , and the users who posted them , and training a number of classifiers to select the best answer using those features . we demonstrate a high predictability of our trained models along with the relative merits of each of the features for such prediction . these models support our argument that in case of cqa , contextual-information such as a user 's profile , can be critical in evaluating and predicting content-quality .
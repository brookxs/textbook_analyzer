does relevance-feedback improve document-retrieval performance ? many authors (1 , 2 , 3 , 5 , 6 , 7) have suggested that overall performance of a document-retrieval-system is improved by relevance-feedback . relevance-feedback denotes the last three steps in the following process : 1) the searcher enters a query , 2) the system prepares a ranked list of suggested documents , 3) the searcher judges some of the documents for relevancy , 4) the searcher informs the system of these documents judged and of the judgement , 5) the system constructs a new query based on the descriptors used in the original query and the descriptors used in the documents judged , 6) the system prepares a second ranked list of suggested documents . the presumption is that the second list is better than the first . by all performance-measures (e.g. - & - ldquo ; fluid ranking - & - rdquo ; and - & - ldquo ; frozen ranking - & - rdquo ;-rrb- , the second list is better than the first . however , if one reranks documents in the original list so as to reflect the searcher 's efforts (step 3) , the corresponding performance-measures are comparable to those for the second list . the marginal difference between the performance-measures for the - & - rdquo ; reranked original - & - rdquo ; list (searcher 's efforts alone) and the second list (which includes computer efforts) makes it unclear if the cost of steps 4 through 6 above can be justified . it is hoped that advocates of relevance-feedback will present - & - ldquo ; reranked original - & - rdquo ; performance-measures as a basis for any performance-improvement claims . this paper also presents three reasonable , easily understood retrieval procedures for which the frozen ranking , the fluid ranking , and the reranked original evaluations are - & - ldquo ; obviously - & - rdquo ; the pertinent way to evaluate . relevance-feedback techniques as implemented in salton 's smart drs appear to show that it is worthwhile for user 's to read abstracts prior to evaluation of full texts . the last indication presented in this paper is that the relevance-feedback performance-improvements noted using smart are due mostly to the user making assessments ; subsequent computer efforts appear to be most likely to result in no further change . for a query for which there is a subsequent change , the change is as likely to be harmful as helpful .
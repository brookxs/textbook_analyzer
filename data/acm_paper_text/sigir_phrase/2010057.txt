quantifying test-collection quality based on the consistency of relevance-judgements relevance-assessments are a key component for test collection-based evaluation of information-retrieval-systems . this paper reports on a feature of such collections that is used as a form of ground-truth data to allow analysis of human assessment error . a wide range of test-collections are retrospectively examined to determine how accurately assessors judge the relevance of documents . our results demonstrate a high level of inconsistency across the collections studied . the level of irregularity is shown to vary across topics , with some showing a very high level of assessment error . we investigate possible influences on the error , and demonstrate that inconsistency in judging increases with time . while the level-of-detail in a topic specification does not appear to influence the errors that assessors make , judgements are significantly affected by the decisions made on previously seen similar documents . assessors also display an assessment inertia . alternate approaches to generating relevance-judgements appear to reduce errors . a further investigation of the way that retrieval-systems are ranked using sets of relevance-judgements produced early and late in the judgement process reveals a consistent influence measured across the majority of examined test-collections . we conclude that there is a clear value in examining , even inserting , ground-truth data in test-collections , and propose ways to help minimise the sources of inconsistency when creating future test-collections .
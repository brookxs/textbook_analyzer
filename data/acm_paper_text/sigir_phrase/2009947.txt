crowdsourcing for book-search evaluation : impact of hit design on comparative system ranking the evaluation of information-retrieval (ir) systems over special-collections , such as large book repositories , is out of reach of traditional methods that rely upon editorial relevance-judgments . increasingly , the use of crowdsourcing to collect relevance-labels has been regarded as a viable alternative that scales with modest costs . however , crowdsourcing suffers from undesirable worker practices and low quality contributions . in this paper we investigate the design and implementation of effective crowdsourcing tasks in the context of book-search evaluation . we observe the impact of aspects of the human-intelligence-task (hit) design on the quality of relevance-labels provided by the crowd . we assess the output in terms of label agreement with a gold-standard data-set and observe the effect of the crowdsourced relevance-judgments on the resulting system rankings . this enables us to observe the effect of crowdsourcing on the entire ir-evaluation process . using the test-set and experimental runs from the inex 2010 book track , we find that varying the hit design , and the pooling and document-ordering strategies leads to considerable differences in agreement with the gold set labels . we then observe the impact of the crowdsourced relevance label sets on the relative system rankings using four ir performance-metrics . system rankings based on map and bpref remain less affected by different label sets while the precision@10 and ndcg@10 lead to dramatically different system rankings , especially for labels acquired from hits with weaker quality controls . overall , we find that crowdsourcing can be an effective tool for the evaluation of ir systems , provided that care is taken when designing the hits .
modelling epistemic uncertainty in ir-evaluation modern information retrieval (ir) test-collections violate the completeness assumption of the cranfield paradigm . in order to maximise the available resources , only a sample of documents (i.e. the pool) are judged for relevance by a human assessor (s) . the subsequent evaluation protocol does not make any distinctions between assessed or unassesseddocuments , as documents that are not in the pool are assumedto be not relevant for the topic . this is beneficial from a practical point-of-view , as the relative-performance can be compared with confidence if the experimental conditions are fair for all systems . however , given the incompleteness of relevance-assessments , two forms of uncertainty emerge during evaluation . the first is aleatory uncertainty , which refers to variation in system-performance across the topic set , which is often addressed through the use of statistical-significance tests . the second form of uncertainty is epistemic , which refers to the amount of knowledge (or ignorance) we have about the estimate of a system 's performance . epistemic uncertainty is a consequence of incompleteness and is not addressed by the current evaluation protocol . in this study , we present a first attempt at modelling both aleatory and epistemic uncertainty associatedwith ir-evaluation . we aim to account for both the variability associated with system-performance and the amount of knowledge known about the performance estimate .
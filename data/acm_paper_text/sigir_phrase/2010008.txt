regularized latent-semantic-indexing topic-modeling can boost the performance of information-retrieval , but its real-world-application is limited due to scalability issues . scaling to larger document-collections via parallelization is an active area of research , but most solutions require drastic steps such as vastly reducing input vocabulary . we introduce regularized latent-semantic-indexing (rlsi) , a new method which is designed for parallelization . it is as effective as existing topic-models , and scales to larger datasets without reducing input vocabulary . rlsi formalizes topic-modeling as a problem of minimizing a quadratic loss-function regularized by l ? and/or l ? norm . this formulation allows the learning-process to be decomposed into multiple sub-optimization problems which can be optimized in parallel , for example via mapreduce . we particularly propose adopting l ? norm on topics and l ? norm on document representations , to create a model with compact and readable topics and useful for retrieval . relevance-ranking experiments on three trec datasets show that rlsi performs better than lsi , plsi , and lda , and the improvements are sometimes statistically significant . experiments on a web dataset , containing about 1.6 million documents and 7 million terms , demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous studies .
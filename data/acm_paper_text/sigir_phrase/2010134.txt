measuring assessor accuracy : a comparison of nist assessors and user-study participants in many situations , humans judging document-relevance are forced to trade-off accuracy for speed . the development of better interactive-retrieval systems and relevance assessing platforms requires the measurement of assessor accuracy , but to date the subjective nature of relevance has prevented such measurement . to quantify assessor performance , we define relevance to be a group 's majority opinion , and demonstrate the value of this approach by comparing the performance of nist assessors to a group of assessors representative of participants in many information-retrieval user studies . using data collected as part of a user-study with 48 participants , we found that nist assessors discriminate between relevant and non-relevant documents better than the average participant in our study , but that nist assessors ' true-positive-rate is no better than that of the study participants . in addition , we found nist assessors to be conservative in their judgment of relevance compared to the average participant .
robust sparse rank learning for non-smooth ranking-measures recently increasing attention has been focused on directly optimizing ranking-measures and inducing sparsity in learning-models . however , few attempts have been made to relate them together in approaching the problem of learning-to-rank . in this paper , we consider the sparse algorithms to directly optimize the normalized-discounted-cumulative-gain (ndcg) which is a widely-used ranking measure . we begin by establishing a reduction framework under which we reduce ranking , as measured by ndcg , to the importance weighted pairwise-classification . furthermore , we provide a sound theoretical guarantee for this reduction , bounding the realized ndcg regret in terms of a properly weighted pairwise-classification regret , which implies that good performance can be robustly transferred from pairwise-classification to ranking . based on the converted pairwise loss-function , it is conceivable to take into account sparsity in ranking-models and to come up with a gradient possessing certain performance-guarantee . for the sake of achieving sparsity , a novel algorithm named rsrank has also been devised , which performs l1-regularization using truncated-gradient descent . finally , experimental-results on benchmark collection confirm the significant advantage of rsrank in comparison with several baseline methods .
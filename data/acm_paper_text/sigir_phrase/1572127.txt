a study of inter-annotator-agreement for opinion-retrieval evaluation of sentiment-analysis , like large-scale ir-evaluation , relies on the accuracy of human assessors to create judgments . subjectivity in judgments is a problem for relevance-assessment and even more so in the case of sentiment annotations . in this study we examine the degree to which assessors agree upon sentence-level sentiment annotation . we show that inter-assessor agreement is not contingent on document-length or frequency of sentiment but correlates positively with automated opinion-retrieval performance . we also examine the individual annotation categories to determine which categories pose most difficulty for annotators .
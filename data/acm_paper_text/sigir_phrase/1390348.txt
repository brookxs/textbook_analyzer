retrieval sensitivity under training using different measures various measures , such as binary-preference (bpref) , inferred-average-precision (infap) , and binary normalised-discounted-cumulative-gain (ndcg) have been proposed as alternatives to mean-average-precision (map) for being less sensitive to the relevance-judgements completeness . as the primary aim of any system-building is to train the system to respond to user queries in a more robust and stable manner , in this paper , we investigate the importance of the choice of the evaluation-measure for training , under different levels of evaluation incompleteness . we simulate evaluation incompleteness by sampling from the relevance-assessments . through large-scale experiments on two standard trec test-collections , we examine retrieval sensitivity when training - i.e. if a training-process , based on any of the four discussed measures has an impact on the final retrieval-performance . experimental-results show that training by bpref , infap and ndcg provides significantly better retrieval-performance than training by map when relevance-judgements completeness is extremely low . when relevance-judgements completeness increases , the measures behave more similarly .
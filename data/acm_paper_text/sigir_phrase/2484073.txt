a mutual information-based framework for the analysis of information-retrieval-systems we consider the problem of information-retrieval-evaluation and the methods and metrics used for such evaluations . we propose a probabilistic-framework for evaluation which we use to develop new information-theoretic evaluation-metrics . we demonstrate that these new metrics are powerful and generalizable , enabling evaluations heretofore not possible . we introduce four preliminary uses of our framework : (1) a measure of conditional rank-correlation , information tau , a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation ; (2) a new evaluation-measure , relevance information correlation , which is correlated with traditional evaluation-measures and can be used to (3) evaluate a collection of systems simultaneously , which provides a natural upper-bound on metasearch performance ; and (4) a measure of the similarity between rankers on judged documents , information difference , which allows us to determine whether systems with similar performance are in fact different .
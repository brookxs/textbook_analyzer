extending average-precision to graded-relevance judgments evaluation-metrics play a critical role both in the context of comparative-evaluation of the performance of retrieval-systems and in the context of learning-to-rank (ltr) as objective-functions to be optimized . many different evaluation-metrics have been proposed in the ir literature , with average-precision (ap) being the dominant one due a number of desirable properties it possesses . however , most of these measures , including average-precision , do not incorporate graded-relevance . in this work , we propose a new measure of retrieval-effectiveness , the graded average-precision (gap) . gap generalizes average-precision to the case of multi-graded relevance and inherits all the desirable characteristics of ap : it has a nice probabilistic interpretation , it approximates the area under a graded precision-recall-curve and it can be justified in terms of a simple but moderately plausible user-model . we then evaluate gap in terms of its informativeness and discriminative power . finally , we show that gap can reliably be used as an objective metric in learning-to-rank by illustrating that optimizing for gap using softrank and lambdarank leads to better performing ranking-functions than the ones constructed by algorithms tuned to optimize for ap or ndcg even when using ap or ndcg as the test-metrics .
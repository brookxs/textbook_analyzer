on the local-optimality of lambdarank a machine-learning-approach to learning-to-rank trains a model to optimize a target evaluation-measure with repect to training-data . currently , existing information-retrieval-measures are impossible to optimize directly except for models with a very small number of parameters . the ir community thus faces a major challenge : how to optimize ir-measures of interest directly . in this paper , we present a solution . specifically , we show that lambdarank , which smoothly approximates the gradient of the target measure , can be adapted to work with four popular ir target evaluation-measures using the same underlying gradient construction . it is likely , therefore , that this construction is extendable to other evaluation-measures . we empirically show that lambdarank finds a locally-optimal-solution for mean ndcg@10, mean ndcg , map and mrr with a 99 \ % confidence rate . we also show that the amount of effective-training data varies with ir measure and that with a sufficiently large-training set-size , matching the training optimization measure to the target evaluation-measure yields the best accuracy .
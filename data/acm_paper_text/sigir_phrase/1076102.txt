evaluation of resources for question-answering-evaluation controlled and reproducible laboratory-experiments , enabled by reusable test-collections , represent a well-established methodology in modern information retrieval research . in order to confidently draw conclusions about the performance of different retrieval-methods using test-collections , their reliability and trustworthiness must first be established . although such studies have been performed for ad-hoc test-collections , currently available resources for evaluating question-answering-systems have not been similarly analyzed . this study evaluates the quality of answer patterns and lists of relevant documents currently employed in automatic-question-answering evaluation , and concludes that they are not suitable for post-hoc experimentation . these resources , created from runs submitted by trec qa track participants , do not produce fair and reliable assessments of systems that did not participate in the original evaluations . potential solutions for addressing this evaluation gap and their shortcomings are discussed .
query-expansion based on a semantic-graph model query-expansion is a classical topic in the field of information-retrieval , which is proposed to bridge the gap between searchers ' information intents and their queries . previous researches usually expand queries based on document-collections , or some external resources such as wordnet and wikipedia [1 , 2 , 3 , 4 , 5] . however , it seems that independently using one of these resources has some defects , document-collections lack semantic-information of words , while wordnet and wikipedia may not include domain-specific-knowledge in certain document-collection . our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific-information but also semantic-information . in our preliminary experiments , we construct a two-layer word-graph and use random-walk algorithm to calculate the weights of each term in pseudo-relevance-feedback documents , then select the highest weighted term to expand original query . the first layer of the word-graph contains terms in related documents , while the second layer contains semantic senses corresponding to these terms . these terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships , such as mutual-information and semantic similarities . we utilized mutual-information , semantic-similarity and uniform-distribution as the weight of term-term relation , sense-sense relation and word-sense relation respectively . though these experiments show that our expansion outperform original queries , we are troubled with some difficult problems . given the framework of semantic-graph model , we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses . we utilized a two-layer graph-model in our preliminary research , where terms from different documents are treated equally . maybe we can introduce the document as a third layer in future work , where we can differ the same terms in different documents according to document-relevance and context . then we need appropriately represent initial weights of this words , senses and relationships . various measures for weights of terms and term relations have been proved effective in other information-retrieval-tasks , such as tfidf , mutual-information (mi) , but there is little research on weights for semantic senses and their relations . for polysemous-words , we add all of their semantic senses to the graph and assume that these senses are uniformly distributed . actually , it is not precise for a word in a special document and query . as we know , a polysemous-word may have only one or two senses in a document , and they are not uniformly distributed . give a word , what we should do is to determine its word-senses in a relevant-document and estimate the distribution of these senses . word-sense-disambiguation may help us in this problem . then , there are many methods to compute word-similarity according to wordnet , which we use to represent the weights of relationships between word-senses . varelas et al implemented some popular methods to compute semantic-similarity by mapping terms to an ontology and examining their relationships in that ontology [4] . we also need to know which algorithm for semantic-similarity is most suitable for our model . additional , wordnet is suitable to calculate word-similarity but not suitable to measure word relevance . the inner hyperlinks of wikipedia could help us to calculate word relevance . we wish to find an effective way to combine the similarity-measure from wordnet and relevance-measure from wikipedia , which may completely reflect word relationships .
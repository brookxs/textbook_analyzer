exploiting user-feedback to learn-to-rank answers in q &#38; a forums : a case-study with stack-overflow collaborative-web sites , such as collaborative encyclopedias , blogs , and forums , are characterized by a loose edit control , which allows anyone to freely edit their content . as a consequence , the quality of this content raises much concern . to deal with this , many sites adopt manual quality-control mechanisms . however , given their size and change rate , manual assessment strategies do not scale and content that is new or unpopular is seldom reviewed . this has a negative impact on the many services provided , such as ranking and recommendation . to tackle with this problem , we propose a learning-to-rank (l2r) approach for ranking answers in q &#38; a forums . in particular , we adopt an approach based on random-forests and represent query and answer pairs using eight different groups of features . some of these features are used in the q &#38; a domain for the first time . our l2r method was trained to learn the answer rating , based on the feedback users give to answers in q &#38; a forums . using the proposed method , we were able (i) to outperform a state-of-the-art baseline with gains of up to 21 % in ndcg , a metric used to evaluate rankings ; we also conducted a comprehensive study of the features , showing that (ii) review and user features are the most important in the q &#38; a domain although text-features are useful for assessing-quality of new answers ; and (iii) the best set of new features we proposed was able to yield the best quality rankings .
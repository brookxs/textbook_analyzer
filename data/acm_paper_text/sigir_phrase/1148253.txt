large-scale semi-supervised linear svms large-scale-learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large-collection of unlabeled-data . in many information-retrieval and data-mining-applications , linear-classifiers are strongly preferred because of their ease of implementation , interpretability and empirical performance . in this work , we present a family of semi-supervised linear support vector classifiers that are designed to handle partially-labeled sparse datasets with possibly very-large number of examples and features . at their core , our algorithms employ recently developed modified finite newton techniques . our contributions in this paper are as follows : (a) we provide an implementation of transductive-svm (tsvm) that is significantly more efficient and scalable than currently used dual techniques , for linear-classification problems involving large , sparse datasets . (b) we propose a variant of tsvm that involves multiple switching of labels . experimental-results show that this variant provides an order of magnitude further improvement in training-efficiency . (c) we present a new algorithm for semi-supervised-learning based on a deterministic-annealing (da) approach . this algorithm alleviates the problem of local minimum in the tsvm optimization-procedure while also being computationally attractive . we conduct an empirical-study on several document-classification tasks which confirms the value of our methods in large-scale semi-supervised settings .
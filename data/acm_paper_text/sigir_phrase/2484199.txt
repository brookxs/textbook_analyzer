batc : a benchmark for aggregation-techniques in crowdsourcing as the volumes of ai problems involving human-knowledge are likely to soar , crowdsourcing has become essential in a wide range of world-wide-web applications . one of the biggest challenges of crowdsourcing is aggregating the answers collected from crowd workers ; and thus , many aggregate techniques have been proposed . however , given a new application , it is difficult for users to choose the best-suited technique as well as appropriate parameter-values since each of these techniques has distinct performance characteristics depending on various factors (e.g. worker expertise , question difficulty) . in this paper , we develop a benchmarking tool that allows to (i) simulate the crowd and (ii) evaluate aggregate techniques in different aspects (accuracy , sensitivity to spammers , etc.) . we believe that this tool will be able to serve as a practical guideline for both researchers and software-developers . while researchers can use our tool to assess existing or new techniques , developers can reuse its components to reduce the development complexity .
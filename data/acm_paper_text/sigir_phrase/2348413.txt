predicting quality flaws in user-generated-content : the case of wikipedia the detection and improvement of low-quality information is a key concern in web-applications that are based on user-generated-content ; a popular example is the online encyclopedia wikipedia . existing research on quality-assessment of user-generated-content deals with the classification as to whether the content is high-quality or low-quality . this paper goes one step further : it targets the prediction of quality flaws , this way providing specific indications in which respects low-quality content needs improvement . the prediction is based on user-defined cleanup tags , which are commonly used in many web-applications to tag content that has some shortcomings . we apply this approach to the english wikipedia , which is the largest and most popular user-generated knowledge source on the web . we present an automatic mining approach to identify the existing cleanup tags , which provides us with a training-corpus of labeled wikipedia articles . we argue that common binary or multiclass-classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw prediction as a one-class-classification problem . we develop a quality flaw model and employ a dedicated machine-learning-approach to predict wikipedia 's most important quality flaws . since in the wikipedia setting the acquisition of significant test-data is intricate , we analyze the effects of a biased sample-selection . in this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances . the flaw prediction-performance is evaluated with 10,000 wikipedia articles that have been tagged with the ten most frequent quality flaws : provided test-data with little noise , four flaws can be detected with a precision close to 1 .
a study on performance volatility in information-retrieval a common practice in comparative-evaluation of information-retrieval (ir) systems is to create a test-collection comprising a set of topics (queries) , a document-corpus , and relevance-judgments , and to monitor the performance of retrieval-systems over such a collection . a typical evaluation of a system involves computing a performance-metric , e.g. , average-precision (ap) , for each topic and then using the average-performance metric , e.g. , mean-average-precision (map) to express the overall system-performance . however , averages do not capture all the important aspects of system-performance , and used alone may not thoroughly express system effectiveness , i.e. , average of performance can mask large variance in individual topic effectiveness . the author hypothesis is that , in addition to the average of overall performance , attention needs to be paid to how a system-performance varies across topics . this variability can be measured by calculating the standard-deviation (sd) of individual-performance scores . we refer to this performance variation as volatility .
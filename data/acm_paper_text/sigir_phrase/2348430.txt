crowdterrier : automatic crowdsourced relevance-assessments with terrier in this demo , we present crowdterrier , an infrastructure extension to the open-source terrier ir platform that enables the semi-automatic generation of relevance-assessments for a variety of document-ranking tasks using crowdsourcing . the aim of crowdterrier is to reduce the time and expertise required to effectively crowdsource relevance-assessments by abstracting away from the complexities of the crowdsourcing process . it achieves this by automating the assessment-process as much as possible , via a close integration of the ir-system that ranks the documents (terrier) and the crowdsourcing marketplace that is used to assess those documents (amazon 's mechanical-turk) .
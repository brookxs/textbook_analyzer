spam-filter evaluation with imprecise ground-truth when trained and evaluated on accurately labeled datasets , online email-spam filters are remarkably effective , achieving error-rates an order of magnitude better than classifiers in similar applications . but labels acquired from user-feedback or third-party adjudication exhibit higher error-rates than the best filters -- even filters trained using the same source of labels . it is appropriate to use naturally occuring labels -- including errors -- as training-data in evaluating spam-filters . erroneous labels are problematic , however , when used as ground-truth to measure filter effectiveness . any measurement of the filter 's error-rate will be augmented and perhaps masked by the label error-rate . using two natural sources of labels , we demonstrate automatic and semi-automatic methods that reduce the influence of labeling errors on evaluation , yielding substantially more precise measurements of true filter error-rates .
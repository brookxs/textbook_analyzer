improving the estimation of relevance-models using large external corpora information-retrieval algorithms leverage various collection statistics to improve performance . because these statistics are often computed on a relatively small evaluation corpus , we believe using larger , non-evaluation corpora should improve performance . specifically , we advocate incorporating external corpora based on language-modeling . we refer to this process as external expansion . when compared to traditional pseudo-relevance-feedback techniques , external expansion is more stable across topics and up to 10 \ % more effective in terms of mean-average-precision . our results show that using a high quality corpus that is comparable to the evaluation corpus can be as , if not more , effective than using the-web . our results also show that external expansion outperforms simulated relevance-feedback . in addition , we propose a method for predicting the extent to which external expansion will improve retrieval-performance . our new measure demonstrates positive correlation with improvements in mean-average-precision .
retrieval-evaluation on focused tasks ranking of retrieval-systems for focused tasks requires large number of relevance-judgments . we propose an approach that minimizes the number of relevance-judgments , where the performance-measures are approximated using a monte-carlo sampling technique . partial measures are taken using relevance-judgments , whereas the remaining part of passages are annotated using a generated relevance probability-distribution based on result rank . we define two conditions for stopping the assessment procedure when the ranking between systems is stable .
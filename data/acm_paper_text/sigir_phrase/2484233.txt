semantic-models for answer-re-ranking in question-answering the task of question-answering (qa) is to find correct answers to users ' questions expressed in natural-language . in the last few years non-factoid qa received more attention . it focuses on causation , manner and reason questions , where the expected answer has the form of a passage of text . the presence of question and answers corpora allows the adoption of learning-to-rank (mlr) algorithms in order to out - put a sensible ranking of the candidate answers . the importance and effectiveness of linguistically motivated features , obtained from syntax , lexical-semantics and semantic-role-labeling , was shown in literature [2-4] , but there are still several different possible semantic-features that have not been taken into account so far and our goal is to find out if their use could lead to performance-improvement . in particular features coming from semantic-models (sm) like distributional semantic-models (dsms) , explicit-semantic-analysis (esa) , latent-dirichlet-allocation (lda) induced topics have never been applied to the task so far . based on the usefulness that those models show in other tasks , we think that sm can have a significant role in improving current state-of-the-art systems ' performance in answer-re-ranking . the questions this research wants to answer are : 1) do semantic-features bring information that is not present in the bag-of-words and syntactic-features ? 2) do they bring different information or does it overlap with that of other features ? 3) are additional semantic-features useful for answer-re-ranking ? does their adoption improve systems ' performance ? 4) which of them is more effective and under which circumstances ? we performed a preliminary evaluation of dsms on the respubliqa 2010 dataset . we built a dsm_based answer scorer that represents the question and the answer as the sums of the vectors of their terms taken term-term co-occurrence-matrix and calculates their cosine-similarity . we replaced the term-term matrix with the ones obtained by random-indexing (ri) , latent-semantic-analysis (lsa) and lsa over the ri . considering each dsm on its own , the results prove that all the dsms are better than the baseline (the standard term-term co-occurrence-matrix) , and the improvement is always significant . the best improvement for the mrr in english is obtained by lsa (+180 %) , while in italian by lsari (+161 %) . we also showed that combining the dsms with overlap based measures via combsum the ranking is significantly better than the baseline obtained by the overlap measures alone . for english we have obtained an improvement in mrr of about 16 % and for italian , we achieve a even higher improvement in mrr of 26 % . finally , adopting ranknet for combining the overlap features and the dsms features , improves the mrr of about 13 % . more details can be found in [1] . in order to investigate the effectiveness of the semantic-features , we still need to incorporate other semantic-features , such as esa , lda and other state-of-the-art linguistic-features . other operators for semantic-compositionality , like product , tensor-product and circular convolution , will also be investigated . moreover we will experiment on different datasets , focus - ing mainly on non-factoid qa . the yahoo!-answers manner questions datasets are a good starting point . a new dataset will also be collected with questions from the users of wikiedi (a qa-system over wikipedia articles , www.wikiedi.it) and answers in the form of paragraphs from wikipedia pages .
an analysis of the coupling between training-set and neighborhood sizes for the k nn classifier we consider the relationship between training-set-size and the parameter k for the k - nearest-neighbors (k nn) classifier . when few examples are available , we observe that accuracy is sensitive to k and that best k tends to increase with training-size . we explore the subsequent risk that k tuned on partitions will be suboptimal after aggregation and re-training . this risk is found to be most severe when little data is available . for larger training sizes , accuracy becomes increasingly stable with respect to k and the risk decreases .
generic soft-pattern models for definitional-question-answering this paper explores probabilistic lexico-syntactic pattern-matching , also known as soft-pattern matching . while previous methods in soft-pattern matching are ad-hoc in computing the degree of match , we propose two formal-matching models : one based on bigrams and the other on the profile-hidden-markov-model (phmm) . both models provide a theoretically sound method to model pattern-matching as a probabilistic process that generates token sequences . we demonstrate the effectiveness of these models on definition sentence-retrieval for definitional-question-answering . we show that both models significantly outperform state-of-the-art manually constructed patterns . a critical difference between the two models is that the phmm technique handles language variations more effectively but requires more training-data to converge . we believe that both models can be extended to other areas where lexico-syntactic pattern-matching can be applied .
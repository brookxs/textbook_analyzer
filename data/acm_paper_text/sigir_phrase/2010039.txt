repeatable and reliable search-system evaluation using crowdsourcing the primary problem confronting any new kind of search-task is how to boot-strap a reliable and repeatable evaluation campaign , and a crowd-sourcing approach provides many advantages . however , can these crowd-sourced evaluations be repeated over long periods of time in a reliable manner ? to demonstrate , we investigate creating an evaluation campaign for the semantic search-task of keyword-based ad-hoc object-retrieval . in contrast to traditional search over web-pages , object-search aims at the retrieval of information from factual assertions about real-world-objects rather than searching over web-pages with textual descriptions . using the first large-scale evaluation campaign that specifically targets the task of ad-hoc web-object retrieval over a number of deployed systems , we demonstrate that crowd-sourced evaluation campaigns can be repeated over time and still maintain reliable results . furthermore , we show how these results are comparable to expert judges when ranking-systems and that the results hold over different evaluation and relevance-metrics . this work provides empirical support for scalable , reliable , and repeatable search-system evaluation using crowdsourcing .
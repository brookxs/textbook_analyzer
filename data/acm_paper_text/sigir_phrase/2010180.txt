modeling document scores for distributed-information-retrieval distributed-information-retrieval (dir) , also known as federated-search , integrates multiple searchable collections and provides direct-access to them through a unified interface [3] . this is done by a centralized broker , that receives user queries , forwards them to appropriate collections and returns merged results to users . in practice , most of federated resources do not cooperate with a broker and do not provide neither their content nor the statistics used for retrieval . this is known as uncooperative dir . in this case a broker creates a resource-representation by sending sample queries to a collection and analyzing retrieved documents . this process is called query-based-sampling . the key issue here is the following : 1.1 how many documents have to be retrieved from a resource in order to obtain a representative sample ? although there have been a number of attempts to address this issue it is still not solved appropriately . for a given user-query resources are ranked according to their similarity to the query or based on the number of relevant documents they contain . since resource representations are usually incomplete , the similarity or the number of relevant documents can not be calculated precisely . resource-selection algorithms proposed in the literature estimate these numbers based on incomplete samples . however these estimates are subjects to error . in practice , inaccurate estimates that have high error should be trusted less then the more accurate estimates with low error . unfortunately none of the existing algorithms can make the calculation of the estimation errors possible . therefore the following questions arise : 2.1 how to estimate resource scores so that the estimation errors can be calculated ? 2.2 how to use these errors in order to improve the resource-selection performance ? existing results-merging algorithms estimate normalized document scores based on scores of documents that appear both in a sample and in a result list . the problem similar to the resource-selection one arises . the normalized document scores are only the estimates and are subjects to error . inaccurate estimates should be trusted less then the more accurate ones . again none of the existing algorithms provide a way for calculating these errors . thus the two question to be address on the results-merging phase are similar to the resource-selection ones : 3.1 how to estimate normalized document scores so that the estimation errors can be calculated ? 3.2 how to use these errors in order to improve the results-merging performance ? in this work we address the above issues by applying score-distribution-models (sdm) to different phases of dir [2] . in particular , we discuss the sdm-based resource-selection technique that allows the calculation of resource score estimation errors and can be extended in order to calculate the number of documents to be sampled from each resource for a given query . we have performed initial experiments comparing the sdm-based resource-selection technique to the state-of-the-art algorithms and we are currently experimenting with the sdm-based results-merging method . we plan to apply the existing score-normalization techniques from meta-search to the dir results-merging problem [1] . however , the sdm-based results-merging approaches require the relevance-scores to be returned together with retrieved documents . it is not yet clear how to relax this strong assumption that does not always hold in practice .
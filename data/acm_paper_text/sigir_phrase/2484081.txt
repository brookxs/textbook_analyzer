a general evaluation measure for document-organization tasks a number of key information-access tasks -- document-retrieval , clustering , filtering , and their combinations -- can be seen as instances of a generic {em document-organization} problem that establishes priority and relatedness relationships between documents (in other words , a problem of forming and ranking clusters) . as far as we know , no analysis has been made yet on the evaluation of these tasks from a global perspective . in this paper we propose two complementary-evaluation measures -- reliability and sensitivity -- for the generic-document organization task which are derived from a proposed set of formal-constraints (properties that any suitable measure must satisfy) . in addition to be the first measures that can be applied to any mixture of ranking , clustering and filtering tasks , reliability and sensitivity satisfy more formal-constraints than previously existing evaluation-metrics for each of the subsumed tasks . besides their formal properties , its most salient feature from an empirical point-of-view is their strictness : a high score according to the harmonic mean of reliability and sensitivity ensures a high score with any of the most popular evaluation-metrics in all the document retrieval , clustering and filtering datasets used in our experiments .
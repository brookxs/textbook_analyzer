a statistical method for system-evaluation using incomplete-judgments we consider the problem of large-scale-retrieval evaluation , and we propose a statistical method for evaluating retrieval-systems using incomplete-judgments . unlike existing techniques that (1) rely on effectively complete , and thus prohibitively expensive , relevance-judgment sets , (2) produce biased estimates of standard performance-measures , or (3) produce estimates of non-standard measures thought to be correlated with these standard measures , our proposed statistical-technique produces unbiased estimates of the standard measures themselves.our proposed technique is based on random-sampling . while our estimates are unbiased by statistical-design , their variance is dependent on the sampling-distribution employed ; as such , we derive a sampling-distribution likely to yield low variance estimates . we test our proposed technique using benchmark trec-data , demonstrating that a sampling pool derived from a set of runs can be used to efficiently and effectively evaluate those runs . we further show that these sampling pools generalize well to unseen runs . our experiments indicate that highly accurate estimates of standard performance-measures can be obtained using a number of relevance-judgments as small as 4 \ % of the typical trec-style judgment pool .
predicting query-difficulty on the web by learning-visual clues we describe a method for predicting query-difficulty in a precision-oriented web-search-task . our approach uses visual-features from retrieved surrogate document representations (titles , snippets , etc.) to predict retrieval-effectiveness for a query . by training a supervised-machine-learning algorithm with manually evaluated queries , visual clues indicative of relevance are discovered . we show that this approach has a moderate correlation of 0.57 with precision-at-10 scores from manual relevance-judgments of the top ten documents retrieved by ten web-search-engines over 896 queries . our findings indicate that difficulty predictors which have been successful in recall-oriented ad-hoc search , such as clarity metrics , are not nearly as correlated with engine-performance in precision-oriented tasks such as this , yielding a maximum correlation of 0.3 . additionally , relying only on visual clues avoids the need for collection statistics that are required by these prior approaches . this enables our approach to be employed in environments where these statistics are unavailable or costly to retrieve , such as metasearch .
modeling concept dynamics for large-scale music-search continuing advances in data-storage and communication-technologies have led to an explosive growth in digital-music-collections . to cope with their increasing scale , we need effective music-information-retrieval (mir) capabilities like tagging , concept-search and clustering . integral to mir is a framework for modelling music documents and generating discriminative signatures for them . in this paper , we introduce a multimodal , layered learning-framework called dmcm . distinguished from the existing approaches that encode music as an ensemble of order-less feature-vectors , our framework extracts from each music document a variety of acoustic features , and translates them into low-level encodings over the temporal-dimension . from them , dmcm elucidates the concept dynamics in the music document , representing them with a novel music signature-scheme called stochastic music concept histogram (smch) that captures the probability distribution over all the concepts . experiment results with two large music collections confirm the advantages of the proposed framework over existing methods on various mir tasks .
evaluating evaluation-measure stability this paper presents a novel way of examining the accuracy of the evaluation-measures commonly used in information-retrieval experiments . it validates several of the rules-of-thumb experimenters use , such as the number of queries needed for a good experiment is at least 25 and 50 is better , while challenging other beliefs , such as the common evaluation-measures are equally reliable . as an example , we show that precision at 30 documents has about twice the average-error-rate as average-precision has . these results can help information-retrieval researchers design-experiments that provide a desired level of confidence in their results . in particular , we suggest researchers using web measures such as precision-at-10 documents will need to use many more than 50 queries or will have to require two methods to have a very-large difference in evaluation scores before concluding that the two methods are actually different .
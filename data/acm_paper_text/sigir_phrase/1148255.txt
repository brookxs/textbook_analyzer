constructing informative prior distributions from domain-knowledge in text-classification supervised-learning approaches to text-classification are in practice often required to work with small and unsystematically collected training-sets . the alternative to supervised-learning is usually viewed to be building classifiers by hand , using a domain expert 's understanding of which features of the text are related to the class of interest . this is expensive , requires a degree of sophistication about linguistics and classification , and makes it difficult to use combinations of weak predictors . we propose instead combining domain-knowledge with training-examples in a bayesian-framework . domain-knowledge is used to specify a prior-distribution for the parameters of a logistic-regression-model , and labeled-training-data is used to produce a posterior-distribution , whose mode we take as the final classifier . we show on three text-categorization data-sets that this approach can rescue what would otherwise be disastrously bad training situations , producing much more effective classifiers .
active-learning to maximize accuracy vs. effort in interactive-information-retrieval we consider an interactive-information-retrieval task in which the user is interested in finding several to many relevant documents with minimal effort . given an initial document-ranking , user-interaction with the system produces relevance-feedback (rf) which the system then uses to revise the ranking . this interactive-process repeats until the user terminates the search . to maximize accuracy relative to user effort , we propose an active-learning strategy . at each iteration , the document whose relevance is maximally uncertain to the system is slotted high into the ranking in order to obtain user-feedback for it . simulated feedback on the robust04 trec collection shows our active-learning approach dominates several standard rf baselines relative to the amount of feedback provided by the user . evaluation on robust04 under noisy feedback and on letor collections further demonstrate the effectiveness of active-learning , as well as value of negative-feedback in this task scenario .
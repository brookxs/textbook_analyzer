deciding on an adjustment for multiplicity in ir experiments we evaluate statistical-inference procedures for small-scale ir experiments that involve multiple-comparisons against the baseline . these procedures adjust for multiple-comparisons by ensuring that the probability of observing at least one false-positive in the experiment is below a given threshold . we use only publicly available test-collections and make our software available for download . in particular , we employ the trec runs and runs constructed from the microsoft learning-to-rank (mslr) data-set . our focus is on non-parametric statistical procedures that include the holm-bonferroni adjustment of the permutation-test p-values , the maxt permutation-test , and the permutation-based closed testing . in trec-based simulations , these procedures retain from 66 % to 92 % of individually significant results (i.e. , those obtained without taking other comparisons into account) . similar retention rates are observed in the mslr simulations . for the largest evaluated query set-size (i.e. , 6400) , procedures that adjust for multiplicity find at most 5 % fewer true differences compared to unadjusted tests . at the same time , unadjusted tests produce many more false-positives .
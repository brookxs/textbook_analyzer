comparing the sensitivity of information-retrieval metrics information-retrieval effectiveness is usually evaluated using measures such as normalized-discounted-cumulative-gain (ndcg) , mean-average-precision (map) and precision at some cutoff (precision@k) on a set of judged queries . recent research has suggested an alternative , evaluating information-retrieval-systems based on user-behavior . particularly promising are experiments that interleave two rankings and track user-clicks . according to a recent study , interleaving experiments can identify large differences in retrieval-effectiveness with much better reliability than other click-based methods . we study interleaving in more detail , comparing it with traditional measures in terms of reliability , sensitivity and agreement . to detect very small differences in retrieval-effectiveness , a reliable outcome with standard metrics requires about 5,000 judged queries , and this is about as reliable as interleaving with 50,000 user impressions . amongst the traditional measures , ndcg has the strongest correlation with interleaving . finally , we present some new forms of analysis , including an approach to enhance interleaving sensitivity .
learning-to-rank with softrank and gaussian-processes in this paper we address the issue of learning-to-rank for document-retrieval using thurstonian models based on sparse gaussian processes . thurstonian models represent each document for a given query as a probability-distribution in a score space ; these distributions over scores naturally give rise to distributions over document rankings . however , in general we do not have observed rankings with which to train the model ; instead , each document in the training-set is judged to have a particular relevance level : for example `` bad '' , `` fair '' , `` good '' , or `` excellent '' . the performance of the model is then evaluated using information-retrieval (ir) metrics such as normalised-discounted-cumulative-gain (ndcg) . recently taylor et al. presented a method called softrank which allows the direct gradient optimisation of a smoothed version of ndcg using a thurstonian model . in this approach , document scores are represented by the outputs of a neural-network , and score distributions are created artificially by adding random noise to the scores . the softrank mechanism is a general one ; it can be applied to different ir-metrics , and make use of different underlying models . in this paper we extend the softrank framework to make use of the score uncertainties which are naturally provided by a gaussian-process (gp) , which is a probabilistic non-linear regression-model . we further develop the model by using sparse gaussian process techniques , which give improved performance and efficiency , and show competitive results against baseline methods when tested on the publicly available letor ohsumed data-set . we also explore how the available uncertainty information can be used in prediction and how it affects model-performance .
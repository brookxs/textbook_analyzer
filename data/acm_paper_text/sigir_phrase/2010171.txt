practical online retrieval-evaluation online-evaluation is amongst the few evaluation-techniques available to the information-retrieval-community that is guaranteed to reflect how users actually respond to improvements developed by the community . broadly speaking , online-evaluation refers to any evaluation of retrieval quality conducted while observing user-behavior in a natural context . however , it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales . the goal of this tutorial is to familiarize information-retrieval researchers with state-of-the-art techniques in evaluating information-retrieval-systems based on natural user clicking behavior , as well as to show how such methods can be practically deployed . in particular , our focus will be on demonstrating how the interleaving approach and other click based techniques contrast with traditional offline-evaluation , and how these online methods can be effectively used in academic-scale research . in addition to lecture-notes , we will also provide sample software and code walk-throughs to showcase the ease with which interleaving and other click-based methods can be employed by students , academics and other researchers .
novelty and diversity in information-retrieval-evaluation evaluation-measures act as objective-functions to be optimized by information-retrieval-systems . such objective-functions must accurately reflect user-requirements , particularly when tuning ir systems and learning ranking-functions . ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation-measures . in this paper , we present a framework for evaluation that systematically rewards novelty and diversity . we develop this framework into a specific evaluation-measure , based on cumulative-gain . we demonstrate the feasibility of our approach using a test-collection based on the trec question-answering track .
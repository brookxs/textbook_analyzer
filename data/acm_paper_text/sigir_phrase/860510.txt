automatic-ranking of retrieval-systems in imperfect environments the empirical-investigation of the effectiveness of information-retrieval (ir) systems requires a test-collection , a set of query topics , and a set of relevance-judgments made by human assessors for each query . previous experiments show that differences in human relevance assessments do not affect the relative-performance of retrieval-systems . based on this observation , we propose and evaluate a new approach to replace the human-relevance-judgments by an automatic-method . ranking of retrieval-systems with our methodology correlates positively and significantly with that of human-based evaluations . in the experiments , we assume a web-like imperfect environment : the indexing information for all documents is available for ranking , but some documents may not be available for retrieval . such conditions can be due to document deletions or network problems . our method of simulating imperfect environments can be used for web-search-engine assessment and in estimating the effects of network conditions (e.g. , network unreliability) on ir-system performance .
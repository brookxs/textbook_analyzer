regularized estimation of mixture-models for robust pseudo-relevance-feedback pseudo-relevance-feedback has proven to be an effective strategy for improving retrieval-accuracy in all retrieval-models . however the performance of existing pseudo-feedback methods is often affected significantly by some parameters , such as the number of feedback documents to use and the relative weight of original query terms ; these parameters generally have to be set by trial-and-error without any guidance . in this paper , we present a more robust method for pseudo-feedback based on statistical-language-models . our main idea is to integrate the original query with feedback documents in a single probabilistic mixture-model and regularize the estimation of the language-model parameters in the model so that the information in the feedback documents can be gradually added to the original query . unlike most existing feedback methods , our new method has no parameter to tune . experiment results on two representative data-sets show that the new method is significantly more robust than a state-of-the-art baseline language-modeling-approach for feedback with comparable or better retrieval-accuracy .
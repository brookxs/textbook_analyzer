evaluating multi-query sessions the standard system-based evaluation-paradigm has focused on assessing the performance of retrieval-systems in serving the best results for a single query . real-users , however , often begin an interaction with a search-engine with a sufficiently under-specified query that they will need to reformulate before they find what they are looking for . in this work we consider the problem of evaluating retrieval-systems over test-collections of multi-query sessions . we propose two families of measures : a model-free family that makes no assumption about the user 's behavior over a session , and a model-based family with a simple model of user-interactions over the session . in both cases we generalize traditional evaluation-metrics such as average-precision to multi-query session evaluation . we demonstrate the behavior of the proposed metrics by using the new trec 2010 session track collection and simulations over the trec-9 query track collection .
tackling concept-drift by temporal inductive-transfer machine-learning is the mainstay for text-classification . however , even the most successful techniques are defeated by many real-world-applications that have a strong time-varying component . to advance research on this challenging but important problem , we promote a natural , experimental framework-the daily classification task-which can be applied to large time-based datasets , such as reuters rcv1.in this paper we dissect concept-drift into three main subtypes . we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv1 . this understanding led us to develop a new learning-model that transfers induced knowledge through time to benefit future classifier-learning tasks . the method avoids two main problems with existing work in inductive-transfer : scalability and the risk of negative transfer . in empirical-tests , it consistently showed more than 10 points f-measure improvement for each of four reuters categories tested .
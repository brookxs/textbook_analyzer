information-retrieval-system evaluation : effort , sensitivity , and reliability the effectiveness of information-retrieval-systems is measured by comparing performance on a common set of queries and documents . significance-tests are often used to evaluate the reliability of such comparisons . previous work has examined such tests , but produced results with limited application . other work established an alternative benchmark for significance , but the resulting test was too stringent . in this paper , we revisit the question of how such tests should be used . we find that the t-test is highly reliable (more so than the sign or wilcoxon-test) , and is far more reliable than simply showing a large percentage difference in effectiveness-measures between ir systems . our results show that past empirical work on significance-tests over-estimated the error of such tests . we also re-consider comparisons between the reliability of precision at rank 10 and mean-average-precision , arguing that past comparisons did not consider the assessor effort required to compute such measures . this investigation shows that assessor effort would be better spent building test-collections with more topics , each assessed in less detail .
low-cost evaluation in information-retrieval search corpora are growing larger and larger : over the last 10 years , the ir research-community has moved from the several hundred thousand documents on the trec disks to the tens of millions of u.s. government web pages of gov2 to the one billion general-interest web-pages in the new clueweb09 collection . but traditional means of acquiring relevance-judgments and evaluating - e.g. pooling documents to calculate average-precision - do not seem to scale well to these new large-collections . they require substantially more cost in human assessments for the same reliability in evaluation ; if the additional cost goes over the assessing budget , errors in evaluation are inevitable . some alternatives to pooling that support low-cost and reliable evaluation have recently been proposed . a number of them have already been used in trec and other evaluation forums (trec million query , legal , chemical , web , relevance-feedback tracks , clef patent ir , inex) . evaluation via implicit-user-feedback (e.g. clicks) and crowdsourcing have also recently gained attention in the community . thus it is important that the methodologies , the analysis they support , and their strengths and weaknesses are well-understood by the ir community . furthermore , these approaches can support small research groups attempting to start investigating new tasks on new corpora with relatively low-cost . even groups that do not participate in trec , clef , or other evaluation conferences can benefit from understanding how these methods work , how to use them , and what they mean as they build test-collections for tasks they are interested in . the goal of this tutorial is to provide attendees with a comprehensive overview of techniques to perform low-cost (in terms of judgment effort) evaluation . a number of topics will be covered , including alternatives to pooling , evaluation-measures robust to incomplete-judgments , evaluating with no relevance-judgments , statistical-inference of evaluation-metrics , inference of relevance-judgments , query-selection , techniques to test the reliability of the evaluation and reusability of the constructed collections . the tutorial should be of interest to a wide range of attendees . those new to the field will come away with a solid understanding of how low-cost evaluation-methods can be applied to construct inexpensive test-collections and evaluate new ir technology , while those with intermediate knowledge will gain deeper insights and further understand the risks and gains of low-cost evaluation . attendees should have a basic knowledge of the traditional evaluation-framework (cranfield) and metrics (such as average-precision and ndcg) , along with some basic knowledge on probability-theory and statistics . more advanced concepts will be explained during the tutorial .
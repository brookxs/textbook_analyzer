automatic-image-annotation and retrieval using cross-media relevance-models libraries have traditionally used manual image-annotation for indexing and then later retrieving their image-collections . however , manual image-annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content . here , we propose an automatic approach to annotating and retrieving images based on a training-set of images . we assume that regions in an image can be described using a small-vocabulary of blobs . blobs are generated from image-features using clustering . given a training-set of images with annotations , we show that probabilistic-models allow us to predict the probability of generating a word given the blobs in an image . this may be used to automatically annotate and retrieve images given a word as a query . we show that relevance-models allow us to derive these probabilities in a natural way . experiments show that the annotation performance of this cross-media relevance-model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state-of-the-art model derived from machine-translation . our approach shows the usefulness of using formal information-retrieval-models for the task of image-annotation and retrieval .
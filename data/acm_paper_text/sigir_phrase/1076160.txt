short comings of latent-models in supervised settings the aspect-model [1 , 2] and the latent-dirichlet-allocation model [3 , 4] are latent generative-models proposed with the objective of modeling discrete data such as text . though it is not explicitly published (to the best of our knowledge) , it is reasonably well known in there search community that the aspect-model does not perform very well in supervised settings and also that latent-models are frequently not identifiable , i.e. their optimal parameters are not unique.in this paper , we make a much stronger claim about the pitfalls of commonly-used latent-models . by constructing a small , synthetic , but by no means unrealistic corpus , we show that latent-models have inherent limitations that prevent them from recovering semantically meaningful parameters from data generated from a reasonable generative distribution . in fact , our experiments with supervised-classification using the aspect-model , showed that its performance was rather poor , even worse than naive-bayes , leading us to the synthetic study.we also analyze the scenario of using tempered em and show that it would not plug the above shortcomings . our analysis suggests that there is also some scope for improvement in the latent-dirichlet-allocation model (lda) [3 , 4] . we then use our insight into the shortcomings of these models , to come up with a promising variant of the lda , that does not suffer from the aforesaid drawbacks . this could potentially lead to much better performance and model-fit , in the supervised scenario .
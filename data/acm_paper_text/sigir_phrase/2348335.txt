group matrix-factorization for scalable topic-modeling topic-modeling can reveal the latent structure of text-data and is useful for knowledge-discovery , search-relevance ranking , document-classification , and so on . one of the major challenges in topic-modeling is to deal with large-datasets and large numbers of topics in real-world-applications . in this paper , we investigate techniques for scaling up the non-probabilistic topic-modeling approaches such as rlsi and nmf . we propose a general topic-modeling method , referred to as group matrix-factorization (gmf) , to enhance the scalability and efficiency of the non-probabilistic approaches . gmf assumes that the text-documents have already been categorized into multiple semantic-classes , and there exist class-specific topics for each of the classes as well as shared topics across all classes . topic-modeling is then formalized as a problem of minimizing a general objective-function with regularizations and/or constraints on the class-specific topics and shared topics . in this way , the learning of class-specific topics can be conducted in parallel , and thus the scalability and efficiency can be greatly improved . we apply gmf to rlsi and nmf , obtaining group rlsi (grlsi) and group nmf (gnmf) respectively . experiments on a wikipedia dataset and a real-world web dataset , each containing about 3 million documents , show that grlsi and gnmf can greatly improve rlsi and nmf in terms of scalability and efficiency . the topics discovered by grlsi and gnmf are coherent and have good readability . further experiments on a search-relevance dataset , containing 30,000 labeled queries , show that the use of topics learned by grlsi and gnmf can significantly improve search-relevance .
Guided linking: efficiently making image-to-transcript correspondence
The problem of annotating unstructured images is labor intensive and d ifficult to automate. Linking is a type of annotation where an image region is tagged by representing a correspondence between the region and other information. Any serious effort at creating a digital edition of a manuscript from nothing but images and their associated information, such as transcripts and editorial remarks, must include the task of creating a large number of links between image regions and the related information. We present an approach to the problem of image linking, which concentrates on the fundamental and labor-intensive task of associating image regions with their textual counterparts. We assume the input to the system is a set of images representing a manuscript, and that associated data, such as a transcript, is available to provide guidance to the automated portion of the system. Our approach targets collections that are damaged and difficult-to-read, such as manuscripts that require intensive editorial annotation. It is essentially impossible to perform fully automated techniques, such as optical character recognition (OCR) or accurate handwriting analysis [2], on these kinds of manuscripts. We approach the problem with a semi-automatic solution that involves a document analysis (DA) module and a graphical user interface. The role of the DA module is to assist the user in formulating an algorithm that will automatically detect and rank regions of interest in the image. We provide the user with the capability to configure the parameters and steps of the analysis algorithm and thereby tune it to the task of identifying and ranking candidate regions for linking. With a graphical user interface, the user can interactively establish correspondences between image regions and other data, refine the correspondences detected by the DA module, and supply simple yet critical cues to the DA module to improve the result of subsequent automated processing. One
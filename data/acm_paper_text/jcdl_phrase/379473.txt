human-evaluation of kea , an automatic keyphrasing system this paper describes an evaluation of the kea automatic keyphrase extr action algorithm . tools that automatically identify keyphrases are desirable because document keyphrases have numerous applications in digital-library-systems , but are costly and time consuming to manually assign . keyphrase-extraction algorithms are usually evaluated by comparison to author-specified keywords , but this methodology has several well-known shortcomings . the results presented in this paper are based on subjective evaluations of the quality and appropriateness of keyphrases by human assessors , and make a number of contributions . first , they validate previous evaluations of kea that rely on author keywords . second , they show kea 's performance is comparable to that of similar systems that have been evaluated by human assessors . finally , they justify the use of author-keyphrases as a performance-metric by showing that authors generally choose good keywords .
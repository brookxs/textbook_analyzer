how do you feel about dancing queen ? : deriving mood & theme annotations from user tags web-2.0 enables information-sharing , collaboration among users and most notably supports active participation and creativity of the users . as a result , a huge amount of manually created metadata describing all kinds of resources is now available . such semantically rich user generated annotations are especially valuable for digital-libraries covering multimedia-resources such as music , where these metadata enable retrieval relying not only on content-based (low level) features , but also on the textual descriptions represented by tags . however , if we analyze the annotations users generate for music tracks , we find them heavily biased towards genre . previous work investigating the types of user provided annotations for music tracks showed that the types of tags which would be really beneficial for supporting retrieval - usage (theme) and opinion (mood) tags - are often neglected by users in the annotation rocess . in this paper we address exactly this problem : in order to support users in tagging and to fill these gaps in the tag space , we develop algorithms for recommending mood and theme annotations . our methods exploit the available user annotations , the lyrics of music tracks , as well as combinations of both . we also compare the results for our recommended mood / theme annotations against genre and style recommendations - a much easier and already studied task . besides evaluating against an expert (allmusic.com) ground-truth , we evaluate the quality of our recommended tags through a facebook-based user-study . our results are very promising both in comparison to experts as well as users and provide interesting insights into possible extensions for music tagging-systems to support music-search .
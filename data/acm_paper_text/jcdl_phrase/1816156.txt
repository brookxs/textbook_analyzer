evaluating topic-models for digital-libraries topic-models could have a huge impact on improving the ways users find and discover content in digital-libraries and search-interfaces through their ability to automatically learn and apply subject tags to each and every item in a collection , and their ability to dynamically create virtual collections on-the-fly . however , much remains to be done to tap this potential , and empirically evaluate the true value of a given topic-model to humans . in this work , we sketch out some sub-tasks that we suggest pave the way towards this goal , and present methods for assessing the coherence and interpretability of topics learned by topic-models . our large-scale user study includes over 70 human-subjects evaluating and scoring almost 500 topics learned from collections from a wide range of genres and domains . we show how scoring-model -- based on pointwise-mutual-information of word-pair using wikipedia , google and medline as external data sources - performs well at predicting human scores . this automated-scoring of topics is an important first step to integrating topic-modeling into digital-libraries
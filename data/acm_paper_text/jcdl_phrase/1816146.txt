improving mood classification in music-digital-libraries by combining lyrics and audio mood is an emerging metadata type and access-point in music-digital-libraries (mdl) and online-music repositories . in this study , we present a comprehensive investigation of the usefulness of lyrics in music-mood-classification by evaluating and comparing a wide range of lyric text-features including linguistic and text stylistic features . we then combine the best lyric features with features extracted from music audio using two fusion methods . the results show that combining lyrics and audio significantly outperformed systems using audio-only features . in addition , the examination of learning-curves shows that the hybrid lyric + audio system needed fewer training-samples to achieve the same or better classification accuracies than systems using lyrics or audio singularly . these experiments were conducted on a unique large-scale dataset of 5,296 songs (with both audio and lyrics for each) representing 18 mood categories derived from social-tags . the findings push forward the state-of-the-art on lyric-sentiment-analysis and automatic music mood classification and will help make mood a practical access-point in music-digital-libraries .
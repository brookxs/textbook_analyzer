can an intermediary collection help users search image-databases without annotations ? developing methods for searching image-databases is a challenging and ongoing area of research . a common approach is to use manual annotations , although generating annotations can be expensive in terms of time and money , and therefore may not be justified in many situations . content-based-search techniques which extract visual-features from image-data can be used , but users are typically forced to express their information-need using example images , or through sketching-interfaces . this can be difficult if no visual example of the information-need is available , or when the information-need can not be easily drawn . in this paper , we consider an alternative approach which allows a user to search for images through an intermediate database . in this approach , a user can search using text in the intermediate database as a way of finding visual examples of their information-need . the visual examples can then be used to search a database that lacks annotations . three experiments are presented which investigate this process . the first experiment automatically selects the image-queries from the intermediary database ; the second instead uses images which have been hand-picked by users . a third experiment , an interactive study , is then presented this study compares the intermediary interface to text-search , where we consider text as an upper-bound of performance . for this last study , an-interface which supports the intermediary search-process is described . results show that while performance does not match manual annotations , users are able to find relevant material without requiring collection annotations .
automatic quality-assessment of content created collaboratively by web-communities : a case-study of wikipedia the old dream of a universal-repository containing all the human-knowledge and culture is becoming possible through the-internet and the-web . moreover , this is happening with the direct collaborative , participation of people . wikipedia is a great example . it is an enormous repository of information with free access and edition , created by the community in a collaborative manner . however , this large amount of information , made available democratically and virtually without any control , raises questions about its relative quality . in this work we explore a significant number of quality-indicators , some of them proposed by us and used here for the first time , and study their capability to assess the quality of wikipedia articles . furthermore , we explore machine-learning-techniques to combine these quality-indicators into one single assessment judgment . through experiments , we show that the most important quality-indicators are the easiest ones to extract , namely , textual-features related to length , structure and style . we were also able to determine which indicators did not contribute significantly to the quality-assessment . these were , coincidentally , the most complex features , such as those based on link-analysis . finally , we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction .
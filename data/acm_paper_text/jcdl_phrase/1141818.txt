using controlled-query-generation to evaluate blind-relevance-feedback algorithms currently in document-retrieval there are many algorithms each with different strengths and weakness . there is some difficulty , however , in evaluating the impact of the test query set on retrieval results . the traditional evaluation-process , the cranfield evaluation paradigm , which uses a corpus and a set of user queries , focuses on making the queries as re-alistic as possible . unfortunately such query sets lack the fine-grained control necessary to test algorithm properties . we present an approach called controlled-query-generation (cqg) that creates query sets from documents in the corpus in a way that regulates the theoretic information-quality of each query . this allows us to generate reproducible and well defined sets of queries of varying length and term-specificity . imposing this level of control over the query sets used for testing retrieval algorithms enables the rigorous simulation of different query environments to identify specific algorithm properties before introducing user queries . in this work , we demonstrate the usefulness of cqg by generating three dif-ferent query environments to investigate characteristics of two blind-relevance-feedback approaches .
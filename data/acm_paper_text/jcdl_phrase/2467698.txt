information-theoretic term-weighting-schemes for document-clustering we propose a new theory to quantify information in probability-distributions and derive a new document-representation-model for text-clustering . by extending shannon-entropy to accommodate a non-linear relation between information and uncertainty , the proposed least information-theory (lit) provides insight into how terms can be weighted based on their probability-distributions in documents vs. in the collection . we derive two basic quantities in the document-clustering context : 1) li binary (lib) which quantifies information due to the observation of a term 's (binary) occurrence in a document ; and 2) li frequency (lif) which measures information for the observation of a randomly picked term from the document . both quantities are computed given term distributions in the document-collection as prior-knowledge and can be used separately or combined to represent documents for text-clustering . experiments on four benchmark text-collections demonstrate strong performances of the proposed methods compared to classic tf * idf . particularly , the lib * lif weighting-scheme , which combines lib and lif , consistently outperforms tf * idf in terms of multiple evaluation-metrics . the least information-measure has a potentially broad range of applications beyond text-clustering .
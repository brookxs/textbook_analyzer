 33.3 Resolving Power  Zipf observed that the frequency of words' occurrence varies dramatically, andPoisson models explore deviations of these occurrence patterns from purely random processes. We now make the first important move toward a theory of why some words occur more frequently and how such statistics can be exploited when building an index automatically. Luhn, as far back as 1957, said clearly:  It is hereby proposed that the frequency of word occurrence in an article furnishes a useful measurement of word significance. [Luhn,1957]  That is, if a word occurs frequently, more frequently than we would expect it to occur within a corpus, then it is reflecting emphasis on the part of the author about that topic. But the raw frequency of occurrence in a document is only one of two critical statistics recommending good keywords.  Consider a document taken from our AIT corpus, and imagine using the keyword ARTIFICIAL INTELLIGENCE with it. By construction, virtually every document in the AIT is about ARTIFICIAL INTELLIGENCE!? Assigning the keyword ARTIFICIAL INTELLIGENCE to every document in AIT would be a mistake, not because this document isn't about ARTIFICIAL INTELLIGENCE, but because this term cannot help us discriminate one subset of our corpus as relevant to any query. If we change our search task to looking not only in our AIT corpus but through a much larger collection (for example, all computer industry newsletters), then associating ARTIFICIAL INTELLIGENCE with those articles in our AIT subcorpus becomes a good idea. This term helps to distinguish AI documents from others.  The second critical characteristic of good Indices now becomes clear: A good index term not only characterizes a document absolutely, as a feature of a document in isolation, but also allows us to discriminate it relative to other documents in the corpus. Hence keywords are not strictly properties of any single document, but they reflect a relationship between an individual document and the collection from which it might be selected.  These two countervailing considerations suggest that the best keywords will not be the most ubiquitous, frequently occurring terms, nor WEIGHTING AND MATCHING AGAINST INDICES       77  Upper cutott  Lower cutoff  Zipf s first law  Rank order of words  Too common  Significant  Too rare  FIGURE 3.3 Resolving Power  those that occur only once or twice, but rather those occurring a moderate number of times. Using Zipf's rank ordering of words as a baseline, Luhn hypothesized a modal function of a word's rank he called resolving power, centered exactly at the middle of this rank ordering. If resolving power is defined as a word's ability to discriminate content, Luhn assumed that this quantity is maximal at the middle and falls off at either very high or very low frequency extremes, as shown in Figure 3.3.* The next step is then to establish maximal and minimal occurrence thresholds defining useful, midfrequency index terms. Unfortunately, Luhn's view does not provide theoretical grounds for selecting these bounds, so we are reduced to the engineering task of tuning them for optimal performance.  We'll begin with the maximal-frequency threshold, which is used to exclude words that occur too frequently. For any particular corpus, it is interesting to contrast this set of most-common words with the negative dictionary of noise words, defined in Section 2.3.2. While there is often great overlap, the negative dictionary list has proven itself to be useful across many different corpora, while the most frequent tokens in a particular corpus may be quite specific to it.  Establishing the low-frequency threshold is less intuitive. Assuming that our index is to be of limited size, including a certain keyword means we must exclude some other. This suggests that a word that occurs in  * After [van Rijsbergen, p. 16, figure 2.1 ]. 78      FINDING OUT ABOUT  Specificity                                Exhaustivity  ¶i  Discritninability                     Representation of        _______________       of  few      "'¶ª"¶¶¶´¶.......¶¶¶...............................................I    many  doc/jkw                             kw/doc  leads to  High precision                              High recall  FIGURE 3.4 Specificity/Exhaustivity Trade-Offs  exactly one document can't possibly be used to help discriminate that document from others regularly. For example, imagine a word - suppose it is DERIVATIVE - that occurs exactly once, in a single document. If we took out that word DERIVATIVE and put in any other word, for example, FOOBAR, in terms of the word frequency co-occurrence statistics that are the basis of all our indexing techniques, the relationship between that document and all the other documents in the collection will remain unchanged. In terms of overlap between what the word DERIVATIVE means, in the FOA sense of what this and other documents are about, a single word occurrence has no meaning!  The most useful words will be those that are not used so often as to be roughly common to all of the documents, and not so rarely as to be (nearly) unique to any one (or a small set of) document. We seek those keywords whose combinatorial properties, when used in concert with one another as part of queries, help to compare and contrast topical areas of interest against one another.   
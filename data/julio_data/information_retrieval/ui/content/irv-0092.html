 Presentation of experimental results  In my discussion of micro-, macro-evaluation, and expected search length, various ways of averaging the effectiveness measure of the set of queries arose in a natural way. I now want to examine the ways in which we can summarise our retrieval results when we have no a priori reason to suspect that taking means is legitimate.  In this section the discussion will be restricted to single number measures such as a normalised symmetric difference, normalised recall, etc. Let us use Z to denote any arbitrary measure. The test queries will be Qi and n in number. Our aim in all this is to make statements about the relative merits of retrieval under different conditions a,b,c, . . . in terms of the measure of effectiveness Z. The 'conditions' a,b,c, . . . may be different search strategies, or information structures, etc. In other words, we have the usual experimental set-up where we control a variable and measure how its change influences retrieval effectiveness. For the moment we restrict these comparisons to one set of queries and the same document collection.  The measurements we have therefore are {Za(Q1), Za(Q2), . . . }, {Zb(Q1), Zb(Q2), . . . }, {Zc(Q1), Zc(Q2), . . . }, . . . where Zx(Q1) is the value of Z when measuring the effectiveness of the response to Qi under conditions x. If we now wish to make an overall comparison between these sets of measurements we could take means and compare these. Unfortunately, the distributions of Z encountered are far from bell-shaped, or symmetric for that matter, so that the mean is not a particularly good 'average' indicator. The problem of summarising IR data has been a hurdle every since the beginning of the subject. Because of the non-parametric nature of the data it is better not to quote a single statistic but instead to show the variation in effectiveness by plotting graphs. Should it be necessary to quote 'average' results it is important that they are quoted alongside the distribution from which they are derived.  There are a number of ways of representing sets of Z-values graphically. Probably the most obvious one is to use a scatter diagram, where the x-axis is scaled for Za and the y-axis for Zb and each plotted point is the pair (Za(Qi), Zb(Qi)). The number of points plotted will equal the number of queries. If we now draw a line at 45[[ring]] to the x-axis from the origin we will be able to see what proportion of the queries did better under condition a than under condition b. There are two disadvantages to this method of representation: the comparison is limited to two conditions, and it is difficult to get an idea of the extent to which two conditions differ.  A more convenient way of showing retrieval results of this kind is to plot them as cumulative frequency distributions, or as they are frequently called by statisticians empirical distribution functions. Let {Z(Q1), Z(Q2), . . . , Z(Qn)} be a set of retrieval results then the empirical distribution function F(z) is a function of z which equals the proportion of Z(Qi)'s which are less than or equal to z. To plot this function we divide the range of z into intervals. If we assume that 0 lt;= z lt;= 1, then a convenient set of intervals is ten. The distributions will take the general shape as shown in Figure 7.14. When the measure Z is such that the smaller its value the more effective the retrieval, then the higher the curve the better. It is quite simple to read off the various quantiles. For example, to find the median we only need to find the z-value corresponding to 0.5 on the F(z) axis. In our diagrams they are 0.2 and 0.4 respectively for conditions a and b.  I have emphasised the measurement of effectiveness from the point of view of the user. If we now wish to compare retrieval on different document collections with different sets of queries then we can still use these measures to indicate which system satisfies the user more. On the other hand, we cannot thereby establish which system is more effective in its retrieval operations. It may be that in system A the sets of relevant documents constitute a smaller proportion of the total set of documents than is the case in system B. In other words, it is much harder to find the relevant documents in system B than in system A. So, any direct comparison must be weighted by the generality measure which gives the number of relevant documents as a proportion of the total number of documents. Alternatively one could use fallout which measures the proportion of non-relevant documents retrieved. The important point here is to be clear about whether we are measuring user satisfaction or system effectiveness.   
 4________  Assessing the Retrieval  We've come a long way since Chapter 1, where we first sketched the full range of activities we might consider FOA. As Chapter 2 considered the various ways of breaking text into indexable features, and Chapter 3 explained the various ways of weighting combinations of these features to identify the best matches to a query, I hope you have been aware of how many alternatives have been mentioned! That is, rarely has there been a single method that can be proven to be better than all others. IR has traditionally been driven by empirical demonstrations, and the range of commercial competitors now trying to provide the "best" search of the WWW makes it likely this performance orientation will continue. But whether we are search engineers, scientists objectively assessing one particular technique, or consumers of WWW search engine technology interested in buying and using the best, a solid basis of performance assessment is criticalSeveral perspectives on assessment are possible. In the first chapter FOA was viewed as a personal activity, adopting the users1 points of view. Section 4.1 will continue in this theme, considering how users assess the results of their retrievals and how they can express their opinions using relevance feedback (relevance feedback). Oddy is credited with first identifying this important stream of data, naturally provided by users as a part of their FOA browsing [Oddy, 1977; Belkin et al, 1982].  105 106       FINDING OUT ABOUT  But in this book we are also concerned with FOA from the IR system builder's point of view. Ideally, we would like to construct a search engine that robustly finds the "right" documents for each query and for each user. The second section of this chapter discusses performance measures of statistical properties that are reliable across large numbers of users and their highly variable queries. The key to these measures is having some insight into which documents should have been retrieved, typically because some idealized omniscient expert has determined (within a specially constructed experimental situation) that certain documents "should" have been retrieved. Alternatively the relevance feedback of many users can be combined to form a consensual opinion of relevance, as described in Section 4.4.  A concrete notion of relevance would seem a fundamental precondition for understanding either an individual's relevance feedback or how this can be used to assess a search engine. But in this respect, information retrieval generally, and relevance feedback in particular, is like many other academic areas of study (including artificial intelligence and genetics) in that the lack of a fully satisfactory definition of the core concept (information, intelligence, genes, and so on) has not entirely stopped progress. That is, a great deal can be done by operationalizing relevance feedback to be simply those relevance assessment behaviors produced as part of an FOA dialog. This operational simplification will hold us until fundamental issues of language and communication are again addressed in Section 8.2.1.   
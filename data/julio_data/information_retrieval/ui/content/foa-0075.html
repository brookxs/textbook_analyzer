 4.3.8 One-Parameter Criteria  This section began with recall and precision, the two most typical measures of search engine performance. From that beginning, richer, more elaborate characterizations of how well the system is performing have been considered. But even having the two measures of recall and precision, it is not a simple matter to decide whether one system is better or worse than another. What are we to think of a system that has good recall but poor precision, relative to another with the opposite feature?  For example, if we wish to optimize a search engine with respect to one or more design parameters (e.g., the exact form of the query/ document matching function, cf. Section 5.3.1), effective optimization becomes much more difficult in multicriterial cases. Such thinking has generated composite measures based on the basic components of recall and precision.  For example, Jardine and van Rijsbergen [ Jardine and van Rijsbergen, 1971; van Rijsbergen, 1973] originally proposed the F-measure for this purpose:  p   ~~ (@2 + l)* Precision-Recall  P "       ^Precision + Recall                            *    j  van Rijsbergen, p. 174 * has since defined the closely related effectiveness  * Van Rijsbergen's original paper used the function symbol E rather than the F we use here. The substitution is made to maintain the E ó "effectiveness" mnemonic. ASSESSING THE RETRIEVAL       133  of measure £, which uses a to smoothly vary the emphasis given to precision versus recall:  /     a            1-cA""  Ea = 1 -           ..     + -rój:                        (4.12)  \Prectswn      Recall/  The transform a = -ott^ converts easily between the two formulations, with E = 1 ó F. Van Rijsbergen, p. 174 also presents an argument that a perfectly even-handed balance of precision against recall at a = 0.5 is most appropriate. Setting a = 0.5, f5 = 1 also has the pleasing consequence that the F, statistic corresponds to the harmonic mean of Precision and Recall  As discussed at some length in Section 7.4, it is possible to view retrieval as a type of classification task: Given a set of features for each document (e.g., the keywords it contains), classifiy it as either Rel or Rel with respect to some query. Lewis and Gale [Lewis and Gale, 1994] used the Fp measure in the context of text classification tasks, and they recommend a focus on the same j8 = 1.0 balance. Classification accuracy measures how often the classification is correct. If we associate the choice to retrieve a document with classifying it as Rel, we can use the variables defined in the contingency table of (Table 4.1):  I Retr H Rel I + I Retr D Rel\  Accuracy- -----------------ó--------------------             (4.13)  NDoc  Sliding Ratio  The fact that the Retr set is ordered makes it useful to compare two rank orderings directly. If the "correct," idealized ranking is known (for example, one corresponding to perfectly decreasing probability of relevance) , then an actual search engine's hitlist ranking can be compared against this standard. More typically, the rankings of two retrieval systems are compared to one another.  Given two rankings, we will prefer the one that ranks relevant documents ahead of irrelevant ones. If our relevance assessments are binary, with each document simply marked as relevant or irrelevant,* the normalized recall measure considered in Section 4.3.6 (or the  * As always, these assessments of relevance are with respect to some particular query. 134      FINDING OUT ABOUT  expected search length measure to be described in Section 4.3.10) is the best we can do in distinguishing the two rankings.  But if we assume instead that it is possible to impose a more refined measure Rel(di) than simply Rel/Rel (e.g., recall the richer preference scale of Figure 4.1), more sophisticated measures are possible. In this case, we prefer a ranking that ranks d\ ahead of dj just in case Rel(d{) gt; Rel{ dj). One way to quantify this preference is to sum the Rel{ d\) for the NRet most highly ranked documents:  NRet  (4.14)  The ratio of this measure, computed for each of the two systems' rankings, is called the sliding ratio score [Pollack, 1968]:  Rank\{dt) lt;NRet  E       Rd(dt)  Rank2(dl) lt;NRet  E       Wdi)  f=i  As NRet increases, this ratio comes closer to unity:  Ranki(di) lt;NRet  E        ReKdi) lim     ------ó---------------= 1                  (4.16)  NRet-+NDoc Ranked,) lt;NRet  i=l  and so it is most useful for distinguishing between two rankings when only a small NRet is considered.  Point Alienation  The sliding ratio measure provides a more discriminating measure but depends entirely on the availability of metric Rel(dj) measures for retrieved documents. As discussed in Section 4.1.1, it is much easier to derive nonmetric assessments directly from relevance feedback data given naturally as part of users' browsing:  e -lt;#-lt;©                                  (4.17)  In an effort to exploit the nonmetric preferences often provided by human subjects5 Guttman [Guttman, 1978] defined a measure known ASSESSING THE RETRIEVAL       135  as point alienation. Bartell has pioneered a variation of it for use with document rankings rated by relevance feedback [Bartell et al., 1994a]. The basic idea is deceptively simple: Compare the difference in rank between two differentially preferred documents to the absolute difference of these ranks:  y.   Rank(d)~Rank(d') ~ f^d, \Rank{d) - Rank(df)\                   l '    j  If d is really preferred over d! - (d y d!) - (e.g., if some user has marked d as Rel but said nothing about df), we can hope that Rank(d) lt; Rank(df),* and so the numerator (Rank(d)~ Rank(df))wHl be negative; if, on the other hand, the two documents are incorrectly ordered by the ranking, the numerator will be positive. Comparing this arithmetic difference to its absolute value, and then summing over the rankings for all pairs of documents (d, df) that are differentially preferred {dy dr) gives Equation 4.18.   
 4.3.5 Ordering the Retr Set  Do not worry about large numbers of results: the best ones come first! (www.AltaVista.com, 1998)  The next step is to move beyond thinking of Retr as simply a set. We will suppose that retrieved documents are returned in some order by the search engine, reflecting its assessment of how well each document matches the query. Following current Web vernacular, we will call this ordering of the Retr set a hitlist and a retrieved document's position its hitKst rank Rank(di). This is a positive integer assigned to each document in the Retr set, in descending order of similarity with respect to the matching function Match{q, d):  Match(q, d)eU Rank(d) e M^  Rankidi) lt; Rank(dj) lt; Matchiq, dª) gt; Match(q, dj)      (4.6)  Sparck Jones | Sparck Jones, 1972] and others have historically referred to a document's rank in Retr as its "coordination level" (cf. Eq. 3.36). Strictly speaking, coordination level refers to the number of ASSESSING THE RETRIEVAL       125  keywords shared by document and query. In Boolean retrieval systems, sensitive only to the presence or absence of keywords, ranking by coordination level may be the only available measure on document/query similarity.  For long queries, hitlist rank and coordination level are likely to be similar, because it is unlikely that different documents will match exactly the same number of words from the query. But for short queries, it is likely that coordination level will only partially order the Retr set. This is why van Rijsbergen, p. 161, speaking of the Boolean systems typical at that time, said, "Unfortunately, the ranking generated by a matching function is rarely a simple ordering, but more commonly a weak ordering." Most modern search engines, however, exploit keyword weightings and can provide much more refined measures, thereby providing a total ordering of the hitlist.  According to the Probability Ranking Principle (cf. Section 5.5.1), a retrieval system is performing optimally if it retrieves documents in order of decreasing probability of relevance. For now we simply assume that there is a total ordering imposed over Retr. We will use the hitlist ranking to effectively define a series of retrievals. Setting a very high threshold on this ordering would mean retrieving a very small set, while setting a lower threshold will retrieve a much larger one.  Now consider a particular query q and the set Relq of relevant documents associated with it. Assuming that Retr is totally ordered makes it possible for us to define the fundamental analytic tool for search engine performance: the Recall/Precision curve (Re/Pre curve). The basic procedure is to consider each retrieved document in hitlist rank order and to ask for the precision and recall of the retrieval of all documents up to and including this one.  Consider the first of the two hypothetical retrievals shown in Table 4.2.  With respect to this query, we will assume there are exactly five relevant documents out of a total of 25 in the corpus. The very first one retrieved is deemed relevant; if we stopped retrieval at this point, our recall would be 0.2 (because we would have retrieved one of five relevant documents), and our precision is perfect (the one retrieved document is relevant). Our good luck continues as we consider the next document, which is also relevant; this generates a second Re/Pre data point of (0.4,1.0). We are not so lucky with the third document retrieved; precision drops to 0.67 and recall remains at 0.4. Proceeding down the 126      FINDING OUT ABOUT    TABLE 4.2 Two Hypothetical Retrievals      Query 1    Query 2    Relevant? NRel Recall Precision Relevant? NRel Recall Precision  1 1 1 0.20 1.00 0 0 0.00 0.00  2 1 2 0.40 1.00 0 0 0.00 0.00  3 0 2 0.40 0.67 0 0 0.00 0.00  4 1 3 0.60 0.75 1 1 0.50 0.25  5 0 3 0.60 0.60 0 1 0.50 0.20  6 0 3 0.60 0.50 0 1 0.50 0.17  7 0 3 0.60 0.43 0 1 0.50 0.14  8 0 3 0.60 0.38 0 1 0.50 0.13  9 0 3 0.60 0.33 0 1 0.50 0.11  10 0 3 0.60 0.30 0 1 0.50 0.10  11 0 3 0.60 0.27 0 1 0.50 0.09  12 0 3 0.60 0.25 0 1 0.50 0.08  13 0 3 0.60 0.23 0 1 0.50 0.08  14 0 3 0.60 0.21 0 1 0.50 0.07  15 1 4 0.80 0.27 1 2 1.00 0.13  16 0 4 0.80 0.25 0 2 1.00 0.13  17 0 4 0.80 0.24 0 2 1.00 0.12  18 0 4 0.80 0.22 0 2 1.00 0.11  19 0 4 0.80 0.21 0 2 1.00 0.11  20 0 4 0.80 0.20 0 2 LOO 0.10  21 0 4 0.80 0.19 0 2 LOO 0.10  22 0 4 0.80 0.18 0 2 LOO 0.09  23 0 4 0.80 0.17 0 2 LOO 0.09  24 0 4 0.80 0.17 0 2 LOO 0.08  25 1 5 1.00 0.20 0 2 LOO 0.08  retrieval in rank order, and plotting each point in this fashion gives the Re/Pre curve shown in Figure 4.10.  At this point we can already make several observations. Asymptotically, we know that the final recall must go to one; once we have  retrieved every document we've also retrieved every relevant document. The precision will be the ratio of the number of relevant documents to the total corpus size. Ordinarily, unless we are interested in very general  queries or very small sets of documents, this ratio will be very close to zero.  The other end of the curve, however, turns out to be much less stable. We would hope that a retrieval system's very first candidate for retrieval, the document with hitlist rank = 1, will be relevant, but it may not be. ASSESSING THE RETRIEVAL       127  1.00 0.90 ¶0.80 ¶0.70 -0.60 -0.50 -0.40 - 0.30 - ï 0.20 - ¶ 0.10 - 0.00  0.00  0.10  0.20  0.30  0.40  0.50 Recall  0.60  0.70  0.80  0.90  1.00  FIGURE 4.10 Recall/Precision Curve  LOO ¶0.90 ¶0.80 -0.70 -0.60 - 0.50 - 0.40 ¶0.30 -0.20 -0.10 - 0.00  0.00        0.10        0.20        0.30        0.40        0.50        0.60        0.70        0.80  Recall  FIGURE 4.11 Instability of Beginning of Re/Pre Curve  0.90  LOO  Figure 4.11 shows a second pair of hypothetical data points (dashed line), corresponding to the case that a single irrelevant document is ranked  higher than the relevant ones. This relatively small change in assessment creates a fairly dramatic effect on the curve, with real consequence once 128      FINDING OUT ABOUT  0.00  0.00  0.10         0.20         0.30         0.40         0.50         0.60          0.70         0.80  Recall FIGURE 4.12 Best/Worst Retrieval Envelope  0.90  LOO  we need to juxtapose multiple queries' curves (see Section 43.7). Such instability is an inevitable consequence of the definitions ofPrecision and Recall: If the first retrieved document happens to be relevant, its Re/Pre coordinates will be less than 1, and ~^j greater than 1; otherwise it will belt;0gt;Ggt;.  Figure 4.12 puts this particular retrieval in the context of the best and worst retrievals we might imagine. The best possible retrieval (hashed) would be to retrieve the five relevant documents first, and then all other documents. This would produce the upper, square Re/Pre curve. Alternatively, the worst possible retrieval (shaded) would retrieve all but the relevant documents before returning these; this produces the lower line.   
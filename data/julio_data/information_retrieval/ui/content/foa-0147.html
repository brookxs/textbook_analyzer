 73 Learning Which Documents to Route  Arguably, the first use of relevance feedback information in an adaptive IR context  came out of the SMART group led by Gerald Salton. As was discussed in  Section 4.2.2, Salton's vector space model lends itself to a representation of documents and queries that suggest ways of producing better matches, as shown in Figure 7.4. 264      FINDING OUT ABOUT  FIGURE 7.4 Document Modifications due to relevance feedback  Rocchio's characterization of the task is best described as routing: We imagine that a stream of all the world's news is available to some institution (for example, a corporation), and many members of this institution (employees) are expected to build long-standing queries characterizing their interests. These queries are passed against the incoming stream of documents, and those matching a particular employee's query are routed to him or her [Schutze et al., 1995a].  From this employee's point of view, he or she will be receiving a stream of documents, all of which are of interest to him or her. If he or she Identifies those documents that are not relevant, the resulting corpus can be viewed as a training set and used to adjust the filter. This is a concrete application of the binary classification technology.  Brauen, in particular, considered "document vector modifications," resulting in "dynamic" document spaces [Brauen, 1969]. Documents marked as relevant can be moved closer to the query that successfully retrieved them in several ways. First and most obviously, features shared by query and document can have their weight increased. Features in the document but not in the query can have their weight decreased, and terms in the query but not in the document can be added to the document's representation with small initial weights.  From the perspective of modern machine learning, Brauen's method would be considered a type of gradient descent learning algorithm: ADAPTIVE INFORMATION RETRIEVAL       265  The disparity between document and query creates a gradient along which small changes are made. One difference, however, is that Brauen imagined a batch learning scenario, with the entire set of labeled documents available simultaneously. Online learning algorithms, on the other hand, make small, incremental adaptive changes in immediate response to every learning instance (document). Each individual step of learning (e.g., weight update in neural networks) must be very small, in order to guarantee convergence toward the globally optimal value. The size of the small change is controlled by 77, typically known as the learning rate. Stochastic approximation theory suggests that this constant should be relatively small for online learning so that the weights are small enough to allow convergence [White, 1989]. Online learning is generally preferred to avoid time delay and data collection complications. Idealizing our learning task to produce a perfect match (i.e., the dot product of query and document is 1.0) on relevant documents and no match on irrelevant documents, we can treat this behavior as the target for our error correction learning. Let R stand for this Boolean relevant/not relevant classification of each document. Then (as discussed further in Section 7.4), we can hope to have available a training set T of documents that have been labeled a priori as R^ = 0 or R = 1gt; specifying whether they are considered relevant. With such evidence as to what documents we do and don't consider relevant, we can define precisely how well we have learned. A typical error measure or loss function can be defined as the squared difference between the actual vector match and the target Rd:  ,      .,  r          Wj         n \2                          a c\     gradient runs in  Error = ^(d-q-Rd)2                           (7.5)    *oth directions  deT   
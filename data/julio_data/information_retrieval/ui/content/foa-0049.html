 3.3.6 Informative Signals versus Noise Words  We begin with a weighting algorithm derived from information theory. Information theory has proven itself to be an extraordinarily useful model of many different situations in which some message must be communicated across a noisy channel and our goal is to devise an encoding for messages that is most robust in the face of this noise.  In our case, we must imagine that the "messages" describe the content of documents in our corpus. On this account, the amount of information we get about this content from a word is inversely proportional to its probability of occurrence. In other words, the least informative word in our corpus is the one that occurs approximately uniformly across the corpus. For example, the word THE occurs at about the same frequency across every document in the collection; its probability of occurrence in any one document is almost uniform. We gain the least information about the document's contents from observing it.1"                                                                                                                              What is  Salton and McGill [Salton and McGIll, 1983], following Dennis    "information"? [Dennis, 1967], use Shannon's classic binary logarithm to measure the amount of information conveyed by each word's occurrence in bits and noise to be the absence of information:  pk = Pr(keyword k occurs)                         (3.14)  Infok= -log pk                                           (3.15)  Noisek = -log(l/p*)                                  (3.16)  Note that our evidence about the probability of a keyword occurring comes from statistics of how frequently it occurs. We must compare how frequently a keyword occurs in a particular document, relative to how frequently it occurs throughout the entire collection. We can calculate the expected noise associated with a keyword across the corpus, and from this we can infer its remaining signal. Signal then becomes another measure we can use to weight the frequency of occurrence of the keyword 84      FINDING OUT ABOUT  fid  Informative word  Noise word  liliiliii  FIGURE 3.6 Hypothetical Word Distributions  document:  (Noisek) = (pjfclog(l/pjt)gt; =  Signalk = log fk - Noisek = fkd * Signalk  A  Po%fd        (3.17)  (3.18) (3.19)  Two hypothetical distributions, for a noise word and a useful index term, are shown in Figure 3.6. A noise word is equally likely to occur anywhere; its distribution is nearly uniform. On the other hand, if all of the occurrences of a keyword are localized in a few documents (conveniently clustered together in the cartoon of Figure 3.6) and mostly zero everywhere else, this is an informative word. You've learned something about the document's content when you see it.  3.3.7 Inverse Document Frequency  Up to this point, we've been concerned only with the total number of times a word occurs across the entire corpus. Karen Sparck Jones has observed that, from a discrimination point of view, what we'd really like to know is the number of documents containing a keyword. This thinking underlies the inverse document frequency (IDF) weighting:  The basis for IDF weighting is the observation that people tend to express their information needs using rather broadly defined, frequently occurring terms, whereas it is the more specific* i.e., WEIGHTING AND MATCHING AGAINST INDICES       85  low-frequency terms that are likely to be of particular importance in identifying relevant material. This is because the number of documents relevant to a query is generally small, and thus any frequently occurring terms must necessarily occur in many irrelevant documents; infrequently occurring terms have a greater probability of occurring in relevant documents - and should thus be considered as being of greater potential when searching a database. [Sparck Jones and Willett, 1997, p. 307]  Rather than looking at the raw occurrence frequencies, we will aggregate occurrences within any document and consider only the number of documents in which a keyword occurs. IDF proposes, again using a "statistical interpretation of term specificity" [Sparck Jones, 1972], that the value of a keyword varies inversely with the log of the number of documents in which it occurs:  (  ugt;kd = fkd * (log ^P + l)                     (3.20)  where Dk is as defined in Equation 3.12.  The formula in Equation 3.20 is still not fully specified in that the count Dk must be normalized with respect to a constant Norm. We could normalize with respect to the total number of documents in the corpus [Sparck Jones, 1972; Croft and Harper, 1979]; another possibility is to normalize against the maximum document frequency (i.e., the most documents any keyword appears in) [Sparck Jones 1979a; Sparck Jones, 1979b]:  XT            iNDoc          or                           /aoi\  Norm = \               _                               (3.21)  [argmaxkDk  Today the most common form of IDF weighting is that used by Robertson and Sparck Jones [Robertson and Sparck Jones, 1976], which normalizes with respect to the number of documents not containing a keyword (NDoc รณ Dk) and adds a constant of 0.5 to both numerator and denominator to moderate extreme values:  /      (NDoc -Djt) + 03\  = fkd * ^log รณ^^5-----j         (3.22)  
 5.5 Hypertext Linkages  A new class of information representation, described in Chapter 4 as the hypertext data structure, is evolving on the Internet. Hypertext data structures must be generated manually although user interface tools may simplify the process. Very little research has been done on the information retrieval aspects of hypertext linkages and automatic mechanisms to use the information of item pointers in creating additional search structures. In effect, hypertext linkages are creating an additional information retrieval dimension. Traditional items can be viewed as two dimensional constructs. The text of the items is one dimension representing the Automatic Indexing                                                                                   133  information in the items. Imbedded references are a logical second dimension that has had minimal use in information search techniques. The major use of the citations has been in trying to determine the concepts within an item and clustering items (Salton-83). Hypertext, with its linkages to additional electronic items, can be viewed as networking between items that extends the contents. To understand the total subject of an item it is necessary to follow these additional information concept paths. The imbedding of the linkage allows the user to go immediately to the linked item for additional information. The issue is how to use this additional dimension to locate relevant information.  The easiest approach is to do nothing and let the user follow these paths to view items. But this is avoiding one of the challenges in information systems on creating techniques to assist the user in finding relevant information. Looking at the Internet at the current time there are three classes of mechanisms to help find information: manually generated indexes, automatically generated indexes and web crawlers (intelligent agents). YAHOO (http://www.yahoo.com) is an example of the first case where information sources (home pages) are indexed manually into a hyperlinked hierarchy. The user can navigate through the hierarchy by expanding the hyperlink on a particular topic to see the more detailed subtopics. At some point the user starts to see the end items. LYCOS (http://www.lycos.com) and Altavista (http://www.altavista.digital.com) automatically go out to other Internet sites and return the text at the sites for automatic indexing. Lycos returns home pages from each site for automatic indexing while Altavista indexes all of the text at a site. None of these approaches use the linkages in items to enhance their indexing.  Webcrawlers (e.g., WebCrawler, OpenText, Pathfinder) and intelligent agents (Coriolis Groups' NetSeeker√¥) are tools that allow a user to define items of interest and they automatically go to various sites on the Internet searching for the desired information. They are better described as a search tool than an indexing tool that a priori analyzes items to assist in finding them via a search.  What is needed is an index algorithm for items that looks at the hypertext linkages as an extension of the concepts being presented in the item where the link exists. Some links that are for references to multi-media imbedded objects would not be part of the indexing process. The Universal Reference Locator (URL) hypertext links can map to another item or to a specific location within an item. The current concept is defined by the information within proximity of the location of the link. The concepts in the linked item, or with a stronger weight the concepts in the proximity of the location included in the link, need to be included in the index of the current item. If the current item is discussing the financial state of Louisiana and a hyperlink is included to a discussion on crop damage due to draughts in the southern states, the index should allow for a t4hif on a search statement including "droughts in Louisiana."  One approach is to view the hyperlink as an extension of the text of the item in another dimension. The index values of the hyperlieked item has a reduced weighted value from contiguous IqxI biased by the type of linkage. The weight of processing tokens appears: 134                                                                                              Chapters  Weighty = (a * Weighty +p*Weightk,,) * (y *Linki,k)  where Weighty is the Weight associated with processing token "j" in item"i" and processing token "1" in item "k" that are related via a hyperlink. Link^ is the weight associated with strength of the link. It could be a one-level link that is weak or strong, or it could be a multilevel transitive link, a, (3 and y are weighting/normalization factors. The values could be stored in an expanded index structure or calculated dynamically if only the hyperlink relationships between items are available.  Taking another perspective, the system could automatically generate hyperlinks between items. Attempts have been made to achieve this capability, but they suffer from working with static versus dynamic growing databases or ignoring the efficiency needed for an operational environment (Allan-95, Furuta-89, Rearick-91). Kellog and Subhas have proposed a new solution based upon document segmentation and clustering (Kellog-96). They link at both the document and document sub-part level using the cover-coefficient based incremental clustering method (C2ICM) to generate links between the document (document sub-parts) pairs for each cluster. (Can-95). The automatic link generation phase is performed in parallel with the clustering phase. Item pairs in the same cluster are candidates for hyperlinking (link-similarity) if they have a similarity above a given threshold. The process is completed in two phases. In the first phase the document seeds and an estimate of the number of clusters is calculated. In the second phase the items are clustered and the links are created. Rather than storing the link information within the item or storing a persistent link ID within the item and the link information externally, they store all of the link information externally. They create HTML items on demand. When analyzing links missed by their algorithm, three common problems were discovered:  misspellings or multiple word representations (e.g., cabinet maker and cabinetmaker)  parser problems with document segmentation caused by punctuation  errors (lines were treated as paragraphs and sentences)  problems occurred when the definition of sobparts (smaller sentences) of items was attempted  A significant portion of errors came from parsing rather than algorithmic problems. This technique has maximum effectiveness for referential links which naturally have higher similarity measures.  
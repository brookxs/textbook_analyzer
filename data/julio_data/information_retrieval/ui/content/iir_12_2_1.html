    Using query likelihood language models in IR Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model . In it, we construct from each document in the collection a language model . Our goal is to rank documents by , where the probability of a document is interpreted as the likelihood that it is relevant to the query. Using Bayes rule (as introduced in probirsec), we have: (98)        The most common way to do this is using the multinomial unigram language model, which is equivalent to a multinomial Naive Bayes model (page 13.3 ), where the documents are the classes, each treated in the estimation as a separate ``language''. Under this model, we have that: (99)    For retrieval based on a language model (henceforth LM ), we treat the generation of queries as a random process. The approach is to Infer a LM for each document. Estimate , the probability of generating the query according to each of these document models. Rank the documents according to these probabilities.     
 7.2.1 Feature Selection  When you are thinking about how you classifiy your email, keywords contained in your email are almost certainly some of the features you think of first. Recall, however, that the keyword vocabulary can be very large. Using this feature space, then, individual document representations will be very sparse. In terms of the vector space model of Section 3.4, many of the vector elements will be zero. To use Littlestone's lovely expression, "Irrelevant attributes abound" [Littlestone, 1988], and so it should come as no surprise that his learning techniques are especially appropriate in the FOA learning applications discussed in Section 7.5.3.  Efforts to control the keyword vocabulary and make the lexical features as meaningful as possible are therefore important preconditions for good classification performance. For example, name-tagging techniques (cf. Section 6.6.1) that reliably identify proper names can provide valuable classification features. A proper name tagger would be one that was especially sophisticated about capitalization, name order, and abbreviation conventions. When both people's proper names and institutional names (government agencies, universities, corporations, etc.) are capitalized, the recognition of complex, multitoken phrases becomes possible.  In part because of the difficult issues lexical, keyword-based representations entail, it is worth thinking briefly about some of the alternatives. There are also less-obvious features we might use to classify documents. Meta-data associated with the document, for example, information about its date and place of publication, is one possibility. Geographic place information associated with a document can also be useful; cf. Section 6.6.1. Finally, recall the bibliographic citations that many documents contain (cf. Section 6.1). The set of references one 260      FINDING OUT ABOUT  document makes to others (representable as links in a graph) can be used as the basis of classification in much the same way as its keywords. In summary, while keywords provide the most obvious set of features on which classifications can be based, these result in very large and sparse learning problems. Other features are also available and may be more useful. It is important to note, however, that careful Bayesian reasoning about dependencies among keyword features is a very difficult problem, as discussed in Section 5.5.7. Attempting to extend this inference to include other heterogeneous types of features must be done carefully.  Distribution-Based Selection  Because our typical assumption has been that keywords occur independently, it should come as no surprise that when we try to reduce from the full set of all keywords in the Vocab to a smaller set, a good way to decide which features are most useful is to pick those that are most independent of any others. That is, we can hope that two keywords that are statistically dependent can be merged into a single one.  As mentioned in Section 3.3.6, entropy captures the amount of randomness in (or uncertainty about) some random variable:  H(X) = -J^Pr(X = x) log (Pr(x))                 (7.1)  When the distribution of a random variable X is conditionally dependent on that of a second random variable Y, the conditional entropy of X on Y (a.La. post-Y entropy of X) can be similarly defined [Papoulis,  1991, pp. 549-54]:  H(X\Y) = -     Â£     Pr(x\y)log(Pr(X = x\Y = y))       (7.2)  X=x, Y=y  If knowledge of values of Y reduces our uncertainty about the distribution of Xy it is natural to think that Y informs us about X. Mutual  information I captures this notion:  - H(X\Y)  Pr(x)log(Pr(x)) + J2Pr(x\y)log(Pr(x\y))   (7.3)  x                                                  x, y  This information is mutual in the sense that it is a symmetric relation ADAPTIVE INFORMATION RETRIEVAL       261  between the two variables; Y tells us as much about X as X does about Y:I(X,Y) = I{Y,X).  If the mutual information I(k{} kj) between all pairs of keywords in K isknown,vanRijsbergen,p. 123 recommends selecting the maximum spanning tree, which maximizes the total information across all edges of the tree:  the MST... incorporates the most significant of the dependencies between the variables subject to the global constraint that the sum of them should be a maximum, [van Rijsbergen, p. 123 ]  Note, however, that the mutual information statistic has an intrinsic bias toward keyword pairs kj, kj in which the individual keyword frequencies fi and fj are intermediate. The post-Y entropy of X can only reduce it [Papoulis, 1991]:  H(X\Y)lt; H(X)                           (7.4)  In terms of keyword frequencies, then, the mutual information of a pair of keywords is limited by their marginal entropies. This implies that very rare and very common keywords are "penalized" with respect to the mutual information measure. For this reason, it is worthwhile considering the relation between mutual information as a measure of keyword interdependencies and the eigenstructure analysis (cf. Section 5.2.3) of the keyword cross-correlation matrix /.  Mutual information considers the full joint probability between the two keywords, while methods like singular value decomposition (SVD) and principal component analysis (PCA) consider only the crosscorrelation matrix. When random variables happen to fit a normal distribution, these correlation statistics are sufficient, but that is unlikely in the case of our keywords. A second important difference is that correlationbased methods construct new feature variables, out of linear combinations of the initial keyword tokens. Mutual information-based methods (or at least van Rijsbergen, p. 123's MST-based construction) select the best variables from a constant set.  Selection Based on "Fit" to a Classification Task  Rather than using distributional statistics among the keywords themselves as the basis for feature selection, it is also possible to look for those 262      FINDING OUT ABOUT  features that are most "fit" with respect to some classification task [Lewis and Hayes, 1994] (cf. Section 7.4).  Both mutual information and correlation statistics can be used in either supervised or unsupervised learning situations, considering either the mutual information J(fc, c) of each keyword k with respect to the class c in the former case or Fisher's linear discriminant [Duda and Hart, 1973, p. 114] in the latter.  Using the classification performance criterion, Yang and Pederson considered a wide range of potential measures and found that simply measuring /*, the document frequency of keyword fc, provided an effective measure over potential keyword features [Yang and Pedersen, 1997]. In fact, using document frequency as a criterion, they were able to remove 98 percent of all keywords while retaining (even improving slightly!?) classification performance. Given the efficient way in which fk can be collected (relative to Ml, x2gt; and other potential measures), the level of performance maintained by such aggressive dimensionality reduction is indeed striking. But because the features selected are sensitive to the particular classification task considered, their utility for other purposes may be suspect. Distribution-based selection methods may therefore be more robust.   
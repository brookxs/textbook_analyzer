 4.3.7 Multiple Retrievals across Varying Queries  It should come as no surprise, given the wide range of activities in which FOA is a crucial component, that there is enormous variability among the kinds of queries produced by users. The next step of our construction  is therefore to go beyond a single query to consider the performance of a system across a set of queries. 130      FINDING OUT ABOUT  0.00  0.00         0.10         0.20         0.30         0.40         0.50         0.60         0.70         0.80  Recall  FIGURE 4.14 Multiple Queries, Fixed Recall Levels  0.90  1.00  One obvious dimension to this variability concerns the "breadth" of the query: How general is it? If the Rel set for a query is known, this can be quantified by generality, comparing the size of Rel to the total number of documents in the corpus:  Generality  =  NDoc  (4.10)  There are many other ways in which queries can vary, and the fact that different retrieval techniques seem to be much more effective on some types of queries than others makes this a critical issue for further research. For now, however, we will treat all queries interchangeably but consider average performance across a set of them.  Figure 4.14 juxtaposes two Re/Pre curves corresponding to two queries. Query 1 is as before, and Query 2 is a more specific query, as evidenced by its lower asymptote. Even with these two queries, we can see that, in general, there is no guarantee that we will have Re/Pre data points at any particular recall level This necessitates interpolation of data points at desired recall levels. The typical interpolation is done at prespecified recall levels, for example, 0, 0.25, 0.5, 0.75, and 1.0. As van Rijsbergen, p. 152 discusses, a number of interpolation techniques ASSESSING THE RETRIEVAL       131  1.0' i-      / Logical, stem  0.8  Numeric, stem  |  0.6    u    Â£  0.4    0.2     1               1                \               1   0 0.2    0.4    0.6    0.8 1.0   Recall   FIGURE 4.15 11-Point Average Re/Pre Curves. From [Salton and Lesk, 1968]. Reproduced with permission of Prentice-Hall  are available, each with its own biases. Because each new relevant document added to our retrieved set will produce an increase in precision (causing the saw-tooth pattern observed in the graph), simply using the next available data point above a desired recall level will produce an overestimate, while using the prior data point will produce an underestimate.  With preestablished recall levels, we can now juxtapose an arbitrary number of queries and average over them at these levels. For 30 years the most typical presentation of results within the IR community was the 11point average curves, like those shown in Figure 4.15 [Salton and Lesk, 1971; Salton and Lesk, 1968]. (These data happen to show performance on the ADI corpus of Boolean versus weighted retrieval methods; they include only the last 10 data points.)  It is not uncommon to see research data reduced even further. If queries are averaged 2X fixed recall levels and then all of these recall levels are averaged together, we can produce a single number that measures retrieval system performance. Note the serious bias this last averaging across recall levels produces, however. It says that we are as interested in how well the system did at the 90 percent recall level as at 10 percent!? Virtually all users care more about the first screenful of hits they retrieve than the last.  This motivates another way to use the same basic recall/precision data. Rather than measuring at fixed recall levels, statistics are collected at the 10-, 25-, and 50-document retrieval levels. Precision within the first 10 or 15 documents is arguably a much closer measure of standard browser effectiveness than any other single number. 132       FINDING OUT ABOUT  All such atempts to reduce the Re/Pre plot to a single number are bound to introduce artifacts of their own. In most cases the full Re/Pre curve picture is certainly worth a thousand words. Plotting the entire curve is straightforward and immediately interpretable, and it lets viewers draw more of their own conclusions.  We must guard against taking our intuitions based on this tiny example (with only 25 documents in the entire corpus) too seriously when considering results from standard corpora and queries. For example, our first query had fully 20 percent of the corpus as relevant; even our second query had 8 percent. In a corpus of a million documents, this would mean 80,000 of them were relevant!? Much more typical are queries with a tiny fraction, perhaps 0.001 percent, relevant. This will mean that the precision asymptote is very nearly zero. Also, we are likely to have many more relevant documents, resulting in a much smoother curve.   
 5.5.4 Binary Independence Model  Perhaps the simplest model proceeds by imagining binary, independent features; it is conventionally called (surprise!) the Binary Independence  Model (BIMJ [RobertsonandSparck Jones, 1976; van Rijsbergen, 1977]. First, the binary assumption is that all the features xt are binary. This is not a very restrictive assumption and is used only to simplify the derivation. MATHEMATICAL FOUNDATIONS       171  The much bigger assumption is that the documents' features occur independently of one another. We have discussed the problems with such an assumption before. Van Rijsbergen, p. 120 quotes J. H. Williams's expression of the paradox:  The assumption of independence of words in a document is usually made as a matter of mathematical convenience. Without the assumption, many of the subsequent mathematical relations could not be expressed. With it, many of the conclusions should be accepted with extreme caution. [Williams, 1965, emphasis in original]  The key advantage it allows is that the probability of a feature vector x becomes simply the product of the marginal probabilities of the individual features:  Pr(x\Rel) = f{Pr(x,- \Rel)                      (5.40)  i  Very convenient - and very unrealistic.^"                                                   Maybe this  Applying this decomposition to our odds calculation gives:                assumption  isn't so bad?  Odds(Rel | x) = Odds(Rel) ï fT Pr{xi^  It will be convenient to introduce the variables pi and qi to capture the probabilities that feature xx is present, given that a document is or is not relevant:  xi = l\Rel)                             (5.42)  qi = Pr(xi = l\Rd)                             (5.43)  The complementary probabilities concerning documents in which the feature is absent can also be defined easily:  1 - pi = Pr{Xi = 0 \Rel)                         (5.44)  1 - ca = Pr(xi = 0 \Rel)                         (5.45)  These definitions break the product into two portions, the first having to do with those features that are present in a particular document 172       FINDING OUT ABOUT  Rel Rel  Q                   D  FIGURE 5.6 Random Variables Underlying Binary Independence Model  and the second with those that are not:  Odds(Rel\x) = Odds(Rel)   Yl ~    Yl ^~~^          (5*46)  Recall that both queries and documents live within the same vector space defined over the features x\. The two products of Equation 5.46 (defined in terms of presence or absence of a feature in a document) can be further broken into four subcases, depending on whether the features occur in the query. We next make another "background" assumption concerning all the features x\ that are not in both the query and the document of current interest; we assume that the probability of these features being present in relevant and irrelevant documents is equal: pi = qi. In other words, for those terms we don't care about (because they don't affect this query/document comparison), we are happy to think that their occurrence is independent of their relevance.  Consider the sets D and Q shown in Figure 5.6 defined in terms of those features Xj present and absent in the document and query, respectively.* Regrouping the two products of Equation 5.46 into four products created by the two sets D and Q, the ^ terms cancel except in the intersection of the query and document (where the feature Xj is present in both) and in Q\ Dgt; the set difference of Q less D:  Odds(Rel\x) = Odds(Rel)'     Yl    ~ '    Yl    ^"^      (5*47)  xteDf\Q 4*     XteQ\D l ~   * Apologies for the unfortunate overuse of the same letter 'q' for denoting both the set Q of features contained in the query and the probability qt of the presence of a feature in Irrelevant documents, but there is no intended, direct connection between these two quantities. MATHEMATICAL FOUNDATIONS       173  In the retrieval situation we will exploit the sparseness that makes it much more efficient to keep track of where a feature does occur (x,- = 1) than all the places it does not (x\ = 0). Since the second product is defined over all the features of q except those in d, if we are careful to "premultiply" each feature in their intersection by a reciprocal, we can then safely multiply everything in the query by the same ratio:  Odds(Rel\x) = Odds(Rel)- U  ]ólL-    T\  The next section will show the utility of separating the last term, which depends on features of the document in question, from the first two, which do not, as part of an online retrieval calculation.  But first, it is worthwhile considering how we might attempt to estimate some of the required statistics [Robertson and Sparck Jones, 1976]. Fuhr [Fuhr, 1992], for example, considers the retrospective case when we have relevance feedback from a user who has evaluated each of the top N documents in an initial retrieval and has found R of these to be relevant (as well as evaluating all the N ó R remaining and found them to be irrelevant!). If a particular feature x\ is present in n of the retrieved documents with r of these relevant, then this bit of relevance feedback provides reasonable estimates for pi and q\\  P, = J                                          (5.49)   
 3.3.2 Word Occurrence as a Poisson Process  When the words contained in a corpus are ranked and shown to be distributed according to a Zipfian distribution, an obvious but important observation can be made: The most frequently occurring words are not really about anything. Words like NOT, OF, THE, OR, TO, BUT, and BE obviously play an important functional role, as part of the syntactic structure of sentences, but it is hard to imagine users asking for documents about OF or about BUT. Define function words to be those that have only (!) a syntactic function, for example, OF, THE, BUT, and distinguish them from content words, which are descriptive in the sense that we're interested in them for the indexing task. This is one of the first - but most certainly not the last - examples FOA makes using a priori determinations of a word's semantic utility based on its statistical properties. For example, we might hope that function words occur randomly throughout arbitrary text, while content words do not. One ubiquitous 74      FINDING OUT ABOUT  model of randomness is as a Poisson process, used in the past to model things like:  ï  raisins' distribution across slices of bread; or  ï  misprints' distribution across printed pages; or  ï  the distribution of people's birthdays across days of the year.  In the case of our documents, we'll start with a slightly simpler Bernoulli model wherein we imagine an author making binary decisions, picking a keyword k with probability p^ Then in a document of length L the probability that a keyword was selected exactly n times in document d is:  Pr(/w = n) = (L) (pk)n(l - pk)l~n                  (3.6)  In other words, we'd expect it to occur an average of pk - L times in a document of length I.  As I ógt; oo and p ó+ 0 (and the mean value X = p ï L ó* 1), the Poisson distribution:  Pr(/W = n) = ói-L                           (3.7)  converges to this same distribution. We will generally be interested in a large set of parameters A.fcgt; each corresponding to a particular keyword  L If we imagine a Bernoulli-like experiment, where individual function  words are placed with low probability and observed across the many "experiments" of words occurring in documents, we can expect that a particular word k will occur n times in a randomly selected document according to a Poisson distribution. (Because documents are of different lengths, we must also take care to normalize them all to the same number of experiments.)  As an example of how a Poisson model might be applied to good use, work pioneered by Bookstein and Swanson in the mid-1970s proposed that function words are distributed according to a relatively constant Poisson distribution, while content words are not [Bookstein and Swanson, 1974; Bookstein and Kraft, 1977, Croft and Harper, 1979]. That is, when a keyword is found in a document, it is for one of two possible reasons: Either it just happens (randomly) to be there, or it really WEIGHTING AND MATCHING AGAINST INDICES       75  means something. Robertson and Walker [Robertson and Walker, 1994] distinguish the latter elite occurrences of a keyword:  We hypothesize that occurrences of a term in a document have a random or stochastic element, which nevertheless reflects a real but hidden distinction between those ... "elite" documents which are about the concept represented by the term and those which are not. We may draw an inference about eliteness from the term frequency, but this inference will of course be probabilistic. Furthermore, relevance (to a query which may of course contain many concepts) is related to eliteness rather than directly to term frequency, which is assumed to depend only on eliteness. [Robertson and Walker, 1994, p. 233, underline not in original]           ?1 s 2  In addition to discriminating function from content words, the Poisson model has been used to measure the degree to which a content word is effective as a keyword for a document [Robertson and Walker, 1994]. If we assume that a potential keyword effectively describes some documents in a corpus but occurs at the level of chance throughout the rest of the corpus, the distribution of this keyword across the corpus can be described as the mixture of a Poisson process with some other distribution.  The so-called two-Poisson model models both distributions (i.e., one over the Rel documents that could accurately be characterized as about this keyword and a second over the rest of the Rel documents, which are not) as Poisson, but with distinct means Xlw and X2wy with the superscripts 1 and 2 referring to the Rel and Rel distributions, respectively. One advantage of assuming that both distributions are Poisson and that we only need to discriminate between two classes (relevant versus nonrelevant) is that a single-parameter pTe\ = Pr(Relevance) controls the probability that the word w is relevant:  about w | k occurrences of w  (3.8)  This probability can then be used as part of a decision theoretic model related to the costs of indexing too many or too few documents with a keyword w (cf. Section 5.5.6). 76      FINDING OUT ABOUT   
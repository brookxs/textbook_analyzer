 8.1.1 WWW Crawling  One important way in which Web search engines extend beyond the  notions of FOA presented here concerns the crawlers that feed them. In all of our discussions, the corpus was imagined to be a static object. For WWW search engines, the underlying set of documents that are to be indexed and made available to users is constantly changing. Further, the task of quickly, reliably, and exhaustively visiting all WWW-linked pages is a fundamental task in and of itself. One good, accessible example of  2 aslgeeves.com/ CONCLUSIONS AND FUTURE DIRECTIONS       295  0.4  8gt;  gt;  o o  ÃŸgt;  s  0.3  0.2  0.1  HotBot AltaVista Northern  Excite   infoSeek   Lycos Light  FIGURE 8.1 Crawler Coverage. From [Lowrence and Giles, 1998].  Reproduced with permission of American Association for the  Advancement of Science  crawler code is provided by the LibWWW Robot,3 part of the WWW Consortium (W3C) LibWWW distribution. A Perl-based crawler interface4 has also been developed by Gisle Aas; ParallelUserAgent,5 developed by Marc Langheinrich, is another Perl alternative.  Naive WWW users often seem to have the tacit belief that every Web crawler is aware of (i.e., has indexed) every document on the Web. More sophisticated users know that there is a certain lag time between the posting of a new page and its inclusion in the Web search engine's index. But the fundamental omissions by most search engine crawlers are still underappreciated. The most concrete data in this respect is due to a recent experiment done by Lawrence and Giles [Lawrence and Giles, 1998], shown in Figure 8.1. Using a statistical extrapolation from the mismatch of documents found by one of the six most important search engines suggests that at that time the Web contained approximately 320 million pages. Of this total, even the best search engine was able to capture only about a third of those documents.  The ecology of these various search engines and their co-evolutionary technological responses to one another create an extremely dynamic situation. Danny Sullivan edits an excellent newsletter, Search Engine Watch,6 that does nothing but track changes in the volatile  3 www.w5.org/Robot/  4 www.Mnpro.no/lwp/  a' http: //www.inf. ethz.ch/-laxigpieta/BaraUelUA/ 296       FINDING OUT ABOUT  marketplace of search engines and portals. The search engine business and supporting technologies can be expected to continue to foment for some time to come. In asymptote, however, current notions of search engines will go extinct for two basic reasons: Their methods do not scale to the Internet, and they only get in the way.  Search Engines Don't Scale  Scalability is a major issue limiting the effectiveness of search engines. The factors contributing to the problem are the large size of the WWW, its rapid growth, and its highly dynamic nature. In order to keep indexes up to date, crawlers periodically revisit every indexed document to see what has been changed, moved, or deleted. Heuristics are used to estimate how frequently a document is changed and needs to be revisited, but the accuracy of such statistics is highly volatile. Moreover, crawlers attempt to find newly added documents either exhaustively or based on user-supplied URLs. Yet Lawrence and Giles have shown that the coverage achieved by search engines is at best around 33 percent, and that coverage is anticorrelated with currency - the more complete an index, the staler the links [Lawrence and Giles, 1998]. More importantly, such disappointing performance comes at high costs in terms of the load imposed on the Net [Eichmann, 1994].  This becomes an important reason for investigating search agents for the WWW like those described in Section 7.6. Online agents do not have a scale problem because they search through the current environment and therefore do not run into stale information. On the other hand, they are less efficient than search engines because they cannot amortize the cost of a search over many queries.  Disintermediation  Section 8.2.1 will discuss FOA as a particular type of "language game." In brief, the FOA language game is played by three players: the text's author, its readers, and the search engine. Authors have something to say and an audience they are trying to say it to. They attempt to characterize their content to intermediates (book publishers, journal editors, WWW search engines) in ways that capture "markets" for what they have to say. Readers have an information need and some ideas about where to look for writings that might satisfy it. These readers sometimes (and now CONCLUSIONS AND FUTURE DIRECTIONS       297  much more often than in the past) characterize their information need to intermediates (librarians, paralegals, WWW search engines) in hopes of being shown documents likely to be relevant to their information needs.  The second fundamental flaw of current search engines, then, is that they are and will forever be only mediators; they neither produce content nor consume it directly. The search engine is caught in the middle of the other two players. It must somehow make the correspondence between the languages used by writers and readers. If it plays its part of the FOA language game well, it reliably connects readers with writers.  Said another way, search engines are simply noise in the channel between author and reader. If they are doing their jobs effectively, they should disappear as transparent background to facilitate easy communication of rich messages. The difficulty browsing users currently experience as they attempt to FOA documents on the WWW makes it clear just how far current search engines are from this ideal.  Traditionally, authors have made conventional assumptions about how their readers would find them. They would sell their book to a publisher, and part of this economic relationship involved the publisher putting its distribution channels at the services of the author. For magazine and newspaper reporters, as well as for fiction authors, periodical publications provided a regular audience for a magazine of contributions. Textbook authors would favor publishers with extensive connections with educational institutions. Scientists would submit articles to peer review under the supervision of editors for professional societies. In every case, multiple levels of mediation between the author and the reader are assumed by the author.  Even if the WWW were only a new technological substrate on which all of these conventional activities occurred, we might expect the level of confusion now present as search engines cross everyone's wires. But it seems likely that the change is even more fundamental: The number of content-producers (writers) is rapidly approaching the number of content-consumers (readers)! Never before has the machinery of producing and distributing media been as widely available as it is today. Our collective expectations as to just what documents are clt;out there," not to mention the care and authority with which they have been authored, are in terrific flux. 298      FINDING OUT ABOUT  Authors trying to be heard through this cacophony must fundamentally rethink their assumptions of how their content will be published. The most obvious examples of this are author-created keyword meta-tags. A wide range of meta-tags are now in use - ranging from ones that carry intellectual property information to ones that carry "decency" ratings; the HTML standard in fact allows an open-ended set of such tags to support any number of additional attributes of the document. Two meta-tags, however, are especially important from the perspective of FOA. The KEYWORDS meta-tag is designed to contain (the author's recommendation for) content descriptors, and the DESCRIPTION meta-tag to provide a proxy string. Both provide explicit mechanisms for authors to convey additional meaning in their writings, beyond words that happen to be in the text of the document itself. They have the additional advantage of being free of any morphological and weighting heuristics used by a particular search engine. Of course, this additional expressive power on the part of authors also makes it at least possible for them (or their Web masters) to attempt to spoof search engines with meta-tags designed simply to draw users to the page. Like much of the law concerning the WWW, exactly what constitutes "good faith" use of meta-tags is a matter of great debate (a recent example is Playboy v. Terri Welles7). Whether in good faith or not, attempts by authors to express themselves clearly are currently compromised by the refusal of search engines to publicly commit to some basic standards of crawling and indexing behavior. Their wide variety in operation, compounded by opaque descriptions of how each works, currently makes articulate expression by  Just how do         an author impossible.^ It is no wonder that searching users become  Alta Vista,            confused.  HotBot, ...  work?!   
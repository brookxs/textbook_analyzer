 7.6.1 Exploiting Linkage for Context*  All samples of language, including the documents indexed by Web search engines, depend heavily on shared context for comprehension. A document's author makes assumptions, often tacit, about the intended audience, and when this document appears in a "traditional" medium (conference proceedings, academic journal, etc.), it is likely that typical readers will understand it as intended. But one of the many things the Web changes is the huge new audience it brings to documents, many of whom will not share the author's intended context.  But because most search engines attempt to Index indiscriminately across the entire WWW, the global word frequency statistics they collect can only reflect gross averages. The utility of an index term, as a  * Portions previously published in |Menczer and Belew, 2000J. 280      FINDING OUT ABOUT  discriminator of relevant from irrelevant items, can become a muddy average of its application across multiple distinct subcorpora within which these words have more focused meaning [Steier and Belew, 1994a; Steier and Belew, 1994b].  Hypertext information environments like the Web contain addiQueryingfor tional structure information [Chakrabarti et al., 1998a] .t This linkage link topology information is typically exploited by browsing users. But linkage topology- the "spatial" structure imposed over documents by their hypertext links to one another - can be used to generate a concrete notion of context within which each document is understood: Two documents and the words they contain are imagined to be in the same context if they are close together in this space. Even in unstructured portions of the Web, authors tend to cluster documents about related topics by letting them point to each other via links. Such linkage topology is useful inasmuch as browsers have a better-than-random expectation that following links can provide them with guidance. If this were not the case, browsing would be a waste of time.  This suggests that agents (infobots, spiders, etc.) that navigate over such structural links might be able to discover this context. For example, agents browsing through pages about ROCK CLIMBING and ROCK 'N ROLL should attribute different weights to the word ROCK, depending on whether the query they are trying to satisfy is about music or sports. Where an agent is situated in an "environment" (neighborhood of highly interlinked documents) provides it with the local context within which to analyze word meanings - a structured, situated approach to polisemy. The words that surround links in a document provide an agent with valuable information to evaluate links and thus guide its path decisions a statistical approach to action selection.  The idea of decentralizing the index-building process is not new. Dividing the task into localized indexing, performed by a set of gatherers, and centralized searching, performed by a set of brokers, has been suggested since the early days of the Web by the Harvest project [Bowman et al., 1994]. Web Watcher [Armstrong et al., 1995] and Letizia [Lieberman, 1997] are agents that learn to mimic the user by looking over his or her shoulder while browsing. Then they perform look-ahead searches and make real-time suggestions for pages that might interest the user. Fab [Balabanovic, 1997] and Amalthaea [Moukas and Zacharia, 1997] are multiagent adaptive filtering systems inspired by genetic ADAPTIVE INFORMATION RETRIEVAL       281  algorithms, artificial life, and market models. Term weighting and relevance feedback are used to adapt a matching between a set of discovery agents (typically search engine parasites) and a set of user profiles (corresponding to single- or multiple-user interests).  Here we focus on InfoSpiders, a multiagent system developed by Fillipo Menczer [Menczer et al, 1995; Menczer, 1997; Menczer and Belew, 1998; Menczer, 1998; Menczer and Belew, 2000]. In InfoSpiders an evolving population of many agents is maintained, with each agent browsing from document to document online, making autonomous decisions about which links to follow and adjusting its strategy. Populationwide dynamics bias the search toward more promising areas and control the total amount of computing resources devoted to the search activity. Basic features of the algorithm are discussed, and then an example of how these agents perform as searchers through a hypertext version of the Encyclopaedia Britannica are presented herein.   
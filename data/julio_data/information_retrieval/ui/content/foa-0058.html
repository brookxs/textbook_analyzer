 3.7 Computing Partial Match Scores  With length normalization to the side, we can concentrate on the main calculation of matching, summing the weight products of terms shared  by query q and document d:  Wkd ' wkq                                  (3.45)  kÄ(qnd)  This mathematical characterization hides a number of messy details associated with actually computing it. We will need to make efficient use of two data structures in particular. The first is the inverted index (recall 98      FINDING OUT ABOUT  Figure 3.5). The critical feature of this organization is that we can, for each term in the query, find the set of all document postings associated with it. In particular, the freq statistic maintained with each posting allows us to compute the weight wu we need for each. Even with these efficiencies, however, the need to consider every document posting for every query term can create a serious processing demand, and one that must be satisfied immediately - the users are waiting!  Because the elements of the sum can be reordered arbitrarily, we will consider a partial ranking (a.k.a. filtering, or pruning) algorithm that attempts to include the dominant elements of the sum but ignore small, inconsequential ones [Salton, 1989; Buckley and Lewit, 1985; Harman andCandela, 1990].  The fact that Wkq and Wkd va*T considerably suggests that, by ordering the inclusion of products into the sum, we may be able to truncate this process when they become smaller than we care to consider.  We therefore begin by sorting the terms in decreasing order of query weights Wkq- Considering terms in this order means we can expect to accumulate the match score beginning with its largest terms. Then the fact that our list of postings was similarly (and not coincidentally) ordered by decreasing frequency means that:  (V; gt; i) Wkdi gt; Wkd}                              (3.46)  Once these weights diminish below some threshold rfl^, we can stop going down the postings list. (In fact, it may be that the weight associated with the very first posting is too small and we can ignore all weights associated with this term.)  The second important data structure is an accumulator queue in which each document's match score is maintained. Because each query term may add an additional term to the match score for a particular document, these accumulators will keep a running total for each document. For moderate corpus sizes, it may not be unreasonable to allocate an accumulator for each document, but this can demand too much memory for very large corpora. Define NAccum to be the number of accumulators we are willing to allocate. Then one obvious way to restrict this set is to only allocate an accumulator when a document's score becomes significant, again in comparison to some threshold xtmeTt. Because we will be processing query terms in decreasing Wkq order and heuristically WEIGHTING AND MATCHING AGAINST INDICES       99  value the space associated with new accumulators more than the slightly longer time to run down posting lists a bit further, we can assume that  ^¶insert *gt; ^add%  Picking appropriate values for these two thresholds is something of a black art, but Persin [Persin, 1994] reports one especially careful experiment in which both are made proportional to the most highly matched document's accumulator A* (i.e., A* is the maximum match score in any document's accumulator):  Tinsert  == ?}insert   *   A                                    (3.47)  *add = riadd ' A*                                     (3.48)  Persin's experiments suggest that values ofr}insert = 0.07, rjadd = 0.001 give retrieval effectiveness near that of full matches (i.e., considering all query-document term products) while minimizing NAccum?                   Partial  These two thresholds divide the range of possible query-document    matching isn't term products into three conditions:                                                       Kr .       .  ugt;kq gt; insert      Always add; create new accumulator A4        wor] s   etter if necessary  lt; ^insert   Add only if accumulator Ad already exists £ iad      Ignore; move on to the next query term  We want to remain flexible with respect to both long and short queries, so we will assume that the query weights Wkq are precomputed and passed to our ranking procedure.* Using our definition for Wkd and focusing first on the TimeTt threshold:  X\nsen ( fkd ' Wk ) ' Wkq  gt; insert  fkd   gt;  ^  idf  r  Jkd  Vmsert  tdfk wkq  * However, it Is generally more efficient to retain "raw" frequency counts in the postings (as integers) rather than length-normalized weights wkd (a$ floats). This means that length normalization of documents is performed after the partial ranking match has been completed. 100      FINDING OUT ABOUT  Algorithm 3.1 Partial Ranking  prank(qry[],A) {  // qry = vector of lt; keyword, weight gt; pairs  // A = queue of lt; docid, score gt; accumulators  // initially empty and returned with most highly ranked  Sort(qry,  descending WgtCmp);  for (q G query ) {  ?insert = iV insert ' A*)/{q.Wgt ï ldfq)\  ?add = (rjadd ' A*)/(q.wgt - idfq); for (fpe fpost(q)){ if (fp.freq lt;= xadd)  break;  newscore = fp-freq ï idfq ï q.wgt; for (dp e dpost(/ª){  fhd = hashfind(docTbl, docno); if(fndORdwgtgt; rimert) { if(fnd)  fhd.score + = newscore; else{  hashadd(docTbl, docno); if(length(A) gt; NAccum)  pop(A); insertQ(newscore, docno. A);  }  A* = max(A*,ftid.score,newscore);  }  } } //eo-posting loops }//eo-qryloop  This finally becomes an operational question we can apply with respect to each posting's frequency /jy. Note that this threshold must be updated every time we move to a new term of the query. Of course, the computation of t^ proceeds similarly. WEIGHTING AND MATCHING AGAINST INDICES       101  All the basic features of a partial ranking algorithm are now in place, and a pseudo-code sketch is shown in Algorithm 3.1. It also includes a few minor complications. First, a hashtable is required to find accumulators associated with a particular docno. Second, the set of accumulators Ad is described as a queue, but it must be slightly trickier than most: It must maintain them in order so that only the top NAccum are maintained, and it must support lengthO queries and a popO function when it is full. Nondeterministic skip lists [Pugh, 1990] are recommended for this purpose [Cutting and Pedersen, 1997].                                                    ?59 69 7, 8S 9   
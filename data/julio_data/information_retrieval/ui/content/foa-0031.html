 2.5.1 Basic Algorithm  We now assume that:  ï  prior technology has successfully broken our stream of characters, our large corpus, into a set of documents;  ï  within each document we have identified individual tokens; and  ï  noise word tokens have been identified.  Then the basic flow of what we will call the postdoc function operates as follows (see Algorithm 2.1).  2  www.apple.com/slierlock/  3  ftp://ftp.os.cornell.eciu/pub/smart/  4  www.mcis.iiiiit.e4u.au/nig/welcome.iitml :lt; www.searGtitools.eom/tools/tooIs.litm.l  * www.glue.umcL edu/dlrg/filter/software,html " http://www.cse.urad.edu/-rik/FOA/ 52       FINDING OUT ABOUT  Algorithm 2.1 Basic Algorithm  for every doc in corpus  while (token = getNonNoiseToken) if(StemP)  token = stem (token) Save Posting(token,doc) in Tree  for every token in Tree  Accumulate ndoc(token), totfreq(token) Sortp G Postings(token)  descending docfreq(p) order write tokengt;ndoc,totfreq,Postings  For every document in the corpus we will iterate through a loop until weVe exhausted every token in that document. So let's call getNonNoiseToken a routine that repeatedly builds tokens from the document's stream, does whatever character assessments are required, checks it against a negative dictionary, and returns a token. If stemming is to be applied, well stem the word at this point. Then we will save a posting for that token's occurrence in that document. A posting is simply a correspondence between a particular word and a particular document, representing the occurrence of that word in that document.* That is, we have a document in front of us and it contains a set of tokens. We are now going to build a representation for each token that tells all of the documents in which ones it occurs. For each keyword we will maintain the token itself as the key used for subsequent access and the head of a linked list of all postings, each containing the document number and the number of occurrences of the keyword in that document. A sketch of these data structures is shown in Figure 2.4.  After going through every document in the corpus in this fashion, we have a large collection of postings. Here we recommend splay trees as an appropriate data structure for these keywords and their postings. In the C implementation shown in Algorithm 2.2, the installTermO  Implementation    function inserts a new posting into the Terms tree J details  * Well discuss either data we might also keep with the posting later; c£. Section 2.5.2. EXTRACTING LEXICAL FEATURES       53  KW_INV token totdoc  head      "aardvarck" 20 65       totfreq I c POSTING locno       freq next     ^ª         o  FIGURE 2.4 Basic Postings Data Structures  Algorithm 2.2 postdoc.c Details  void postDoc (int docno, FILE *docf, long int bpos, long int epos, char *proxy){  GetTermString(proxyPos, Noise, MaxTermSize,newterm); GetTerm(docf, Noise, MaxTermSize,newterm); InstallTerm(newterm, docno, Terms);  During the processing of each document, it will prove important to know how many keywords are extracted from it. This will be known as the documents length, denoted lengths this quantity is important when normalizing documents of different lengths. One way to implement this computation is to maintain a small separate file doclend.d containing only this one number for each document.  When the set of documents has been exhausted, we need to write out this inverted representation to a file for subsequent processing. For every token in the splay tree (typically the traversal will be in lexicographic order), we will organize all its postings. First, we count the number of occurrences of the keyword across all the documents in the corpus; we will call this variable totfreqk. A second, less obvious statistic we will maintain is how many documents contain this keyword; this variable will be called docfreqi. If there is exactly one occurrence of a 54      FINDING OUT ABOUT  KW token totdoc  wgt head  "aardvarck" 20 65 0.634   totfreq  FPOST  freq   dochd next   o o  FIGURE 2.5 Refined Postings Data Structures  keyword in each document, then these two numbers will be the same. But typically there are multiple occurrences of the same keyword in a single document and totfreq gt; docfreqk. Both variables will be important to us in determining appropriate weights for the Index relation (cf. Chapter 3).  After going through all of the documents and accumulating for each these two statistics, we must sort the postings in decreasing frequency order. The reason for this won't be apparent until we discuss the matching algorithms (cf. Section 3.5), but it turns out to be important that documents that use a keyword most often are at the beginning of the list.  Once the documents' postings have been sorted into descending order of frequency, it is likely that several of the documents in this list will have the same frequency, and we can exploit this fact to compress their representation. Figure 2.5 shows the POSTING list broken into a list of FPOST sublists, one for unique frequency count.   
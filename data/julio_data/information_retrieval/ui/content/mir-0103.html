 6.3.3    Modeling Natural Language Text is composed of symbols from a finite alphabet. We can divide the symbols in two disjoint subsets: symbols that separate words and symbols that belong to words. It is well known that symbols are not uniformly distributed. If we consider just letters (a to z), we observe that vowels are usually more frequent than most consonants. For example, in English, the letter V has the highest frequency. A simple model to generate text is the binomial model In it, each symbol is generated with a certain probability. However, natural language has a dependency on previous symbols. For example, in English, a letter ki' cannot appear after a letter V and vowels or certain consonants have a higher probability 146        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES of occurring. Therefore, the probability of a symbol depends on previous symbols. We can use a finite-context or Markovian model to reflect this dependency. The model can consider one, two, or more letters to generate the next symbol. If we use k letters, we say that it is a fc-order model (so the binomial model is considered a 0-order model). We can use these models taking words as symbols. For example, text generated by a 5-order model using the distribution of words in the Bible might make sense (that is, it can be grammatically correct), but will be different from the original. More complex models include finite-state models (which define regular languages), and grammar models (which define context free and other languages). However, finding the right grammar for natural language is still a difficult open problem. The next issue is how the different words are distributed inside each document. An approximate model is Zipf's Law [847, 310], which attempts to capture the distribution of the frequencies (that is, number of occurrences) of the words in the text. The rule states that the frequency of the i-ih most frequent word is l/ie times that of the most frequent word. This implies that in a text of n words with a vocabulary of V words, the z-th most frequent word appears n/{ie Hy{9)) times, where Hy{9) is the harmonic number of order 9 of V, defined as so that the sum of all frequencies is n. The left side of Figure 6.2 illustrates the distribution of frequencies considering that the words are arranged in decreasing order of their frequencies. The value of 6 depends on the text. In the most simple formulation, 0 = 1, and therefore Hy{9) = O(logn). However, this simplified version is very inexact, and the case 9 gt; 1 (more precisely, between 1.5 and 2.0) fits better the real data [26]. This case is very different, since the distribution is much more skewed, and Hy{6) = 0(1). Experimental data suggests that a better model is fc/(c-h£)* where c is an additional parameter and k is such that all frequencies add to n. This is called a Mandelbrot distribution [561]. Since the distribution of words is very skewed (that is, there are a few hundred words which take up 50% of the text), words that are too frequent, such as stopwords, can be disregarded. A stopword is a word which does not carry meaning in natural language and therefore can be ignored (that is, made not searchable), such as 4a,' 'the,' 'by,' etc. Fortunately the most frequent words are stopwords and therefore, half of the words appearing in a text do not need to be considered. This allows us, for instance, to significantly reduce the space overhead of indices for natural language texts. For example, the most frequent words in the TREC-2 collection (see Chapter 3 for details on this reference collection and others) are "the,1 fcof,' 'and,1 "a,1 "to1 and in1 (see also Chapter 7). Another issue is the distribution of words in the documents of a collection. A simple model is to consider that each word appears the same number of times in every document.   However, this is not true in practice.   A better model is TEXT 147 Words Text size Figure 6.2    Distribution of sorted word frequencies (left) and size of the vocabulary (right). to consider a negative binomial distribution, which says that the fraction of documents containing a word k times is a + k-l k -a-k where p and a are parameters that depend on the word and the document collection. For example, for the Brown Corpus [276] and the word *said\ we have p = 9.24 and a = 0.42 [171]. The latter reference gives other models derived from a Poisson distribution. The next issue is the number of distinct words in a document. This set of words is referred to as the document vocabulary. To predict the growth of the vocabulary size in natural language text, we use the so-called Heaps' Law [352]. This is a very precise law which states that the vocabulary of a text of size n words is of size V = KnÆ = 0{nP), where K and /3 depend on the particular text. The right side of Figure 6.2 illustrates how the vocabulary size varies with the text size. K is normally between 10 and 100, and f3 is a positive value less than one. Some experiments [26, 42] on the TREC-2 collection show that the most common values for /3 are between 0.4 and 0.6. Hence, the vocabulary of a text grows sublinearly with the text size, in a proportion close to its square root. Notice that the set of different words of a language is fixed by a constant (for example, the number of different English words is finite). However, the limit is so high that it is much more accurate to assume that the size of the vocabulary is O(n) instead of O(l), although the number should stabilize for huge enough texts. On the other hand, many authors argue that the number keeps growing anyway because of typing or spelling errors. Heaps' law also applies to collections of documents because, as the total text size grows, the predictions of the model become more accurate. Furthermore, this model is also valid for the World Wide Web (see Chapter 13). The last issue is the average length of words. This relates the text size in 148        TEXT AND MULTIMEDIA LANGUAGES AND PROPERTIES words with the text size in bytes (without accounting for punctuation and other extra symbols). For example, in the different subcollections of the TREC-2 collection, the average word length is very close to 5 letters, and the range of variation of this average in each subcollection is small (from 4.8 to 5.3 letters). If we remove the stopwords, the average length of a word increases to a number between 6 and 7 (letters). If we take only the words of the vocabulary, the average length is higher (about 8 or 9). This defines the total space needed for the vocabulary. Heaps' law implies that the length of the words in the vocabulary increases logarithmically with the text size and thus, that longer and longer words should appear as the text grows. However, in practice, the average length of the words in the overall text is constant because shorter words are common enough (e.g. stop-words). This balance between short and long words, such that the average word length remains constant, has been noticed many times in different contexts, and can also be explained by a finite-state model in which: (a) the space character has probability close to 0.2; (b) the space character cannot appear twice subsequently; and (c) there are 26 letters [561]. This simple model is consistent with Zipf's and Heaps' laws. The models presented in this section are used in Chapters 8 and 13, in particular Zipf's and Heaps' laws.  
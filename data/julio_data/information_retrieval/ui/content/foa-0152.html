 7.4.2 Training a Classifier  The parameters © controlling the classifier could come from many places, but of course here we are concerned with learning them. In terms of the training set T:  T={(dhCi)}                             (7.14)  we seek the parameters © with the highest probability of having produced T. Depending on the model we employ, how we decompose 0 into its constituent parameters 0{ will differ.  One piece of this is easy to estimate: The prior probability of the class Pr(c) is how frequently one classification is observed in T relative to the others. Using a "twiddle" hat to distinguish estimates 0 of the probabilities from their true values 0:  S- = W\                                   lt;7'15)  Estimating 0ck is more complicated. The fact that both the multi variate Bernoulli and the multinomial models of document generation involve the product of the keywords' 0ck should make it obvious that our cumulative estimate will be very sensitive to any one of these values; consider, for example, what happens if even one of these terms is zero!  Within the Bayesian framework,^" these statistical sensitivities are ad~    Probabilists' dressed by providing priors for the underlying word-events of document    religious wars generation.  * For simplicity, we assume that the documents1 lengths are independent of the classes, i.e., that knowing a document's length tells us little about which class it should be in.  
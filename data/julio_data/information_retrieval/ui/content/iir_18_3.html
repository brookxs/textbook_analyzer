    Low-rank approximations We next state a matrix approximation problem that at first seems to have little to do with information retrieval. We describe a solution to this matrix problem using singular-value decompositions, then develop its application to information retrieval. Given an matrix and a positive integer , we wish to find an matrix of rank at most , so as to minimize the Frobenius norm of the matrix difference , defined to be (238)               low-rank approximation The singular value decomposition can be used to solve the low-rank matrix approximation problem. We then derive from it an application to approximating term-document matrices. We invoke the following three-step procedure to this end: Given , construct its SVD in the form shown in (232); thus, . Derive from the matrix formed by replacing by zeros the smallest singular values on the diagonal of . Compute and output as the rank- approximation to .     18.1   Theorem. (239)  End theorem. Recalling that the singular values are in decreasing order , we learn from Theorem 18.3 that is the best rank- approximation to , incurring an error (measured by the Frobenius norm of ) equal to . Thus the larger is, the smaller this error (and in particular, for , the error is zero since ; provided , then and thus ).   To derive further insight into why the process of truncating the smallest singular values in helps generate a rank- approximation of low error, we examine the form of : (240)   (241)   (242)              Exercises. Compute a rank 1 approximation to the matrix in Example 235, using the SVD as in Exercise 236. What is the Frobenius norm of the error of this approximation? Consider now the computation in Exercise 18.3 . Following the schematic in Figure 18.2 , notice that for a rank 1 approximation we have being a scalar. Denote by the first column of and by the first column of . Show that the rank-1 approximation to can then be written as . reduced can be generalized to rank approximations: we let and denote the ``reduced'' matrices formed by retaining only the first columns of and , respectively. Thus is an matrix while is a matrix. Then, we have (243) where is the square submatrix of with the singular values on the diagonal. The primary advantage of using (243) is to eliminate a lot of redundant columns of zeros in and , thereby explicitly eliminating multiplication by columns that do not affect the low-rank approximation; this version of the SVD is sometimes known as the reduced SVD or truncated SVD and is a computationally simpler representation from which to compute the low rank approximation. For the matrix in Example 18.2, write down both and .    
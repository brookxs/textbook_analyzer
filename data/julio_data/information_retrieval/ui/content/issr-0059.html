 4.4.2 N-Gram Data Structure  As shown in Figure 4.7, an n-gram is a data structure that ignores words and treats the input as a continuous data, optionally limiting its processing by interword symbols. The data structure consists of fixed length overlapping symbol  segments that define the searchable processing tokens. These tokens have logical linkages to all the items in which the tokens are found.   Inversion lists, document  vectors (described in Chapter 5) and other proprietary data structures are used to store the linkage data structure and are used in the search process. In some cases just the least frequently occurring n-gram is kept as part of a first pass search process (Yochum-85). Examples of these implementations are found in Chapter 5.  The choice of the fixed length word fragment size has been studied in many contexts. Yochum and D'Amore investigated the impacts of different values for "n/1 Fatah Comlekoglu (Comlekoglu-90) investigated n-gram data structures using an inverted file system for n=2 to n=26. Trigrams (n-grams of length 3) were determined to be the optimal length, trading off information versus size of data structure.    The Aquaintance System uses longer n-grams., ignoring word Chapter 4  boundaries. The advantage of n-grams is that they place a finite limit on the number of searchable tokens.  MaxSegn = (A,)n  The maximum number of unique n-grams that can be generated, MaxSeg, can be calculated as a function of n which is the length of the n-grams, and X which is the number of processable symbols from the alphabet (i.e., non-interword symbols).  Although there is a savings in the number of unique processing tokens and implementation techniques allow for fast processing on minimally sized machines, false hits can occur under some architectures. For example, a system that uses trigrams and does not include interword symbols or the character position of the n-gram in an item finds an item containing "retain detail" when searching for "retail" (i.e., all of the trigrams associated with "retail" are created in the processing of "retain detail"). Inclusion of interword symbols would not have helped in this example. Inclusion of character position of the n-gram would have discovered that the n-grams "ret," "eta," "tai," "ail" that define "retail" are not all consecutively starting within one character of each other. The longer the n-gram, the less likely this type error is to occur because of more information in the word fragment. But the longer the n-gram, the more it provides the same result as full word data structures since most words are included within a single n-gram. Another disadvantage of n-grams is the increased size of inversion lists (or other data structures) that store the linkage data structure. In effect, use of n-grams expands the number of processing tokens by a significant factor. The average word in the English language is between six and seven characters in length. Use of trigrams increases the number of processing tokens by a factor of five (see Figure 4.7) if interword symbols are not included. Thus the inversion lists increase by a factor of five.  Because of the processing token bounds of n-gram data structures, optimized performance techniques can be applied in mapping items to an n-gram searchable structure and in query processing. There is no semantic meaning in a particular n-gram since it is a fragment of processing token and may not represent a concept. Thus n-grams are a poor representation of concepts and their relationships. But the juxtaposition of n-grams can be used to equate to standard word indexing, achieving the same levels of recall and within 85 per cent precision levels with a significant improvement in performance (Adams-92). Vector representations of the n-grams from an item can be used to calculate the similarity between items.   
 Information System Evaluation                                                                  267  11.3 Measurement Example-TREC-ResuSts  Until the creation of the Text Retrieval Conferences (TREC) by the Defense Advance Research Projects Agency (DARPA) and the National Institute of Standards and Technology (NIST), experimentation in the area of information retrieval was constrained by the researcher's ability to manually create a test database. One of the first test databases was associated with the Cranfield I and II tests (Cleverdon-62, Cleverdon-66). It contained 1400 documents and 225 queries. It became one of the standard test sets and has been used by a large number of researchers. Other test collections have been created by Fox and Sparck Jones (Fox83, Sparck Jones-79). Although there has been some standard usage of the same test data, in those cases the evaluation techniques varied sufficiently so that it has been almost impossible to compare results and derive generalizations. This lack of a common base for experimentation constrained the ability of researchers to explain relationships between different experiments and thus did not provide a basis to determine system improvements (Sparck Jones-81). Even if there had been a better attempt at uniformity in use of the standard collections, all of the standard test sets suffered from a lack of size that prevented realistic measurements for operational environments.  The goal of the Text Retrieval Conference was to overcome these problems by making a very large, diverse test data set available to anyone interested in using it as a basis for their testing and to provide a yearly conference to share the results. There have been five TREC-conferences since 1992, usually held in the Fall. Two types of retrieval are examined at TREC: "adhoc" query, and "routing" (dissemination), In TREC-the normal two word Ltad hoc" is concatenated into a single word. As experience has been gained from TREC-1 to TREC-5, the details and focus of the experiments have evolved. TREC-provides a set of training documents and a set of test documents, each over 1 Gigabyte in size. It also provides a set of training search topics (along with relevance judgments from the database) and a set of test topics. The researchers send to the TREC-sponsor the list of the top 200 items in ranked order that satisfy the search statements. These lists are used in determining the items to be manually reviewed for relevance and for calculating the results from each system. The search topics are "user need" statements rather than specific queries. This allows maximum flexibility for each researcher to translate the search statement to a query appropriate for their system and assists in the determination of whether an item is relevant.  Figure 11.3 describes the sources and the number and size of items in the test database (Harrnan-95). Figure 11.3 also includes statistics on the number of terms in an item and number of unique terms in the test databases. The database was initially composed of disks I and 2. In later TRECs, disk 3 of data was added to focus on the routing tests. Figure 11.3b includes in the final column the statistics for the Cranfield test collection. Comparing the Cranfield collection to the contents of disk 1 shows that the TREC-test database is approximately 200 times larger and the 268                                                                                               Chapter 11  average length of the items is doubled. Also the dictionary size of unique words is 20 times larger. All of the documents are formatted in Standard Generalized Markup Language (SGML) with a Document Type Definition (DTD) included for each collection allowing easy parsing. SGML is a superset of HTML and is one of the major standards used by the publishing industry.  It was impossible to perform relevance judgments on all of the items in the test databases (over 700,000 items) to be used in recall and fallout formulas. The option of performing a random sample that would find the estimated 200 or more relevant items for each test search would require a very large sample size to be manually analyzed. Instead, the pooling method proposed by Sparck Jones was used. The top 200 documents based upon the relevance rank from each of the researchers was pooled, redundant items were eliminated and the resultant set was manually reviewed for relevance. In general one-third of the possible items retrieved were unique (e.g., out of 3300 items 1278 were unique in TREC-1) (Harman-93). This ratio also been shown to be true in other experiments (Katzer82). In TREC, each test topic was judged by one person across all of the possible documents to ensure consistency of relevance judgment.  The search Topics in the initial TREC-consisted of a Number, Domain (e.g., Science and Technology), Title, Description of what constituted a relevant item, Narrative natural language text for the search, and Concepts which were specific search terms.  The following describes the source contents of each of the disks shown in Figure 11.3 available for TREC analysis:  Diskl  WSJ - Wall street journal (1987, 1988, 1989)  AP    - AP Newswire (1989)  ZIFF - Articles from Computer Select disks (ZIFF-Davis Publishing)  FR    - Federal Register (1989)  DOE - Short Abstracts from DOE Publications  Disk 2  WSJ    - Wall Street Journal (1990, 1991, 1992)  AP      - AP Newswire (1988)  ZIFF   - Articles from Computer Select disks (ZIFF-Davis Publishing)  FR      - Federal register (1988)  Disk 3  SJM1M - San Jose Mercury News (1993) AP      - AP Newswire (1990)  ZIFF   - Articles from Computer Select disks (ZIFF-Davis Publishing) PAT   -U.S. Patents (1993) Information System Evaluation  269  Subset of collection WSJ (disks 12) AP ZIFF FR        (disks DOE Cranfield   SJMN (disk 3)   I2)  test      PAT (disk 3)  database  Size     of    Collection        (Mbytes)        (diskl) 270 259 245 ^62 186 1.5  (disk 2) 247 241 178 211    (disk 3) 290 242 349 245    Number of Records        (disk 1) 98,732 84,678 75,180 25,960 226,087 1400  (disk 2) 74,520 79,919 56,920 19,860    (disk 3) 90,257 78,321 161,021 6,711    Median Number        Terms per record        (diskl) 182 353 181 313 82 79  (disk 2) 218 346 167 315    (disk 3) 279 358 119 2896    Average Number        of Terms per record        (diskl) 329 375 412 1017 89 88  (disk 2) 377 370 394 1073    (disk 3) 337 379 263 3543    Total Number        of Unique Terms        (disk 1) 156,298 197,608 173,501 126,258  8226  Figure 11.3b TREC-Training and Adhoc Test Collection  Collection Source Size in MBytes Mean     Terms      pei record Median      Terms per record Total Records  ZIFF (disk 3) 249 263 119 161,021  FR (1994) 283 456 390 55,554  IR Digest 7 2,383 2,225 455  News Groups 237 340 235 102,598  Virtual Worlds 28 416 225 10,152  Figure 11.3a Routing Test Database (from TREC-5 Conference Proceedings to be published, Harmon-96)  Precision and recall were calculated in the initial TREC. To experiment with a measure called Relative Operating Characteristic (ROC) curves, calculation of Probability of Detection (same as Recall formula) and calculation of Probability of False Alarm (same as Fallout) was also tried. This use of a set of common evaluation formulas between systems allows for consistent comparison between different executions of the same algorithm and between different algorithms. The results are represented on Recall-Precision and Recall-Fallout graphs (ROC curves). Figure 11.4 shows how the two graphs appear. The x-axis plots the recall from zero to 1.0 based upon the assumption that the relevant items judged in the pooling technique account for all relevant items. The precision or fallout value at each of the discrete recall values is calculated based upon reviewing the items, in relevance 270  Chapter  rank score order, that it requires to reach that recall value. For example, assume there are 200 relevant items. A particular system, to achieve a recall of 40 per cent (.4) requiring retrieval of 80 of the relevant items, requires retrieving the top 160 items with the highest relevance scores. Associated with the Precision/Recall graph, for the x-axis value of .4, the y-axis value would be 80/160 or .5. There are sufficient sources of potential errors in generating the graphs, that they should only be used as relative comparisons between algorithms rather than absolute performance indicators. It has been proven they do provide useul comparative information.  In addition to the search measurements, other standard information on system performance such as system timing, storage, and specific descriptions on the tests are collected on each system. This data is useful because the TREC-objective is to support the migration of techniques developed in a research environment into operational systems.  TREC-5 was held in November 1996. The results from each conference have varied based upon understanding from previous conferences and new objectives. A general trend has been followed to make the tests in each TRECcloserto realistic operational uses of information systems (Harman-96).  0 .2 .4   .6   .8 1.0  Recall  0 .2 .4   .6   .8 1.0  Recall  Figure 11.4 Examples of TREC-Result Charts  TREC-1 (1992) was constrained by researchers trying to get their systems to work with the very large test databases. TREC-2 in August 1993 was the first real test of the algorithms which provided insights for the researchers into areas in which their systems needed work. The search statements (user need statements) were very large and complex. They reflect long-standing information needs versus adhoc requests. By TREC-3, the participants were experimenting with techniques for query expansion and the importance of constraining searches to passages within items versus the total item. There were trade offs available between manual and automatic query expansion and the benefits from combining results from multiple retrieval techniques. Some of the experiments were driven by the introduction of shorter and less complex search statements.  The "concept" field, which contained Information System Evaluation  271  terms related to the query that a user might be expected to be aware of, was eliminated from the search statements. This change was a major source for the interest into query expansion techniques. TREC-4 introduced significantly shorter queries (average reduction from 119 terms in TREC-3 to 16 terms in TREC-4) and introduced five new areas of testing called "tracks" (Harman-96). The queries were shortened by dropping the title and a narrative field, which provided additional description of a relevant item.  The multilingual track expanded TREC-4 to test a search in a Spanish test set of 200 Mbytes of articles from the "El Norte" newspaper. The interactive track modified the previous adhoc search testing from a batch to an interactive environment. Since there are no standardized tools for evaluating this environment, the TREC-5 goals included development of evaluation methodologies as well as investigating the search aspects. The database merging task investigated methods for merging results from multiple subcollections into a single Hit file. The confusion track dealt with corrupted data. Data of this type are found in Optical Character Reader (OCR) conversion of hardcopy to characters or speech input. The database for TREC-had random errors created in the text. Usually in real world situations, the errors in these systems tend not to be totally random but bursty or oriented towards particular characters. Finally, additional tests were performed on the routing (dissemination) function that focused on three different objectives: high precision, high recall and balanced precision and recall. Rather than ranking all items, a binary text classification system approach was pursued where each item is either accepted or rejected (Lewis-96, Lewis-95).  Adhoc Manual vs Automatic  0.00      0.10     0.20      0.30     0.40     0.50  Figure 11.5 TREC-1 Adhoc manual versus Automatic Query (from TREC-1 Conference Proceedings, page 15, Harmon-93) 272  Chapter 1  Routing Manual vs Automatic  0.00     0.10      0.20      0.30  0.40     0.50      0.60     0.70     0.80 Recall  0.90      1.00  Figure 11.6 TREC-1 Routing Manual versus Automatic Results (from TREC-1 Conference Proceedings, page 18, Harmon-93)  Insights into the advancements in information retrieval can be gained by looking at changes in results between TRECs mitigated by the changes in the test search statements. Adhoc query results from TREC-1 were calculated for automatic and manual query construction. Automatic query construction is based upon automatic generation of the query from the Topic fields. Manual construction is also generated from the Topic field manually with some machine assistance if desired. Figures 11.5 shows the Precision/Recall results top two systems for each hmethod. The precision values were very low compared to later TRECs. It also shows that there was very little difference between manual construction of a query and automatic construction.  Routing (dissemination) also allowed for both an automatic and a manual query construction process. The generation of the query followed the same guidelines as the generation of the queries for the adhoc process. Figure 11.6 shows the results from the top two manual and automatic routing systems. In this case, unlike the adhoc query process, the automatic query building process is better as shown by the results from the "fund" system.  By TREC-3 and TREC-4 the systems were focusing on how to accommodate the shorter queries. It is clear that if the shorter queries had been executed for TREC-1, the results would have been worse than those described. Figures 11.7 and 11.8 show the precision recall results for Automatic and Manual adhoc searches for TREC-3 and TREC-4 (Harman-96). The significant reduction in query size caused even the best algorithms shown in the figures to perform worse in TREC-4 than in TREC-3. The systems that historically perform best at TRECs (e.g., City University, London cityal, 1NQUERY - INQ201, Cornell University Information System Evaluation  273  a o   TREC-3 vs TREC-4 Automatic  0.8 ^    0.6    0.4    0.2    0.0    ó´ó cityal (TREC-3) -¶^ó INQ101 (TREC-3) ~-~=ió CmlEA (TREC-3) ó-#ó pircsl     (TREC-3)  óaó CmlAE (TREC^) ó´ó pircsl (TREC-4) ómó cityal (TREC-4) ó*ó INQ201 (TREC-4)  0.0  0.2  0.4                0.6  Recall  0.8  1.0  Figure 11.7 Automatic AdHoc Query results from TREC-3 and TREC-4 (from TREC-5 Conference Proceedings to be published, Harmon-96)  TREC-3 vs TREC-4 Manual  *¶-¶ INQ102     (TREC-3)  *-ï Brkly7      (TREC-3)  *ïó ASSCTV1 (TREC-3)  ï?.....pircs2        (TEIEC-3)  *ó CnQst2      (TREC-4)  ïó INQ202     (TREC-4)  ?ó BrklylO     (TREC-4)  ?ó pircs2        (TREC-4)  Recall  Figure 11.8 Manual AdHoc Query results fromTREC3 and TREC4  (from TREC-5 Conference Proceedings to be published, Harmon-96) 274                                                                                              Chapter 11  CrnlEA) all experienced 14 per cent to 36 per cent drops in retrieval performance. The manual experiments also suffered from a similar significant decrease in performance. The following is the legend to the Figures:  Crnl EA - Cornell University - SMART system - vector based weighted system pircsl     - Queens College - PIRCS system - spreading activation on 550  word subdocuments from documents  cityal     - City University, London - Okapi system - probabilistic term weighting INQ201 - University of Massachusetts - INQUERY system - probabilistic  weighting using inference netscitri2            -   RMIT, Australia  standard cosine with OKAPI measure  CnQst2   - Excalibur Corporation - Retrievalware - two pass weights brkly-10 - University of California, Berkley - logistic regression model based on  6 measures of document relevance ASSCTV1 - Mead Data Central, Inc. - query expansion via thesaurus  Even though all systems experienced significant problems when the size of the queries was reduced, a comparison between Figure 11.5 and Figures 11.7 and 11.8 shows a significant improvement in the Precision/Recall capabilities of the systems. A significant portion of this improvement occurred between TREC-1 and TREC-2.  By participating on a yearly basis, systems can determine the effects of changes they make and compare them with how other approaches are doing. Many of the systems change their weighting and similarity measures between TRECs. INQUERY determined they needed better weighting formulas for long documents so they used the City University algorithms for longer items and their own version of a probabilistic weighting scheme for shorter items. Another example of the learning from previous TRECs is the Cornell "SMART" system that made major modifications to their cosine weighting formula introducing a non-cosine length normalization technique that performs well for all lengths of documents. They also changed their expansion of a query by using the top 20 highest ranked items from a first pass to generate additional query terms for a second pass. They used 50 terms in TREC-4 versus the 300 terms used in TREC-3. These changes produced significant improvements and made their technique the best in the Automatic Adhoc for TREC-4 versus being lower in TREC-3.  In the manual query method, most systems used the same search algorithms. The difference was in how they manually generated the query. The major techniques are the automatic generation of a query that is edited, total manual generation of the query using reference information (e.g., online dictionary or thesaurus) and a more complex interaction using both automatic generation and manual expansion.  When TREC-introduced the more realistic short search statements, the value of previously discovered techniques had to be reevaluated. Passage retrieval (limiting the similarity measurement to a logical subsets of the item) had a major impact in TREC-3 but minimal utility in TREC-4. Also more systems began making use of    multiple algorithms and selecting the best combination  based upon Information System Evaluation  275  characteristics of the items being searched. A lot more effort was spent on testing better ways of expanding queries (due to their short length) while limiting the expanded terms to reduce impacts on precision. The automatic techniques showed a consistent degradation from TREC-3 to TREC-4. For the Manual Adhoc results, starting at about a level of .6, there was minimal difference between the TRECs. The Routing systems are very similar to the Adhoc systems. The   TREC-3 vs TREC-4 Routing Comparison  0.8     0.6     0.4     0.2 -0.0  i   ïï¶#-   INQI03 (TREC-3)  óª.....cityrl    (TREC-3)  --*.....pircs3    (TREC-3)  -~v~- CmlRR (TREC-3) ó-nó nyuir2    (TREC-3)  ó¶ó INQ203 (TREC-4) ï cityr2 (TREC-4) ó*ó pircsC (TREC-4) -*ó CmIRE (TREC-4) óió nyuge2 (TREC-4)  0.0  0.2  0.4  0.6  0.8  1.0  Recall  Figure 11.9 Routing results from TREC-3 and TREC-4 (from TREC-5 Conference Proceedings to be published, Harmon-96)  researchers tended to use the same algorithms with minor modifications to adjust for the lack of a permanent database in dissemination systems. Not surprisingly, the same systems that do well in the Adhoc tests do well in the Routing tests. There was significant diversity on how the search statements were expanded (see TREC-4 proceedings). Unlike the Adhoc results, the comparison of TREC-3 and TREC-4 Routing shown in Figure 11.9 has minimal changes with a slight increase in precision. The following is the legend for the Routing comparison for systems not defined in the adhoc legend:  nyuge2  - GE Corporate Research and New York University - use of natural language processing to identify syntactic phrases  nyuir2    - New York University - use of natural language processing to identify syntactic phrases 276  Chapter 11  cityr      - City University, London  As with the adhoc results, comparing Figure 11.6 with Figure 11.8 shows the significant improvement in Routing capability between TREC-1 and the later  Spanish TREC-4  0.2  0.0  ó  UCFSP1  ó  SIN010 ..... xerox-sp2  CralSE  ó  gmuauto ......BrklySP3  cilri-sp2 DCUSPO ACQSPA cmlmlO  0.0  Figure 11.10 Results of TREC-4 Spanish Track (from TREC-5 Conference Proceedings to be published, Harmon-96)  TRECs. TREC-5 results were very close to those from TREC-4 but the queries had become more difficult so actual improvements came from not seeing a degradation in the Precision/Recall and Routing graphs.  The multilingual track expanded between TREC-4 and TREC-5 by the introduction of Chinese in addition to the previous Spanish tests. The concept in TREC-5 is that the algorithms being developed should be language independent (with the exception of stemming and stopwords). In TREC-4, the researchers who spent extra time in linguistic work in a foreign language showed better results (e.g., INQUERY enhanced their noun-phrase identifier in their statistical thesaurus generator). The best results in came from the University of Central Florida, which built an extensive synonym list. Figure 11.10 shows the results of the Spanish adhoc search in TREC-4. In TREC-5 significant improvements in precision were made in the systems participating from TREC-4. In Spanish, the Precision-Recall charts are better than those for the Adhoc tests, but the search statements were not as constrained as in the ad hoc. In Chinese, the results varied significantly between the participants with some results worse than the adhoc and some better. This being the Information System Evaluation                                                                   277  first time for Chinese, it is too early to judge the overall types of performance to be expected. But for Spanish, the results indicate the applicability to the developed algorithms to other languages. Experiments with Chinese demonstrates the applicability to a language based upon pictographs that represent words versus an alphabet based language.  The confusion track was preliminary in TREC-4. By TREC-5 the test database had expanded by taking the Federal Register (250Mbytes), creating dvi image files and then running NIST OCR programs against it. This produced approximately 5 per cent corruption typical of OCR operations. A second dataset with closer to 20 per cent corruption was produced by down-sampling the images and redoing the OCR (Voorhees-96). A set of known item topics was created by selecting items that seemed to be unique and creating a description of them. These were used and the evaluation metric was the rank of the item in the Hit file. Most of the search systems used some version of n-gram indexing (see Chapter 4). The results are too preliminary to draw any major conclusions from them.  The results in TREC 8, held in November 1999 did not show any significant improvement over the best TREC 3 or TREC four results in this text for automatic searching. The manual searching did show some improvement because the user interaction techniques are improving with experience. One participant, Readware, did perform significantly better than the other participants. The major change with TREC 8 was the introduction of the Question/Answer track. The goal of the track is to encourage research into systems that return answers versus lists of documents. The user is looking for an answer to an information need and does not want to have to browse through long items to locate the specific information of interest.  The experiment was run based upon 200 fact based short answer questions. The participants returned a ranked list of up to five document-id/string location pairs for each query. The strings were limited to either 50 or 250 characters. The answers were judged based upon the proposed string including units if asked for (e.g., world's population) and for famous objects answers had to pertain to that specific object.  Most researchers processed the request using their normal search algorithms, but included "blind feedback" to increase the precision of the higher ranked hits. Then techniques were used to parse the returned document around the words that caused the hit using natural language techniques to focus on the likely strings to be returned. Most of the participants only tried to return the 250-character string range.  The TREC-series of conferences have achieved their goal of defining a standard test forum for evaluating information retrieval search techniques. It provides a realistic environment with known results. It has been evolving to equate closer to a real world operational environment that allows transition of the test results to inclusion of commercial products with known benefits. By being an open forum, it has encouraged participation by most of the major organizations developing algorithms for information retrieval search.  
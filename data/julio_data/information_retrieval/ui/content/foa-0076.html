 4.3.9 Test Corpora  By test corpora we refer to collections of documents that have associated with them a series of queries for which relevance assesments are available. One of the earliest such test sets was a collection of 1400 research papers on aerodynamics developed by C. Cleverdon in the mid-1960s, known as the Cranfield corpus [Cleverdon and Mills, 1963]. For most of the 1980s, a set of corpora known as CACM, CISI, INSPEC, MED, and NPL (sometimes referred to as the Cornell corpora) were developed, maintained, and distributed by Gerald Salton and his students at Cornell; it became the de facto standard for testing within the IR community. For some time, the most influential test corpora have been the TREC corpora l associated with the Text Retrieval Evaluation Conference meetings [Harman, 1995].  Table 4.3 gives a sample of statistics for a number of the most widely used corpora. One obvious trend is the increasing size of these collections over time. The Reuters corpus2 classification labels are invaluable for  ' Note that my notation deviates from BarteiTs somewhat. In particular, we assume here that Rank() increases from most- to least-highly ranked document, so that the first element of the hitlist has Rank = 1. potomac. ncsl. nlst. go v/ tree/  www.research.att.com/ lewis/reutersS 1578.html 136      FINDING OUT ABOUT  TABLE 4.3 Common Test Corpora  Collection       NDocs      NQrys     Size (MB)     Term/Doc     Q-DRelAss  ADI 82 35     AIT 2109 14 2 400 gt; 10,000  CACM 3204 64 2 24.5   CISI 1460 112 2 46.5   Cranfield 1400 225 2 53.1   LISA 5872 35 3    Medline 1033 30 1    NPL 11,429 93 3    OSHMED 34,8566 106 400 250 16,140  Reuters 21,578 672 28 131   TREC 740,000 200 2000 89-3543 Â£3 100,000  training classifiers (cf. Section 7.4). With our AIT corpus, the OSHMED [Hersh, 1994] is one of the few to provide multiple relevance assessments of the same (q, d) pair.  Figure 4.16 shows a sample query from the TREC experiments. For this query, and hundreds like it, considerable manual effort has gone into assessing whether documents in the TREC corpus should be considered "relevant." Note the way the "basic" query (Line 2) has been embellished with general and specific topical orientation (Lines 1 and 3), important terms and abbreviations have been explicated, etc. This is much more  1  gt;         Science and Technology  2 gt;         ADDS treatments  3 gt;         Document will mention a specific AIDS or ARC treatment.  4 gt;         To be r, a document must Include a reference to at least one specific potential Acquired  Immune Deficiency Syndrome (AIDS or AIDS Related Complex treatment.  5 gt;         1. Acquired Immune Deficiency Syndrome (AIDS, AIDS Related Complex (ARC  6 gt;        2. treatment, drug, pharmaceutical  7 gt;         3. test, trials, study  8 gt;         4. AJZT, TEA  9 gt;        5. Genentech, Burroughs-Wellcome  10 gt;       ARC - AIDS Related Complex  11  gt;      .  A set of symptoms similar to AIDS.  12  gt;       A2T - Azidothyinidine, a drug for the treatment of Acquired Immune Deficiency  Syndrome, its related pneumonia, and for severe AIDS Related Complex,  13 gt;       TPA - Tissue Plasminogen Activator - a blood clot-dissolving drug.  14 gt;       treatment - any drug or procedure used to reduce the debilitating effects of AIDS or  ARC.  FIGURE 4.16 TREC Query ASSESSING THE RETRIEVAL       137  information than most users typically provide, but it also allows much more refined assessment of systems' performance.  As the testing procedures of the TREC participants have developed over the years, multiple "tracks" have formed, corresponding to typical search engine usage patterns. The task on which we have focused throughout this section is termed ad hoc retrieval, in the sense that a constant corpus is repeatedly searched with respect to a series of ad hoc queries. This is distinguished from the routing task, which assumes a relatively constant standing set of queries (for example, corresponding to the interests of various employees of the same corporation). Then, an ongoing stream of documents is compared, with relevant documents routed to appropriate recipients.  More recently, a special type of routing termed filtering has also been considered. In the filtering task, the standing query is allowed to adapt to the stream of relevance feedback generated by the users as they receive and evaluate routed documents (cf. Section 7.3).   
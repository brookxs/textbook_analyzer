 5.1 Derivation of Zipf's Law for Random Texts  As before, we begin by defining a word to be any sequence of characters separated by spaces. Let us therefore consider an alphabet of M characters, interspersed with a specially designated space character 0. We will consider an especially simple model (similar to that used by many others [Li, 1992; Miller, 1957; Hill, 1970; Hill, 1974]) in which a random monkey generates words by hitting all keys - space and letters - with equal probability p:  p = pT{%) = Pr(A) ==...= Pr(Z) = ó^ó           (5.1)  M+ 1  We can use lexicographic trees to conveniently organize words of length fc, say, by the order in which the k characters occur prior to the terminating space, as shown in Figure 5.1 This shows a set of M + 1 trees, each rooted in the words' starting character. Leaf nodes at level k  149 150       FINDING OUT ABOUT  o  Pr("A")  FIGURE 5.1 Lexicographic Tree Underlying Zipfian Distribution  Double counting  spaces?!  are all labeled with the probability of the sequence of k ó 1 characters prior to the space occurring at level k.  One immediate observation is that Njt, the number of words W{ of length k or less, is:  Nk = Number(wi I i lt; k) = Y" Nf = M(1~M }           (5gt;2)  In an infinitely long sequence of characters generated according to Equation 5.1, we will expect to find a "word" Wk terminating at level k (i.e., a string of k unbroken nonspace characters bracketed by two spaces) with probability defined in terms of the independent character probabilities p:  L                         ,5.3)  We can compute c, the constant of proportionality, by including all the Mk words of length k and summing these probabilities over all possible  words (including unrealistic, infinitely long ones!):^"  ,                       (M+l)2  Mlcp        lgt;c  h    ï¶ï pk =  Next consider the rank of these words. Because the probability of a  word's occurrence is an exponentially decreasing function of its length, MATHEMATICAL FOUNDATIONS       151  we know that the M highest ranked words are the one-character words; next come the M2 two-letter words; and so on. Using Equation 5.2 we therefore know how the rank rgt; of all words wk terminating on level k must be bounded above and below:  Nfc-i lt; rklt;Nk  where f  denotes a compromise "average" rank for all the Mk equiprobable words.*  Note that Equations 5.4 and 5.5 define the words' probability and rank, respectively, in terms of the common metric k. As Li [Li, 1992] notes, Zipf's law is fundamentally about this transformation, from an exponential distribution onto a rank variable.  Solving both equations for k:  k=  ln(M+ 1)  i__ f 2(M ó l)f)t    i , _       V    M+l     +  ~            lnM  we can now set them equal and derive an expression for a word's probability in terms of its rank:  'z(M-\)fk M+ 1  ln(M+l)              lnM  1   /2(M-l)ffc  This has the functional form required by Mandelbrot's generalized Zipf's law (cf. Equation 3.2):  _        C Pk ~ (ft + B)´ where  1   /   M+l   V            M+l           ln(Af+l)  MV2(M     1)7   '         2(M     1)'                         P;  1   /   M+l   V MV2(M- 1)7   '  2(M- 1)'              lnM  * The model can be extended by replacing this simple average with distributional information, for example, incorporating realistic character frequency information. 152       FINDING OUT ABOUT  1.25  1.15  1.05  80                            100  0.95  FIGURE 5.2 a as Function of M, Number of Distinct Characters   
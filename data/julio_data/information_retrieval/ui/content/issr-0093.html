 6.3 Item Clustering  Clustering of items is very similar to term clustering for the generation of thesauri. Manual item clustering is inherent in any library or filing system. In this case someone reads the item and determines the category or categories to which it belongs. When physical clustering occurs, each item is usually assigned to one category. With the advent of indexing, an item is physically stored in a primary category, but it can be found in other categories as defined by the index terms assigned to the item.  With the advent of electronic holdings of items, it is possible to perform automatic clustering of the items. The techniques described for the clustering of terms in Sections 6.2.2.1 through 6.2.2.3 also apply to item clustering. Similarity between documents is based upon two items that have terms in common versus terms with items in common. Thus, the similarity function is performed between rows of the item matrix. Using Figure 6.2 as the set of items and their terms and similarity equation:  SIM(Item,, Itemj) = I (Term^ ) (TeraijjÂ£)  as k goes from 1 to 8 for the eight terms, an Item-Item matrix is created (Figure 6.9). Using a threshold of 10 produces the Item Relationship matrix shown in Figure 6.10. Document and Term Clustering  155   Item 1 Item 2 Item 3 Item 4 Item 5  Item 1  11 3 6 22  Item 2 11  12 10 36  Item 3 3 12  6 9  Item 4 6 10 6  11  Item 5 22 36 9 11   Figure 6.9 Item/Item Matrix   Iteml Item 2 Item3 Item 4 ItemS  Iteml  1 0 0 1  Item2 1  1 1 1  Item 3 0 1  0 0  Item4 0 1 0  3  Item5 1 1 0 1   Figure 6.10 Item Relationship Matrix  Using the Clique algorithm for assigning items to classes produces the following classes based upon Figure 6.10:  Class 1 = Item 1, Item 2, Item 5  Class 2 = Item 2, Item 3  Class 3 = Item 2, Item 4, Item 5  Application of the single link technique produces:  Class 1 = Item 1, Item 2, Item 5, Item 3, Item 4  All the items are in this one cluster, with Item 3 and Item 4 added because of their similarity to Item 2. The Star technique (i.e., always selecting the lowest nonassigned item) produces:  Class 1 - Item 1, Item 2, Item 5 Class 2 - Item 3, Item 2 Class 3 - Item4, Itera2, ItemS  Using the String technique and stopping when all items are assigned to classes produces the following:  Class 1 - Item 1, Item 2, Item 3  Class 2 - Item 4, Item 5 156                                                                                               Chapter 6  In the vocabulary domain homographs introduce ambiguities and erroneous hits. In the item domain multiple topics in an item may cause similar problems. This is especially true when the decision is made to partition the document space. Without precoordination of semantic concepts, an item that discusses "Politics" in "America" and "Economics" in "Mexico" could get clustered with a class that is focused around "Politics" in "Mexico."  Clustering by starting with existing clusters can be performed in a manner similar to the term model. Lets start with item 1 and item 3 in Class 1, and item 2 and item 4 in Class 2. The centroids are:  Class 1 = 3/2, 4/2, 0/2, 0/2, 3/2, 2/2, 4/2, 3/2 Class 2 = 3/2, 2/2, 4/2, 6/2, 1/2, 2/2, 2/2, 1/2  The results of recalculating the similarities of each item to each centroid and reassigning terms is shown in Figure 6.11.  Class 1         Class 2                          Assign  Class 1 Class 2 Class 2 Class 2 Class 2  Figure 6.11 Item Clustering with Initial Clusters  Finding the centroid for Class 2, which now contains four items, and recalculating the similarities does not result in reassignment for any of the items.  Instead of using words as a basis for clustering items, the Acquaintance system uses n-grams (Damashek-95, Cohen-95). Not only does their algorithm cluster items, but when items can be from more than one language, it will also recognize the different languages.   
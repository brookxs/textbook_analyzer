 2.5.4    Probabilistic Model In this section, we describe the classic probabilistic model introduced in 1976 by Roberston and Sparck Jones [677] which later became known as the binary independence retrieval (BIR) model.   Our discussion is intentionally brief and focuses mainly on highlighting the key features of the model. With this purpose in mind, we do not detain ourselves in subtleties regarding the binary independence assumption for the model. The section on bibliographic discussion points to references which cover these details. The probabilistic model attempts to capture the IR problem within a probabilistic framework. The fundamental idea is as follows. Given a user query, there is a set of documents which contains exactly the relevant documents and CLASSIC INFORMATION RETRIEVAL        31 no other. Let us refer to this set of documents as the ideal answer set. Given the description of this ideal answer set, we would have no problems in retrieving its documents. Thus, we can think of the querying process as a process of specifying the properties of an ideal answer set (which is analogous to interpreting the IR problem as a problem of clustering). The problem is that we do not know exactly what these properties are. All we know is that there are index terms whose semantics should be used to characterize these properties. Since these properties are not known at query time, an effort has to be made at initially guessing what they could be. This initial guess allows us to generate a preliminary probabilistic description of the ideal answer set which is used to retrieve a first set of documents. An interaction with the user is then initiated with the purpose of improving the probabilistic description of the ideal answer set. Such interaction could proceed as follows. The user takes a look at the retrieved documents and decides which ones are relevant and which ones are not (in truth, only the first top documents need to be examined). The system then uses this information to refine the description of the ideal answer set. By repeating this process many times, it is expected that such a description will evolve and become closer to the real description of the ideal answer set. Thus, one should always have in mind the need to guess at the beginning the description of the ideal answer set. Furthermore, a conscious effort is made to model this description in probabilistic terms. The probabilistic model is based on the following fundamental assumption. Assumption (Probabilistic Principle) Given a user query q and a document d3 in the collection, the probabilistic model tries to estimate the probability that the user will find the document d3 interesting (i.e., relevant). The model assumes that this probability of relevance depends on the query and the document representations only. Further, the model assumes that there is a subset of all documents which the user prefers as the answer set for the query q. Such an ideal answer set is labeled R and should maximize the overall probability of relevance to the user. Documents in the set R are predicted to be relevant to the query. Documents not in this set are predicted to be non-relevant This assumption is quite troublesome because it does not state explicitly how to compute the probabilities of relevance. In fact, not even the sample space which is to be used for defining such probabilities is given. Given a query qy the probabilistic model assigns to each document dj, as a measure of its similarity to the query, the ratio P{d3 relevant-to q)/P(d3 non-relevant-to q) which computes the odds of the document d3 being relevant to the query q. Taking the odds of relevance as the rank minimizes the probability of an erroneous judgement [282, 785]. Definition For the probabilistic model, the index term weight variables are all binary i.e., w^j £ {0,1}, Wi,q Ä {0,1}. A query q is a subset of index tenn^ Let R be the set of documents known (or initially guessed) to be relevant Let R be the complement of R (i.e., the set of non-relevant documents). Let P(R\dj) 32        MODELING be the probability that the document dj is relevant to the query q and P(R\dj) be the probability that dj is non-relevant to q. The similarity sim(d3,q) of the document dj to the query q is defined as the ratio Using Bayes' rule, .   ,_     .      P(dj\R)xP{R) stm(d1)q) =       lx__!------ióiP(dj\R)xP(R) P(dj\R) stands for the probability of randomly selecting the document d3 from the set R of relevant documents. Further, P(R) stands for the probability that a document randomly selectedjrom the entire collection is relevant. The meanings attached to P(dj\R) and P(R) are analogous and complementary. Since P(R) and P(R) are the same for all the documents in the collection, we write, sim(dj,q)    -¶ Assuming independence of index terms, sim(dj,q)    ~         *   J P(kl\R) stands for the probability that the index term ki is present in a document randomly selected from the set R. P(ki\R) stands for the probability that the index term ki is not present in a document randomly selected from the set R. The probabilities associated with the set R have meanings which are analogous to the ones just described. Taking logarithms, recalling that P(ki\R) + P(kt\R) = 1, and ignoring factors which are constant for all documents in the context of the same query, we can finally write   which is a key expression for ranking computation in the probabilistic model. Since we do not know the set R at the beginning, it is necessaryjto devise a method for initially computing the probabilities P(k,\R) and P(kt\R). There are many alternatives for such computation. We discuss a couple of them below. CLASSIC INFORMATION RETRIEVAL        33 In the very beginning (i.e., immediately after the query specification), there are no retrieved documents. Thus, one has to make simplifying assumptions such as: (a) assume that P(ki\R) is constant for all index terms ki (typically, equal to 0.5) and (b) assume that the distribution of index terms among the non-relevant documents can be approximated by the distribution of index terms among all the documents in the collection. These two assumptions yield P(ki\R)    =    0.5 where, as already denned, m is the number of documents which contain the index term ki and N is the total number of documents in the collection. Given this initial guess, we can then retrieve documents which contain query terms and provide an initial probabilistic ranking for them. After that, this initial ranking is improved as follows. Let V be a subset of the documents initially retrieved and ranked by the probabilistic model Such a subset can be defined, for instance, as the top r ranked documents where r is a previously defined threshold. Further, let V% be the subset of V composed of the documents in V which contain the index term k%. For simplicity, we also use V and Vi to refer to the number of elements in these sets (it should always be clear when the used variable refers to the set or to the number of elements in it). For improving the probabilistic ranking, we need to improve our guesses for P(ki\R) and P(ki\R). This can be accomplished with the following assumptions: (a) we can approximate P(ki\R) by the distribution of the index term ki among the documents retrieved so far, and (b) we can approximate P(ki\R) by considering that all the non-retrieved documents are not relevant. With these assumptions, we can write, This process can then be repeated recursively. By doing so, we are able to improve on our guesses for the probabilities P(ki\R) and P(k{\R) without any assistance from a human subject (contrary to the original idea). However, we can also use assistance from the user for definition of the subset V as originally conceived. The last formulas for P(ki\R) and P(ki\R) pose problems for small values of V and Vz which arise in practice (such as V = 1 and Vt =0). To circumvent these problems, an adjustment factor is often added in which yields R)    =    Vi+∞-5 P(ki\R)   = V + l n% - Vj + 0.5 AT - V + 1 34        MODELING An adjustment factor which is constant and equal to 0.5 is not always satisfactory. An alternative is to take the fraction rii/N as the adjustment factor which yields Rgt;         N-V + l This completes our discussion of the probabilistic model. The main advantage of the probabilistic model, in theory, is that documents are ranked in decreasing order of their probability of being relevant. The disadvantages include: (1) the need to guess the initial separation of documents into relevant and non-relevant sets; (2) the fact that the method does not take into account the frequency with which an index term occurs inside a document (i.e., all weights are binary); and (3) the adoption of the independence assumption for index terms. However, as discussed for the vector model, it is not clear that independence of index terms is a bad assumption in practical situations.  
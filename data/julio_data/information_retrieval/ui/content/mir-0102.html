 6.3.2    Information Theory Written text has a certain semantics and is a way to communicate information. Although it is difficult to formally capture how much information is there in a given text, the distribution of symbols is related to it. For example, a text where one symbol appears almost all the time does not convey much information. Information theory defines a special concept, entropy, to capture information content (or equivalently, information uncertainty). If the alphabet has a symbols, each one appearing with probability pz (probability here is defined as the symbol frequency over the total number of symbols) in a text, the entropy of this text is defined as E= -' 2=1 In this formula the a symbols of the alphabet are coded in binary, so the entropy is measured in bits. As an example, for a = 2, the entropy is 1 if both symbols appear the same number of times or 0 if only one symbol appears. We say that the amount of information in a text can be quantified by its entropy. The definition of entropy depends on the probabilities (frequencies) of each symbol. To obtain those probabilities we need a text model. So we say that the amount of information in a text is measured with regard to the text model. This concept is also important, for example, in text compression, where the entropy is a limit on how much the text can be compressed, depending on the text model. In our case we are interested in natural language, as we now discuss.  
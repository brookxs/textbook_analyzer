 5.2.2 Vector Weighting  One of the earliest systems that investigated statistical approaches to information retrieval was the SMART system at Cornell University (Buckley-95, Salton-83). The system is based upon a vector model. The semantics of every item are represented as a vector. A vector is a one-dimensional set of values, where the order/position of each value in the set is fixed and represents a particular domain. In information retrieval, each position in the vector typically represents a processing token. There are two approaches to the domain of values in the vector: binary and weighted. Under the binary approach, the domain contains the value of one or zero, with one representing the existence of the processing token in the item. In the weighted approach, the domain is typically the set of all real positive numbers. The value for each processing token represents the relative importance of that processing token in representing the semantics of the item. Figure 5.2 shows how an item that discusses petroleum refineries in Mexico would be represented . In the example, the major topics discussed are indicated by the index terms for each column (i.e., Petroleum, Mexico, Oil, Taxes, Refineries and Shipping).  Binary vectors require a decision process to determine if the degree that a particular processing token represents the semantics of an item is sufficient to include it in the vector. In the example for Figure 5.2, a five-page item may have had only one sentence like "Standard taxation of the shipment of the oil to refineries is enforced." For the binary vector, the concepts of i4Tax" and "Shipment* are below the threshold of importance (e.g., assume threshold is 1.0) 112  Chapter 5  Petroleum Mexico   Oil    Taxes Refineries Shipping Binary                (       1,1,1,0,       1        ,    0      )  Weighted            (      2.8    ,     1.6   , 3.5,   .3   ,       3.1    ,     .1    )  Figure 5.2 Binary and Vector Representation of an Item  and they not are included in the vector. A weighted vector acts the same as a binary vector but it provides a range of values that accommodates a variance in the value of the relative importance of a processing token in representing the semantics of the item. The use of weights also provides a basis for determining the rank of an item.  The vector approach allows for a mathematical and a physical representation using a vector space model. Each processing token can be considered another dimension in an item representation space. In Chapter 7 it is shown that a query can be represented as one more vector in the same ndimensional space. Figure 5.3 shows a three-dimensional vector representation assuming there were only three processing tokens, Petroleum Mexico and Oil.  Oil  Figure 53 Vector Representation Automatic Indexing                                                                                    113  The original document vector has been extended by additional information such as citations/references to add more information for search and clustering purposes. There have not been significant improvements in retrieval using these techniques. Introduction of text generated from multimedia sources introduces a new rationale behind extending the vocabulary associated with an item. In the case where the text is not generated directly by an author but is the result of audio transcription, the text will contain a significant number of word errors. Audio transcription maps the phonemes that are in an audio item to the words most closely approximating those phonemes in a dictionary. Good audio transcription of broadcast news still has 15% of the words in error and conversational speech still has 40% or more of the words in error. These will be valid words but the wrong word. One mechanism to reduce the impact of the missing words is to use the existing database to expand the document. This is accomplished by using the transcribed document as a query against the existing database, selecting a small number of the highest ranked results, determining the most important (highest frequency) words across those items and adding those words to the original document. The new document will then be normalized and reweighted based upon the added words (Singhal-99). This technique reduced the losses in retrieval effectiveness from 15-27% to 7-13% when the audio transcriptions had high errors (40% or more). It has marginal benefit when the transcription has errors in the 15% range.  There are many algorithms that can be used in calculating the weights used to represent a processing token. Part of the art in information retrieval is deriving changes to the basic algorithms to account for normalization (e.g., accounting for variances in number of words in items). The following subsections present the major algorithms starting with the most simple term frequency algorithm.   
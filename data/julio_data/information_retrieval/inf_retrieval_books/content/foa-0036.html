 3.2 Remember Zipf  Looking at our corpus as a very long string of characters, something that even a monkey could generate, provides a useful baseline against which we can evaluate larger constructs.  Associate with each word w its frequency F{w)gt; the number of times it occurs anywhere in the corpus. Now imagine that we've sorted the vocabulary according to frequency so that the most frequently occurring word will have rank r = 1, the next most frequently used word will have r = 2, and so on.  George Kingsley Zipf (1902-50) has become famous for noticing that the distribution we find true of our corpus is in fact very reliably true of any large sample of natural language we might consider. Zipf {Zipf, 1949] observed that the words' rank-frequency distribution can be fit very closely by the relation:  F(r) = ó,    a´l,     C´0.1                    (3.1)  This empirical rule is now known as Zipf's law.  But why should this pattern of word usage, something we can reasonably expect to vary with author or type of publication, be so WEIGHTING AND MATCHING AGAINST INDICES       63  100,000 y     Log(Freq)  10,000 - 1000  - 100  - 10 - Log(Rank)  H  10                         100                       1000  FIGURE 3.1 Zipfian Distribution of AIT Words  10,000  universal? What's more, the notion of "word" used in this formula has varied radically: in tabulations of word frequencies by Yule and Thorndike, words were stemmed to their root form; Yule counted only nouns [Yule, 1924; Thorndike, 1937]. Dewey [Dewey, 1929] and Thorndike collected statistics from multiple sources; others were collected from a single work (for example, James Joyce's Ulysses). The frequency distribution for a small subset of (nonnoise words in) our AIT corpus is shown in Figure 3.1. Note the nearly linear, negatively sloped relation when frequency is plotted as a function of rank, and both are plotted on log scales.   
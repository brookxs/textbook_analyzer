 9.1.2    Performance Measures When we employ parallel computing, we usually want to know what sort of performance improvement we are obtaining over a comparable sequential program running on a uniprocessor.   A number of metrics are available to measure the performance of a parallel algorithm. One such measure is the speedup obtained with the parallel algorithm relative to the best available sequential algorithm for solving the same problem, defined as: __ Running time of best available sequential algorithm Running time of parallel algorithm I The processors used in a MIMD system may be identical to those used in SISD systems, or they may provide additional functionality, such as hardware cache coherence for shared memory. 232        PARALLEL AND DISTRIBUTED IR Ideally, when running a parallel algorithm on N processors, we would obtain perfect speedup, or S = N. In practice, perfect speedup is unattainable either because the problem cannot be decomposed into N equal subtasks, the parallel architecture imposes control overheads (e.g., scheduling or synchronization), or the problem contains an inherently sequential component. Amdahl's law [18] states that the maximal speedup obtainable for a given problem is related to /, the fraction of the problem that must be computed sequentially. The relationship is given by: Another measure of parallel algorithm performance is efficiency, given by: where S is speedup and N is the number of processors. Ideal efficiency occurs when 0=1 and no processor is ever idle or performs unnecessary work. As with perfect speedup, ideal efficiency is unattainable in practice. Ultimately, the performance improvement of a parallel program over a sequential program should be viewed in terms of the reduction in real time required to complete the processing task combined with the additional monetary cost associated with the parallel hardware required to run the parallel program. This gives the best overall picture of parallel program performance and cost effectiveness.  
 WEIGHTING AND MATCHING AGAINST INDICES       89  3.4.2 Vector Length Normalization  One good example involves the length of document and query vectors. So far, we have placed no constraint on the number of keywords associated with a document. This means that long documents, which, caeteris paribus, can be expected to give rise to more keyword indices, can be expected to match (more precisely, have nonzero inner product with) more queries and be retrieved more often. Somehow (as discussed in Section 1.4) this doesn't seem fair: The author of a very short document who worked hard to compress the meaning into a pithy few paragraphs is less likely to have his or her document retrieved, relative to a wordy writer who says everything six times in six different ways!  These possibilities have been captured by Robertson and Walker in a pair of hypotheses regarding a document's scope versus its verbosity:  Some documents may simply cover more material than others ... (the "Scope hypothesis"). An opposite view would have long documents like short documents but longer: in other words, a long document covers a similar scope to a short document, but simply uses more words (the "Verbosity hypothesis"). [Robertson and Walker, 1994, p. 235]  Once we have decided that about-ness is conserved across documents, all documents' vectors will have constant length. If we make the same assumption about the query vector, then all of the vectors will lie on the surface of a sphere, as shown in Figure 3.8. Without loss of generality, we will assume that the radius of the sphere is unity.  Making Weights Sensitive to Document Length  Unfortunately, this very simple normalization is often inadequate, as can be shown in terms of the inverse document frequency (IDF) weights discussed in Section 3.3.7. IDF weighting highlights the distinction between inter- and intradocument keyword occurrences. Because its primary focus is on discrimination among documents, Intradocument occurrences of the same keyword become insignificant. This makes IDF very sensitive to the definition of how document boundaries are defined (cf. Section 2.2), as suggested by Figure 3.9. 90      FINDING OUT ABOUT  kw3  kwl  FIGURE 3.8 Length Normalization of Vector Space  Egt;oc10G  Ñ     __    __     parallel __    _  ó    ó    ó    ó    ó    ó DOC2  parallel ó  .__  -*ó   ó  ó  Doqo  T  -Docjooo  FIGURE 3.9 Sensitivity of IDF to "Document" Size  The IDF weight that results from encapsulating more text within the same "document" is, in a sense* the converse of normalizing the number of keywords assigned to every document. In either case, the advantage of using the paragraph as our canonical document (cf. Section 1.4), and/or relying on all documents in the corpus to be of nearly uniform size (as in the AIT dissertation abstracts) is apparent. WEIGHTING AND MATCHING AGAINST INDICES       91  Probablity      Probability    /\ i of retrieval      of relevance ~T i  Pivot  03  B Pivoted normalization Old normalization/  i 77 1 /         a / slope = tan(aj Pivot  Document length                                                    Old normalization factor  FIGURE 3.10 Pivot-Based Document Length Normalization. From [Singhal et al., 1996]. Reproduced with permission of the Association of Computing Machinery.  The OKAPI retrieval system of Robertson et al. [Robertson and Walker, 1994] has proven itself successful (in retrieval competitions like TREC; cf. Section 4.3.3) by combining IDF weightings with corpus-specific sensitivities to the lengths of the documents retrieved. They propose that the average length of all documents in a corpus, digital libraryen, provides a "natural" reference point against which other documents' lengths can be compared.  Define Len(d) to be the number of keywords associated with the document. OKAPI then normalizes the first component of our weighting formula, keyword frequency, by a term that is sensitive to each document's deviation from this corpuswide average:  Wkd =  fki  (kgt;Len(d)/Dlen)+ fkd  log  (NDoc - Djb) + 0.5 Dk + 0.5  (3.30)  Robertson and Walker report that k w 1.0 ó 2.0 | Q J seems to work best, where | Q | is the number of query terms.  Singhal et al. [Singhal et alo 1996] approach the problem of length normalization by doing a post hoc analysis of the distributions of retrieved versus relevant documents (in the TREC corpus) as a function of their length. A sketch of typical curves is shown in Figure 3.10.A. The 92      FINDING OUT ABOUT  fact that these two distributions cross suggests a corpus-specific length normalization pivot value, p, below which match scores are reduced and above which they are increased. The amount of this linear increase or decrease, shown as the length normalization slope m of the length normalization function in Figure 3.10.B, is the second corpus-specific parameter of Singhal et al.'s model. Returning to the "generic" form of the weighting function originally given in Equation 3.13, the pivot-based length normalization is:  wkd =----------------¶--------------discrimk            (3.31)  (1 ó m) ï p + m- norm  where norm is whatever other normalization factor (e.g., cosine) is already in use; several possible values are given in the next section.  Both OKAPI and pivot-based document length normalizations rely on the specification of additional corpus-specific parameters (k\ and p, m, respectively). Although the addition of yet more "knobs to twiddle" is generally to be avoided in a retrieval system, recent experience with machine learning techniques suggests the possibility of training such parameters to best match each corpus. This approach is sometimes called a regression technique and is discussed more fully in Chapter 7.   
 2.1.9      Multimedia Queries  The user interface becomes far more complex with the introduction of the availability of multimedia items. All of the previous discussions still apply for search of the textual portions of a multimedia database. But in addition, the user has to be able to specify search terms for the other modalities. The current systems only focus on specification of still images as another search criteria. The still image could be used to search images that are part of an item. They also could be used to locate a specific scene in a video product. As described later, in the video modality, scene changes are extracted to represent changes in the information presentation. The scene changes are represented as a series of images. Additionally, where there is static text in the video, the current technology allows for OCRing the text (e.g., in the latest release of the VIRAGE system). The ability to search for audio as a match makes less sense as a user specification. To adequately perform the search you would have to simulate the audio segment and then look for a match. Instead audio sources are converted to searchable text via audio transcription. This allows queries to be applied to the text. But, like Optical Character Reading (OCR) output, the transcribed audio will contain many errors 38                                                                                               Chapter 2  (accuracies of 85-90% are the best that can be achieved from news broadcasts, conversational speech is in the range of 60%). Thus the search algorithms must allow for errors in the data. The errors are very different compared to OCR. OCR errors will usually create a text string that is not a valid word. In automatic speech recognition (ASR), all errors are other valid words since ASR selects entries ONLY from a dictionary of words. Audio also allows the user to search on specific speakers, since speaker identification is relatively accurate against audio sources.  The correlation between different parts of a query against different modalities is usually based upon time or location. The most common example would be on time. For example if a video news program has been indexed, the user could have access to the scene changes, the transcribed audio, the closed captioning and the index terms that a user has assigned while displaying the video. The query could be "Find where Bill Clinton is discussing Cuban refugees and there is a picture of a boat". All of the separate tracks of information are correlated on a time basis. The system would return those locations where Bill Clinton is identified as the speaker (user the audio track and speaker identification), where in any of the text streams (OCRed text from the video, transcribed audio, closed captioning, or index terms) there is discussion of refugees and Cuba, and finally during that time segment there is at least one scene change that includes a boat.   
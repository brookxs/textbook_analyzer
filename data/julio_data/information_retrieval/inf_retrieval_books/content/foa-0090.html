 156       FINDING OUT ABOUT  5.2.3 Singular Value Decomposition  Just as students' HEIGHT and WEIGHT are correlated along the dimension SIZE, we can guess that (at least some small sets of) keywords are correlated and that the vector space representation of a document corpus (cf. Section 3.4) might be simplified in a similar way. The goal is to "reduce the dimensionality" of the documents' representation in the same way we reduced that of our students' sizes.  But although we can draw a simple picture revealing the correlational structure between two dimensions, the picture is much more complicated when we try to conceive of the full V ´ 105 space of index terms. For the D vectors we seek a smaller k lt; V-dimensional solution. We still have as many documents as we had before, but we're going to use a different set of descriptors.  One of the most effective ways to characterize the correlational structure among large sets of objects is via eigenfactor analysis. The technique we consider is called singular value decomposition. This factors any rectangular matrix into three terms:  / = ULAT                                  (5.14)  where U and A are each orthonorinal and 1 is a diagonal matrix "conMathematical       necting" them.t  details                      As before (cf. Equation 3.39), using inner product to measure simi larity, we can define X:  X = ]f                                    (5.15)  Because / is a D x V matrix, X is a D x D symmetric matrix capturing all (^) interdocument similarities. Then U is the system of eigenvectors of X, and L has the square roots of the eignenvalues, y/TT^ along its diagonal. By convention, we order these in decreasing order:  ' gt;  y/h;               (5.16)  Large eigenvalues correspond to dominant correlations and so, just as looking for the SIZE dimension that captured the main interaction between HEIGHT and WEIGHT, we will rely on the first k dimensions to MATHEMATICAL FOUNDATIONS       157  Terms  FIGURE 5.4 SVD Decomposition  capture the dominant modes of interaction in /:  Jk=UkLkATk                                 (5.17)  This operation is shown schematically in Figure 5.4.1"  As always, whenever we throw something away (viz., the small eigenvectors), the result must be an approximation. That is, there will be a difference between our reduced-dimension representation Jk and the original /.  One easy way to measure this discrepancy is by referring to the D2 interdocument similarities latent in the X matrix and considering how different they are in the approximate matrix Xk  Xk = Jkj£                                   (5.18)  using the Li norm || ï ||2 to measure deviation. Then:  Err= || X-Xkh                          (5.19)  In fact, approximating X with its reduced fc-rank SVD decomposition turns out to be optimal, in the sense that it results in minimal Err over all other rank-fc alternatives [Bartell et aL, 1992].   
 The SMART measures  In 1966, Rocchio gave a derivation of two overall indices of merit based on recall and precision. They were proposed for the evaluation of retrieval systems which ranked documents, and were designed to be independent of cut-off.  The first of these indices is normalised recall. It roughly measures the effectiveness of the ranking in relation to the best possible and worst possible ranking. The situation is illustrated in Figure 7.9 for 25 documents where we plot on the y-axis and the ranks on the x-axis.  Normalised recall (Rnorm) is the area between the actual case and the worst as a proportion of the area between the best and the worst. If n is the number of relevant documents, and ri the rank at which the ith document is retrieved, then the area between the best and actual case can be shown to be (after a bit of algebra):  (see Salton[23], page 285).  A convenient explicit form of normalised recall is:  where N is the number of documents in the system and N - n the area between the best and the worst case (to see this substitute ri = N - i + 1 in the formula for Ab - Aa). The form ensures that Rnorm lies between 0 (for the worst case) and 1 (for the best case).  In an analogous manner normalised precision is worked out. In Figure 7.10 we once more have three curves showing (1) the best case, (2) the actual case, and (3) the worst case in terms of the precision values at different rank positions.  The calculation of the areas is a bit more messy but simple to do (see Salton[23], page 298). The area between the actual and best case is now given by:  The log function appears as a result of approximating [[Sigma]] 1/r by its continuous analogue [[integral]] 1/r dr, which is logr + constant.  The area between the worst and best case is obtained in the same way as before using the same substitution, and is:  The explicit form, with appropriate normalisation, for normalised precision is therefore:  Once again it varies between 0 (worst) and 1 (best).  A few comments about these measures are now in order. Firstly their behaviour is consistent in the sense that if one of them is 0 (or 1) then the other is 0 (or 1). In other words they both agree on the best and worst performance. Secondly, they differ in the weights assigned to arbitrary positions of the precision-recall curve, and these weights may differ considerably from those which the user feels are pertinent (Senko[21]). Or, as Salton[23] (page 289) puts it: 'the normalised precision measure assigns a much larger weight to the initial (low) document ranks than to the later ones, whereas the normalised recall measure assigns a uniform weight to all relevant documents'. Unfortunately, the weighting is arbitrary and given. Thirdly, it can be shown that normalised recall and precision have interpretations as approximations to the average recall and precision values for all possible cut-off levels. That is, if R (i) is the recall at rank position i, and P (i) the corresponding precision value, then:  Fourthly, whereas Cooper has gone to some trouble to take account of the random element introduced by ties in the matching function, it is largely ignored in the derivation of Pnorm and Rnorm.  One further comment of interest is that Robertson15 has shown that normalised recall has an interpretation as the area under the Recall-Fallout curve used by Swets.  Finally mention should be made of two similar but simpler measures used by the SMART system. They are:  and do not take into account the collection size N, n is here the number of relevant documents for the particular test query.   
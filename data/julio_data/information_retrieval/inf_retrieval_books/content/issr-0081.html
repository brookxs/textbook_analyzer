 Automatic Indexing                                                                                   125  5.3.1 Index Phrase Generation  The goal of indexing is to represent the semantic concepts of an item in the information system to support finding relevant information. Single words have conceptual context, but frequently they are too general to help the user find the desired information. Term phrases allow additional specification and focusing of the concept to provide better precision and reduce the user's overhead of retrieving non-relevant items. Having the modifier "grass" or "magnetic" associated with the term "field" clearly disambiguates between very different concepts. One of the earliest statistical approaches to determining term phrases proposed by Salton was use of a COHESION factor between terms (Salton-83):  COHESION^ = SIZE-FACTOR * (PAIR-FREQk,h / TOTFk * TOTFH)  where SIZE-FACTOR is a normalization factor based upon the size of the vocabulary and PAIR-FREQkj, is the total frequency of co-occurrence of the pair Teraik Ternih in the item collection. Co-occurrence may be defined in terms of adjacency, word proximity, sentence proximity, etc. This initial algorithm has been modified in the SMART system to be based on the following guidelines (BUCKLEY-95):  any pair of adjacent non-stop words is a potential phrase any pair must exist in 25 or more items  phrase weighting uses a modified version of the SMART system single term algorithm  normalization is achieved by dividing by the length of the single-term subvector.  Natural language processing can reduce errors in determining phrases by determining inter-item dependencies and using that information to create the term  phrases used in the indexing process. Statistical approaches tend to focus on two term phrases. A major advantage of natural language approaches is their ability to  produce multiple-term phrases to denote a single concept. If a phrase such as "industrious intelligent students" was used often, a statistical approach would create phrases such as "industrious intelligent" and "intelligent student." A natural language approach would create phrases such as "industrious student," "intelligent student" and "industrious intelligent student."  The first step in a natural language determination of phrases is a lexical analysis of the input. In its simplest form this is a part of speech tagger that, for example, identifies noun phrases by recognizing adjectives and nouns. Precise part 126                                                                                               Chapter5  of speech taggers exist that are accurate to the 99 per cent range. Additionally, proper noun identification tools exist that allow for accurate identification of names, locations and organizations since these values should be indexed as phrases and not undergo stemming. Greater gains come from identifying syntactic and semantic level dependencies creating a hierarchy of semantic concepts. For example, "nuclear reactor fusion" could produce term phrases of "nuclear reactor" and "nuclear fusion." In the ideal case all variations of a phrase would be reduced to a single canonical form that represents the semantics for a phrase. Thus, where possible the phrase detection process should output a normalized form. For example, "blind Venetian" and "Venetian who is blind" should map to the same phrase. This not only increases the precision of searches, but also increases the frequency of occurrence of the common phrase. This, in turn, improves the likelihood that the frequency of occurrence of the common phrase is above the threshold required to index the phrase. Once the phrase is indexed, it is available for search, thus participating in an item's selection for a search and the rank associated with an item in the Hit file. One solution to finding a common form is to transform the phrases into a operator-argument form or a header-modifier form. There is always a category of semantic phrases that comes from inferring concepts from an item that is non-determinable. This comes from the natural ambiguity inherent in languages that is discussed in Chapter 1.  A good example of application of natural language to phrase creation is in the natural language information retrieval system at New York University developed in collaboration with GE Corporate Research and Development (Carballo-95). The text of the item is processed by a fast syntactical process and extracted phrases are added to the index in addition to the single word terms. Statistical analysis is used to determine similarity links between phrases and identification of subphrases. Once the phrases are statistically noted as similar, a filtering process categorizes the link onto a semantic relationship (generality, specialization, antonymy, complementation, synonymy, etc.).  The Tagged Text Parser (TTP), based upon the Linguistic String Grammar (Sager-81), produces a regularized parse tree representation of each sentence reflecting the predicate-argument structure (Strzalkowski-93). The tagged text parser contains over 400 grammar production rules. Some examples of the part of speech tagger identification are given in Figure 5.8.  CLASS                                         EXAMPLES  determiners                                   a, the  singular nouns                              paper, notation, structure, language  plural nouns                                  operations, data, processes  preposition                                    in, by, of, for  adjective                                       high, concurrent  present tense verb                          presents, associates  present participal                           multiprogramming  5.8 Part of Speech Tags Automatic Indexing                                                                                   127  The TTP parse trees are header-modifier pairs where the header is the main concept and the modifiers are the additional descriptors that form the concept and eliminate ambiguities. Figure 5.9 gives an example of a regularized parse tree structure generated for the independent clause:  The former Soviet President has been a local hero   ever since a Russian tank invaded Wisconsin  |assert  perf[HAVE]  verb[BE]  subject  np  noun[President] t_pos[The] adj [former] adj [Soviet] object np  noun [hero] t__pos[a] adj [local] adv[ever] sub_ord [since] verb[invade] subject np  nounftank] t__pos[a] adj[Russian] object np noun[Wisconsin]  Figure 5.9 TTP Parse Tree  This structure allows for identification of potential term phrases usually based upon noun identification. To determine if a header-modifier pair warrants indexing, Strzalkowski calculates a value for Informational Contribution (IC) for each element in the pair. Higher values of IC indicate a potentially stronger semantic relationship between terms. The basis behind the IC formula is a conditional probability between the terms. The formula for IC between two terms (x,y) is; 128                                                                                                Chapter 5  where fxy is the frequency of (x,y) in the database, nx is the number of pairs in which "x" occurs at the same position as in (x,y) and D(x) is the dispersion parameter which is the number of distinct words with which x is paired. When IC=1, x occurs only with y (fx,y=nx and dx = 1).  Nominal compounds are the source of many inaccurate identifications in creating header-modifier pairs. Use of statistical information on frequency of occurrence of phrases can eliminate some combinations that occur infrequently and are not meaningful.  The next challenge is to assign weights to term phrases. The most popular term weighting scheme uses term frequencies and inverse document frequencies with normalization based upon item length to calculate weights assigned to terms (see Section 5.2.2.2). Term phrases have lower frequency occurrences than the individual terms. Using natural language processing, the focus is on semantic relationships versus frequency relationships. Thus weighting schemes such as inverse document frequency require adjustments so that the weights are not overly diminished by the potential lower frequency of the phrases. For example, the weighting scheme used in the New York University system uses the following formula for weighting phrases:  weight(PhraseO = (Ci*log(termf) + C2*oc(N,i))*IDF  where a(N,i) is 1 for ilt;N and 0 otherwise and Ci and C2 are normalizing factors.  The N assumes the phrases are sorted by IDF value and allows the top "N" highest  IDF (inverse document frequency) scores to have a greater effect on the overall  weight than other terms.   
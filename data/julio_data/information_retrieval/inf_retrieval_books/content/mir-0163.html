 9.1.1    Parallel Computing Parallel computing is the simultaneous application of multiple processors to solve a single problem, where each processor works on a different part of the problem. With parallel computing, the overall time required to solve the problem can be reduced to the amount of time required by the longest running part. As long as the problem can be further decomposed into more parts that will run in parallel, we can add more processors to the system, reduce the time required to solve the problem, and scale up to larger problems. Processors can be combined in a variety of ways to form parallel architectures. Flynn [259] has defined a commonly used taxonomy of parallel architectures based on the number of the instruction and data streams in the architecture. The taxonomy includes four classes: 誰  SISD single instruction stream, single data stream 誰  SIMD single instruction stream, multiple data stream 誰  MISD multiple instruction stream, single data stream 誰  MIMD multiple instruction stream, multiple data stream. The SISD class includes the traditional von Neumann [134] computer running sequential programs, e.g., uniprocessor personal computers. SIMD computers consist of AT processors operating on N data streams, with each processor executing the same instruction at the same time.   Machines in this class are often massively parallel computers with many relatively simple processors, a communication network between the processors, and a control unit that supervises the synchronous operation of the processors, e.g., the Thinking Machines CM-2. The processors may use shared memory, or each processor may have its own local memory. Sequential programs require significant modification to make effective use of a SIMD architecture, and not all problems lend themselves to a SIMD implementation. MISD computers use N processors operating on a single data stream in shared memory. Each processor executes its own instruct ion stream, such that multiple operations are performed simultaneously on the same data item. MISD architectures are relatively rare. Systolic arrays are the best known example. MIMD is the most general and most popular class of parallel architectures. A MIMD computer contains N processors, A" instruction streams, and Ar data streams.   The processors are similar to those used in a SISD computer; each INTRODUCTION        231 processor has its own control unit, processing unit, and local memory.f MIMD systems usually include shared memory or a communication network that connects the processors to each other. The processors can work on separate, unrelated tasks, or they can cooperate to solve a single task, providing a great deal of flexibility. MIMD systems with a high degree of processor interaction are called tightly coupled, while systems with a low degree of processor interaction are loosely coupled. Examples of MIMD systems include multiprocessor PC servers, symmetric multiprocessors (SMPs) such as the Sun HPC Server, and scalable parallel processors such as the IBM SP2. Although MIMD typically refers to a single, self-contained parallel computer using two or more of the same kind of processor, MIMD also characterizes distributed computing architectures. In distributed computing, multiple computers connected by a local or wide area network cooperate to solve a single problem. Even though the coupling between processors is very loose in a distributed computing environment, the basic components of the MIMD architecture remain. Each computer contains a processor, control unit, and local memory, and the local or wide area network forms the communication network between the processors. The main difference between a MIMD parallel computer and a distributed computing environment is the cost of interprocessor communication, which is considerably higher in a distributed computing environment. As such, distributed programs are usually coarse grained, while programs running on a single parallel computer tend to be finer grained. Granularity refers to the amount of computation relative to the amount of communication performed by the program. Coarse-grained programs perform large amounts of computation relative to communication; fine-grained programs perform large amounts of communication relative to computation. Of course, an application may use different levels of granularity at different times to solve a given problem.  
 Significance tests  Once we have our retrieval effectiveness figures we may wish to establish that the difference in effectiveness under two conditions is statistically significant. It is precisely for this purpose that many statistical tests have been designed. Unfortunately, I have to agree with the findings of the Comparative Systems Laboratory[28] in 1968, that there are no known statistical tests applicable to IR. This may sound like a counsel of defeat but let me hasten to add that it is possible to select a test which violates only a few of the assumptions it makes. Two good sources which spell out the pre-conditions for non-parametric tests are Siegal[29] and Conover[30]. A much harder but also more rewarding book on non-parametrics is Lehmann[31].  Parametric tests are inappropriate because we do not know the form of the underlying distribution. In this class we must include the popular t-test. The assumptions underlying its use are given in some detail by Siegel (page 19), needless to say most of these are not met by IR data. One obvious failure is that the observations are not drawn from normally distributed populations.  On the face of it non-parametric tests might provide the answer. There are some tests for dealing with the case of related samples. In our experimental set-up we have one set of queries which is used in different retrieval environments. Therefore, without questioning whether we have random samples, it is clear that the sample under condition a is related to the sample under condition b. When in this situation a common test to use has been the Wilcoxon Matched-Pairs test. Unfortunately again some important assumptions are not met. The test is done on the difference Di = Za (Qi) - Zb (Qi), but it is assumed that Di is continuous and that it is derived from a symmetric distribution, neither of which is normally met in IR data.  It seems therefore that some of the more sophisticated statistical tests are inappropriate. There is, however, one simple test which makes very few assumptions and which can be used providing its limitations are noted. This one is known in the literature as the sign test (Siegel[29], page 68 and Conover[30], page 121). It is applicable in the case of related samples. It makes no assumptions about the form of the underlying distribution. It does, however, assume that the data are derived from a continuous variable and that the Z (Qi) are statistically independent. These two conditions are unlikely to be met in a retrieval experiment. Nevertheless, given that some of the conditions are not met, it can be used conservatively.  The way it works is as follows: Let {Za (Q1), Za (Q2), . . .,}, {Zb (Q1), Zb (Q2). . .,} be our two sets of measurements under conditions a and b respectively. Within each pair (Za (Qi), Zb (Qi)) a comparison is made, and each pair is classified as ' + ' if Za (Qi) gt; Zb (Qi), as ' - ' if Za (Qi) lt; Zb (Qi) or 'tie' if Za (Qi) = Za (Qi). Pairs which are classified as 'tie' are removed from the analysis thereby reducing the effective number of measurements. The null hypothesis we wish to test is that:  P (Za gt; Zb ) = P (Za lt; Zb ) = [1]/2  Under this hypothesis we expect the number of pairs which have Za gt; Zb to equal the number of pairs which have Za lt; Zb . Another way of stating this is that the two populations from which Za and Zb are derived have the same median.  In IR this test is usually used as a one-tailed test, that is, the alternative hypothesis prescribes the superiority of retrieval under condition a over condition b, or vice versa. A table for small samples n lt;= 25 giving the probability under the null hypothesis for each possible combination of '+''s and '-''s may be found in Siegal[29] (page 250). To give the reader a feel for the values involved: in a sample of 25 queries the null hypothesis will be rejected at the 5 per cent level if there are at least 14 differences in the direction predicted by the alternative hypothesis.  The use of the sign test raises a number of interesting points. The first of these is that unlike the Wilcoxon test it only assumes that the Z's are measured on an ordinal scale, that is, the magnitude of |Za - Zb | is not significant. This is a suitable feature since we are usually only seeking to find which strategy is better in an average sense and do not wish the result to be unduly influenced by excellent retrieval performance on one query. The second point is that some care needs to be taken when comparing Za and Zb. Because our measure of effectiveness can be calculated to infinite precision we may be insisting on a difference when in fact it only occurs in the tenth decimal place. It is therefore important to decide beforehand at what value of [[propersubset]] we will equate Za and Zb when |Za - Zb | lt;= [[propersubset]].  Finally, although I have just explained the use of the sign test in terms of single number measures, it is also used to detect a significant difference between precision-recall graphs. We now interpret the Z's as precision values at a set of standard recall values. Let this set be SR = {0,1, 0.2, . . ., 1.0}, then corresponding to each R[[propersubset]] SR we have a pair (Pa (R) Pb (R)). The Pa's and Pb's are now treated in the same way as the Za's and Zb's. Note that when doing the evaluation this way, the precision-recall values will have already been averaged over the set of queries by one of the ways explained before.   
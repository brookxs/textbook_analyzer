 7.3 Relevance Feedback  As discussed in the early chapters in this text, one of the major problems in finding relevant items lies in the difference in vocabulary between the authors 176                                                                                               Chapter 7  and the user. Thesuari and semantic networks provide utility in generally expanding a user's search statement to include potential related search terms. But this still does not correlate to the vocabulary used by the authors that contributes to a particular database. There is also a significant risk that the thesaurus does not include the latest jargon being used, acronyms or proper nouns. In an interactive system, users can manually modify an inefficient query or have the system automatically expand the query via a thesaurus. The user can also use relevant items that have been found by the system (irrespective of their ranking) to improve future searches, which is the basis behind relevance feedback. Relevant items (or portions of relevant items) are used to reweight the existing query terms and possibly expand the user's search statement with new terms.  The first major work on relevance feedback was published in 1965 by Rocchio (republished in 1971: Rocchio-71). Rocchio was documenting experiments on reweighting query terms and query expansion based upon a vector representation of queries and items. The concepts are also found in the probabilistic model presented by Robertson and Sparck Jones (Robertson-76). The relevance feedback concept was that the new query should be based on the old query modified to increase the weight of terms in relevant items and decrease the weight of terms that are in non-relevant items. This technique not only modified the terms in the original query but also allowed expansion of new terms from the relevant items. The formula used is:  r ,.i                nr ^  where  Qn          = the revised vector for the new query  Qo          = the original query  r             = number of relevant items  DR,        = the vectors for the relevant items  nr           = number of non-relevant items  DNRj      = the vectors for the non-relevant items.  The factors r and nr were later modified to be constants that account for the number of items along with the importance of that particular factor in the equation.  Additionally a constant was added to Qo to allow adjustments to the importance of the weight assigned to the original query.   This led to the revised version of the  formula: User Search Techniques  177  where a, P, and y are the constants associated with each factor (usually \ln or Mnr  r  times a constant). The factor P 2^ DR i is referred to as positive feedback because it is using the user judgments on relevant items to increase the values of terms for  nr  the next iteration of searching.  The factor y^T/)JV/?j  is referred to as negative  feedback since it decreases the values of terms in the query vector. Positive feedback is weighted significantly greater than negative feedback. Many times only positive feedback is used in a relevance feedback environment. Positive feedback is more likely to move a query closer to a user's information needs. Negative feedback may help, but in some cases it actually reduces the effectiveness of a query. Figure 7.6 gives an example of the impacts of positive and negative feedback. The filled circles represent non-relevant items; the other circles represent relevant items. The oval represents the items that are returned from the query. The solid box is logically where the query is initially. The hollow box is the query modified by relevance feedback (positive only or negative only in the Figure).   o∞ o      / o o \  o ^ o o    o  o  o∞o   on oo  oo p          1   o     o  o     o   o  /\   o o      /o o \  o o∞ o L o)  oo∞ o    o   o o   o o    o     o 1 cL      o  Positive Feedback                                        Negative Feedback  Figure 7.6 Impact of Relevance Feedback  Positive feedback moves the query to retrieve items similar to the items retrieved and thus in the direction of more relevant items. Negative feedback moves the query away from the non-relevant items retrieved, but not necessarily closer to more relevant items.  Figure 7.7 shows how the formula is applied to three items (two relevant and one non-relevant). If we use the factors a = 1, p = % (14 times a constant s/i), y 178  Chapter 7  = % (1/1 times a constant XA) in the foregoing formula we get the following revised query (NOTE: negative values are changed to a zero value in the revised Query vector):  Qn = (3, 0,0,2, 0) + Va (2+1, 4+3,0+0,0+0, 2+0) - V4 (0, 0, 4, 3, 2) = (33/4,l3/4,0{-lKl1/4,0)   Term 1 Term 2 Term 3 Term 4 Term 5  Qo 3 0 0 2 0  DOClr 2 4 0 0 2  DOC2r 1 3 0 0 0  DOC3nr 0 0 4 3 3  Qn 33/4 VA 0 VA 0  Figure 7.7 Query Modification via Relevance Feedback  5  Using the unnormalized similarity formula SIM(Qk,DOCj) =   /j TERM ^ TERMu produces the results shown in Figure 7.8:  /=!   DOCl DOC2 DOC3  Qo 6 3 6  Qn 14V4 9.0 3.75  Figure 7.8 Effect of Relevance Feedback  In addition to showing the benefits of relevance feedback, this example illustrates the problems of identifying information. Although DOC3 is not relevant to the user, the initial query produced one of the highest similarity measures for it. This was caused by a query term (Term 4) of interest to the user that has a significant weight in DOC3. The fewer the number of terms in a user query, the more likely a specific term to cause non-relevant items to be returned. The modification to the query by the relevance feedback process significantly increased the similarity measure values for the two relevant items (DOCl and DOC2) while decreasing the value of the non-relevant item. It is also of interest to note that the new query added a weight to Term 2 that was not in the original query. One reason that the user might not have initially had a value to Term 2 is that it might not have been in the user's vocabulary. For example, the user may have been searching on "PC" and '"word processor1" and not been aware that many authors use the specific term "Macintosh" rather than "PC."  Relevance feedback, in particular positive feedback, has been proven to be of significant value in producing better queries. Some of the early experiments on User Search Techniques                                                                             179  the SMART system (Ide-69, Ide-71, Salton-83) indicated the possible improvements that would be gained by the process. But the small collection sizes and evaluation techniques put into question the actual gains by using relevance feedback. One of the early problems addressed in relevance feedback is how to treat query terms that are not found in any retrieved relevant items. Just applying the algorithm would have the effect of reducing the relative weight of those terms with respect to other query terms. From the user's perspective, this may not be desired because the term may still have significant value to the user if found in the future iterations of the search process. Harper and van Rijisbergen addressed this issue in their proposed EMIM weighting scheme (Harper-785 Harper-80). Relevance feedback has become a common feature in most information systems. When the original query is modified based upon relevance feedback, the systems ensure that the original query terms are in the modified query, even if negative feedback would have eliminated them. In some systems the modified query is presented to the user to allow the user to readjust the weights and review the new terms added.  Recent experiments with relevance feedback during the TREC sessions have shown conclusively the advantages of relevance feedback. Queries using relevance feedback produce significantly better results than those being manually enhanced. When users enter queries with a few number of terms, automatic relevance feedback based upon just the rank values of items has been used. This concept in information systems called pseudo-relevance feedback, blind feedback or local context analysis (Xu-96) does not require human relevance judgments. The highest ranked items from a query are automatically assumed to be relevant and applying relevance feedback (positive only) used to create and execute an expanded query. The system returns to the user a Hit file based upon the expanded query. This technique also showed improved performance over not using the automatic relevance feedback process. In the automatic query processing tests from TREC (see Chapter 10) most systems use the highest ranked hits from the first pass to generate the relevance feedback for the second pass.   
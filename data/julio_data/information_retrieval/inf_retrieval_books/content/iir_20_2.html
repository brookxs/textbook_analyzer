    Crawling seed set 4 5 URL frontier 19 This seemingly simple recursive traversal of the web graph is complicated by the many demands on a practical web crawling system: the crawler has to be distributed, scalable, efficient, polite, robust and extensible while fetching pages of high quality. We examine the effects of each of these issues. Our treatment follows the design of the Mercator crawler that has formed the basis of a number of research and commercial crawlers. As a reference point, fetching a billion pages (a small fraction of the static Web at present) in a month-long crawl requires fetching several hundred pages each second. We will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate. Before proceeding to this detailed description, we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy: Only one connection should be open to any given host at a time. A waiting time of a few seconds should occur between successive requests to a host. Politeness restrictions detailed in Section 20.2.1 should be obeyed.   Subsections Crawler architecture Distributing the crawler DNS resolution The URL frontier   
 2.8.2    Inference Network Model The two most traditional schools of thought in probability are based on the frequentist view and the epistemological view. The frequentist view takes probability as a statistical notion related to the laws of chance. The epistemological 50 MODELING Figure 2.9    Basic inference network model. view interprets probability as a degree of belief whose specification might be devoid of statistical experimentation. This second viewpoint is important because we frequently refer to probabilities in our daily lives without a clear definition of the statistical experiment which yielded those probabilities. The inference network model [772, 771] takes an epistemological view of the information retrieval problem. It associates random variables with the index terms, the documents, and the user queries. A random variable associated with a document dj represents the event of observing that document (i.e., the model assumes that documents are being observed in the search for relevant documents). The observation of the document dj asserts a belief upon the random variables associated with its index terms. Thus, observation of a document is the cause for an increased belief in the variables associated with its index terms. Index term and document variables are represented as nodes in the network. Edges are directed from a document node to its term nodes to indicate that observation of the document yields improved belief on its term nodes. The random variable associated with the user query models the event that the information request specified by the query has been met. This random variable is also represented by a node in the network. The belief in this (query) node is a function of the beliefs in the nodes associated with the query terms. Thus, edges are directed from the index term nodes to the query node. Figure 2.9 illustrates an inference network for information retrieval. The document d3 has A?2, kii and kt as its index terms. This is modeled by directing the edges from the node d3 to the nodes 2, k^ and kt. The query q is composed of the index terms k\, 2, and fcj. This is modeled by directing the edges from the nodes k\% 2* and k{ to the node q. Notice that Figure 2.9 also includes three extra nodes: Q2, #1, and /. The nodes $2 and qi are used to model an (alternative) Boolean formulation qj for the query q (in this case, q\ = (k\ A klt;i) V kt).  When such ALTERNATIVE PROBABILISTIC MODELS        51 (additional) information is available, the user information need / is supported by both q and q\. In what follows, we concentrate our attention on the support provided to the query node q by the observation of a document d3. Later on, we discuss the impact of considering multiple query representations for an information need /. This is important because, as Turtle and Croft have demonstrated, a keyword-based query formulation (such as q) can be combined with a Boolean-like query formulation (such as q{) to yield improved retrieval performance for the same information need. The complete inference network model also includes text nodes and query concept nodes but the model discussed above summarizes the essence of the approach. A simplifying assumption is made which states that all random variables in the network are binary. This seems arbitrary but it does simplify the modeling task and is general enough to capture all the important relationships in the information retrieval problem. Definition Let k be a t-dimensional vector defined by k = (i, 2, ï ï ï ,kt) where k\, k2, ..., kt are binary random variables i.e., k% Ä {0,1}. These variables define the 2l possible states for k. Further, let d3 be a binary random variable associated with a document dj and let q be a binary random variable associated with the user query. Notice that q is used to refer to the query, to the random variable associated with it, and to the respective node in the network. This is also the case for dj and for each index term ki. We allow this overloading in syntax because it should always be clear whether we are referring to either the query or to its associated random variable. The ranking of a document dj with respect to a query q is a measure of how much evidential support the observation of dj provides to the query q. In an inference network, the ranking of a document dj is computed as P(q A dj) where q and d3 are short representations for q = 1 and dj = 1, respectively. In general, such a ranking is given by Vfc  VA: P(qAdj)    =    l-P(qAdj) 52        MODELING which is obtained by basic conditioning and the application of Bayes' rule. Notice that P{q\dj x k) = P(q\k) because the ki nodes separate the query node q from the document node d3.  Also, the notation q A dj is a short representation for The instantiation of a document node d3 (i.e., the observation of the document) separates its children index term nodes making them mutually independent (see Bayesian theory for details). Thus, the degree of belief asserted to each index term node ki by instantiating the document node dj can be computed separately. This implies that P(k\d3) can be computed in product form which yields (from equation 2.6), P(q/\dj)    =    TP(q\k)x P{k%\dj) ] x P(d3)      (2.7) P{q/\d3)    =    l-P(q/\dj where P{ki\d3) = 1 -P{kl\d3). Through proper specification of the probabilities P(q\k)i P(kt\dj), and P(dj), we can make the inference network cover a wide range of useful information retrieval ranking strategies. Later on, we discuss how to use an inference network to subsume the Boolean model and tf-idf ranking schemes. Let us first cover the specification of the P(d3) probabilities. Prior Probabilities for Inference Networks Since the document nodes are the root nodes in an inference network, they receive a prior probability distribution which is of our choosing. This prior probability reflects the probability associated to the event of observing a given document dj (to simplify matters, a single document node is observed at a time). Since we have no prior preferences for any document in particular, we usually adopt a prior probability distribution which is uniform. For instance, in the original work on inference networks [772, 771], the probability of observing a document d3 is set to l/N where N is the total number of documents in the system. Thus, The choice of the value 1/Ar for the prior probability P{dj) is a simple and natural specification given that our collection is composed of N documents. However, other specifications for P(d3) might also be interesting. For instance, in the cosine formula of the vector model, the contribution of an index term to ALTERNATIVE PROBABILISTIC MODELS        53 the rank of the document d3 is inversely proportional to the norm of the vector dj. The larger the norm of the document vector, the smaller is the relative contribution of its index terms to the document final rank. This effect can be taken into account through proper specification of the prior probabilities P(dj) as follows. =   \-P{d3) where \d3 | stands for the norm of the vector d3. Therefore, in this case, the larger the norm of a document vector, the smaller its associated prior probability. Such specification reflects a prior knowledge that we have about the behavior of vector-based ranking strategies (which normalize the ranking in the document space). As commanded by Bayesian postulates, previous knowledge of the application domain should be asserted in the specification of the priors in the network, as we have just done. Inference Network for the Boolean Model Here we demonstrate how an inference network can be tuned to subsume the Boolean model. First, for the Boolean model, the prior probabilities P{d3) are all set to I/AT because the Boolean model makes no prior distinction on documents. Thus, p(d3) = i Regarding the conditional probabilities P(ki\dj) and P(q\k), the specification is as follows. 0    otherwise P(kt\dj)    =    l-Pihldj) which basically states that, when the document dj is being observed, only the nodes associated with the index terms of the document dj are active (i.e., have an induced probability greater than 0). For instance, observation of a document node dj whose term vector is composed of exactly the index terms A:2, kt, and kt (see Figure 2.9) activates the index term nodes {k2,kt,kt} and no others. Once the beliefs in the index term nodes have been computed, we can use them to compute the evidential support they provide to the user query q as 54        MODELING follows. P(q\k)    =    /  1    if 3$ô   I   (Qcc Ä qdnf) A(Vfct, 9i{k) = gi(qcc)) 1  ;          \ 0    otherwise P{q\k)    =    1-P(lt;?|fc) where qcc and gdn/ are as defined for the classic Boolean model. The above equation basically states that one of the conjunctive components of the user query (expressed in disjunctive normal form) must be matched by the set of active terms in k (in this case, those activated by the document observed) exactly. Substituting the above definitions for P(q\h), P(ki\dj), and P(dj) into equation 2.7, it can be easily shown that the set of documents retrieved is exactly the set of documents returned by the Boolean model as defined in section 2.5.2. Thus, an inference network can be used to subsume the Boolean model without difficulties. Inference Network for tf-idf Ranking Strategies For tf-idf ranking strategies (i.e., those related to the vector model), we adopt prior probabilities which reflect our prior knowledge of the importance of document normalization. Thus, we set the prior P(dj) to l/|dj| as follows. P(dj)    =    -1-                                                                     (2.8) P(dj)    =    1 Further, we have to decide where to introduce the tf (term-frequency) and the idf (inverse-document-frequency) factors in the network. For that purpose, we consider that the tf and idf factors are normalized (as in equation 2.1) and that these normalized factors are strictly smaller than 1. We first focus on capturing the impact of the tf factors in the network. Normalized tf factors are taken into account through the beliefs asserted upon the index term nodes as follows. P(ki\dj)    =   fij                                                                   (2.9) These equations simply state that, according to the observed document dj, the relevance of a term hi is determined by its normalized term-frequency factor. We are now in a position to consider the influence of idf factors. They are taken into account through the specification of the impact of index term nodes ALTERNATIVE PROBABILISTIC MODELS        55 on the query node. Define a vector ki given by, ki=k  \   (9i(k) = 1 A V^ 9j{%) = 0) The vector k% is a reference to the state of the vector k in which the node ki is active and all others are inactive. The motivation is that tf-idf ranking strategies sum up the individual contributions of index terms and ki allows us to consider the influence of the term ki in isolation. We are now ready to define the influence of the index term nodes in the query node q as _    I idfi    if  k = hA gi(q) = 1                                     ( "   \ o     if j^£vft($) = o                                 (2*10) P(q\k)    =    1-P(q\k) where idfz here is a normalized version of the idf factor defined in equation 2.2. By applying equations 2.8, 2.9, and 2.10 to equation 2.7, we can then write JJ x ói P{ds) I =    f IJPCJfeil^) ) x P{dj) x Y^p(ki\di) x __         I   '       y    _____    V                          \                             T ¶         V   1 /7 T      V    óóóóóóóó        Ky t   A       15       lt;*ª                          7                             11   1   A   6Ci/ 2   ^ Mjl       Vt      rf- ~^A    (g=1                        1~^J =    l-PfgAd,-) which provides a tf-idf-like ranking. Unfortunately, Cj depends on a product of the various probabilities P(ki\dj) which vary from document to document and thus the ranking is distinct from the one provided by the vector model. Despite this peculiarity in the tf-idf ranking generated, it has been shown that an inference network is able to provide good retrieval performance with general collections. The reason is that the network allows us to consistently combine evidence from distinct evidential sources to improve the final ranking, as we now discuss. Combining Evidential Sources In Figure 2.9, the first query node q is the standard keyword-based query formulation for the user information need /. The second query q\ is a Boolean-iike query formulation for the same information need (i.e., an additional evidential source collected from a specialist). The joint support these two query formulations provide to the information need node / can be modeled through, for 56        MODELING instance, an OR operator (i.e., / = q V qi). In this case, the ranking provided by the inference network is computed as, P(I A dj)    =   ^2 PCW x p(k\do) x p(ds) k =    X^1 ~ P(5l*) P(^)) x P(^ldi) x P(^) k which might yield a retrieval performance which surpasses the retrieval performance obtained with each of the query nodes in isolation as demonstrated in [771].  
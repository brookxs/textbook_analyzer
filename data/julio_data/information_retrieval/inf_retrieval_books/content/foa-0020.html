 1.7 How Well Are We Doing?  Suppose you and I each build an FOA search tool; how might we decide which does the better job? How might a potential customer decide on their relative values? If we use a new search engine that seems to work much better, how can we determine which of its many features are critical to this success? If we are to make a science of FOA, or even if we only wish to build consistent, reliable tools, it is vital that we establish a methodology by which the performance of search engines can be rigorously evaluated.  Just as your evaluation of a human question-answerer (professor, reference librarian, etc.) might well depend on subjective factors (how well you "communicate") and factors that go beyond the performance of the search engine (does any available document contain a satisfying answer?), evaluation of search engines is notoriously difficult. The field of IR has made great progress, however, by adopting a methodology for search engine evaluation that has allowed objective assessment of a task that is closely related to FOA. Here we will sketch this simplified notion of the FOA task.  The first step is to focus on a particular query. With respect to this query, we identify the set of documents Rel that are determined to be Omniscient         relevant to it."'" Then a good search engine is one that can retrieve all  relevance            an(j onjy ^ documents in jRel. Figure 1.10 shows both Rel and Retr, the  set of documents actually retrieved in response to the query, in terms of a Venn diagram. Clearly, the number of documents that were designated both relevant and retrieved, Retr D Rel, will be a key measure of success.  But we must compare the size of the set \Retr n Rel | to something, and several standards of comparison are possible. For example, if we are very concerned that the search engine retrieve every relevant document, OVERVIEW       35  High-recall retrieval  ^ - "*"  Corpus  FIGURE 1.10 Comparison of Retrieved versus Relevant Documents  then it is appropriate to compare the intersection to the number of documents marked as relevant, | Rel |. This measure of search engine performance is known as recall:  \Retr n Rel \  Recall =  However, we might instead be worried about how much of what the users see is relevant, so an equally reasonable standard of comparison is what number of the documents retrieved, | Retr |, are in fact relevant. This measure is known as precision:  \Retr fl Rel\ Precision =  \Retr  (1.2)  Note that even in this simple measure of search engine performance,  we have identified two legitimate criteria. In real applications, our users will often vary as to whether high precision or high recall is more important. For example, a lawyer looking for every prior ruling (i.e., judicial opinions, retrievable as separate documents) that is on point for his  or her case will be more interested in high-recall behavior. The typical undergraduate, on the other hand, who is quickly searching the Web for a term paper due the next day, knows all too well that there may be many, many relevant documents somewhere out there. But the student cares much more that the first screen of hits be full of relevant leads. 36      FINDING OUT ABOUT  Examples of high-recall and high-precision retrievals are also shown in Figure 1.10.  To be useful, this same analysis must be extended to consider the order in which documents are retrieved, and it must consider performance across a broad range of typical queries rather than just one. These and other issues of evaluation are taken up in Chapter 4.   
 3.2.1    Recall and Precision Consider an example information request / (of a test reference collection) and its set R of relevant documents. Let \R\ be the number of documents in this set. Assume that a given retrieval strategy (which is being evaluated) processes the information request / and generates a document answer set A. Let \A\ be the number of documents in this set. Further, let \Ra\ be the number of documents in the intersection of the sets R and A. Figure 3.1 illustrates these sets. The recall and precision measures are defined as follows. ï Recall is the fraction of the relevant documents (the set R) which has been retrieved i.e., Recall = \R\ ï Precision is the fraction of the retrieved documents (the set A) which is relevant i.e., Precision = \A\ Recall and precision, as defined above, assume that all the documents in the answer set A have been examined (or seen). However, the user is not usually presented with all the documents in the answer set A at once.   Instead, the Relevant Docs in Answer Set                 ^-------_^ Collection Relevant Docs         Answer Set' |*|                       \A\ Figure 3.1    Precision and recall for a given example information request. 76        RETRIEVAL EVALUATION documents in A are first sorted according to a degree of relevance (i.e., a ranking is generated). The user then examines this ranked list starting from the top document. In this situation, the recall and precision measures vary as the user proceeds with his examination of the answer set A. Thus, proper evaluation requires plotting a precision versus recall curve as follows. As before, consider a reference collection and its set of example information requests. Let us focus on a given example information request for which a query q is formulated. Assume that a set Rq containing the relevant documents for q has been defined. Without loss of generality, assume further that the set Rq is composed of the following documents Rq = {^3,^5,^9, ^25, ^39^44, ^56^71)^89? d\2z]                                   (3.1) Thus, according to a group of specialists, there are ten documents which are relevant to the query q. Consider now a new retrieval algorithm which has just been designed. Assume that this algorithm returns, for the query g, a ranking of the documents in the answer set as follows. Ranking for query q: 1.	d\2Z ï	6.  CI9 ï	11.	"38 2.		7. d5n	12.	d48 3.	d56 ï	8.  di29	13. 4.	de	9-  ^187	14.	dn 5.	d8	10.  ^25 ï	15. The documents that are relevant to the query q are marked with a bullet after the document number. If we examine this ranking, starting from the top document, we observe the following points. First, the document di23 which is ranked as number 1 is relevant. Further, this document corresponds to 10% of all the relevant documents in the set Rq. Thus, we say that we have a precision of 100% at 10% recall. Second, the document d$ß which is ranked as number 3 is the next relevant document. At this point, we say that we have a precision of roughly 66% (two documents out of three are relevant) at 20% recall (two of the ten relevant documents have been seen). Third, if we proceed with our examination of the ranking generated we can plot a curve of precision versus recall as illustrated in Figure 3.2. The precision at levels of recall higher than 50% drops to 0 because not all relevant documents have been retrieved. This precision versus recall curve is usually based on 11 (instead often) standard recall levels which are 0%, 10%, 20%, ..., 100%. For the recall level 0%, the precision is obtained through an interpolation procedure as detailed below. In the above example, the precision and recall figures are for a single query. Usually, however, retrieval algorithms are evaluated by running them for several distinct queries. In this case, for each query a distinct precision versus recall curve is generated. To evaluate the retrieval performance of an algorithm over RETRIEVAL PERFORMANCE EVALUATION 77 Figure 3.2    Precision at 11 standard recall levels. all test queries, we average the precision figures at each recall level as follows. P(r) = Pi(r) (3.2) t=l where P(r) is the average precision at the recall level r, Nq is the number of queries used, and Pi(r) is the precision at recall level r for the i-th query. Since the recall levels for each query might be distinct from the 11 standard recall levels, utilization of an interpolation procedure is often necessary. For instance, consider again the set of 15 ranked documents presented above. Assume that the set of relevant documents for the query q has changed and is now given by (3.3) In this case, the first relevant document in the ranking for query q is d5e which provides a recall level of 33.3% (with precision also equal to 33.3%) because, at this point, one-third of all relevant documents have already been seen. The second relevant document is di29 which provides a recall level of 66.6% (with precision equal to 25%). The third relevant document is d$ which provides a recall level of 100% (with precision equal to 20%). The precision figures at the 11 standard recall levels are interpolated as follows. Let fj, j E {0,1,2,..., 10}, be a reference to the j-th standard recall level (i.e., rs is a reference to the recall level 50%). Then, P(rj)-= max r3lt;rlt;r^t   P(r) (3.4) 78 RETRIEVAL EVALUATION 120 100 -gt; 80 -60 20 40 60 Recall 80 100 120 Figure  3.3    Interpolated precision at  11 standard recall levels relative to Rq  = which states that the interpolated precision at the j-th standard recall level is the maximum known precision at any recall level between the j-th recall level and the (j -f l)-th recall level. In our last example, this interpolation rule yields the precision and recall figures illustrated in Figure 3.3. At recall levels 0%, 10%, 20%, and 30%, the interpolated precision is equal to 33.3% (which is the known precision at the recall level 33.3%). At recall levels 40%, 50%, and 60%, the interpolated precision is 25% (which is the precision at the recall level 66.6%). At recall levels 70%, 80%, 90%, and 100%, the interpolated precision is 20% (which is the precision at recall level 100%). The curve of precision versus recall which results from averaging the results for various queries is usually referred to as precision versus recall figures. Such average figures are normally used to compare the retrieval performance of distinct retrieval algorithms. For instance, one could compare the retrieval performance of a newly proposed retrieval algorithm with the retrieval performance of the classic vector space model. Figure 3.4 illustrates average precision versus recall figures for two distinct retrieval algorithms. In this case, one algorithm has higher precision at lower recall levels while the second algorithm is superior at higher recall levels. One additional approach is to compute average precision at given document cutoff values. For instance, we can compute the average precision when 5, 10, 15, 20, 30, 50, or 100 relevant documents have been seen. The procedure is analogous to the computation of average precision at 11 standard recall levels but provides additional information on the retrieval performance of the ranking algorithm. Average precision versus recall figures are now a standard evaluation strategy for information retrieval systems and are used extensively in the information retrieval literature.    They are useful because thev allow us to evaluate RETRIEVAL PERFORMANCE EVALUATION        79 100 120 Figure 3.4    Average recall versus precision figures for two distinct retrieval algorithms. quantitatively both the quality of the overall answer set and the breadth of the retrieval algorithm. Further, they are simple, intuitive, and can be combined in a single curve. However, precision versus recall figures also have their disadvantages and their widespread usage has been criticized in the literature. We return to this point later on. Before that, let us discuss techniques for summarizing precision versus recall figures by a single numerical value. Single Value Summaries Average precision versus recall figures are useful for comparing the retrieval performance of distinct retrieval algorithms over a set of example queries. However, there are situations in which we would like to compare the retrieval performance of our retrieval algorithms for the individual queries. The reasons are twofold. First, averaging precision over many queries might disguise important anomalies in the retrieval algorithms under study. Second, when comparing two algorithms, we might be interested in investigating whether one of them outperforms the other for each query in a given set of example queries (notice that this fact can be easily hidden by an average precision computation). In these situations, a single precision value (for each query) can be used. This single value should be interpreted as a summary of the corresponding precision versus recall curve. Usually, this single value summary is taken as the precision at a specified recall level. For instance, we could evaluate the precision when we observe the first relevant document and take this precision as the single value summary. Of course, as seems obvious, this is not a good approach. More interesting strategies can be adopted as we now discuss. 80        RETRIEVAL EVALUATION Average Precision at Seen Relevant Documents The idea here is to generate a single value summary of the ranking by averaging the precision figures obtained after each new relevant document is observed (in the ranking). For instance, consider the example in Figure 3.2. The precision figures after each new relevant document is observed are 1, 0.66, 0.5, 0.4, and 0.3. Thus, the average precision at seen relevant documents is given by (l+0.66+0.5-H).4-H).3)/5 or 0.57. This measure favors systems which retrieve relevant documents quickly (i.e., early in the ranking). Of course, an algorithm might present a good average precision at seen relevant documents but have a poor performance in terms of overall recall. R-Precision The idea here is to generate a single value summary of the ranking by computing the precision at the R-th position in the ranking, where R is the total number of relevant documents for the current query (i.e., number of documents in the set Rq). For instance, consider the examples in Figures 3.2 and 3.3. The value of R-precision is 0.4 for the first example (because R = 10 and there are four relevant documents among the first ten documents in the ranking) and 0.33 for the second example (because R = 3 and there is one relevant document among the first three documents in the ranking). The R-precision measure is a useful parameter for observing the behavior of an algorithm for each individual query in an experiment. Additionally, one can also compute an average R-precision figure over all queries. However, using a single number to summarize the full behavior of a retrieval algorithm over several queries might be quite imprecise. Precision Histograms The R-precision measures for several queries can be used to compare the retrieval history of two algorithms as follows. Let RPa{i) and RPb(i) be the R-precision values of the retrieval algorithms A and B for the i-th query. Define, for instance, the difference RPA/B(i) = RPA(i) - RPB(i)                                                             (3.5) A value of RPa/b^) equal to 0 indicates that both algorithms have equivalent performance (in terms of R-precision) for the i-th query.   A positive value of RPa/b(^) indicates a better retrieval performance by algorithm A (for the i-th query) while a negative value indicates a better retrieval performance by algorithm B. Figure 3.5 Illustrates the RPaib^) values (labeled R-Precision A/B) for two hypothetical retrieval algorithms over ten example queries.  The algorithm .4 is superior for eight queries while the algorithm B performs better for the two other queries (numbered 4 and 5). This type of bar graph is called a precision histogram and allows us to quickly compare the retrieval performance history of two algorithms through visual inspection. Summary Table Statistics Single mine measures can also be stored in a table to provide a statistical summary regarding the set of all the queries in a retrieval task. For instance, these RETRIEVAL PERFORMANCE EVALUATION        81 1,5  , 1,0 0,5 Q. n U 1             2            3 -1,0 ' igt;          6           7          8           9          10 -1,5 Query Number Figure 3.5    A precision histogram for ten hypothetical queries. summary table statistics could include: the number of queries used in the task, the total number of documents retrieved by all queries, the total number of relevant documents which were effectively retrieved when all queries are considered, the total number of relevant documents which could have been retrieved by all queries, etc. Precision and Recall Appropriateness Precision and recall have been used extensively to evaluate the retrieval performance of retrieval algorithms. However, a more careful reflection reveals problems with these two measures [451, 664, 754]. First, the proper estimation of maximum recall for a query requires detailed knowledge of all the documents in the collection. With large collections, such knowledge is unavailable which implies that recall cannot be estimated precisely. Second, recall and precision are related measures which capture different aspects of the set of retrieved documents. In many situations, the use of a single measure which combines recall and precision could be more appropriate. Third, recall and precision measure the effectiveness over a set of queries processed in batch mode. However, with modern systems, interactivity (and not batch processing) is the key aspect of the retrieval process. Thus, measures which quantify the informativeness of the retrieval process might now be more appropriate. Fourth, recall and precision are easy to define when a linear ordering of the retrieved documents is enforced. For systems which require a weak ordering though, recall and precision might be inadequate. 82        RETRIEVAL EVALUATION  
 3.3.3 Multimedia Indexing  Indexing associated with multimedia differs from the previous discussions of indexing. The automated indexing takes place in multiple passes of the information versus just a direct conversion to the indexing structure. The first pass in most cases is a conversion from the analog input mode into a digital structure. Then algorithms are applied to the digital structure to extract the unit of processing of the different modalities that will be used to represent the item. In an abstract sense this could be considered the location of a processing token in the modality. This unit will then undergo the final processing that will extract the searchable features that represent the unit. Indexing video or images can be accomplished at the raw data level (e.g., the aggregation of raw pixels), the feature level distinguishing primitive attributes such as color and luminance, and at the semantic level where meaningful objects are recognized (e.g., an airplane in the image/video frame). An example is processing of video. The system (e.g., Virage) will periodically collect a frame of video input for processing. It might compare that frame to the last frame captured to determine the differences between the frames. If the difference is below a threshold it will discard the frame. For a frame requiring processing, it will define a vector that represents the different features associated with that frame. Each dimension of the vector represents a different feature level aspect of the frame. The vector then becomes the unit of processing in the search system. This is similar to processing an image. Semantic level indexing requires pattern recognition of objects within the images.  Examples can be found Cataloging and Indexing                                                                              65  in MITs Photobook (Pentland-94), IBM's QBIC (Niblack-93) and the MultiMedia Datablade from Informix/Virage (Bach-96.)  If you consider an analog audio input, the system will convert the audio to digital format and determine the phonemes associated with the utterances. The phonemes will be used as input to a Hidden Markov Search model (see Chapter 4 and Chapter 10), that will determine with a confidence level the words that were spoken. A single phoneme can be divided into four states for the Markov model. It is the textual words assocaited with the audio that becomes the searchable structure.  In addition to storing the extracted index searchable data, a multimedia item needs to also store some mechanism to correlate the different modalities during search. There are two main mechanisms that are used, positional and temporal. Positional is used when the modalities are interspersed in a linear sequential composition. For example a document that has images or audio inserted, can be considered a linear structure and the only relationship between the modalities will be the juxtaposition of each modality. This would allow for a query that would specify location of an image of a boat within one paragraph of "Cuba and refugees".  The second mechanism is based upon time because the modalities are executing concurrently. The typical video source off television is inherently a multimedia source. It contains video, audio, and potentially closed captioning. Also the creation of multimedia presentations are becoming more common using the Synchronized Multimedia Integration Language (SMIL). It is a mark-up language designed to support multimedia presentations that integrate text (e.g., from slides or free running text) with audio, images and video. In both of these examples, time is the mechanism that is used to synchronize the different modalities. Thus the indexing must include a time-offset parameter versus a physical displacement. It also suggests that the proximity to increase precision will be based upon time concurrency (or ranges) versus physical proximity.   
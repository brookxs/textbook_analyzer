 7.53 When Irrelevant Attributes Abound  In documents where "irrelevant attributes abound" [Littlestone, 1988] (e.g., when any one document contains a small fraction of the full vocabulary but still more than are important in a classifier), the rapid "winnowing" of features is critical. One approach is to use boosting methods, which begin with many very weak hypotheses but focus in on the most successful [Schapire et aL, 1998].  Kivinen and Warmuth's exponentiated gradient EG algorithm extends from Winnow's binary features and output to real-valued quantities [Warmuth, 1997]. The result shares much with the Widrow-Hoff model (cf. Section 7.3.1), but rather than having weight changes making an additive change in the direction of the gradient, EG recommends making a multiplicative change to each element of the document vector. Using R4 as in Equation 7.6:  d'=d  (7.18) ADAPTIVE INFORMATION RETRIEVAL       275  FIGURE 7.8 Combining Experts  where A! is the updated document vector and |d/7| is the length of the document vector after all weights have been updated. That is, weights are always renormalized so that their sum remains one (and nonnegative). Renormalization is an important feature of EG, which, in conjunction with the multiplicative increases in those "relevant" features that are shared with a query, results in quick (i.e., exponentially fast) reduction to zero for irrelevant weights [Lewis et al., 1996]. Callan has also found that rather than training the EG classifier with zero for incorrect classifications and unity for correct ones, using more moderate target values pegged to the minimum and maximum feature values is more successful [Callan, 1998].  Another very recent approach is to apply Vapnik's support vector machines (SVM) [Vapnik, 1995]. Rather than searching for dichotomizing planes within a representational space that has been predefined (e.g., the hyperplanes that gradient descent methods adjust), SVMs search in the "dual" space defined by the set of training instances for kernels (representations) wherein the classes can be conveniently separated! As Joachims [Joachims, 1998] recently emphasized, the way in which these techniques avoid searching the vast space of potential keyword features seems to make SVM a very appropriate technology for this application.   
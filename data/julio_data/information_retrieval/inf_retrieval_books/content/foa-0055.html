 3.5.1 Measures of Association  Given a vocabulary of lexical indexing features, our central concern will be how to moderate the raw frequency counts of these features in documents (and, to a lesser extent, queries) based on distributional characteristics of that feature across the corpus. We will be led to use real-valued weights to capture these relations, but we begin with cruder methods: simply counting shared keywords.  The most direct way to say that a query and a document are similar is to measure the intersection of their respective feature sets. Let Kq be the set of keyword features used in a query and Kd be the analogous set found in a document. The coordination level is exactly how many terms in the query overlap with terms in the document:  CoordLevel(q} d) = | (Kq n Kd) |                   (3.36)  If we have many, many features and our query and documents are highly variable, then the presence of any significant overlap may be enough to  identify the set of documents of interest. On the other hand, if there is a great deal of overlap between our query and many of the documents,  then simply counting how big this intersection is will look like a gross measure. This fine line between one or two documents matching a query and an avalanche of thousands occurs regularly as part of FOA,  One part of the problem is normalization - the coordination level of intersecting features shared by both query and document seems a good measure of their similarity, but compared to what? It's easy to show that WEIGHTING AND MATCHING AGAINST INDICES       95  this normalization matters. Consider the case where CoordLevel{q, d) = 1, and imagine first that Kq = Kd = l also; i.e., a single-word query and a one-word document. In this case the two match on the one feature they each have. Intuitively, this is a perfect match; you couldn't do any better. But now imagine that the query includes 10 keywords, the document contains 1000 words, but still CoordLevel(q, d) = 1. The same intuition suggests that this should be judged a much poorer match, but our measure does not reveal this. Unless we normalize by something that reflects how many other features we might have matched, we can't have a useful measure of association.  One natural normalizer is to take the average of the number of terms in the query and in the document and compare the size of the intersection to it. This gives us the Dice coefficient:  |(icqn Kd)\  SimDice(q, d) = 2"?      J\                            (3.37)  Kq\ + \Kd\  The average number of features may or may not be appropriate, again depending on what a typical query and typical document are. Often a document has many, many keywords associated with it, and queries have only one or two. This average is not a very good characterization of either. Another perspective on similarity says that missing features are as significant as shared ones. The simple matching coefficient gives equal weight to those features included in both query and document and those excluded from both and normalizes by the total number of keyword features NKw:  - \(K*n **)!+ \((NKw ~ Klt;Jn (NKw ~ K^)\  NKw\  (3.38)   
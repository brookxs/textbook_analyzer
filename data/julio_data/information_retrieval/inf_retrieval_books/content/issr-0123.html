 10.4 Imagery Retrieval  Increasing volumes of imagery ó from web page images to personal collections from digital cameras ó have escalated the need for more effective and efficient imagery access. Researchers have identified needs for indexing and search of not only the metadata associated with the imagery (e.g., captions, annotations) but also retrieval directly on the content of the imagery. Initial algorithm development has focused on the automatic indexing of visual features of imagery (e.g., color, texture, shape) which can be used as a means for retrieving similar Multimedia Information Retrieval  247  images without the burden of manual indexing (Niblack and Jain, 1993, 1994, 1995). However, the ultimate objective is semantic based access to imagery.  Flicker et al.'s (1997) Query By Image Content (QBIC) system (www.qbic.almadenJbm.com, Seybold 1994) and its commercial version, Ultimedia Manager (www.ibm.com/software/data/umm/umm.html), exemplifies this imagery attribute indexing approach. QBIC supports access to imagery collections on the basis of visual properties such as color, shape, texture, and sketches (viewing from the Internet will show colors described in the text.). In their approach, query facilities for specifying color parameters, drawing desired shapes, or selecting textures replace the traditional keyword query found in text retrieval. For example, Figure 10.4a illustrates a query to a database of all US stamps prior to 1995 in which QBIC is asked to retrieve red images. The "red stamps" results are displayed in Figure 10.4b. If there are text captions associated with the imagery these of course can be exploited. For example, if we further refine this search by adding the keyword "president" we obtain the results shown in Figure 10.4c in which all stamps are both red in color and are related to "president". For the careful reader, the female stamp in the bottom right hand corner of Figure 10.4c is of Martha Washington from the presidential stamp collection.  QBIC  Stamps database courtesy of W  First select tpwemtopr by didnag´aw p´nªjng´i*to Hack cffamctaagSe of the desirec mat (ftaximtfigs of toe ´jS ´eo) M new ´gkms m pessted white, jwtMj Tbm sslact ft cete Ssxm toe este jacket to ptditi year kicUbs^b. Yam am ´tfa´fctci:ttgt;. the cote ªg´ttªlt;w Mtieryei-jfdessrsdRGBvTauftisit; She Uxt areas Repeattfaa far each desaowi cote and m pilt;± up to 5 cdat/pettenittgs pan  e it (ai%usl ths color percentage^. Select  rectange with  You r´pant etty selection, e  Figure 10.4a. QBIC Query by Color red 248  Chapter 10  Bt  £lt;St Vfew  So   frnrnrieatar  Figure 10.4b. Retrieved red stamps  Figure 10.4c. Red stamps of Presidents Multimedia Information Retrieval                                                              249  Using QBIC the user can also specify queries such as "find images with a coarsely textured, red round object and a green square". Because robust, domain independent object identification remains difficult and manual image annotation is tedious, the authors have developed automated and semiautomated object outlining tools (e.g., foreground/background models to extract objects) to facilitate database population.  More recently researchers have explored the application of content based imagery access to video retrieval. For example, Flicker et al. (1997) perform shot detection, extract a representative frame (r-frame, sometimes called keyframe) for each shot, and derive a layered representation of moving objects. This enables queries such as "find me all shots panning left to right" which yield a list of relevancy ranked r-frames (which acts as a thumbnail), selection of which retrieves the associated video shot.  Additional research in image processing has addressed specific kinds of content-based retrieval problems. Consider face processing, where we distinguish face detection (i.e., identifying a face or faces in a scene), face recognition (authenticating that a given face is of a particular person), and face retrieval (find the closest matching face in a repository given an example or some search criteria). For example, for the past few years the US Immigration and Naturalization Service has been using a face recognition system (www.faceit.com) to "watch" and approve registered "fast lane" drivers crossing the Otay Mesa port of entry at the US/Mexico border. Using a radiofrequency (RF) tag on the automobile, the system retrieves a picture of the driver registered with the automobile from a database, which is then matched to an image taken in real-time of the actual driver. If the verification is successful, the car is permitted to proceed without delay; if not, the vehicle is routed to an inspection station. Since FaceltÆ can find the head anywhere in the field of view of the camera, it works on any kind of vehicle (car, van, or sports utility). System performance can be assessed using measurements analogous to those used in text retrieval, such as precision and recall.  Researchers have also developed systems to track human movement (e.g., heads, hands, feet) and to differentiate human expressions (Pentland, 1997) such as a smile, surprise, anger, or disgust. This expression recognition is related to research in emotion recognition (Picard, 1997) in the context of human computer interaction. Face recognition is also important in video retrieval. For example, Wactlar et al's (2000) Informedia Digital Video Library system extracts information from audio and video and supports full content search over digitized video sources. Among other capabilities, Informedia provides a facility called named face which automatically associates a name with a face and enables the user to search for a face given a name and vice versa.   
 6.9 Text-Based Intelligence  Knowledge representation has always been a central issue for AI, and as a subdiscipline within computer science its primary contribution is probably the beginnings of a computational theory of knowledge. Although it is still too early to speak of such a theory, some key aspects of good knowledge representation are becoming clear [Belew and Forrest, 1988].  The text captured in document corpora was not entered with the intention of being part of a knowledge base. These are documents written by someone as part of a natural communication process, and any search engine technology simply gives this document added life. Alternatively, we can say that the document was intended to become part of a "knowledge base," but one that predates (at least the AI) use of that term: People publish their documents with the explicit hope that their ideas can become part of our collective wisdom and be used by others. 248      FINDING OUT ABOUT  Note the ease with which an author-as-knowledge engineer can express his or her knowledge. Hypertext knowledge bases are accessible to every writer. In this view, hypertext solves the key AI problem of the knowledge acquisition bottleneck, providing a knowledge representation language with the ease, flexibility, and expressiveness of natural language - by actually using natural language! The cost paid is the weakness of the inferences that can be made from a textual foundation: Contrast the strong theorem-proving notions of inference of Section 6.5.1 with the many confounded associations that arise in Swanson's analysis of latent knowledge in Section 6.5.3.  Grounding Symbols in Texts  According to Hamad's grounding hypothesis, if computers are ever to understand natural language as fully as humans, they must have an equally vast corpus of experience from which to draw [Harnad, 1987]. We propose that the huge volumes of natural language text managed by hypertext systems provide exactly the corpus of "experience" needed for such understanding. Each word in every document in a hypertext system constitutes a separate experiential "data point" about what that word means. The exciting prospect of using search engines as a basis for natural language-understanding systems is that their understanding of words, and concepts built from these words, will reflect the richness of this huge base of textual "experience." There are, of course, differences between the text-based "experience" and first-person, human experience, and these imply fundamental limits on language understanding derived from this source.  In this view, the computer's experience of the world is secondhand, from documents written by people about the world and subsequently through users' queries of the system. The "trick" is to learn what words mean by interacting with users who already know what the words mean, with the documents of the textual corpus forming the common referential base of experience.  The hypertext itself is in fact only the first source of information, viz., how authors use and juxtapose words. The second, ongoing source of experience is the subsequent interactions with users, a new population of people who use these same words and then react positively or negatively to the system's interpretation of them. Both the original authors and the INFERENCE BEYOND THE INDEX      249  browsing users function as the text-based intelligent system's "eyes" into the real world and how it looks to humans. That insight is something no video camera will ever give any robot.   
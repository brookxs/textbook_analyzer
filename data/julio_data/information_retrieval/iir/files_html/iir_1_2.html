<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>A first take at building an inverted index</title> 
  <meta name="description" content="A first take at building an inverted index" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="processing-boolean-queries-1.html" /> 
  <link rel="previous" href="an-example-information-retrieval-problem-1.html" /> 
  <link rel="up" href="boolean-retrieval-1.html" /> 
  <link rel="next" href="processing-boolean-queries-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html950" href="processing-boolean-queries-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html944" href="boolean-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html938" href="an-example-information-retrieval-problem-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html946" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html948" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html951" href="processing-boolean-queries-1.html">Processing Boolean queries</a> 
  <b> Up:</b> 
  <a name="tex2html945" href="boolean-retrieval-1.html">Boolean retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html939" href="an-example-information-retrieval-problem-1.html">An example information retrieval</a> &nbsp; 
  <b> <a name="tex2html947" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html949" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION00620000000000000000"></a><a name="sec:indexfirsttake"></a> <a name="p:indexfirsttake"></a> <br /> A first take at building an inverted index </h1> 
  <p> To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are: </p> 
  <ol> 
   <li>Collect the documents to be indexed: <br /> 
    <!-- MATH
 $\framebox{\weestrut Friends, Romans, countrymen.}$
 --> <img width="228" height="27" align="BOTTOM" border="0" src="img44.png" alt="\framebox{\weestrut Friends, Romans, countrymen.}" /> 
    <!-- MATH
 $\framebox{\weestrut So let it be with Caesar}$
 --> <img width="172" height="27" align="BOTTOM" border="0" src="img45.png" alt="\framebox{\weestrut So let it be with Caesar}" /> ... </li> 
   <li>Tokenize the text, turning each document into a list of tokens: <br /> 
    <!-- MATH
 $\framebox{\weestrut Friends}$
 --> <img width="65" height="27" align="BOTTOM" border="0" src="img46.png" alt="\framebox{\weestrut Friends}" /> 
    <!-- MATH
 $\framebox{\weestrut Romans}$
 --> <img width="69" height="27" align="BOTTOM" border="0" src="img47.png" alt="\framebox{\weestrut Romans}" /> 
    <!-- MATH
 $\framebox{\weestrut countrymen}$
 --> <img width="97" height="27" align="BOTTOM" border="0" src="img48.png" alt="\framebox{\weestrut countrymen}" /> 
    <!-- MATH
 $\framebox{\weestrut So}$
 --> <img width="29" height="27" align="BOTTOM" border="0" src="img49.png" alt="\framebox{\weestrut So}" /> ... </li> 
   <li>Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms: 
    <!-- MATH
 $\framebox{\weestrut friend}$
 --> <img width="55" height="27" align="BOTTOM" border="0" src="img50.png" alt="\framebox{\weestrut friend}" /> 
    <!-- MATH
 $\framebox{\weestrut roman}$
 --> <img width="58" height="27" align="BOTTOM" border="0" src="img51.png" alt="\framebox{\weestrut roman}" /> 
    <!-- MATH
 $\framebox{\weestrut countryman}$
 --> <img width="98" height="27" align="BOTTOM" border="0" src="img52.png" alt="\framebox{\weestrut countryman}" /> 
    <!-- MATH
 $\framebox{\weestrut so}$
 --> <img width="27" height="27" align="BOTTOM" border="0" src="img53.png" alt="\framebox{\weestrut so}" /> ... </li> 
   <li>Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings. </li> 
  </ol> We will define and discuss the earlier stages of processing, that is, steps 1-3, in Section&nbsp; 
  <a href="determining-the-vocabulary-of-terms-1.html#sec:dictionary-terms">2.2</a> (page&nbsp; 
  <a href="determining-the-vocabulary-of-terms-1.html#p:dictionary-terms"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>). Until then you can think of 
  <i>tokens</i> and 
  <i>normalized tokens</i> as also loosely equivalent to 
  <i>words</i>. Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by 
  <a name="1066"></a> 
  <i>sort-based indexing</i> . 
  <p> </p> 
  <div align="CENTER"> 
   <p><a name="fig:indexstart"></a><a name="p:indexstart"></a></p> 
   <img width="647" height="834" border="0" src="img54.png" alt="\begin{figure}
% latex2html id marker 1068
\begin{tabular}{p{2.3in}p{2.6in}}
\te...
...n each document) or the position(s) of the term in each
document.}
\end{figure}" /> 
  </div> 
  <p> Within a document collection, we assume that each document has a unique serial number, known as the document identifier <a name="p:docid"></a> (<a name="1190"></a> <i>docID</i> ). During index construction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure <a href="#fig:indexstart">1.4</a> . The core indexing step is <a name="1193"></a> <i>sorting</i> this list so that the terms are alphabetical, giving us the representation in the middle column of Figure <a href="#fig:indexstart">1.4</a> . Multiple occurrences of the same term from the same document are then merged.<a name="tex2html9" href="footnode.html#foot1546"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a>Instances of the same term are then grouped, and the result is split into a <a name="1198"></a> <i>dictionary</i> and <a name="1200"></a> <i>postings</i> , as shown in the right column of Figure <a href="#fig:indexstart">1.4</a> . Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the <a name="1203"></a> <i>document frequency</i> , which is here also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search. </p> 
  <p> In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter <a href="index-compression-1.html#ch:icompress">5</a> we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section <a href="faster-postings-list-intersection-via-skip-pointers-1.html#sec:skip-pointers">2.3</a> ), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory <a name="1207"></a>caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure <a href="an-example-information-retrieval-problem-1.html#fig:invertedindex-picture">1.3</a> ), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory. </p> 
  <p> <b>Exercises.</b> </p> 
  <ul> 
   <li>Draw the inverted index that would be built for the following document collection. (See Figure <a href="an-example-information-retrieval-problem-1.html#fig:invertedindex-picture">1.3</a> for an example.) 
    <blockquote> 
     <b>Doc 1</b>&nbsp;&nbsp;&nbsp;&nbsp;new home sales top forecasts 
     <br /> 
     <b>Doc 2</b>&nbsp;&nbsp;&nbsp;&nbsp;home sales rise in july 
     <br /> 
     <b>Doc 3</b>&nbsp;&nbsp;&nbsp;&nbsp;increase in home sales in july 
     <br /> 
     <b>Doc 4</b>&nbsp;&nbsp;&nbsp;&nbsp;july new home sales rise 
    </blockquote> <p> </p></li> 
   <li><a name="ex:small-collection"></a> <a name="p:small-collection"></a> Consider these documents: 
    <blockquote> 
     <b>Doc 1</b>&nbsp;&nbsp;&nbsp;&nbsp;breakthrough drug for schizophrenia 
     <br /> 
     <b>Doc 2</b>&nbsp;&nbsp;&nbsp;&nbsp;new schizophrenia drug 
     <br /> 
     <b>Doc 3</b>&nbsp;&nbsp;&nbsp;&nbsp;new approach for treatment of schizophrenia 
     <br /> 
     <b>Doc 4</b>&nbsp;&nbsp;&nbsp;&nbsp;new hopes for schizophrenia patients 
    </blockquote> 
    <ol> 
     <li>Draw the term-document incidence matrix for this document collection. </li> 
     <li>Draw the inverted index representation for this collection, as in Figure&nbsp;<a href="an-example-information-retrieval-problem-1.html#fig:invertedindex-picture">1.3</a> (page&nbsp;<a href="an-example-information-retrieval-problem-1.html#p:invertedindex-picture"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>). </li> 
    </ol> <p> </p></li> 
   <li>For the document collection shown in Exercise <a href="#ex:small-collection">1.2</a> , what are the returned results for these queries: 
    <ol> 
     <li>schizophrenia AND drug </li> 
     <li>for AND NOT(drug OR approach) </li> 
    </ol> <p> </p></li> 
  </ul> 
  <p> </p> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html950" href="processing-boolean-queries-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html944" href="boolean-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html938" href="an-example-information-retrieval-problem-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html946" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html948" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html951" href="processing-boolean-queries-1.html">Processing Boolean queries</a> 
  <b> Up:</b> 
  <a name="tex2html945" href="boolean-retrieval-1.html">Boolean retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html939" href="an-example-information-retrieval-problem-1.html">An example information retrieval</a> &nbsp; 
  <b> <a name="tex2html947" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html949" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
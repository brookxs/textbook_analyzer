<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Time complexity and optimality of kNN</title> 
  <meta name="description" content="Time complexity and optimality of kNN" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="previous" href="k-nearest-neighbor-1.html" /> 
  <link rel="up" href="k-nearest-neighbor-1.html" /> 
  <link rel="next" href="linear-versus-nonlinear-classifiers-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3720" href="linear-versus-nonlinear-classifiers-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3714" href="k-nearest-neighbor-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3710" href="k-nearest-neighbor-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3716" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3718" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3721" href="linear-versus-nonlinear-classifiers-1.html">Linear versus nonlinear classifiers</a> 
  <b> Up:</b> 
  <a name="tex2html3715" href="k-nearest-neighbor-1.html">k nearest neighbor</a> 
  <b> Previous:</b> 
  <a name="tex2html3711" href="k-nearest-neighbor-1.html">k nearest neighbor</a> &nbsp; 
  <b> <a name="tex2html3717" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3719" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h2><a name="SECTION001931000000000000000"> Time complexity and optimality of kNN</a> </h2> 
  <br /> 
  <p></p> 
  <div align="CENTER"> 
   <table cellpadding="3"> 
    <tbody> 
     <tr> 
      <td align="CENTER" colspan="2"><b>kNN with preprocessing of training set</b></td> 
     </tr> 
     <tr> 
      <td align="LEFT">training</td> 
      <td align="LEFT"> 
       <!-- MATH
 $\Theta(|\docsetlabeled| L_{ave})$
 --> <img width="83" height="33" align="MIDDLE" border="0" src="img914.png" alt="$\Theta(\vert\docsetlabeled\vert L_{ave})$" /></td> 
     </tr> 
     <tr> 
      <td align="LEFT">testing</td> 
      <td align="LEFT"> 
       <!-- MATH
 $\Theta( L_{a} + |\docsetlabeled|  M_{ave} M_{a})= \Theta(|\docsetlabeled|  M_{ave} M_{a})$
 --> <img width="277" height="33" align="MIDDLE" border="0" src="img1154.png" alt="$\Theta( L_{a} + \vert\docsetlabeled\vert M_{ave} M_{a})= \Theta(\vert\docsetlabeled\vert M_{ave} M_{a})$" /></td> 
     </tr> 
     <tr> 
      <td align="CENTER" colspan="2"><b>kNN without preprocessing of training set</b></td> 
     </tr> 
     <tr> 
      <td align="LEFT">training</td> 
      <td align="LEFT"><img width="39" height="33" align="MIDDLE" border="0" src="img1155.png" alt="$\Theta(1)$" /></td> 
     </tr> 
     <tr> 
      <td align="LEFT">testing</td> 
      <td align="LEFT"> 
       <!-- MATH
 $\Theta( L_{a} + |\docsetlabeled|  L_{ave} M_{a}) = \Theta(|\docsetlabeled|  L_{ave} M_{a})$
 --> <img width="265" height="33" align="MIDDLE" border="0" src="img1156.png" alt="$\Theta( L_{a} + \vert\docsetlabeled\vert L_{ave} M_{a}) = \Theta(\vert\docsetlabeled\vert L_{ave} M_{a})$" /></td> 
     </tr> 
    </tbody> 
   </table> Training and test times for kNN classification. 
   <img width="38" height="32" align="MIDDLE" border="0" src="img203.png" alt="$ M_{ave}$" /> is the average size of the vocabulary of documents in the collection. 
   <a name="tab:knncomp"></a> 
   <a name="p:knncomp"></a> 
  </div> 
  <br /> 
  <p> Table <a href="#tab:knncomp">14.3</a> gives the time complexity of kNN. kNN has properties that are quite different from most other classification algorithms. Training a kNN classifier simply consists of determining <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> and preprocessing documents. In fact, if we preselect a value for <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> and do not preprocess, then kNN requires no training at all. In practice, we have to perform preprocessing steps like tokenization. It makes more sense to preprocess training documents once as part of the training phase rather than repeatedly every time we classify a new test document. </p> 
  <p> Test time is 
   <!-- MATH
 $\Theta(|\docsetlabeled| M_{ave} M_{a})$
 --> <img width="112" height="33" align="MIDDLE" border="0" src="img1157.png" alt="$\Theta(\vert\docsetlabeled\vert M_{ave} M_{a})$" /> for kNN. It is linear in the size of the training set as we need to compute the distance of each training document from the test document. Test time is independent of the number of classes <img width="11" height="32" align="MIDDLE" border="0" src="img27.png" alt="$J$" />. kNN therefore has a potential advantage for problems with large <img width="11" height="32" align="MIDDLE" border="0" src="img27.png" alt="$J$" />. </p> 
  <p> In kNN classification, we do not perform any estimation of parameters as we do in Rocchio classification (centroids) or in Naive Bayes (priors and conditional probabilities). kNN simply memorizes all examples in the training set and then compares the test document to them. For this reason, kNN is also called <a name="20266"></a> <i>memory-based learning</i> or <a name="20268"></a> <i>instance-based learning</i> . It is usually desirable to have as much training data as possible in machine learning. But in kNN large training sets come with a severe efficiency penalty in classification. </p> 
  <p> <a name="p:knninvertedindexcomplexity"></a> Can kNN testing be made more efficient than 
   <!-- MATH
 $\Theta(|\docsetlabeled| M_{ave} M_{a})$
 --> <img width="112" height="33" align="MIDDLE" border="0" src="img1157.png" alt="$\Theta(\vert\docsetlabeled\vert M_{ave} M_{a})$" /> or, ignoring the length of documents, more efficient than 
   <!-- MATH
 $\Theta(|\docsetlabeled|)$
 --> <img width="55" height="33" align="MIDDLE" border="0" src="img1158.png" alt="$\Theta(\vert\docsetlabeled\vert)$" />? There are fast kNN algorithms for small dimensionality <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> (Exercise <a href="exercises-2.html#ex:knndim2dim3">14.8</a> ). There are also approximations for large <img width="20" height="32" align="MIDDLE" border="0" src="img186.png" alt="$M$" /> that give error bounds for specific efficiency gains (see Section <a href="references-and-further-reading-14.html#sec:vclassfurther">14.7</a> ). These approximations have not been extensively tested for text classification applications, so it is not clear whether they can achieve much better efficiency than 
   <!-- MATH
 $\Theta(|\docsetlabeled|)$
 --> <img width="55" height="33" align="MIDDLE" border="0" src="img1158.png" alt="$\Theta(\vert\docsetlabeled\vert)$" /> without a significant loss of accuracy. </p> 
  <p> The reader may have noticed the similarity between the problem of finding nearest neighbors of a test document and ad hoc retrieval, where we search for the documents with the highest similarity to the query (Section <a href="queries-as-vectors-1.html#sec:queryvector">6.3.2</a> , page <a href="queries-as-vectors-1.html#p:queryvector">6.3.2</a> ). In fact, the two problems are both <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> nearest neighbor problems and only differ in the relative density of (the vector of) the test document in kNN (10s or 100s of non-zero entries) versus the sparseness of (the vector of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries). We introduced the inverted index for efficient ad hoc retrieval in Section <a href="an-example-information-retrieval-problem-1.html#sec:example-problem">1.1</a> (page <a href="an-example-information-retrieval-problem-1.html#p:invertedindex">1.1</a> ). Is the inverted index also the solution for efficient kNN? </p> 
  <p> An inverted index restricts a search to those documents that have at least one term in common with the query. Thus in the context of kNN, the inverted index will be efficient if the test document has no term overlap with a large number of training documents. Whether this is the case depends on the classification problem. If documents are long and no stop list is used, then less time will be saved. But with short documents and a large stop list, an inverted index may well cut the average test time by a factor of 10 or more. </p> 
  <p> The search time in an inverted index is a function of the length of the postings lists of the terms in the query. Postings lists grow sublinearly with the length of the collection since the vocabulary increases according to Heaps' law - if the probability of occurrence of some terms increases, then the probability of occurrence of others must decrease. However, most new terms are infrequent. We therefore take the complexity of inverted index search to be <img width="41" height="33" align="MIDDLE" border="0" src="img124.png" alt="$\Theta(T)$" /> (as discussed in Section <a href="positional-indexes-1.html#sec:positional-index">2.4.2</a> , page <a href="positional-indexes-1.html#p:positional-index">2.4.2</a> ) and, assuming average document length does not change over time, 
   <!-- MATH
 $\Theta(T)=\Theta(|\docsetlabeled|)$
 --> <img width="115" height="33" align="MIDDLE" border="0" src="img1159.png" alt="$\Theta(T)=\Theta(\vert\docsetlabeled\vert)$" />. </p> 
  <p> <a name="p:knnmacroaverage"></a> As we will see in the next chapter, kNN's effectiveness is close to that of the most accurate learning methods in text classification (Table <a href="experimental-results-1.html#tab:reuters-joachims">15.2</a> , page <a href="experimental-results-1.html#p:reuters-joachims">15.2</a> ). A measure of the quality of a learning method is its <a name="20284"></a> <i>Bayes error rate</i> , the average error rate of classifiers learned by it for a particular problem. kNN is not optimal for problems with a non-zero Bayes error rate - that is, for problems where even the best possible classifier has a non-zero classification error. The error of 1NN is asymptotically (as the training set increases) bounded by twice the Bayes error rate. That is, if the optimal classifier has an error rate of <img width="12" height="32" align="MIDDLE" border="0" src="img58.png" alt="$x$" />, then 1NN has an asymptotic error rate of less than <img width="20" height="32" align="MIDDLE" border="0" src="img1160.png" alt="$2x$" />. This is due to the effect of noise - we already saw one example of noise in the form of noisy features in Section <a href="feature-selection-1.html#sec:feature">13.5</a> (page <a href="feature-selection-1.html#p:noisefeature">13.5</a> ), but noise can also take other forms as we will discuss in the next section. Noise affects two components of kNN: the test document and the closest training document. The two sources of noise are additive, so the overall error of 1NN is twice the optimal error rate. For problems with Bayes error rate 0, the error rate of 1NN will approach 0 as the size of the training set increases. </p> 
  <p> <b>Exercises.</b> </p> 
  <ul> 
   <li>Explain why kNN handles multimodal classes better than Rocchio. <p> </p></li> 
  </ul> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3720" href="linear-versus-nonlinear-classifiers-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3714" href="k-nearest-neighbor-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3710" href="k-nearest-neighbor-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3716" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3718" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3721" href="linear-versus-nonlinear-classifiers-1.html">Linear versus nonlinear classifiers</a> 
  <b> Up:</b> 
  <a name="tex2html3715" href="k-nearest-neighbor-1.html">k nearest neighbor</a> 
  <b> Previous:</b> 
  <a name="tex2html3711" href="k-nearest-neighbor-1.html">k nearest neighbor</a> &nbsp; 
  <b> <a name="tex2html3717" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3719" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>The bias-variance tradeoff</title> 
  <meta name="description" content="The bias-variance tradeoff" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="references-and-further-reading-14.html" /> 
  <link rel="previous" href="classification-with-more-than-two-classes-1.html" /> 
  <link rel="up" href="vector-space-classification-1.html" /> 
  <link rel="next" href="references-and-further-reading-14.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3762" href="references-and-further-reading-14.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3756" href="vector-space-classification-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3750" href="classification-with-more-than-two-classes-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3758" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3760" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3763" href="references-and-further-reading-14.html">References and further reading</a> 
  <b> Up:</b> 
  <a name="tex2html3757" href="vector-space-classification-1.html">Vector space classification</a> 
  <b> Previous:</b> 
  <a name="tex2html3751" href="classification-with-more-than-two-classes-1.html">Classification with more than</a> &nbsp; 
  <b> <a name="tex2html3759" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3761" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001960000000000000000"></a> <a name="sec:secbiasvariance"></a> <a name="p:secbiasvariance"></a> <br /> The bias-variance tradeoff </h1> 
  <p> Nonlinear classifiers are more powerful than linear classifiers. For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier. Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification? </p> 
  <p> To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning. The tradeoff helps explain why there is no universally optimal learning method. Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem. </p> 
  <p> Throughout this section, we use linear and nonlinear classifiers as prototypical examples of ``less powerful'' and ``more powerful'' learning, respectively. This is a simplification for a number of reasons. First, many nonlinear models subsume linear models as a special case. For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier. Second, there are nonlinear models that are less complex than linear models. For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier. Third, the complexity of learning is not really a property of the classifier because there are many aspects of learning (such as feature selection, cf. feature, regularization, and constraints such as margin maximization in Chapter <a href="support-vector-machines-and-machine-learning-on-documents-1.html#ch:svm">15</a> ) that make a learning method either more powerful or less powerful without affecting the type of classifier that is the final result of learning - regardless of whether that classifier is linear or nonlinear. We refer the reader to the publications listed in Section <a href="references-and-further-reading-14.html#sec:vclassfurther">14.7</a> for a treatment of the bias-variance tradeoff that takes into account these complexities. In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification. </p> 
  <p> We first need to state our objective in text classification more precisely. In Section&nbsp;<a href="the-text-classification-problem-1.html#sec:classificationproblem">13.1</a> (page&nbsp;<a href="the-text-classification-problem-1.html#p:classificationproblem"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>), we said that we want to minimize classification error on the test set. The implicit assumption was that training documents and test documents are generated according to the same underlying distribution. We will denote this distribution 
   <!-- MATH
 $P(\langle d,c\rangle)$
 --> <img width="64" height="33" align="MIDDLE" border="0" src="img1197.png" alt="$P(\langle d,c\rangle)$" /> where <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" /> is the document and <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" /> its label or class. graphclassmodelbernoulligraph were examples of <a name="20573"></a>generative models that decompose 
   <!-- MATH
 $P(\langle d,c\rangle)$
 --> <img width="64" height="33" align="MIDDLE" border="0" src="img1197.png" alt="$P(\langle d,c\rangle)$" /> into the product of <img width="35" height="33" align="MIDDLE" border="0" src="img870.png" alt="$P(c)$" /> and <img width="48" height="33" align="MIDDLE" border="0" src="img1187.png" alt="$P(d\vert c)$" />. typicallineartypicalnonlinear depict generative models for 
   <!-- MATH
 $\langle d,c\rangle$
 --> <img width="39" height="33" align="MIDDLE" border="0" src="img1198.png" alt="$\langle d,c\rangle$" /> with 
   <!-- MATH
 $d \in \mathbb{R}^2$
 --> <img width="52" height="36" align="MIDDLE" border="0" src="img1199.png" alt="$d \in \mathbb{R}^2$" /> and 
   <!-- MATH
 $c \in \{\mbox{square},\mbox{solid circle} \}$
 --> <img width="177" height="33" align="MIDDLE" border="0" src="img1200.png" alt="$c \in \{\mbox{square},\mbox{solid circle} \}$" />. </p> 
  <p> In this section, instead of using the number of correctly classified test documents (or, equivalently, the error rate on test documents) as evaluation measure, we adopt an evaluation measure that addresses the inherent uncertainty of labeling. In many text classification problems, a given document representation can arise from documents belonging to different classes. This is because documents from different classes can be mapped to the same document representation. For example, the one-sentence documents China sues France and France sues China are mapped to the same document representation 
   <!-- MATH
 $d' = \{ \mbox{\term{China}} ,
\mbox{\term{France}} , \mbox{\term{sues}} \}$
 --> <img width="180" height="35" align="MIDDLE" border="0" src="img1201.png" alt="$d' = \{ \mbox{\term{China}} ,
\mbox{\term{France}} , \mbox{\term{sues}} \}$" /> in a bag of words model. But only the latter document is relevant to the class <img width="33" height="35" align="MIDDLE" border="0" src="img1202.png" alt="$c'=$" /> legal actions brought by France (which might be defined, for example, as a standing query by an international trade lawyer). </p> 
  <p> To simplify the calculations in this section, we do not count the number of errors on the test set when evaluating a classifier, but instead look at how well the classifier estimates the conditional probability <img width="48" height="33" align="MIDDLE" border="0" src="img868.png" alt="$P(c\vert d)$" /> of a document being in a class. In the above example, we might have 
   <!-- MATH
 $P(c'|d') = 0.5$
 --> <img width="99" height="35" align="MIDDLE" border="0" src="img1203.png" alt="$P(c'\vert d') = 0.5$" />. </p> 
  <p> Our goal in text classification then is to find a classifier <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> such that, averaged over documents <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" />, <img width="36" height="33" align="MIDDLE" border="0" src="img1204.png" alt="$\gamma(d)$" /> is as close as possible to the true probability <img width="48" height="33" align="MIDDLE" border="0" src="img868.png" alt="$P(c\vert d)$" />. We measure this using mean squared error: <br /> </p> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray}
\mbox{MSE}(\gamma) = 
E_{\onedoc}
[\gamma(\onedoc)
- P(c|\onedoc)]^2
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="213" height="38" align="MIDDLE" border="0" src="img1205.png" alt="$\displaystyle \mbox{MSE}(\gamma) =
E_{\onedoc}
[\gamma(\onedoc)
- P(c\vert\onedoc)]^2$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (148)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> where 
  <img width="21" height="32" align="MIDDLE" border="0" src="img1206.png" alt="$E_{\onedoc}$" /> is the expectation with respect to 
  <img width="36" height="33" align="MIDDLE" border="0" src="img815.png" alt="$P(d)$" />. The mean squared error term gives partial credit for decisions by 
  <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> that are close if not completely right. 
  <p> We define a classifier <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> to be <a name="20590"></a> <i>optimal</i> for a distribution 
   <!-- MATH
 $P(\onedoclabeled)$
 --> <img width="64" height="33" align="MIDDLE" border="0" src="img1207.png" alt="$P(\onedoclabeled)$" /> if it minimizes 
   <!-- MATH
 $\mbox{MSE}(\gamma)$
 --> <img width="61" height="33" align="MIDDLE" border="0" src="img1208.png" alt="$\mbox{MSE}(\gamma)$" />. </p> 
  <p> Minimizing MSE is a desideratum for <i>classifiers</i>. We also need a criterion for <i>learning methods</i>. Recall that we defined a learning method <img width="13" height="32" align="MIDDLE" border="0" src="img861.png" alt="$\Gamma$" /> as a function that takes a labeled training set 
   <!-- MATH
 $\docsetlabeled$
 --> <img width="18" height="32" align="MIDDLE" border="0" src="img856.png" alt="$\docsetlabeled$" /> as input and returns a classifier <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" />. </p> 
  <p> For learning methods, we adopt as our goal to find a <img width="13" height="32" align="MIDDLE" border="0" src="img861.png" alt="$\Gamma$" /> that, averaged over training sets, learns classifiers <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> with minimal MSE. We can formalize this as minimizing <a name="20595"></a> <i>learning error</i> : <br /> </p> 
  <div align="CENTER"> 
   <a name="p:learningerror"></a> 
   <a name="learningerror"></a> 
   <!-- MATH
 \begin{eqnarray}
\mbox{learning-error}(\Gamma) = E_{\docsetlabeled}
[\mbox{MSE}(\Gamma(\docsetlabeled))]
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="264" height="33" align="MIDDLE" border="0" src="img1209.png" alt="$\displaystyle \mbox{learning-error}(\Gamma) = E_{\docsetlabeled}
[\mbox{MSE}(\Gamma(\docsetlabeled))]$" /></td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (149)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> where 
  <!-- MATH
 $E_{\docsetlabeled}$
 --> 
  <img width="26" height="32" align="MIDDLE" border="0" src="img1210.png" alt="$E_{\docsetlabeled}$" /> is the expectation over labeled training sets. To keep things simple, we can assume that training sets have a fixed size - the distribution 
  <!-- MATH
 $P(\onedoclabeled)$
 --> 
  <img width="64" height="33" align="MIDDLE" border="0" src="img1207.png" alt="$P(\onedoclabeled)$" /> then defines a distribution 
  <!-- MATH
 $P(\docsetlabeled)$
 --> 
  <img width="43" height="33" align="MIDDLE" border="0" src="img1211.png" alt="$P(\docsetlabeled)$" /> over training sets. 
  <p> We can use learning error as a criterion for selecting a learning method in statistical text classification. A learning method <img width="13" height="32" align="MIDDLE" border="0" src="img861.png" alt="$\Gamma$" /> is <a name="20605"></a> <i>optimal</i> for a distribution 
   <!-- MATH
 $P(\docsetlabeled)$
 --> <img width="43" height="33" align="MIDDLE" border="0" src="img1211.png" alt="$P(\docsetlabeled)$" /> if it minimizes the learning error. </p> 
  <p> <br /> </p> 
  <div align="CENTER"> 
   <a name="basicidentity"></a> 
   <a name="bvdecomposition"></a> 
   <!-- MATH
 \begin{eqnarray}
E[x-\alpha]^2 & = & E x^2 -2Ex\alpha +\alpha^2\\
& = & (Ex)^2 -2Ex\alpha +\alpha^2\\
& & + E x^2 - 2(Ex)^2 +(Ex)^2\\
& = & [Ex-\alpha]^2\\
& & + E x^2 - E2x(Ex) +E(Ex)^2\\
& = & [Ex-\alpha]^2 +
E[x-Ex]^2\\
\\
E_\docsetlabeled E_d[ \Gamma_\docsetlabeled
(\onedoc)-P(c|d)]^2 
& = & 
E_d E_\docsetlabeled[\Gamma_\docsetlabeled (\onedoc)-P(c|d)]^2 \\
& = & 
E_d[\ \ 
[E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)-P(c|d)]^2
\\
&&+
E_\docsetlabeled[\Gamma_\docsetlabeled
  (\onedoc)-E_\docsetlabeled\Gamma_\docsetlabeled
  (\onedoc)]^2\ \  ]
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="68" height="38" align="MIDDLE" border="0" src="img1212.png" alt="$\displaystyle E[x-\alpha]^2$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="120" height="38" align="MIDDLE" border="0" src="img1213.png" alt="$\displaystyle E x^2 -2Ex\alpha +\alpha^2$" /></td> 
      <td width="10" align="RIGHT"> (150)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="134" height="38" align="MIDDLE" border="0" src="img1214.png" alt="$\displaystyle (Ex)^2 -2Ex\alpha +\alpha^2$" /></td> 
      <td width="10" align="RIGHT"> (151)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td>&nbsp;</td> 
      <td align="LEFT" nowrap=""><img width="169" height="38" align="MIDDLE" border="0" src="img1215.png" alt="$\displaystyle + E x^2 - 2(Ex)^2 +(Ex)^2$" /></td> 
      <td width="10" align="RIGHT"> (152)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="68" height="38" align="MIDDLE" border="0" src="img1216.png" alt="$\displaystyle [Ex-\alpha]^2$" /></td> 
      <td width="10" align="RIGHT"> (153)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td>&nbsp;</td> 
      <td align="LEFT" nowrap=""><img width="192" height="38" align="MIDDLE" border="0" src="img1217.png" alt="$\displaystyle + E x^2 - E2x(Ex) +E(Ex)^2$" /></td> 
      <td width="10" align="RIGHT"> (154)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="162" height="38" align="MIDDLE" border="0" src="img1218.png" alt="$\displaystyle [Ex-\alpha]^2 +
E[x-Ex]^2$" /></td> 
      <td width="10" align="RIGHT"> (155)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td>&nbsp;</td> 
      <td>&nbsp;</td> 
      <td width="10" align="RIGHT"> (156)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="167" height="38" align="MIDDLE" border="0" src="img1219.png" alt="$\displaystyle E_\docsetlabeled E_d[ \Gamma_\docsetlabeled
(\onedoc)-P(c\vert d)]^2$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="167" height="38" align="MIDDLE" border="0" src="img1220.png" alt="$\displaystyle E_d E_\docsetlabeled[\Gamma_\docsetlabeled (\onedoc)-P(c\vert d)]^2$" /></td> 
      <td width="10" align="RIGHT"> (157)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="180" height="38" align="MIDDLE" border="0" src="img1221.png" alt="$\displaystyle E_d[\ \
[E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)-P(c\vert d)]^2$" /></td> 
      <td width="10" align="RIGHT"> (158)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td>&nbsp;</td> 
      <td align="LEFT" nowrap=""><img width="196" height="38" align="MIDDLE" border="0" src="img1222.png" alt="$\displaystyle +
E_\docsetlabeled[\Gamma_\docsetlabeled
(\onedoc)-E_\docsetlabeled\Gamma_\docsetlabeled
(\onedoc)]^2\ \ ]$" /></td> 
      <td width="10" align="RIGHT"> (159)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> Arithmetic transformations for the bias-variance decomposition. For the derivation of Equation&nbsp; 
  <a href="#bvdecomposition">157</a>, we set 
  <!-- MATH
 $\alpha =
  P(c|d)$
 --> 
  <img width="79" height="33" align="MIDDLE" border="0" src="img1223.png" alt="$\alpha =
P(c\vert d)$" /> and 
  <!-- MATH
 $x=\Gamma_\docsetlabeled (d)$
 --> 
  <img width="78" height="33" align="MIDDLE" border="0" src="img1224.png" alt="$x=\Gamma_\docsetlabeled (d)$" /> in Equation&nbsp; 
  <a href="#basicidentity">150</a>. 
  <a name="fig:biasvarmath"></a> 
  <a name="p:biasvarmath"></a> 
  <p> Writing 
   <!-- MATH
 $\Gamma_\docsetlabeled$
 --> <img width="25" height="32" align="MIDDLE" border="0" src="img1225.png" alt="$\Gamma_\docsetlabeled$" /> for 
   <!-- MATH
 $\Gamma(\docsetlabeled)$
 --> <img width="41" height="33" align="MIDDLE" border="0" src="img1226.png" alt="$\Gamma(\docsetlabeled)$" /> for better readability, we can transform Equation <a href="#learningerror">149</a> as follows: <br /> </p> 
  <div align="CENTER"> 
   <a name="eqn:biasvarraw"></a> 
   <a name="eqn:biasvar"></a> 
   <!-- MATH
 \begin{eqnarray}
\mbox{learning-error}(\Gamma) &= &E_{\docsetlabeled}
[\mbox{MSE}(\Gamma_\docsetlabeled)]
 \\
&= &
E_{\docsetlabeled} 
E_{\onedoc}
[\Gamma_\docsetlabeled (\onedoc)
- P(c|\onedoc)]^2\\
&= & 
E_{\onedoc}
[\mbox{bias}(\Gamma,\onedoc) +
\mbox{variance}(\Gamma,\onedoc) ]\\
\mbox{bias} (\Gamma,\onedoc)
& = &
[
P(c|\onedoc)
- 
E_{\docsetlabeled}
\Gamma_\docsetlabeled (\onedoc)
]^2
\\
\mbox{variance}(\Gamma,\onedoc)
& = &
E_{\docsetlabeled}
[
\Gamma_\docsetlabeled (\onedoc)
-
E_{\docsetlabeled}
\Gamma_\docsetlabeled (\onedoc)
]^2
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="126" height="33" align="MIDDLE" border="0" src="img1227.png" alt="$\displaystyle \mbox{learning-error}(\Gamma)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="104" height="33" align="MIDDLE" border="0" src="img1228.png" alt="$\displaystyle E_{\docsetlabeled}
[\mbox{MSE}(\Gamma_\docsetlabeled)]$" /></td> 
      <td width="10" align="RIGHT"> (160)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="167" height="38" align="MIDDLE" border="0" src="img1229.png" alt="$\displaystyle E_{\docsetlabeled}
E_{\onedoc}
[\Gamma_\docsetlabeled (\onedoc)
- P(c\vert\onedoc)]^2$" /></td> 
      <td width="10" align="RIGHT"> (161)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="214" height="33" align="MIDDLE" border="0" src="img1230.png" alt="$\displaystyle E_{\onedoc}
[\mbox{bias}(\Gamma,\onedoc) +
\mbox{variance}(\Gamma,\onedoc) ]$" /></td> 
      <td width="10" align="RIGHT"> (162)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="70" height="33" align="MIDDLE" border="0" src="img1231.png" alt="$\displaystyle \mbox{bias} (\Gamma,\onedoc)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="149" height="38" align="MIDDLE" border="0" src="img1232.png" alt="$\displaystyle [
P(c\vert\onedoc)
-
E_{\docsetlabeled}
\Gamma_\docsetlabeled (\onedoc)
]^2$" /></td> 
      <td width="10" align="RIGHT"> (163)</td> 
     </tr> 
     <tr valign="MIDDLE"> 
      <td nowrap="" align="RIGHT"><img width="102" height="33" align="MIDDLE" border="0" src="img1233.png" alt="$\displaystyle \mbox{variance}(\Gamma,\onedoc)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="170" height="38" align="MIDDLE" border="0" src="img1234.png" alt="$\displaystyle E_{\docsetlabeled}
[
\Gamma_\docsetlabeled (\onedoc)
-
E_{\docsetlabeled}
\Gamma_\docsetlabeled (\onedoc)
]^2$" /></td> 
      <td width="10" align="RIGHT"> (164)</td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <br clear="ALL" /> 
  <p></p> where the equivalence between and 
  <a href="#eqn:biasvar">162</a> is shown in Equation&nbsp; 
  <a href="#bvdecomposition">157</a> in Figure 
  <a href="#fig:biasvarmath">14.6</a> . Note that 
  <img width="12" height="31" align="MIDDLE" border="0" src="img994.png" alt="$\onedoc$" /> and 
  <!-- MATH
 $\docsetlabeled$
 --> 
  <img width="18" height="32" align="MIDDLE" border="0" src="img856.png" alt="$\docsetlabeled$" /> are independent of each other. In general, for a random document 
  <img width="12" height="31" align="MIDDLE" border="0" src="img994.png" alt="$\onedoc$" /> and a random training set 
  <!-- MATH
 $\docsetlabeled$
 --> 
  <img width="18" height="32" align="MIDDLE" border="0" src="img856.png" alt="$\docsetlabeled$" />, 
  <!-- MATH
 $\docsetlabeled$
 --> 
  <img width="18" height="32" align="MIDDLE" border="0" src="img856.png" alt="$\docsetlabeled$" /> does not contain a labeled instance of 
  <img width="12" height="31" align="MIDDLE" border="0" src="img994.png" alt="$\onedoc$" />. 
  <p> <a name="20637"></a> <i>Bias</i> is the squared difference between <img width="48" height="33" align="MIDDLE" border="0" src="img868.png" alt="$P(c\vert d)$" />, the true conditional probability of <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" /> being in <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />, and 
   <!-- MATH
 $\Gamma_\docsetlabeled (\onedoc)$
 --> <img width="47" height="33" align="MIDDLE" border="0" src="img1235.png" alt="$\Gamma_\docsetlabeled (\onedoc)$" />, the prediction of the learned classifier, averaged over training sets. Bias is large if the learning method produces classifiers that are consistently wrong. Bias is small if (i) the classifiers are consistently right or (ii) different training sets cause errors on different documents or (iii) different training sets cause positive and negative errors on the same documents, but that average out to close to 0. If one of these three conditions holds, then 
   <!-- MATH
 $E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)$
 --> <img width="69" height="33" align="MIDDLE" border="0" src="img1236.png" alt="$E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)$" />, the expectation over all training sets, is close to <img width="48" height="33" align="MIDDLE" border="0" src="img868.png" alt="$P(c\vert d)$" />. </p> 
  <p> Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary, a linear hyperplane. If the <a name="20639"></a> <i>generative model</i> 
   <!-- MATH
 $P(\onedoclabeled)$
 --> <img width="64" height="33" align="MIDDLE" border="0" src="img1207.png" alt="$P(\onedoclabeled)$" /> has a complex nonlinear class boundary, the bias term in Equation&nbsp;<a href="#eqn:biasvar">162</a> will be high because a large number of points will be consistently misclassified. For example, the circular enclave in Figure <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicalnonlinear">14.11</a> does not fit a linear model and will be misclassified consistently by linear classifiers. </p> 
  <p> We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier. If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method. But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average. </p> 
  <p> Nonlinear methods like kNN have low bias. We can see in Figure <a href="rocchio-classification-1.html#fig:knnboundaries">14.6</a> that the decision boundaries of kNN are variable - depending on the distribution of documents in the training set, learned decision boundaries can vary greatly. As a result, each document has a chance of being classified correctly for some training sets. The average prediction 
   <!-- MATH
 $E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)$
 --> <img width="69" height="33" align="MIDDLE" border="0" src="img1236.png" alt="$E_\docsetlabeled\Gamma_\docsetlabeled (\onedoc)$" /> is therefore closer to <img width="48" height="33" align="MIDDLE" border="0" src="img1237.png" alt="$P(c\vert\onedoc)$" /> and bias is smaller than for a linear learning method. </p> 
  <p> <a name="20644"></a> <i>Variance</i> is the variation of the prediction of learned classifiers: the average squared difference between 
   <!-- MATH
 $\Gamma_\docsetlabeled (d)$
 --> <img width="47" height="33" align="MIDDLE" border="0" src="img1238.png" alt="$\Gamma_\docsetlabeled (d)$" /> and its average 
   <!-- MATH
 $E_\docsetlabeled \Gamma_\docsetlabeled (d)$
 --> <img width="69" height="33" align="MIDDLE" border="0" src="img1239.png" alt="$E_\docsetlabeled \Gamma_\docsetlabeled (d)$" />. Variance is large if different training sets 
   <!-- MATH
 $\docsetlabeled$
 --> <img width="18" height="32" align="MIDDLE" border="0" src="img856.png" alt="$\docsetlabeled$" /> give rise to very different classifiers 
   <!-- MATH
 $\Gamma_\docsetlabeled$
 --> <img width="25" height="32" align="MIDDLE" border="0" src="img1225.png" alt="$\Gamma_\docsetlabeled$" />. It is small if the training set has a minor effect on the classification decisions 
   <!-- MATH
 $\Gamma_\docsetlabeled$
 --> <img width="25" height="32" align="MIDDLE" border="0" src="img1225.png" alt="$\Gamma_\docsetlabeled$" /> makes, be they correct or incorrect. Variance measures how inconsistent the decisions are, not whether they are correct or incorrect. </p> 
  <p> Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes. The decision lines produced by linear learning methods in and <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicalnonlinear">14.11</a> will deviate slightly from the main class boundaries, depending on the training set, but the class assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected. The circular enclave in Figure <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicalnonlinear">14.11</a> will be consistently misclassified. </p> 
  <p> Nonlinear methods like kNN have high variance. It is apparent from Figure <a href="rocchio-classification-1.html#fig:knnboundaries">14.6</a> that kNN can model very complex boundaries between two classes. It is therefore sensitive to noise documents of the sort depicted in Figure <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicallinear">14.10</a> . As a result the variance term in Equation&nbsp;<a href="#eqn:biasvar">162</a> is large for kNN: Test documents are sometimes misclassified - if they happen to be close to a noise document in the training set - and sometimes correctly classified - if there are no noise documents in the training set near them. This results in high variation from training set to training set. </p> 
  <p> High-variance learning methods are prone to <a name="20651"></a> <i>overfitting</i> the training data. The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution 
   <!-- MATH
 $P(\onedoclabeled)$
 --> <img width="64" height="33" align="MIDDLE" border="0" src="img1207.png" alt="$P(\onedoclabeled)$" />. In overfitting, the learning method also learns from noise. Overfitting increases MSE and frequently is a problem for high-variance learning methods. </p> 
  <p> We can also think of variance as the <a name="20653"></a> <i>model complexity</i> or, equivalently, <a name="20655"></a> <i>memory capacity</i> of the learning method - how detailed a characterization of the training set it can remember and then apply to new data. This capacity corresponds to the number of independent parameters available to fit the training set. Each kNN neighborhood <img width="19" height="32" align="MIDDLE" border="0" src="img1240.png" alt="$S_k$" /> makes an independent classification decision. The parameter in this case is the estimate 
   <!-- MATH
 $\hat{P}(c|S_k)$
 --> <img width="56" height="38" align="MIDDLE" border="0" src="img1241.png" alt="$\hat{P}(c\vert S_k)$" /> from Figure <a href="k-nearest-neighbor-1.html#fig:knnalgorithm">14.7</a> . Thus, kNN's capacity is only limited by the size of the training set. It can memorize arbitrarily large training sets. In contrast, the number of parameters of Rocchio is fixed - <img width="11" height="32" align="MIDDLE" border="0" src="img27.png" alt="$J$" /> parameters per dimension, one for each centroid - and independent of the size of the training set. The Rocchio classifier (in form of the centroids defining it) cannot ``remember'' fine-grained details of the distribution of the documents in the training set. </p> 
  <p> According to Equation&nbsp;<a href="#learningerror">149</a>, our goal in selecting a learning method is to minimize learning error. The fundamental insight captured by Equation&nbsp;<a href="#eqn:biasvar">162</a>, which we can succinctly state as: learning-error = bias + variance, is that the learning error has two components, bias and variance, which in general cannot be minimized simultaneously. When comparing two learning methods <img width="20" height="32" align="MIDDLE" border="0" src="img1242.png" alt="$\Gamma_1$" /> and <img width="20" height="32" align="MIDDLE" border="0" src="img1243.png" alt="$\Gamma_2$" />, in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance. The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias). Instead, we have to weigh the respective merits of bias and variance in our application and choose accordingly. This tradeoff is called the <a name="p:biasvariance"></a> <a name="20662"></a> <i>bias-variance tradeoff</i> . </p> 
  <p> Figure <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicallinear">14.10</a> provides an illustration, which is somewhat contrived, but will be useful as an example for the tradeoff. Some Chinese text contains English words written in the Roman alphabet like CPU, ONLINE, and GPS. Consider the task of distinguishing Chinese-only web pages from mixed Chinese-English web pages. A search engine might offer Chinese users without knowledge of English (but who understand loanwords like CPU) the option of filtering out mixed pages. We use two features for this classification task: number of Roman alphabet characters and number of Chinese characters on the web page. As stated earlier, the distribution 
   <!-- MATH
 $P(\onedoclabeled$
 --> <img width="56" height="33" align="MIDDLE" border="0" src="img1244.png" alt="$P(\onedoclabeled$" />) of the generative model generates most mixed (respectively, Chinese) documents above (respectively, below) the short-dashed line, but there are a few noise documents. </p> 
  <p> In Figure <a href="linear-versus-nonlinear-classifiers-1.html#fig:typicallinear">14.10</a> , we see three classifiers: </p> 
  <ul> 
   <li><b>One-feature classifier.</b> Shown as a dotted horizontal line. This classifier uses only one feature, the number of Roman alphabet characters. Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents). So a learning method producing this type of classifier has low variance. But its bias is high since it will consistently misclassify squares in the lower left corner and ``solid circle'' documents with more than 50 Roman characters. </li> 
   <li><b>Linear classifier.</b> Shown as a dashed line with long dashes. Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified. The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets. Thus, very few documents (documents close to the class boundary) will be inconsistently classified. </li> 
   <li><b>``Fit-training-set-perfectly'' classifier.</b> Shown as a solid line. Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set. This method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test set right. But the variance of this learning method is high. Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified - something that a linear learning method is unlikely to do. </li> 
  </ul> 
  <p> It is perhaps surprising that so many of the best-known text classification algorithms are linear. Some of these methods, in particular linear SVMs, regularized logistic regression and regularized linear regression, are among the most effective known methods. The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise <a href="exercises-2.html#ex:separablehigh">14.8</a> ). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases. </p> 
  <p> </p> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3762" href="references-and-further-reading-14.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3756" href="vector-space-classification-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3750" href="classification-with-more-than-two-classes-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3758" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3760" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3763" href="references-and-further-reading-14.html">References and further reading</a> 
  <b> Up:</b> 
  <a name="tex2html3757" href="vector-space-classification-1.html">Vector space classification</a> 
  <b> Previous:</b> 
  <a name="tex2html3751" href="classification-with-more-than-two-classes-1.html">Classification with more than</a> &nbsp; 
  <b> <a name="tex2html3759" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3761" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
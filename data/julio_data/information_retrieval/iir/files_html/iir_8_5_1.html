<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Critiques and justifications of the concept of relevance</title> 
  <meta name="description" content="Critiques and justifications of the concept of relevance" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="previous" href="assessing-relevance-1.html" /> 
  <link rel="up" href="assessing-relevance-1.html" /> 
  <link rel="next" href="a-broader-perspective-system-quality-and-user-utility-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html2472" href="a-broader-perspective-system-quality-and-user-utility-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html2466" href="assessing-relevance-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html2462" href="assessing-relevance-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html2468" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html2470" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html2473" href="a-broader-perspective-system-quality-and-user-utility-1.html">A broader perspective: System</a> 
  <b> Up:</b> 
  <a name="tex2html2467" href="assessing-relevance-1.html">Assessing relevance</a> 
  <b> Previous:</b> 
  <a name="tex2html2463" href="assessing-relevance-1.html">Assessing relevance</a> &nbsp; 
  <b> <a name="tex2html2469" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html2471" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h2><a name="SECTION001351000000000000000"></a><a name="sec:relevance"></a> <a name="p:relevance"></a> <br /> Critiques and justifications of the concept of relevance </h2> 
  <p> The advantage of system evaluation, as enabled by the standard model of relevant and nonrelevant documents, is that we have a fixed setting in which we can vary IR systems and system parameters to carry out comparative experiments. Such formal testing is much less expensive and allows clearer diagnosis of the effect of changing system parameters than doing user studies of retrieval effectiveness. Indeed, once we have a formal measure that we have confidence in, we can proceed to optimize effectiveness by machine learning methods, rather than tuning parameters by hand. Of course, if the formal measure poorly describes what users actually want, doing this will not be effective in improving user satisfaction. Our perspective is that, in practice, the standard formal measures for IR evaluation, although a simplification, are good enough, and recent work in optimizing formal evaluation measures in IR has succeeded brilliantly. There are numerous examples of techniques developed in formal evaluation settings, which improve effectiveness in operational settings, such as the development of document length normalization methods within the context of TREC ( and <a href="okapi-bm25-a-non-binary-model-1.html#sec:okapi-bm25">11.4.3</a> ) and machine learning methods for adjusting parameter weights in scoring (Section <a href="learning-weights-1.html#sec:mlr">6.1.2</a> ). </p> 
  <p> That is not to say that there are not problems latent within the abstractions used. The relevance of one document is treated as independent of the relevance of other documents in the collection. (This assumption is actually built into most retrieval systems - documents are scored against queries, not against each other - as well as being assumed in the evaluation methods.) Assessments are binary: there aren't any nuanced assessments of relevance. Relevance of a document to an information need is treated as an absolute, objective decision. But judgments of relevance are subjective, varying across people, as we discussed above. In practice, human assessors are also imperfect measuring instruments, susceptible to failures of understanding and attention. We also have to assume that users' information needs do not change as they start looking at retrieval results. Any results based on one collection are heavily skewed by the choice of collection, queries, and relevance judgment set: the results may not translate from one domain to another or to a different user population. </p> 
  <p> Some of these problems may be fixable. A number of recent evaluations, including INEX, some TREC tracks, and NTCIR have adopted an ordinal notion of relevance with documents divided into 3 or 4 classes, distinguishing slightly relevant documents from highly relevant documents. See Section&nbsp;<a href="evaluation-of-xml-retrieval-1.html#sec:inex">10.4</a> (page&nbsp;<a href="evaluation-of-xml-retrieval-1.html#p:inex"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>) for a detailed discussion of how this is implemented in the INEX evaluations. </p> 
  <p> One clear problem with the relevance-based assessment that we have presented is the distinction between relevance and <a name="p:marginalrelevance"></a> <a name="10890"></a> <i>marginal relevance</i> : whether a document still has distinctive usefulness after the user has looked at certain other documents (<a href="bibliography-1.html#carbonell98mmr">Carbonell and Goldstein, 1998</a>). Even if a document is highly relevant, its information can be completely redundant with other documents which have already been examined. The most extreme case of this is documents that are duplicates - a phenomenon that is actually very common on the World Wide Web - but it can also easily occur when several documents provide a similar precis of an event. In such circumstances, marginal relevance is clearly a better measure of utility to the user. Maximizing marginal relevance requires returning documents that exhibit diversity and novelty. One way to approach measuring this is by using distinct facts or entities as evaluation units. This perhaps more directly measures true utility to the user but doing this makes it harder to create a test collection. </p> 
  <p> <b>Exercises.</b> </p> 
  <ul> 
   <li>Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you've written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}. 
    <blockquote> 
     <table cellpadding="3"> 
      <tbody> 
       <tr> 
        <td align="RIGHT">docID</td> 
        <td align="LEFT">Judge 1</td> 
        <td align="LEFT">Judge 2</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">1</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">2</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">3</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">1</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">4</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">1</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">5</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">6</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">7</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">8</td> 
        <td align="LEFT">1</td> 
        <td align="LEFT">0</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">9</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">1</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">10</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">1</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">11</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">1</td> 
       </tr> 
       <tr> 
        <td align="RIGHT">12</td> 
        <td align="LEFT">0</td> 
        <td align="LEFT">1</td> 
       </tr> 
      </tbody> 
     </table> 
    </blockquote> 
    <ol> 
     <li>Calculate the kappa measure between the two judges. </li> 
     <li>Calculate precision, recall, and <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> of your system if a document is considered relevant only if the two judges agree. </li> 
     <li>Calculate precision, recall, and <img width="19" height="32" align="MIDDLE" border="0" src="img522.png" alt="$F_1$" /> of your system if a document is considered relevant if either judge thinks it is relevant. </li> 
    </ol> <p> </p></li> 
  </ul> 
  <p> </p> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html2472" href="a-broader-perspective-system-quality-and-user-utility-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html2466" href="assessing-relevance-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html2462" href="assessing-relevance-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html2468" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html2470" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html2473" href="a-broader-perspective-system-quality-and-user-utility-1.html">A broader perspective: System</a> 
  <b> Up:</b> 
  <a name="tex2html2467" href="assessing-relevance-1.html">Assessing relevance</a> 
  <b> Previous:</b> 
  <a name="tex2html2463" href="assessing-relevance-1.html">Assessing relevance</a> &nbsp; 
  <b> <a name="tex2html2469" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html2471" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
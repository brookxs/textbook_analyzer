<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Feature selection</title> 
  <meta name="description" content="Feature selection" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="evaluation-of-text-classification-1.html" /> 
  <link rel="previous" href="properties-of-naive-bayes-1.html" /> 
  <link rel="up" href="text-classification-and-naive-bayes-1.html" /> 
  <link rel="next" href="mutual-information-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html3529" href="mutual-information-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3523" href="text-classification-and-naive-bayes-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3517" href="a-variant-of-the-multinomial-model-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3525" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3527" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3530" href="mutual-information-1.html">Mutual information</a> 
  <b> Up:</b> 
  <a name="tex2html3524" href="text-classification-and-naive-bayes-1.html">Text classification and Naive</a> 
  <b> Previous:</b> 
  <a name="tex2html3518" href="a-variant-of-the-multinomial-model-1.html">A variant of the</a> &nbsp; 
  <b> <a name="tex2html3526" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3528" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001850000000000000000"></a> <a name="sec:feature"></a> <a name="p:feature"></a> <a name="16947"></a> <a name="16948"></a> <a name="16949"></a> <br /> Feature selection </h1> 
  <i>Feature selection</i> is the process of selecting a subset of the terms occurring in the training set and using only this subset as features in text classification. Feature selection serves two main purposes. First, it makes training and applying a classifier more efficient by decreasing the size of the effective vocabulary. This is of particular importance for classifiers that, unlike NB, are expensive to train. Second, feature selection often increases classification accuracy by eliminating noise features. A 
  <a name="16951"></a> 
  <a name="p:noisefeature"></a> 
  <a name="16953"></a> 
  <i>noise feature</i> is one that, when added to the document representation, increases the classification error on new data. Suppose a rare term, say arachnocentric, has no information about a class, say China, but all instances of arachnocentric happen to occur in China documents in our training set. Then the learning method might produce a classifier that misassigns test documents containing arachnocentric to China. Such an incorrect generalization from an accidental property of the training set is called 
  <a name="16961"></a> 
  <a name="16962"></a> 
  <i>overfitting</i> . 
  <p> </p> 
  <div align="CENTER"> 
   <a name="fig:featselalg"></a> 
   <a name="p:featselalg"></a> 
   <a name="16976"></a> 
   <table> 
    <caption align="BOTTOM"> 
     <strong>Figure:</strong> Basic feature selection algorithm for selecting the 
     <img width="11" height="31" align="MIDDLE" border="0" src="img1002.png" alt="$\ktopk$" /> best features. 
    </caption> 
    <tbody> 
     <tr> 
      <td><img width="369" height="131" border="0" src="img1003.png" alt="\begin{figure}\par
\begin{algorithm}{SelectFeatures}{\docsetlabeled,c,\ktopk}
V ...
...eaturesWithLargestValues}(L,\ktopk)}
\end{algorithm}\par\par
\par
\end{figure}" /></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <p> We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features). It may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification, but when discussing <a name="16980"></a>the bias-variance tradeoff in Section&nbsp;<a href="the-bias-variance-tradeoff-1.html#sec:secbiasvariance">14.6</a> (page&nbsp;<a href="the-bias-variance-tradeoff-1.html#p:secbiasvariance"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>), we will see that weaker models are often preferable when limited training data are available. </p> 
  <p> The basic feature selection algorithm is shown in Figure <a href="#fig:featselalg">13.6</a> . For a given class <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />, we compute a utility measure <img width="49" height="33" align="MIDDLE" border="0" src="img1004.png" alt="$A(\tcword,c)$" /> for each term of the vocabulary and select the <img width="11" height="31" align="MIDDLE" border="0" src="img1002.png" alt="$\ktopk$" /> terms that have the highest values of <img width="49" height="33" align="MIDDLE" border="0" src="img1004.png" alt="$A(\tcword,c)$" />. All other terms are discarded and not used in classification. We will introduce three different utility measures in this section: mutual information, 
   <!-- MATH
 $A(\tcword,c) = I(\wvar_\tcword;C_c)$
 --> <img width="133" height="33" align="MIDDLE" border="0" src="img1005.png" alt="$A(\tcword,c) = I(\wvar_\tcword;C_c)$" />; the <img width="21" height="36" align="MIDDLE" border="0" src="img21.png" alt="$\chi ^2$" /> test, 
   <!-- MATH
 $A(\tcword,c) = X^2(t,c)$
 --> <img width="123" height="36" align="MIDDLE" border="0" src="img1006.png" alt="$A(\tcword,c) = X^2(t,c)$" />; and frequency, 
   <!-- MATH
 $A(\tcword,c) = N(\tcword,c)$
 --> <img width="118" height="33" align="MIDDLE" border="0" src="img1007.png" alt="$A(\tcword,c) = N(\tcword,c)$" />. </p> 
  <p> Of the two NB models, the Bernoulli model is particularly sensitive to noise features. A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low. </p> 
  <p> This section mainly addresses feature selection for two-class classification tasks like China versus not-China. Section <a href="comparison-of-feature-selection-methods-1.html#sec:featurecomparison">13.5.5</a> briefly discusses optimizations for systems with more than two classes<a name="16987"></a>. </p> 
  <p> <br /></p> 
  <hr /> 
  <!--Table of Child-Links--> 
  <a name="CHILD_LINKS"><strong>Subsections</strong></a> 
  <ul> 
   <li><a name="tex2html3531" href="mutual-information-1.html">Mutual information</a> </li> 
   <li><a name="tex2html3532" href="feature-selectionchi2-feature-selection-1.html"><img width="21" height="36" align="MIDDLE" border="0" src="img21.png" alt="$\chi ^2$" /> Feature selectionChi2 Feature selection</a> 
    <ul> 
     <li><a name="tex2html3533" href="assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html">Assessing <img width="21" height="36" align="MIDDLE" border="0" src="img21.png" alt="$\chi ^2$" /> as a feature selection methodAssessing chi-square as a feature selection method</a> </li> 
    </ul> <br /> </li> 
   <li><a name="tex2html3534" href="frequency-based-feature-selection-1.html">Frequency-based feature selection</a> </li> 
   <li><a name="tex2html3535" href="feature-selection-for-multiple-classifiers-1.html">Feature selection for multiple classifiers</a> </li> 
   <li><a name="tex2html3536" href="comparison-of-feature-selection-methods-1.html">Comparison of feature selection methods</a> </li> 
  </ul> 
  <!--End of Table of Child-Links--> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html3529" href="mutual-information-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html3523" href="text-classification-and-naive-bayes-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html3517" href="a-variant-of-the-multinomial-model-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html3525" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html3527" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html3530" href="mutual-information-1.html">Mutual information</a> 
  <b> Up:</b> 
  <a name="tex2html3524" href="text-classification-and-naive-bayes-1.html">Text classification and Naive</a> 
  <b> Previous:</b> 
  <a name="tex2html3518" href="a-variant-of-the-multinomial-model-1.html">A variant of the</a> &nbsp; 
  <b> <a name="tex2html3526" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html3528" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
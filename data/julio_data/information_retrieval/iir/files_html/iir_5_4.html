<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>References and further reading</title> 
  <meta name="description" content="References and further reading" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="previous" href="postings-file-compression-1.html" /> 
  <link rel="up" href="index-compression-1.html" /> 
  <link rel="next" href="scoring-term-weighting-and-the-vector-space-model-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html1803" href="scoring-term-weighting-and-the-vector-space-model-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html1797" href="index-compression-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html1793" href="gamma-codes-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html1799" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html1801" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html1804" href="scoring-term-weighting-and-the-vector-space-model-1.html">Scoring, term weighting and</a> 
  <b> Up:</b> 
  <a name="tex2html1798" href="index-compression-1.html">Index compression</a> 
  <b> Previous:</b> 
  <a name="tex2html1794" href="gamma-codes-1.html">Gamma codes</a> &nbsp; 
  <b> <a name="tex2html1800" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html1802" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001040000000000000000"></a> <a name="sec:icompresssecfurther"></a> <a name="p:icompresssecfurther"></a> <br /> References and further reading </h1> 
  <p> Heaps' law was discovered by <a href="bibliography-1.html#heaps78information">Heaps (1978)</a>. See also <a href="bibliography-1.html#baezayates99">Baeza-Yates and Ribeiro-Neto (1999)</a>. A detailed study of vocabulary growth in large collections is (<a href="bibliography-1.html#williams05searchable">Williams and Zobel, 2005</a>). Zipf's law is due to <a href="bibliography-1.html#zipf49human">Zipf (1949)</a>. <a href="bibliography-1.html#witten90source">Witten and Bell (1990)</a> investigate the quality of the fit obtained by the law. Other term distribution models, including K mixture and two-poisson model, are discussed by <a href="bibliography-1.html#manning99foundations">Manning and Sch&uuml;tze (1999, Chapter&nbsp;15)</a>. <a href="bibliography-1.html#carmel01static">Carmel et&nbsp;al. (2001)</a>, <a href="bibliography-1.html#buttcher06document">B&uuml;ttcher and Clarke (2006)</a>, <a href="bibliography-1.html#blanco07boosting">Blanco and Barreiro (2007)</a>, and <a href="bibliography-1.html#ntoulas07pruning">Ntoulas and Cho (2007)</a> show that lossy compression can achieve good compression with no or no significant decrease in retrieval effectiveness. </p> 
  <p> Dictionary compression is covered in detail by <a href="bibliography-1.html#witten99gigabytes">Witten et&nbsp;al. (1999, Chapter&nbsp;4)</a>, which is recommended as additional reading. </p> 
  <p> Subsection&nbsp;<a href="variable-byte-codes-1.html#bytealigned">5.3.1</a> is based on (<a href="bibliography-1.html#scholer02inverted">Scholer et&nbsp;al., 2002</a>). The authors find that variable byte codes process queries two times faster than either bit-level compressed indexes or uncompressed indexes with a 30% penalty in compression ratio compared with the best bit-level compression method. They also show that compressed indexes can be superior to uncompressed indexes not only in disk usage, but also in query processing speed. Compared with VB codes, ``variable nibble'' codes showed 5% to 10% better compression and up to one third worse effectiveness in one experiment (<a href="bibliography-1.html#anh05invertedindex">Anh and Moffat, 2005</a>). <a href="bibliography-1.html#trotman03compressing">Trotman (2003)</a> also recommends using VB codes unless disk space is at a premium. In recent work, <a name="tex2html1805" href="bibliography-1.html#anh06improved">Anh and Moffat (2006a</a>;<a name="tex2html1806" href="bibliography-1.html#anh05invertedindex">2005)</a> and <a href="bibliography-1.html#zukowski06superscalar">Zukowski et&nbsp;al. (2006)</a> have constructed word-aligned binary codes that are both faster in decompression and at least as efficient as VB codes. <a href="bibliography-1.html#zhang07performance">Zhang et&nbsp;al. (2007)</a> investigate the increased effectiveness of caching when a number of different compression techniques for postings lists are used on modern hardware. </p> 
  <p> <a name="6672"></a><img width="11" height="31" align="MIDDLE" border="0" src="img282.png" alt="$\delta$" /> codes (Exercise <a href="gamma-codes-1.html#ex:deltacode">5.3.2</a> ) and <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> codes were introduced by <a href="bibliography-1.html#elias75universal">Elias (1975)</a>, who proved that both codes are universal. In addition, <img width="11" height="31" align="MIDDLE" border="0" src="img282.png" alt="$\delta$" /> codes are asymptotically optimal for 
   <!-- MATH
 $H(P) \rightarrow \infty$
 --> <img width="81" height="33" align="MIDDLE" border="0" src="img346.png" alt="$H(P) \rightarrow \infty$" />. <img width="11" height="31" align="MIDDLE" border="0" src="img282.png" alt="$\delta$" /> codes perform better than <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> codes if large numbers (greater than 15) dominate. A good introduction to information theory, including the concept of <a name="6675"></a> <i>entropy</i> , is (<a href="bibliography-1.html#cover91elements">Cover and Thomas, 1991</a>). While Elias codes are only asymptotically optimal, arithmetic codes (<a href="bibliography-1.html#witten99gigabytes">Witten et&nbsp;al., 1999</a>, Section&nbsp;2.4) can be constructed to be arbitrarily close to the optimum <img width="42" height="33" align="MIDDLE" border="0" src="img13.png" alt="$H(P)$" /> for any <img width="14" height="32" align="MIDDLE" border="0" src="img115.png" alt="$P$" />. </p> 
  <p> Several additional index compression techniques are covered by Witten et al. (1999; Sections 3.3 and 3.4 and Chapter&nbsp;5). They recommend using <a name="6679"></a> <a name="6680"></a> <i>parameterized codes</i> for index compression, codes that explicitly model the probability distribution of gaps for each term. For example, they show that <a name="6682"></a> <a name="6683"></a> <i>Golomb codes</i> achieve better compression ratios than <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> codes for large collections. <a href="bibliography-1.html#moffat92parameterised">Moffat and Zobel (1992)</a> compare several parameterized methods, including <a name="6686"></a>LLRUN (<a href="bibliography-1.html#fraenkel85novel">Fraenkel and Klein, 1985</a>). </p> 
  <p> The distribution of gaps in a postings list depends on the assignment of docIDs to documents. A number of researchers have looked into assigning docIDs in a way that is conducive to the efficient compression of gap sequences (<a href="bibliography-1.html#moffat96exploiting">Moffat and Stuiver, 1996</a>; <a href="bibliography-1.html#blandford02index">Blandford and Blelloch, 2002</a>; <a href="bibliography-1.html#silvestri04assigning">Silvestri et&nbsp;al., 2004</a>; <a href="bibliography-1.html#blanco06tsp">Blanco and Barreiro, 2006</a>; <a href="bibliography-1.html#silvestri07sorting">Silvestri, 2007</a>). These techniques assign docIDs in a small range to documents in a cluster where a cluster can consist of all documents in a given time period, on a particular web site, or sharing another property. As a result, when a sequence of documents from a cluster occurs in a postings list, their gaps are small and can be more effectively compressed. </p> 
  <p> Different considerations apply to the compression of term frequencies and word positions than to the compression of docIDs in postings lists. See <a href="bibliography-1.html#scholer02inverted">Scholer et&nbsp;al. (2002)</a> and <a href="bibliography-1.html#zobel06inverted">Zobel and Moffat (2006)</a>. <a href="bibliography-1.html#zobel06inverted">Zobel and Moffat (2006)</a> is recommended in general as an in-depth and up-to-date tutorial on inverted indexes, including index compression. </p> 
  <p> This chapter only looks at index compression for Boolean retrieval. For <a name="6696"></a> <i>ranked retrieval</i> (Chapter <a href="scoring-term-weighting-and-the-vector-space-model-1.html#ch:termvspace">6</a> ), it is advantageous to order postings according to term frequency instead of docID. During query processing, the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" /> documents found so far. It is not a good idea to precompute and store weights in the index (as opposed to frequencies) because they cannot be compressed as well as integers (see impactordered). </p> 
  <p> <i>Document compression</i> can also be important in an efficient information retrieval system. <a href="bibliography-1.html#moura00fast">de&nbsp;Moura et&nbsp;al. (2000)</a> and <a href="bibliography-1.html#brisaboa06lightweight">Brisaboa et&nbsp;al. (2007)</a> describe compression schemes that allow direct searching of terms and phrases in the compressed text, which is infeasible with standard text compression utilities like gzip and compress. </p> 
  <p> <b>Exercises.</b> </p> 
  <ul> 
   <li>We have defined unary codes as being ``10'': sequences of 1s terminated by a 0. Interchanging the roles of 0s and 1s yields an equivalent ``01'' unary code. When this 01 unary code is used, the construction of a <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> code can be stated as follows: (1) Write <img width="16" height="32" align="MIDDLE" border="0" src="img284.png" alt="$G$" /> down in binary using 
    <!-- MATH
 $b = \lfloor \log_2 j \rfloor +1$
 --> <img width="114" height="33" align="MIDDLE" border="0" src="img347.png" alt="$b = \lfloor \log_2 j \rfloor +1$" /> bits. (2) Prepend <img width="52" height="33" align="MIDDLE" border="0" src="img348.png" alt="$(b-1)$" /> 0s. (i) Encode the numbers in Table <a href="gamma-codes-1.html#tab:icompresstb3">5.5</a> in this alternative <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> code. (ii) Show that this method produces a well-defined alternative <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" /> code in the sense that it has the same length and can be uniquely decoded. <p> </p></li> 
   <li>Unary code is not a universal code in the sense defined above. However, there exists a distribution over gaps for which unary code is optimal. Which distribution is this? <p> </p></li> 
   <li>Give some examples of terms that violate the assumption that gaps all have the same size (which we made when estimating the space requirements of a <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" />-encoded index). What are general characteristics of these terms? <p> </p></li> 
   <li>Consider a term whose postings list has size <img width="13" height="32" align="MIDDLE" border="0" src="img104.png" alt="$n$" />, say, <img width="79" height="32" align="MIDDLE" border="0" src="img349.png" alt="$n=10{,}000$" />. Compare the size of the <img width="14" height="32" align="MIDDLE" border="0" src="img12.png" alt="$\gamma $" />-compressed gap-encoded postings list if the distribution of the term is uniform (i.e., all gaps have the same size) versus its size when the distribution is not uniform. Which compressed postings list is smaller? <p> </p></li> 
   <li>Work out the sum in Equation&nbsp;<a href="gamma-codes-1.html#totalindexsizeeq">12</a> and show it adds up to about 251 MB. Use the numbers in Table <a href="blocked-sort-based-indexing-1.html#tab:icompresstb1">4.2</a> , but do not round <img width="20" height="32" align="MIDDLE" border="0" src="img350.png" alt="$ L c$" />, <img width="11" height="32" align="MIDDLE" border="0" src="img252.png" alt="$c$" />, and the number of vocabulary blocks. <p> </p></li> 
  </ul> 
  <p> </p> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html1803" href="scoring-term-weighting-and-the-vector-space-model-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html1797" href="index-compression-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html1793" href="gamma-codes-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html1799" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html1801" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html1804" href="scoring-term-weighting-and-the-vector-space-model-1.html">Scoring, term weighting and</a> 
  <b> Up:</b> 
  <a name="tex2html1798" href="index-compression-1.html">Index compression</a> 
  <b> Previous:</b> 
  <a name="tex2html1794" href="gamma-codes-1.html">Gamma codes</a> &nbsp; 
  <b> <a name="tex2html1800" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html1802" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>  
 </body>
</html>
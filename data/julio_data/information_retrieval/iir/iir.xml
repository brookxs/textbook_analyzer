<toc>
  <section>
    <id>iir_1</id>
    <title>Boolean retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/boolean-retrieval-1.html</url>
    <file>boolean-retrieval-1.html</file>
    <text> Boolean retrieval The meaning of the term information retrieval can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study, information retrieval might be defined thus: Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).   IR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term ``unstructured data'' refers to data which does not have clear, semantically overt, easy-for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly ``unstructured''. This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web pages). IR is also used to facilitate ``semistructured'' search such as finding a document where the title contains Java and the body contains threading. The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically. Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In web search , the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in webcharlink. At the other extreme is personal information retrieval . In the last few years, consumer operating systems have integrated information retrieval (such as Apple's Mac OS X Spotlight or Windows Vista's Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of enterprise, institutional, and domain-specific search , where retrieval might be provided for collections such as a corporation's internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios. In this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1 ) and the central inverted index data structure (Section 1.2 ). We will then examine the Boolean retrieval model and how Boolean queries are processed ( and 1.4 ).   Subsections An example information retrieval problem A first take at building an inverted index Processing Boolean queries The extended Boolean model versus ranked retrieval References and further reading</text>
    <subsections>
       <section>
         <id>iir_1_1</id>
         <title>An example information retrieval problem</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/an-example-information-retrieval-problem-1.html</url>
         <file>an-example-information-retrieval-problem-1.html</file>
         <text> An example information retrieval problem A fat book which many people own is Shakespeare's Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar and NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as grepping through text, after the Unix command grep, which performs this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of . With modern computers, for simple querying of modest collections (the size of Shakespeare's Collected Works is a bit under one million words of text in total), you really need nothing more. But for many purposes, you do need more: To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words. To allow more flexible matching operations. For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as ``within 5 words'' or ``within the same sentence''. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words. The way to avoid linearly scanning the texts for each query is to index the documents in advance. Let us stick with Shakespeare's Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document - here a play of Shakespeare's - whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document incidence matrix , as in Figure 1.1 . Terms are the indexed units (further discussed in Section 2.2 ); they are usually words, and for the moment you can think of them as words, but the information retrieval literature normally speaks of terms because some of them, such as perhaps I-9 or Hong Kong are not usually thought of as words. Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it.   To answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND: 110100 AND 110111 AND 101111 = 100100 1.2 The Boolean retrieval model is a model for information retrieval in which we can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators and, or, and not. The model views each document as just a set of words.  Figure: Results from Shakespeare for the query Brutus AND Caesar AND NOT Calpurnia. Let us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation. Suppose we have documents. By documents we mean whatever units we have decided to build a retrieval system over. They might be individual memos or chapters of a book (see Section 2.1.2 (page ) for further discussion). We will refer to the group of documents over which we perform retrieval as the (document) collection . It is sometimes also referred to as a corpus (a body of texts). Suppose each document is about 1000 words long (2-3 book pages). If we assume an average of 6 bytes per word including spaces and punctuation, then this is a document collection about 6 GB in size. Typically, there might be about distinct terms in these documents. There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle. We will discuss and model these size assumptions in Section 5.1 (page ). Our goal is to develop a system to address the ad hoc retrieval task. This is the most standard IR task. In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query. An information need is the topic about which the user desires to know more, and is differentiated from a query , which is what the user conveys to the computer in an attempt to communicate the information need. A document is relevant if it is one that the user perceives as containing information of value with respect to their personal information need. Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like ``pipeline leaks'' and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture. To assess the effectiveness of an IR system (i.e., the quality of its search results), a user will usually want to know two key statistics about the system's returned results for a query: Precision : What fraction of the returned results are relevant to the information need? Recall : What fraction of the relevant documents in the collection were returned by the system? 8 We now cannot build a term-document matrix in a naive way. A matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory. But the crucial observation is that the matrix is extremely sparse, that is, it has few non-zero entries. Because each document is 1000 words long, the matrix has no more than one billion 1's, so a minimum of 99.8% of the cells are zero. A much better representation is to record only the things that do occur, that is, the 1 positions. This idea is central to the first major concept in information retrieval, the inverted index . The name is actually redundant: an index always maps back from terms to the parts of a document where they occur. Nevertheless, inverted index, or sometimes inverted file , has become the standard term in information retrieval.The basic idea of an inverted index is shown in Figure 1.3 . We keep a dictionary of terms (sometimes also referred to as a vocabulary or lexicon ; in this book, we use dictionary for the data structure and vocabulary for the set of terms). Then for each term, we have a list that records which documents the term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called a posting .The list is then called a postings list (or ), and all the postings lists taken together are referred to as the postings . The dictionary in Figure 1.3 has been sorted alphabetically and each postings list is sorted by document ID. We will see why this is useful in Section 1.3 , below, but later we will also consider alternatives to doing this (Section 7.1.5 ).   </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_1_2</id>
         <title>A first take at building an inverted index</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html</url>
         <file>a-first-take-at-building-an-inverted-index-1.html</file>
         <text> A first take at building an inverted index To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are: Collect the documents to be indexed: ... Tokenize the text, turning each document into a list of tokens: ... Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms: ... Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings. 2.2  tokens normalized tokens words  sort-based indexing   Within a document collection, we assume that each document has a unique serial number, known as the document identifier ( docID ). During index construction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4 . The core indexing step is sorting this list so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4 . Multiple occurrences of the same term from the same document are then merged.Instances of the same term are then grouped, and the result is split into a dictionary and postings , as shown in the right column of Figure 1.4 . Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the document frequency , which is here also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search. In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3 ), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3 ), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory. Exercises. Draw the inverted index that would be built for the following document collection. (See Figure 1.3 for an example.) Doc 1    new home sales top forecasts Doc 2    home sales rise in july Doc 3    increase in home sales in july Doc 4    july new home sales rise Consider these documents: Doc 1    breakthrough drug for schizophrenia Doc 2    new schizophrenia drug Doc 3    new approach for treatment of schizophrenia Doc 4    new hopes for schizophrenia patients Draw the term-document incidence matrix for this document collection. Draw the inverted index representation for this collection, as in Figure 1.3 (page ). For the document collection shown in Exercise 1.2 , what are the returned results for these queries: schizophrenia AND drug for AND NOT(drug OR approach) </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_1_3</id>
         <title>Processing Boolean queries</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/processing-boolean-queries-1.html</url>
         <file>processing-boolean-queries-1.html</file>
         <text> Processing Boolean queries How do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the simple conjunctive query : over the inverted index partially shown in Figure 1.3 (page ). We: Locate Brutus in the Dictionary Retrieve its postings Locate Calpurnia in the Dictionary Retrieve its postings Intersect the two postings lists, as shown in Figure 1.5 .  intersection  merging  merge algorithm  Figure: Intersecting the postings lists for Brutus and Calpurnia from Figure 1.3 .  Figure 1.6: Algorithm for the intersection of two postings lists and . There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6 ): we maintain pointers into both lists and walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are and , the intersection takes operations. Formally, the complexity of querying is ,where is the number of documents in the collection.Our indexing methods gain us just a constant, not a difference in time complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries like: Query optimization is the process of selecting how to organize the work of answering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of terms, for instance: For each of the terms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document frequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 (page ), we execute the above query as: This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list. Consider now the optimization of more general queries, such as: As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term.  Figure 1.7: Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms. For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7 . The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings intersection can still be done by the algorithm in Figure 1.6 , but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5 . Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common. Exercises. For the queries below, can we still run through the intersection in time , where and are the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve? Brutus and not Caesar Brutus or not Caesar Extend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider: c. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra) Can we always merge in linear time? Linear in what? Can we do better than this? We can use distributive laws for and and or to rewrite queries. Show how to rewrite the query in Exercise 1.3 into disjunctive normal form using the distributive laws. Would the resulting query be more or less efficiently evaluated than the original form of this query? Is this result true in general or does it depend on the words and the contents of the document collection? Recommend a query processing order for d. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes) given the following postings list sizes: Term Postings size eyes 213312 kaleidoscope 87009 marmalade 107913 skies 271658 tangerine 46653 trees 316812 If the query is: e. friends AND romans AND (NOT countrymen) how could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing. For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn't. Write out a postings merge algorithm, in the style of Figure 1.6 (page ), for an OR query. How should the Boolean query AND NOT be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_1_4</id>
         <title>The extended Boolean model versus ranked retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-extended-boolean-model-versus-ranked-retrieval-1.html</url>
         <file>the-extended-boolean-model-versus-ranked-retrieval-1.html</file>
         <text> The extended Boolean model versus ranked retrieval The Boolean retrieval model contrasts with ranked retrieval models such as the vector space model (Section 6.3 ), in which users largely use free text queries , that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boolean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A proximity operator is a way of specifying that two terms in a query must occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph.  Worked example. Commercial Boolean searching: Westlaw.westlaw Westlaw (http://www.westlaw.com/) is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called ``Terms and Connectors'' by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called ``Natural Language'' by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw:  Information need: Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company. Query: "trade secret" /s disclos! /s prevent /s employe!  Information need: Requirements for disabled people to be able to access a workplace. Query: disab! /p access! /s work-site work-place (employment /3 place)    Information need: Cases about a host's responsibility for drunk guests. Query: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest Note the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator), & is AND and /s, /p, and / ask for matches in the same sentence, same paragraph or within words respectively. Double quotes give a phrase search (consecutive words); see Section 2.4 (page ). The exclamation mark (!) gives a trailing wildcard query wildcard; thus liab! matches all words starting with liab. Additionally work-site matches any of worksite, work-site or work site; see Section 2.2.1 (page ). Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user. Many users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw's own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground. End worked example. In this chapter, we have looked at the structure and construction of a basic inverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In dictionaryranking-ir-system we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do: We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words. It is often useful to search for compounds or phrases that denote a concept such as ``operating system''. As the Westlaw examples show, we might also wish to do proximity queries such as Gates near Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need term frequency information (the number of times a term occurs in a document) in postings lists. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or ``rank'') the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query. With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying , most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance. Exercises. Write a query using Westlaw syntax which would find any of the words professor, teacher, or lecturer in the same sentence as a form of the verb explain. Try using the Boolean search features on a couple of major web search engines. For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and top hits. Do they make sense in terms of Boolean logic? Often they haven't for major search engines. Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer. What bound should the number of results from the first two queries place on the third query? Is this bound observed? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_1_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-1.html</url>
         <file>references-and-further-reading-1.html</file>
         <text> References and further reading The practical pursuit of computerized information retrieval began in the late 1940s (Cleverdon, 1991, Liddy, 2005). A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval. However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later. The article of Bush (1945) provided lasting inspiration for the new field: ``Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, `memex' will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.'' Information Retrieval Mooers, 1950 In 1958, much newspaper attention was paid to demonstrations at a conference (see Taube and Wooster, 1958) of IBM ``auto-indexing'' machines, based primarily on the work of H. P. Luhn. Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems. For example Mooers (1961) dissented: ``It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design. This view is as widely and uncritically accepted as it is wrong.'' Lee and Fox, 1988 The book (Witten et al., 1999) is the standard reference for an in-depth comparison of the space and time efficiency of the inverted index versus other possible data structures; a more succinct and up-to-date presentation appears in Zobel and Moffat (2006). We further discuss several approaches in Chapter 5 . Friedl (2006) covers the practical usage of regular expressions for searching. The underlying computer science appears in (Hopcroft et al., 2000). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_2</id>
    <title>The term vocabulary and postings lists</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-term-vocabulary-and-postings-lists-1.html</url>
    <file>the-term-vocabulary-and-postings-lists-1.html</file>
    <text> The term vocabulary and postings lists Recall the major steps in inverted index construction: Collect the documents to be indexed. Tokenize the text. Do linguistic preprocessing of tokens. Index the documents that each term occurs in. 2.1 2.2  tokens  terms 1 4 2.3 2.4   Subsections Document delineation and character sequence decoding Obtaining the character sequence in a document Choosing a document unit Determining the vocabulary of terms Tokenization Dropping common terms: stop words Normalization (equivalence classing of terms) Accents and diacritics. Capitalization/case-folding. Other issues in English. Other languages. Stemming and lemmatization Faster postings list intersection via skip pointers Positional postings and phrase queries Biword indexes Positional indexes Positional index size. Combination schemes References and further reading</text>
    <subsections>
       <section>
         <id>iir_2_1</id>
         <title>Document delineation and character sequence decoding</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/document-delineation-and-character-sequence-decoding-1.html</url>
         <file>document-delineation-and-character-sequence-decoding-1.html</file>
         <text> Document delineation and character sequence decoding   Subsections Obtaining the character sequence in a document Choosing a document unit </text>
         <subsections>
            <section>
              <id>iir_2_1_1</id>
              <title>Obtaining the character sequence in a document</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/obtaining-the-character-sequence-in-a-document-1.html</url>
              <file>obtaining-the-character-sequence-in-a-document-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_1_2</id>
              <title>Choosing a document unit</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/choosing-a-document-unit-1.html</url>
              <file>choosing-a-document-unit-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_2_2</id>
         <title>Determining the vocabulary of terms</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/determining-the-vocabulary-of-terms-1.html</url>
         <file>determining-the-vocabulary-of-terms-1.html</file>
         <text> Determining the vocabulary of terms   Subsections Tokenization Dropping common terms: stop words Normalization (equivalence classing of terms) Accents and diacritics. Capitalization/case-folding. Other issues in English. Other languages. Stemming and lemmatization </text>
         <subsections>
            <section>
              <id>iir_2_2_1</id>
              <title>Tokenization</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html</url>
              <file>tokenization-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_2_2</id>
              <title>Dropping common terms: stop words</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html</url>
              <file>dropping-common-terms-stop-words-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_2_3</id>
              <title>Normalization (equivalence classing of terms)</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/normalization-equivalence-classing-of-terms-1.html</url>
              <file>normalization-equivalence-classing-of-terms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_2_4</id>
              <title>Stemming and lemmatization</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html</url>
              <file>stemming-and-lemmatization-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_2_3</id>
         <title>Faster postings list intersection via skip pointers</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/faster-postings-list-intersection-via-skip-pointers-1.html</url>
         <file>faster-postings-list-intersection-via-skip-pointers-1.html</file>
         <text> Faster postings list intersection via skip pointers In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3 (page ): we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are and , the intersection takes operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn't changing too fast. One way to do this is to use a skip list by augmenting postings lists with skip pointers (at indexing time), as shown in Figure 2.9 . Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers.  Postings lists with skip pointers.The postings intersection can use a skip pointer when the end point is still less than the item on the other list.  Figure 2.10: Postings lists intersection with skip pointers. Consider first efficient merging, with Figure 2.9 as an example. Suppose we've stepped through the lists in the figure until we have matched on each list and moved it to the results list. We advance both pointers, giving us on the upper list and on the lower list. The smallest item is then the element on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to . We thus avoid stepping to and on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown in Figure 2.10 . Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries. Where do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length , use evenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms. Building effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective. Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you're running a search engine with everything in memory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 (page ) and the impact of index size on system speed in Chapter 5 . Exercises. Why are skip pointers not useful for queries of the form OR ? We have a two-word query. For one term the postings list consists of the following 16 entries: [4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180] and for the other it is the one entry postings list: [47]. Work out how many comparisons would be done to intersect the two postings lists with the following two strategies. Briefly justify your answers: Using standard postings lists Using postings lists stored with skip pointers, with a skip length of , as suggested in Section 2.3 . Consider a postings intersection between this postings list, with skip pointers: xunit=0.6cm,arcangle=30 and the following intermediate result postings list (which hence has no skip pointers): 3    5    89    95    97    99    100    101 Trace through the postings intersection algorithm in Figure 2.10 (page ). How often is a skip pointer followed (i.e., is advanced to )? How many postings comparisons will be made by this algorithm while intersecting the two lists? How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_2_4</id>
         <title>Positional postings and phrase queries</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/positional-postings-and-phrase-queries-1.html</url>
         <file>positional-postings-and-phrase-queries-1.html</file>
         <text> Positional postings and phrase queries Many complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university. is not a match. Most recent search engines support a double quotes syntax (``stanford university'') for phrase queries , which has proven to be very easily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 (page ) in the context of ranked retrieval.   Subsections Biword indexes Positional indexes Positional index size. Combination schemes</text>
         <subsections>
            <section>
              <id>iir_2_4_1</id>
              <title>Biword indexes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/biword-indexes-1.html</url>
              <file>biword-indexes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_4_2</id>
              <title>Positional indexes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/positional-indexes-1.html</url>
              <file>positional-indexes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_2_4_3</id>
              <title>Combination schemes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/combination-schemes-1.html</url>
              <file>combination-schemes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_2_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-2.html</url>
         <file>references-and-further-reading-2.html</file>
         <text> References and further reading Exhaustive discussion of the character-level processing of can be found in Lunde (1998). Character bigram indexes are perhaps the most standard approach to indexing Chinese, although some systems use word segmentation. Due to differences in the language and writing system, word segmentation is most usual for Japanese (Luk and Kwok, 2002, Kishida et al., 2005). The structure of a character -gram index over unsegmented text differs from that in Section 3.2.2 (page ): there the -gram dictionary points to postings lists of entries in the regular dictionary, whereas here it points directly to document postings lists. For further discussion of Chinese word segmentation, see Tseng et al. (2005), Sproat and Emerson (2003), Sproat et al. (1996), and Gao et al. (2005). Lita et al. (2003) present a method for truecasing . Natural language processing work on computational morphology is presented in (Sproat, 1992, Beesley and Karttunen, 2003). Language identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level -gram language identification algorithm. While other methods such as looking for particular distinctive function words and letter combinations have been used, with the advent of widespread digital text, many people have explored the character -gram technique, and found it to be highly successful (Beesley, 1998, Dunning, 1994, Cavnar and Trenkle, 1994). Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al. (2006) for a recent survey. Experiments on and discussion of the positive and negative impact of stemming in English can be found in the following works: Salton (1989), Krovetz (1995), Hull (1996), Harman (1991). Hollink et al. (2004) provide detailed results for the effectiveness of language-specific methods on 8 European languages. In terms of percent change in mean average precision (see page 8.4 ) over a baseline system, diacritic removal gains up to 23% (being especially helpful for Finnish, French, and Swedish). Stemming helped markedly for Finnish (30% improvement) and Spanish (10% improvement), but for most languages, including English, the gain from stemming was in the range 0-5%, and results from a lemmatizer were poorer still. Compound splitting gained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather than language-particular methods, indexing character -grams (as we suggested for Chinese) could often give as good or better results: using within-word character 4-grams rather than words gave gains of 37% in Finnish, 27% in Swedish, and 20% in German, while even being slightly positive for other languages, such as Dutch, Spanish, and English. Tomlinson (2003) presents broadly similar results. Bar-Ilan and Gutman (2005) suggest that, at the time of their study (2003), the major commercial web search engines suffered from lacking decent language-particular processing; for example, a query on www.google.fr for l'électricité did not separate off the article l' but only matched pages with precisely this string of article+noun. The classic presentation of for IR can be found in Moffat and Zobel (1996). Extended techniques are discussed in Boldi and Vigna (2005). The main paper in the algorithms literature is Pugh (1990), which uses multilevel skip pointers to give expected list access (the same expected efficiency as using a tree data structure) with less implementational complexity. In practice, the effectiveness of using skip pointers depends on various system parameters. Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al. (2002, p. 217) report that, with modern CPUs, using skip lists instead slows down search because it expands the size of the postings list (i.e., disk I/O dominates performance). In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs. Johnson et al. (2006) report that 11.7% of all queries in two 2002 web query logs contained phrase queries , though Kammenhuber et al. (2006) report only 3% phrase queries for a different data set. Silverstein et al. (1999) note that many queries without explicit phrase operators are actually implicit phrase searches. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_3</id>
    <title>Dictionaries and tolerant retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/dictionaries-and-tolerant-retrieval-1.html</url>
    <file>dictionaries-and-tolerant-retrieval-1.html</file>
    <text> Dictionaries and tolerant retrieval In Chapters 1 2 we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a wildcard query : a query such as *a*e*i*o*u*, which seeks documents containing any term that includes all the five vowels in sequence. The * symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated. We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3 . Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection. Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1 2 , in which each vocabulary term has a postings list with the documents in the collection.   Subsections Search structures for dictionaries Wildcard queries General wildcard queries Permuterm indexes k-gram indexes for wildcard queries Spelling correction Implementing spelling correction Forms of spelling correction Edit distance k-gram indexes for spelling correction Context sensitive spelling correction Phonetic correction References and further reading</text>
    <subsections>
       <section>
         <id>iir_3_1</id>
         <title>Search structures for dictionaries</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/search-structures-for-dictionaries-1.html</url>
         <file>search-structures-for-dictionaries-1.html</file>
         <text> Search structures for dictionaries Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the corresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot - and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain. At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2 . Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years' time.  A binary search tree.In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest. Search trees overcome many of these issues - for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the binary tree , in which each internal node has two children. The search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is ) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained. To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the B-tree - a search tree in which every internal node has a number of children in the interval , where and are appropriate positive integers; Figure 3.2 shows an example with and . Each branch under an internal node again represents a test for a range of character sequences, as in the binary tree example of Figure 3.1 . A B-tree may be viewed as ``collapsing'' multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers and are determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees.  A B-tree.In this example every internal node has between 2 and 4 children. It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_3_2</id>
         <title>Wildcard queries</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/wildcard-queries-1.html</url>
         <file>wildcard-queries-1.html</file>
         <text> Wildcard queries Wildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which leads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart). A query such as mon* is known as a trailing wildcard query , because the * symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set of terms in the dictionary with the prefix mon. Finally, we use lookups on the standard inverted index to retrieve all documents containing any term in . But what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider leading wildcard queries, or queries of the form *mon. Consider a reverse B-tree on the dictionary - one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms in the vocabulary with a given prefix. In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set of dictionary terms beginning with the prefix se, then the reverse B-tree to enumerate the set of terms ending with the suffix mon. Next, we take the intersection of these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single * symbol using two B-trees, the normal B-tree and a reverse B-tree.   Subsections General wildcard queries Permuterm indexes k-gram indexes for wildcard queries</text>
         <subsections>
            <section>
              <id>iir_3_2_1</id>
              <title>General wildcard queries</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/general-wildcard-queries-1.html</url>
              <file>general-wildcard-queries-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_3_2_2</id>
              <title>k-gram indexes for wildcard queries</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/k-gram-indexes-for-wildcard-queries-1.html</url>
              <file>k-gram-indexes-for-wildcard-queries-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_3_3</id>
         <title>Spelling correction</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/spelling-correction-1.html</url>
         <file>spelling-correction-1.html</file>
         <text> Spelling correction We next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney's spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on -gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.   Subsections Implementing spelling correction Forms of spelling correction Edit distance k-gram indexes for spelling correction Context sensitive spelling correction </text>
         <subsections>
            <section>
              <id>iir_3_3_1</id>
              <title>Implementing spelling correction</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/implementing-spelling-correction-1.html</url>
              <file>implementing-spelling-correction-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_3_3_2</id>
              <title>Forms of spelling correction</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/forms-of-spelling-correction-1.html</url>
              <file>forms-of-spelling-correction-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_3_3_3</id>
              <title>Edit distance</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/edit-distance-1.html</url>
              <file>edit-distance-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_3_3_4</id>
              <title>k-gram indexes for spelling correction</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/k-gram-indexes-for-spelling-correction-1.html</url>
              <file>k-gram-indexes-for-spelling-correction-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_3_3_5</id>
              <title>Context sensitive spelling correction</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/context-sensitive-spelling-correction-1.html</url>
              <file>context-sensitive-spelling-correction-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_3_4</id>
         <title>Phonetic correction</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/phonetic-correction-1.html</url>
         <file>phonetic-correction-1.html</file>
         <text> Phonetic correction phonetic Algorithms for such phonetic hashing are commonly collectively known as soundex algorithms. However, there is an original soundex algorithm, with various variants, built on the following scheme: Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index. Do the same with query terms. When the query calls for a soundex match, search this soundex index. Retain the first letter of the term. Change all occurrences of the following letters to '0' (zero): 'A', E', 'I', 'O', 'U', 'H', 'W', 'Y'. Change letters to digits as follows: B, F, P, V to 1. C, G, J, K, Q, S, X, Z to 2. D,T to 3. L to 4. M, N to 5. R to 6. Repeatedly remove one out of each pair of consecutive identical digits. Remove all zeros from the resulting string. Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits. For an example of a soundex map, Hermann maps to H655. Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index. This algorithm rests on a few observations: (1) vowels are viewed as interchangeable, in transcribing names; (2) consonants with similar sounds (e.g., D and T) are put in equivalence classes. This leads to related names often having the same soundex codes. While these rules work for many cases, especially European languages, such rules tend to be writing system dependent. For example, Chinese names can be written in Wade-Giles or Pinyin transcription. While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently. Exercises. Find two differently spelled proper nouns whose soundex codes are the same. Find two phonetically similar proper nouns whose soundex codes are different.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_3_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-3.html</url>
         <file>references-and-further-reading-3.html</file>
         <text> References and further reading Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries. Garfield (1976) gives one of the first complete descriptions of the permuterm index. Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes. One of the earliest formal treatments of spelling correction was due to Damerau (1964). The notion of edit distance that we have used is due to Levenshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974). Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that -gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings. Gusfield (1997) is a standard reference on string algorithms such as edit distance. Probabilistic models (``noisy channel'' models) for spelling correction were pioneered by Kernighan et al. (1990) and further developed by Brill and Moore (2000) and Toutanova and Moore (2002). In these models, the mis-spelled query is viewed as a probabilistic corruption of a correct query. They have a similar mathematical basis to the language model methods presented in Chapter 12 , and also provide ways of incorporating phonetic similarity, closeness on the keyboard, and data from the actual spelling mistakes of users. Many would regard them as the state-of-the-art approach. Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs. The soundex algorithm is attributed to Margaret K. Odell and Robert C. Russelli (from U.S. patents granted in 1918 and 1922); the version described here draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate various phonetic matching algorithms, finding that a variant of the soundex algorithm performs poorly for general spelling correction, but that other algorithms based on the phonetic similarity of term pronunciations perform well. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_4</id>
    <title>Index construction</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/index-construction-1.html</url>
    <file>index-construction-1.html</file>
    <text> Index construction In this chapter, we look at how to construct an inverted index. We call this process index construction or indexing ; the process or machine that performs it the indexer . The design of indexing algorithms is governed by hardware constraints. We therefore begin this chapter with a review of the basics of computer hardware that are relevant for indexing. We then introduce blocked sort-based indexing (Section 4.2 ), an efficient single-machine algorithm designed for static collections that can be viewed as a more scalable version of the basic sort-based indexing algorithm we introduced in Chapter 1 . Section 4.3 describes single-pass in-memory indexing, an algorithm that has even better scaling properties because it does not hold the vocabulary in memory. For very large collections like the web, indexing has to be distributed over computer clusters with hundreds or thousands of machines. We discuss this in Section 4.4 . Collections with frequent changes require dynamic indexing introduced in Section 4.5 so that changes in the collection are immediately reflected in the index. Finally, we cover some complicating issues that can arise in indexing - such as security and indexes for ranked retrieval - in Section 4.6 . Index construction interacts with several topics covered in other chapters. The indexer needs raw text, but documents are encoded in many ways (see Chapter 2 ). Indexers compress and decompress intermediate files and the final index (see Chapter 5 ). In web search, documents are not on a local file system, but have to be spidered or crawled (see Chapter 20 ). In enterprise search , most documents are encapsulated in varied content management systems, email applications, and databases. We give some examples in Section 4.7 . Although most of these applications can be accessed via http, native Application Programming Interfaces (APIs) are usually more efficient. The reader should be aware that building the subsystem that feeds raw text to the indexing process can in itself be a challenging problem.   Subsections Hardware basics Blocked sort-based indexing Single-pass in-memory indexing Distributed indexing Dynamic indexing Other types of indexes References and further reading</text>
    <subsections>
       <section>
         <id>iir_4_1</id>
         <title>Hardware basics</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/hardware-basics-1.html</url>
         <file>hardware-basics-1.html</file>
         <text> Hardware basics   Table 4.1: Typical system parameters in 2007. The seek time is the time needed to position the disk head in a new position. The transfer time per byte is the rate of transfer from disk to memory when the head is in the right position.   Symbol Statistic Value     average seek time 5 ms s     transfer time per byte 0.02 s s       processor's clock rate     lowlevel operation             (e.g., compare & swap a word) 0.01 s s       size of main memory several GB       size of disk space 1 TB or more    When building an information retrieval (IR) system, many decisions are based on the characteristics of the computer hardware on which the system runs. We therefore begin this chapter with a brief review of computer hardware. Performance characteristics typical of systems in 2007 are shown in Table 4.1 . A list of hardware basics that we need in this book to motivate IR system design follows. Access to data in memory is much faster than access to data on disk. It takes a few clock cycles (perhaps seconds) to access a byte in memory, but much longer to transfer it from disk (about seconds). Consequently, we want to keep as much data as possible in memory, especially those data that we need to access frequently. We call the technique of keeping frequently used disk data in main memory caching . When doing a disk read or write, it takes a while for the disk head to move to the part of the disk where the data are located. This time is called the seek time and it averages 5 ms for typical disks. No data are being transferred during the seek. To maximize data transfer rates, chunks of data that will be read together should therefore be stored contiguously on disk. For example, using the numbers in Table 4.1 it may take as little as 0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is stored as one chunk, but up to seconds if it is stored in 100 noncontiguous chunks because we need to move the disk head up to 100 times. Operating systems generally read and write entire blocks. Thus, reading a single byte from disk can take as much time as reading the entire block. Block sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part of main memory where a block being read or written is stored a buffer . Data transfers from disk to memory are handled by the system bus, not by the processor. This means that the processor is available to process data during disk I/O. We can exploit this fact to speed up data transfers by storing compressed data on disk. Assuming an efficient decompression algorithm, the total time of reading and then decompressing compressed data is usually less than reading uncompressed data. Servers used in IR systems typically have several gigabytes (GB) of main memory, sometimes tens of GB. Available disk space is several orders of magnitude larger. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_2</id>
         <title>Blocked sort-based indexing</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/blocked-sort-based-indexing-1.html</url>
         <file>blocked-sort-based-indexing-1.html</file>
         <text> Blocked sort-based indexing The basic steps in constructing a nonpositional index are depicted in Figure 1.4 (page ). We first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. In this chapter, we describe methods for large collections that require the use of secondary storage. To make index construction more efficient, we represent terms as termIDs (instead of strings as we did in Figure 1.4 ), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection; or, in a two-pass approach, we compile the vocabulary in the first pass and construct the inverted index in the second pass. The index construction algorithms described in this chapter all do a single pass through the data. Section 4.7 gives references to multipass algorithms that are preferable in certain applications, for example, when disk space is scarce. We work with the Reuters-RCV1 collection as our model collection in this chapter, a collection with roughly 1 GB of text. It consists of about 800,000 documents that were sent over the Reuters newswire during a 1-year period between August 20, 1996, and August 19, 1997. A typical document is shown in Figure 4.1 , but note that we ignore multimedia information like images in this book and are only concerned with text. Reuters-RCV1 covers a wide range of international topics, including politics, business, sports, and (as in this example) science. Some key statistics of the collection are shown in Table 4.2 . Reuters-RCV1 has 100 million tokens. Collecting all termID-docID pairs of the collection using 4 bytes each for termID and docID therefore requires 0.8 GB of storage. Typical collections today are often one or two orders of magnitude larger than Reuters-RCV1. You can easily see how such collections overwhelm even large computers if we try to sort their termID-docID pairs in memory. If the size of the intermediate files during index construction is within a small factor of available memory, then the compression techniques introduced in Chapter 5 can help; however, the postings file of many large collections cannot fit into memory even after compression.   Table: Collection statistics for Reuters-RCV1. Values are rounded for the computations in this book. The unrounded values are: 806,791 documents, 222 tokens per document, 391,523 (distinct) terms, 6.04 bytes per token with spaces and punctuation, 4.5 bytes per token without spaces and punctuation, 7.5 bytes per term, and 96,969,056 tokens. The numbers in this table correspond to the third line (``case folding'') in icompresstb5.   Symbol Statistic Value     documents 800,000     avg. # tokens per document 200     terms 400,000       avg. # bytes per token (incl. spaces/punct.) 6       avg. # bytes per token (without spaces/punct.) 4.5       avg. # bytes per term 7.5     tokens 100,000,000     Figure 4.1: Document from the Reuters newswire. With main memory insufficient, we need to use an external sorting algorithm , that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks as we explained in Section 4.1 . One solution is the blocked sort-based indexing algorithm or BSBI in Figure 4.2 . BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. The algorithm parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full (PARSENEXTBLOCK in Figure 4.2 ). We choose the block size to fit comfortably into memory to permit a fast in-memory sort. The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk. Applying this to Reuters-RCV1 and assuming we can fit 10 million termID-docID pairs into memory, we end up with ten blocks, each an inverted index of one part of the collection.    Merging in blocked sort-based indexing.Two blocks (``postings lists to be merged'') are loaded from disk into memory, merged in memory (``merged postings lists'') and written back to disk. We show terms instead of termIDs for better readability. In the final step, the algorithm simultaneously merges the ten blocks into one large merged index. An example with two blocks is shown in Figure 4.3 , where we use to denote the document of the collection. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. In each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary. How expensive is BSBI? Its time complexity is because the step with the highest time complexity is sorting and is an upper bound for the number of items we must sort (i.e., the number of termID-docID pairs). But the actual indexing time is usually dominated by the time it takes to parse the documents (PARSENEXTBLOCK) and to do the final merge (MERGEBLOCKS). Exercise 4.6 asks you to compute the total index construction time for RCV1 that includes these steps as well as inverting the blocks and writing them to disk. Notice that Reuters-RCV1 is not particularly large in an age when one or more GB of memory are standard on personal computers. With appropriate compression (Chapter 5 ), we could have created an inverted index for RCV1 in memory on a not overly beefy server. The techniques we have described are needed, however, for collections that are several orders of magnitude larger. Exercises. If we need comparisons (where is the number of termID-docID pairs) and two disk seeks for each comparison, how much time would index construction for Reuters-RCV1 take if we used disk instead of memory for storage and an unoptimized sorting algorithm (i.e., not an external sorting algorithm)? Use the system parameters in Table 4.1 . How would you create the dictionary in blocked sort-based indexing on the fly to avoid an extra pass through the data? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_3</id>
         <title>Single-pass in-memory indexing</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/single-pass-in-memory-indexing-1.html</url>
         <file>single-pass-in-memory-indexing-1.html</file>
         <text> Single-pass in-memory indexing  single-pass in-memory indexing  SPIMI  Figure 4.4: Inversion of a block in single-pass in-memory indexing The SPIMI algorithm is shown in Figure 4.4 . The part of the algorithm that parses documents and turns them into a stream of term-docID pairs, which we call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on the token stream until the entire collection has been processed. Tokens are processed one by one (line 4) during each successive call of SPIMI-INVERT. When a term occurs for the first time, it is added to the dictionary (best implemented as a hash), and a new postings list is created (line 6). The call in line 7 returns this postings list for subsequent occurrences of the term. A difference between BSBI and SPIMI is that SPIMI adds a posting directly to its postings list (line 10). Instead of first collecting all termID-docID pairs and then sorting them (as we did in BSBI), each postings list is dynamic (i.e., its size is adjusted as it grows) and it is immediately available to collect postings. This has two advantages: It is faster because there is no sorting required, and it saves memory because we keep track of the term a postings list belongs to, so the termIDs of postings need not be stored. As a result, the blocks that individual calls of SPIMI-INVERT can process are much larger and the index construction process as a whole is more efficient. Because we do not know how large the postings list of a term will be when we first encounter it, we allocate space for a short postings list initially and double the space each time it is full (lines 8-9). This means that some memory is wasted, which counteracts the memory savings from the omission of termIDs in intermediate data structures. However, the overall memory requirements for the dynamically constructed index of a block in SPIMI are still lower than in BSBI. When memory has been exhausted, we write the index of the block (which consists of the dictionary and the postings lists) to disk (line 12). We have to sort the terms (line 11) before doing this because we want to write postings lists in lexicographic order to facilitate the final merging step. If each block's postings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each block. Each call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last step of SPIMI (corresponding to line 7 in Figure 4.2 ; not shown in Figure 4.4 ) is then to merge the blocks into the final inverted index. In addition to constructing a new dictionary structure for each block and eliminating the expensive sorting step, SPIMI has a third important component: compression. Both the postings and the dictionary terms can be stored compactly on disk if we employ compression. Compression increases the efficiency of the algorithm further because we can process even larger blocks, and because the individual blocks require less space on disk. We refer readers to the literature for this aspect of the algorithm (Section 4.7 ). The time complexity of SPIMI is because no sorting of tokens is required and all operations are at most linear in the size of the collection. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_4</id>
         <title>Distributed indexing</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/distributed-indexing-1.html</url>
         <file>distributed-indexing-1.html</file>
         <text> Distributed indexing  clusters   distributed indexing  distributed index term-partitioned index . Most large search engines prefer a document-partitioned index (which can be easily generated from a term-partitioned index). We discuss this topic further in Section 20.3 (page ). The distributed index construction method we describe in this section is an application of MapReduce , a general architecture for distributed computing. MapReduce is designed for large computer clusters. The point of a cluster is to solve large computing problems on cheap commodity machines or nodes that are built from standard parts (processor, memory, disk) as opposed to on a supercomputer with specialized hardware. Although hundreds or thousands of machines are available in such clusters, individual machines can fail at any time. One requirement for robust distributed indexing is, therefore, that we divide the work up into chunks that we can easily assign and - in case of failure - reassign. A master node directs the process of assigning and reassigning tasks to individual worker nodes. The map and reduce phases of MapReduce split up the computing job into chunks that standard machines can process in a short time. The various steps of MapReduce are shown in Figure 4.5 and an example on a collection consisting of two documents is shown in Figure 4.6 . First, the input data, in our case a collection of web pages, are split into splits where the size of the split is chosen to ensure that the work can be distributed evenly (chunks should not be too large) and efficiently (the total number of chunks we need to manage should not be too large); 16 or 64 MB are good sizes in distributed indexing. Splits are not preassigned to machines, but are instead assigned by the master node on an ongoing basis: As a machine finishes processing one split, it is assigned the next one. If a machine dies or becomes a laggard due to hardware problems, the split it is working on is simply reassigned to another machine.  Figure 4.5: An example of distributed indexing with MapReduce. Adapted from Dean and Ghemawat (2004). In general, MapReduce breaks a large computing problem into smaller parts by recasting it in terms of manipulation of key-value pairs . For indexing, a key-value pair has the form (termID,docID). In distributed indexing, the mapping from terms to termIDs is also distributed and therefore more complex than in single-machine indexing. A simple solution is to maintain a (perhaps precomputed) mapping for frequent terms that is copied to all nodes and to use terms directly (instead of termIDs) for infrequent terms. We do not address this problem here and assume that all nodes share a consistent term termID mapping. The map phase of MapReduce consists of mapping splits of the input data to key-value pairs. This is the same parsing task we also encountered in BSBI and SPIMI, and we therefore call the machines that execute the map phase parsers . Each parser writes its output to local intermediate files, the segment files (shown as in Figure 4.5 ). For the reduce phase , we want all values for a given key to be stored close together, so that they can be read and processed quickly. This is achieved by partitioning the keys into term partitions and having the parsers write key-value pairs for each term partition into a separate segment file. In Figure 4.5 , the term partitions are according to first letter: a-f, g-p, q-z, and . (We chose these key ranges for ease of exposition. In general, key ranges need not correspond to contiguous terms or termIDs.) The term partitions are defined by the person who operates the indexing system (Exercise 4.6 ). The parsers then write corresponding segment files, one for each term partition. Each term partition thus corresponds to segments files, where is the number of parsers. For instance, Figure 4.5 shows three a-f segment files of the a-f partition, corresponding to the three parsers shown in the figure. Collecting all values (here: docIDs) for a given key (here: termID) into one list is the task of the inverters in the reduce phase. The master assigns each term partition to a different inverter - and, as in the case of parsers, reassigns term partitions in case of failing or slow inverters. Each term partition (corresponding to segment files, one on each parser) is processed by one inverter. We assume here that segment files are of a size that a single machine can handle (Exercise 4.6 ). Finally, the list of values is sorted for each key and written to the final sorted postings list (``postings'' in the figure). (Note that postings in Figure 4.6 include term frequencies, whereas each posting in the other sections of this chapter is simply a docID without term frequency information.) The data flow is shown for a-f in Figure 4.5 . This completes the construction of the inverted index. Parsers and inverters are not separate sets of machines. The master identifies idle machines and assigns tasks to them. The same machine can be a parser in the map phase and an inverter in the reduce phase. And there are often other jobs that run in parallel with index construction, so in between being a parser and an inverter a machine might do some crawling or another unrelated task. To minimize write times before inverters reduce the data, each parser writes its segment files to its local disk. In the reduce phase, the master communicates to an inverter the locations of the relevant segment files (e.g., of the segment files of the a-f partition). Each segment file only requires one sequential read because all data relevant to a particular inverter were written to a single segment file by the parser. This setup minimizes the amount of network traffic needed during indexing.  Map and reduce functions in MapReduce. In general, the map function produces a list of key-value pairs. All values for a key are collected into one list in the reduce phase. This list is then processed further. The instantiations of the two functions and an example are shown for index construction. Because the map phase processes documents in a distributed fashion, termID-docID pairs need not be ordered correctly initially as in this example. The example shows terms instead of termIDs for better readability. We abbreviate Caesar as C and conquered as c'ed. Figure 4.6 shows the general schema of the MapReduce functions. Input and output are often lists of key-value pairs themselves, so that several MapReduce jobs can run in sequence. In fact, this was the design of the Google indexing system in 2004. What we describe in this section corresponds to only one of five to ten MapReduce operations in that indexing system. Another MapReduce operation transforms the term-partitioned index we just created into a document-partitioned one. MapReduce offers a robust and conceptually simple framework for implementing index construction in a distributed environment. By providing a semiautomatic method for splitting index construction into smaller tasks, it can scale to almost arbitrarily large collections, given computer clusters of sufficient size. Exercises. For splits, segments, and term partitions, how long would distributed index creation take for Reuters-RCV1 in a MapReduce architecture? Base your assumptions about cluster machines on Table 4.1 . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_5</id>
         <title>Dynamic indexing</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/dynamic-indexing-1.html</url>
         <file>dynamic-indexing-1.html</file>
         <text> Dynamic indexing Thus far, we have assumed that the document collection is static. This is fine for collections that change infrequently or never (e.g., the Bible or Shakespeare). But most collections are modified frequently with documents being added, deleted, and updated. This means that new terms need to be added to the dictionary, and postings lists need to be updated for existing terms. The simplest way to achieve this is to periodically reconstruct the index from scratch. This is a good solution if the number of changes over time is small and a delay in making new documents searchable is acceptable - and if enough resources are available to construct a new index while the old one is still available for querying. If there is a requirement that new documents be included quickly, one solution is to maintain two indexes: a large main index and a small auxiliary index that stores new documents. The auxiliary index is kept in memory. Searches are run across both indexes and results merged. Deletions are stored in an invalidation bit vector. We can then filter out deleted documents before returning the search result. Documents are updated by deleting and reinserting them. Each time the auxiliary index becomes too large, we merge it into the main index. The cost of this merging operation depends on how we store the index in the file system. If we store each postings list as a separate file, then the merge simply consists of extending each postings list of the main index by the corresponding postings list of the auxiliary index. In this scheme, the reason for keeping the auxiliary index is to reduce the number of disk seeks required over time. Updating each document separately requires up to disk seeks, where is the average size of the vocabulary of documents in the collection. With an auxiliary index, we only put additional load on the disk when we merge auxiliary and main indexes. Unfortunately, the one-file-per-postings-list scheme is infeasible because most file systems cannot efficiently handle very large numbers of files. The simplest alternative is to store the index as one large file, that is, as a concatenation of all postings lists. In reality, we often choose a compromise between the two extremes (Section 4.7 ). To simplify the discussion, we choose the simple option of storing the index as one large file here. In this scheme, we process each posting times because we touch it during each of merges where is the size of the auxiliary index and the total number of postings. Thus, the overall time complexity is . (We neglect the representation of terms here and consider only the docIDs. For the purpose of time complexity, a postings list is simply a list of docIDs.)  Figure: Logarithmic merging. Each token (termID,docID) is initially added to in-memory index by LMERGEADDTOKEN. LOGARITHMICMERGE initializes and . We can do better than by introducing indexes , , , ...of size , , .... Postings percolate up this sequence of indexes and are processed only once on each level. This scheme is called logarithmic merging (Figure 4.7 ). As before, up to postings are accumulated in an in-memory auxiliary index, which we call . When the limit is reached, the postings in are transferred to a new index that is created on disk. The next time is full, it is merged with to create an index of size . Then is either stored as (if there isn't already an ) or merged with into (if exists); and so on. We service search requests by querying in-memory and all currently valid indexes on disk and merging the results. Readers familiar with the binomial heap data structure will recognize its similarity with the structure of the inverted indexes in logarithmic merging. Overall index construction time is because each posting is processed only once on each of the levels. We trade this efficiency gain for a slow down of query processing; we now need to merge results from indexes as opposed to just two (the main and auxiliary indexes). As in the auxiliary index scheme, we still need to merge very large indexes occasionally (which slows down the search system during the merge), but this happens less frequently and the indexes involved in a merge on average are smaller. Having multiple indexes complicates the maintenance of collection-wide statistics. For example, it affects the spelling correction algorithm in Section 3.3 (page ) that selects the corrected alternative with the most hits. With multiple indexes and an invalidation bit vector, the correct number of hits for a term is no longer a simple lookup. In fact, all aspects of an IR system - index maintenance, query processing, distribution, and so on - are more complex in logarithmic merging. Because of this complexity of dynamic indexing, some large search engines adopt a reconstruction-from-scratch strategy. They do not construct indexes dynamically. Instead, a new index is built from scratch periodically. Query processing is then switched from the new index and the old index is deleted. Exercises. For and , perform a step-by-step simulation of the algorithm in Figure 4.7 . Create a table that shows, for each point in time at which tokens have been processed ( ), which of the three indexes are in use. The first three lines of the table are given below.         2 0 0 0 0     4 0 0 0 1     6 0 0 1 0   </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_6</id>
         <title>Other types of indexes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/other-types-of-indexes-1.html</url>
         <file>other-types-of-indexes-1.html</file>
         <text> Other types of indexes In the indexes we have considered so far, postings lists are ordered with respect to docID. As we see in Chapter 5, this is advantageous for compression - instead of docIDs we can compress smaller gaps between IDs, thus reducing space requirements for the index. However, this structure for the index is not optimal when we build ranked (Chapters 6 7 ) - as opposed to Boolean - retrieval systems . In ranked retrieval, postings are often ordered according to weight or impact , with the highest-weighted postings occurring first. With this organization, scanning of long postings lists during query processing can usually be terminated early when weights have become so small that any further documents can be predicted to be of low similarity to the query (see Chapter 6 ). In a docID-sorted index, new documents are always inserted at the end of postings lists. In an impact-sorted index impactordered, the insertion can occur anywhere, thus complicating the update of the inverted index. Security is an important consideration for retrieval systems in corporations. A low-level employee should not be able to find the salary roster of the corporation, but authorized managers need to be able to search for it. Users' results lists must not contain documents they are barred from opening; the very existence of a document can be sensitive information.  Figure: A user-document matrix for access control lists. Element is 1 if user has access to document and 0 otherwise. During query processing, a user's access postings list is intersected with the results list returned by the text part of the index. User authorization is often mediated through access control lists or ACLs. ACLs can be dealt with in an information retrieval system by representing each document as the set of users that can access them (Figure 4.8 ) and then inverting the resulting user-document matrix. The inverted ACL index has, for each user, a ``postings list'' of documents they can access - the user's access list. Search results are then intersected with this list. However, such an index is difficult to maintain when access permissions change - we discussed these difficulties in the context of incremental indexing for regular postings lists in Section 4.5. It also requires the processing of very long postings lists for users with access to large document subsets. User membership is therefore often verified by retrieving access information directly from the file system at query time - even though this slows down retrieval. We discussed indexes for storing and retrieving terms (as opposed to documents) in Chapter 3 . Exercises. Can spelling correction compromise document-level security? Consider the case where a spelling correction is based on documents to which the user does not have access. Exercises. Total index construction time in blocked sort-based indexing is broken down in Table 4.3. Fill out the time column of the table for Reuters-RCV1 assuming a system with the parameters given in Table 4.1 . Table: The five steps in constructing an index for Reuters-RCV1 in blocked sort-based indexing. Line numbers refer to Figure 4.2 .     Step Time     1 reading of collection (line 4)       2 10 initial sorts of records each (line 5)       3 writing of 10 blocks (line 6)       4 total disk transfer time for merging (line 7)       5 time of actual merging (line 7)         total     Table 4.4: Collection statistics for a large collection.   Symbol Statistic Value     # documents 1,000,000,000     # tokens per document 1000     # distinct terms 44,000,000   Repeat Exercise 4.6 for the larger collection in Table 4.4 . Choose a block size that is realistic for current technology (remember that a block should easily fit into main memory). How many blocks do you need? Assume that we have a collection of modest size whose index can be constructed with the simple in-memory indexing algorithm in Figure 1.4 (page ). For this collection, compare memory, disk and time requirements of the simple algorithm in Figure 1.4 and blocked sort-based indexing. Assume that machines in MapReduce have 100 GB of disk space each. Assume further that the postings list of the term the has a size of 200 GB. Then the MapReduce algorithm as described cannot be run to construct the index. How would you modify MapReduce so that it can handle this case? For optimal load balancing, the inverters in MapReduce must get segmented postings files of similar sizes. For a new collection, the distribution of key-value pairs may not be known in advance. How would you solve this problem? Apply MapReduce to the problem of counting how often each term occurs in a set of files. Specify map and reduce operations for this task. Write down an example along the lines of Figure 4.6 . We claimed (on page 4.5 ) that an auxiliary index can impair the quality of collection statistics. An example is the term weighting method idf , which is defined as where is the total number of documents and is the number of documents that term occurs in idf. Show that even a small auxiliary index can cause significant error in idf when it is computed on the main index only. Consider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm Flossie). </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_4_7</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-4.html</url>
         <file>references-and-further-reading-4.html</file>
         <text> References and further reading Witten et al. (1999, Chapter 5) present an extensive treatment of the subject of index construction and additional indexing algorithms with different tradeoffs of memory, disk space, and time. In general, blocked sort-based indexing does well on all three counts. However, if conserving memory or disk space is the main criterion, then other algorithms may be a better choice. See Witten et al. (1999), Tables 5.4 and 5.5; BSBI is closest to ``sort-based multiway merge,'' but the two algorithms differ in dictionary structure and use of compression. Moffat and Bell (1995) show how to construct an index ``in situ,'' that is, with disk space usage close to what is needed for the final index and with a minimum of additional temporary files (cf. also Harman and Candela (1990)). They give Lesk (1988) and Somogyi (1990) credit for being among the first to employ sorting for index construction. The SPIMI method in Section 4.3 is from (Heinz and Zobel, 2003). We have simplified several aspects of the algorithm, including compression and the fact that each term's data structure also contains, in addition to the postings list, its document frequency and house keeping information. We recommend Heinz and Zobel (2003) and Zobel and Moffat (2006) as up-do-date, in-depth treatments of index construction. Other algorithms with good scaling properties with respect to vocabulary size require several passes through the data, e.g., FAST-INV (Harman et al., 1992, Fox and Lee, 1991). The MapReduce architecture was introduced by Dean and Ghemawat (2004). An open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/. Ribeiro-Neto et al. (1999) and Melnik et al. (2001) describe other approaches to distributed indexing. Introductory chapters on distributed IR are (Baeza-Yates and Ribeiro-Neto, 1999, Chapter 9) and (Grossman and Frieder, 2004, Chapter 8). See also Callan (2000). Lester et al. (2005) and Büttcher and Clarke (2005a) analyze the properties of logarithmic merging and compare it with other construction methods. One of the first uses of this method was in Lucene (http://lucene.apache.org). Other dynamic indexing methods are discussed by Büttcher et al. (2006) and Lester et al. (2006). The latter paper also discusses the strategy of replacing the old index by one built from scratch. Heinz et al. (2002) compare data structures for accumulating the vocabulary in memory. Büttcher and Clarke (2005b) discuss security models for a common inverted index for multiple users. A detailed characterization of the Reuters-RCV1 collection can be found in (Lewis et al., 2004). NIST distributes the collection (see http://trec.nist.gov/data/reuters/reuters.html). Garcia-Molina et al. (1999, Chapter 2) review computer hardware relevant to system design in depth. An effective indexer for enterprise search needs to be able to communicate efficiently with a number of applications that hold text data in corporations, including Microsoft Outlook, IBM's Lotus software, databases like Oracle and MySQL, content management systems like Open Text, and enterprise resource planning software like SAP. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_5</id>
    <title>Index compression</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/index-compression-1.html</url>
    <file>index-compression-1.html</file>
    <text> Index compression Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems. One benefit of compression is immediately clear. We need less disk space. As we will see, compression ratios of 1:4 are easy to achieve, potentially cutting the cost of storing the index by 75%. There are two more subtle benefits of compression. The first is increased use of caching. Search systems use some parts of the dictionary and the index much more than others. For example, if we cache the postings list of a frequently used query term , then the computations necessary for responding to the one-term query can be entirely done in memory. With compression, we can fit a lot more information into main memory. Instead of having to expend a disk seek when processing a query with , we instead access its postings list in memory and decompress it. As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small. As a result, we are able to decrease the response time of the IR system substantially. Because memory is a more expensive resource than disk space, increased speed owing to caching - rather than decreased space requirements - is often the prime motivator for compression. The second more subtle advantage of compression is faster transfer of data from disk to memory. Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form. For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression. So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists. If the main goal of compression is to conserve disk space, then the speed of compression algorithms is of no concern. But for improved cache utilization and faster disk-to-memory transfer, decompression speeds must be high. The compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression. In this chapter, we define a posting as a docID in a postings list. For example, the postings list (6; 20, 45, 100), where 6 is the termID of the list's term, contains three postings. As discussed in Section 2.4.2 (page ), postings in most search systems also contain frequency and position information; but we will only consider simple docID postings here. See Section 5.4 for references on compressing frequencies and positions. This chapter first gives a statistical characterization of the distribution of the entities we want to compress - terms and postings in large collections (Section 5.1 ). We then look at compression of the dictionary, using the dictionary-as-a-string method and blocked storage (Section 5.2 ). Section 5.3 describes two techniques for compressing the postings file, variable byte encoding and encoding.   Subsections Statistical properties of terms in information retrieval Heaps' law: Estimating the number of terms Zipf's law: Modeling the distribution of terms Dictionary compression Dictionary as a string Blocked storage Postings file compression Variable byte codes Gamma codes References and further reading</text>
    <subsections>
       <section>
         <id>iir_5_1</id>
         <title>Statistical properties of terms in information retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/statistical-properties-of-terms-in-information-retrieval-1.html</url>
         <file>statistical-properties-of-terms-in-information-retrieval-1.html</file>
         <text> Statistical properties of terms in information retrieval As in the last chapter, we use Reuters-RCV1 as our model collection (see Table 4.2 , page 4.2 ). We give some term and postings statistics for the collection in Table 5.1 . ``'' indicates the reduction in size from the previous line. ``T%'' is the cumulative reduction from unfiltered. The table shows the number of terms for different levels of preprocessing (column 2). The number of terms is the main factor in determining the size of the dictionary. The number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection. The expected size of a positional index is related to the number of positions it must encode (column 4). In general, the statistics in Table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly. Stemming and case folding reduce the number of (distinct) terms by 17% each and the number of nonpositional postings by 4% and 3%, respectively. The treatment of the most frequent words is also important. The rule of 30 states that the 30 most common words account for 30% of the tokens in written text (31% in the table). Eliminating the 150 most common words from indexing (as stop words; cf. Section 2.2.2 , page 2.2.2 ) cuts 25% to 30% of the nonpositional postings. But, although a stop list of 150 words reduces the number of postings by a quarter or more, this size reduction does not carry over to the size of the compressed index. As we will see later in this chapter, the postings lists of frequent words require only a few bits per posting after compression.   Table 5.1: The effect of preprocessing on the number of terms, nonpositional postings, and tokens for Reuters-RCV1. ``'' indicates the reduction in size from the previous line, except that ``30 stop words'' and ``150 stop words'' both use ``case folding'' as their reference line. ``T%'' is the cumulative (``total'') reduction from unfiltered. We performed stemming with the Porter stemmer (Chapter 2 , page 2.2.4 ).                 tokens (number of position       (distinct) terms nonpositional postings entries in postings)                   number T% number T% number T%     unfiltered 484,494     109,971,179     197,879,290         no numbers 473,723 2 2 100,680,242 8 8 179,158,204 9 9     case folding 391,523 17 19 96,969,056 3 12 179,158,204 0 9     30 stop words 391,493 0 19 83,390,443 14 24 121,857,825 31 38     150 stop words 391,373 0 19 67,001,847 30 39 94,516,599 47 52     stemming 322,383 17 33 63,812,300 4 42 94,516,599 0 52    The deltas in the table are in a range typical of large collections. Note, however, that the percentage reductions can be very different for some text collections. For example, for a collection of web pages with a high proportion of French text, a lemmatizer for French reduces vocabulary size much more than the Porter stemmer does for an English-only collection because French is a morphologically richer language than English. The compression techniques we describe in the remainder of this chapter are lossless , that is, all information is preserved. Better compression ratios can be achieved with lossy compression , which discards some information. Case folding, stemming, and stop word elimination are forms of lossy compression. Similarly, the vector space model (Chapter 6 ) and dimensionality reduction techniques like latent semantic indexing (Chapter 18 ) create compact representations from which we cannot fully restore the original collection. Lossy compression makes sense when the ``lost'' information is unlikely ever to be used by the search system. For example, web search is characterized by a large number of documents, short queries, and users who only look at the first few pages of results. As a consequence, we can discard postings of documents that would only be used for hits far down the list. Thus, there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness. Before introducing techniques for compressing the dictionary, we want to estimate the number of distinct terms in a collection. It is sometimes said that languages have a vocabulary of a certain size. The second edition of the Oxford English Dictionary (OED) defines more than 600,000 words. But the vocabulary of most large collections is much larger than the OED. The OED does not include most names of people, locations, products, or scientific entities like genes. These names need to be included in the inverted index, so our users can search for them.   Subsections Heaps' law: Estimating the number of terms Zipf's law: Modeling the distribution of terms</text>
         <subsections>
            <section>
              <id>iir_5_1_1</id>
              <title>Heaps' law: Estimating the number of terms</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/heaps-law-estimating-the-number-of-terms-1.html</url>
              <file>heaps-law-estimating-the-number-of-terms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_5_1_2</id>
              <title>Zipf's law: Modeling the distribution of terms</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html</url>
              <file>zipfs-law-modeling-the-distribution-of-terms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_5_2</id>
         <title>Dictionary compression</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/dictionary-compression-1.html</url>
         <file>dictionary-compression-1.html</file>
         <text> Dictionary compression This section presents a series of dictionary data structures that achieve increasingly higher compression ratios. The dictionary is small compared with the postings file as suggested by Table 5.1 . So why compress it if it is responsible for only a small percentage of the overall space requirements of the IR system? One of the primary factors in determining the response time of an IR system is the number of disk seeks necessary to process a query. If parts of the dictionary are on disk, then many more disk seeks are necessary in query evaluation. Thus, the main goal of compressing the dictionary is to fit it in main memory, or at least a large portion of it, to support high query throughput. Although dictionaries of very large collections fit into the memory of a standard desktop machine, this is not true of many other application scenarios. For example, an enterprise search server for a large corporation may have to index a multiterabyte collection with a comparatively large vocabulary because of the presence of documents in many different languages. We also want to be able to design search systems for limited hardware such as mobile phones and onboard computers. Other reasons for wanting to conserve memory are fast startup time and having to share resources with other applications. The search system on your PC must get along with the memory-hogging word processing suite you are using at the same time.  Figure 5.3: Storing the dictionary as an array of fixed-width entries.   Subsections Dictionary as a string Blocked storage</text>
         <subsections>
            <section>
              <id>iir_5_2_1</id>
              <title>Dictionary as a string</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/dictionary-as-a-string-1.html</url>
              <file>dictionary-as-a-string-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_5_2_2</id>
              <title>Blocked storage</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/blocked-storage-1.html</url>
              <file>blocked-storage-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_5_3</id>
         <title>Postings file compression</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/postings-file-compression-1.html</url>
         <file>postings-file-compression-1.html</file>
         <text> Postings file compression   Table: Encoding gaps instead of document IDs. For example, we store gaps 107, 5, 43, ..., instead of docIDs 283154, 283159, 283202, ... for computer. The first docID is left unchanged (only shown for arachnocentric).     encoding postings list                   the docIDs ...   283042   283043   283044   283045 ...       gaps       1   1   1   ...     computer docIDs ...   283047   283154   283159   283202 ...       gaps       107   5   43   ...     arachnocentric docIDs 252000   500100                     gaps 252000 248100                    Recall from Table 4.2 (page 4.2 ) that Reuters-RCV1 has 800,000 documents, 200 tokens per document, six characters per token, and 100,000,000 postings where we define a posting in this chapter as a docID in a postings list, that is, excluding frequency and position information. These numbers correspond to line 3 (``case folding'') in Table 5.1 . Document identifiers are bits long. Thus, the size of the collection is about and the size of the uncompressed postings file is . To devise a more efficient representation of the postings file, one that uses fewer than 20 bits per document, we observe that the postings for frequent terms are close together. Imagine going through the documents of a collection one by one and looking for a frequent term like computer. We will find a document containing computer, then we skip a few documents that do not contain it, then there is again a document with the term and so on (see Table 5.3 ). The key idea is that the gaps between postings are short, requiring a lot less space than 20 bits to store. In fact, gaps for the most frequent terms such as the and for are mostly equal to 1. But the gaps for a rare term that occurs only once or twice in a collection (e.g., arachnocentric in Table 5.3 ) have the same order of magnitude as the docIDs and need 20 bits. For an economical representation of this distribution of gaps, we need a variable encoding method that uses fewer bits for short gaps. To encode small numbers in less space than large numbers, we look at two types of methods: bytewise compression and bitwise compression. As the names suggest, these methods attempt to encode gaps with the minimum number of bytes and bits, respectively.   Subsections Variable byte codes Gamma codes</text>
         <subsections>
            <section>
              <id>iir_5_3_1</id>
              <title>Variable byte codes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/variable-byte-codes-1.html</url>
              <file>variable-byte-codes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_5_3_2</id>
              <title>Gamma codes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/gamma-codes-1.html</url>
              <file>gamma-codes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_5_4</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-5.html</url>
         <file>references-and-further-reading-5.html</file>
         <text> References and further reading Heaps' law was discovered by Heaps (1978). See also Baeza-Yates and Ribeiro-Neto (1999). A detailed study of vocabulary growth in large collections is (Williams and Zobel, 2005). Zipf's law is due to Zipf (1949). Witten and Bell (1990) investigate the quality of the fit obtained by the law. Other term distribution models, including K mixture and two-poisson model, are discussed by Manning and Schütze (1999, Chapter 15). Carmel et al. (2001), Büttcher and Clarke (2006), Blanco and Barreiro (2007), and Ntoulas and Cho (2007) show that lossy compression can achieve good compression with no or no significant decrease in retrieval effectiveness. Dictionary compression is covered in detail by Witten et al. (1999, Chapter 4), which is recommended as additional reading. Subsection 5.3.1 is based on (Scholer et al., 2002). The authors find that variable byte codes process queries two times faster than either bit-level compressed indexes or uncompressed indexes with a 30% penalty in compression ratio compared with the best bit-level compression method. They also show that compressed indexes can be superior to uncompressed indexes not only in disk usage, but also in query processing speed. Compared with VB codes, ``variable nibble'' codes showed 5% to 10% better compression and up to one third worse effectiveness in one experiment (Anh and Moffat, 2005). Trotman (2003) also recommends using VB codes unless disk space is at a premium. In recent work, Anh and Moffat (2006a;2005) and Zukowski et al. (2006) have constructed word-aligned binary codes that are both faster in decompression and at least as efficient as VB codes. Zhang et al. (2007) investigate the increased effectiveness of caching when a number of different compression techniques for postings lists are used on modern hardware. codes (Exercise 5.3.2 ) and codes were introduced by Elias (1975), who proved that both codes are universal. In addition, codes are asymptotically optimal for . codes perform better than codes if large numbers (greater than 15) dominate. A good introduction to information theory, including the concept of entropy , is (Cover and Thomas, 1991). While Elias codes are only asymptotically optimal, arithmetic codes (Witten et al., 1999, Section 2.4) can be constructed to be arbitrarily close to the optimum for any . Several additional index compression techniques are covered by Witten et al. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommend using parameterized codes for index compression, codes that explicitly model the probability distribution of gaps for each term. For example, they show that Golomb codes achieve better compression ratios than codes for large collections. Moffat and Zobel (1992) compare several parameterized methods, including LLRUN (Fraenkel and Klein, 1985). The distribution of gaps in a postings list depends on the assignment of docIDs to documents. A number of researchers have looked into assigning docIDs in a way that is conducive to the efficient compression of gap sequences (Moffat and Stuiver, 1996; Blandford and Blelloch, 2002; Silvestri et al., 2004; Blanco and Barreiro, 2006; Silvestri, 2007). These techniques assign docIDs in a small range to documents in a cluster where a cluster can consist of all documents in a given time period, on a particular web site, or sharing another property. As a result, when a sequence of documents from a cluster occurs in a postings list, their gaps are small and can be more effectively compressed. Different considerations apply to the compression of term frequencies and word positions than to the compression of docIDs in postings lists. See Scholer et al. (2002) and Zobel and Moffat (2006). Zobel and Moffat (2006) is recommended in general as an in-depth and up-to-date tutorial on inverted indexes, including index compression. This chapter only looks at index compression for Boolean retrieval. For ranked retrieval (Chapter 6 ), it is advantageous to order postings according to term frequency instead of docID. During query processing, the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked documents found so far. It is not a good idea to precompute and store weights in the index (as opposed to frequencies) because they cannot be compressed as well as integers (see impactordered). Document compression can also be important in an efficient information retrieval system. de Moura et al. (2000) and Brisaboa et al. (2007) describe compression schemes that allow direct searching of terms and phrases in the compressed text, which is infeasible with standard text compression utilities like gzip and compress. Exercises. We have defined unary codes as being ``10'': sequences of 1s terminated by a 0. Interchanging the roles of 0s and 1s yields an equivalent ``01'' unary code. When this 01 unary code is used, the construction of a code can be stated as follows: (1) Write down in binary using bits. (2) Prepend 0s. (i) Encode the numbers in Table 5.5 in this alternative code. (ii) Show that this method produces a well-defined alternative code in the sense that it has the same length and can be uniquely decoded. Unary code is not a universal code in the sense defined above. However, there exists a distribution over gaps for which unary code is optimal. Which distribution is this? Give some examples of terms that violate the assumption that gaps all have the same size (which we made when estimating the space requirements of a -encoded index). What are general characteristics of these terms? Consider a term whose postings list has size , say, . Compare the size of the -compressed gap-encoded postings list if the distribution of the term is uniform (i.e., all gaps have the same size) versus its size when the distribution is not uniform. Which compressed postings list is smaller? Work out the sum in Equation 12 and show it adds up to about 251 MB. Use the numbers in Table 4.2 , but do not round , , and the number of vocabulary blocks. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_6</id>
    <title>Scoring, term weighting and the vector space model</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/scoring-term-weighting-and-the-vector-space-model-1.html</url>
    <file>scoring-term-weighting-and-the-vector-space-model-1.html</file>
    <text> Scoring, term weighting and the vector space model Thus far we have dealt with indexes that support Boolean queries: a document either matches or does not match a query. In the case of large document collections, the resulting number of matching documents can far exceed the number a human user could possibly sift through. Accordingly, it is essential for a search engine to rank-order the documents matching a query. To do this, the search engine computes, for each matching document, a score with respect to the query at hand. In this chapter we initiate the study of assigning a score to a (query, document) pair. This chapter consists of three main ideas. We introduce parametric and zone indexes in Section 6.1 , which serve two purposes. First, they allow us to index and retrieve documents by metadata such as the language in which a document is written. Second, they give us a simple means for scoring (and thereby ranking) documents in response to a query. Next, in Section 6.2 we develop the idea of weighting the importance of a term in a document, based on the statistics of occurrence of the term. In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document. This view is known as vector space scoring. 6.4 7 As we develop these ideas, the notion of a query will assume multiple nuances. In Section 6.1 we consider queries in which specific query terms occur in specified regions of a matching document. Beginning Section 6.2 we will in fact relax the requirement of matching specific regions of a document; instead, we will look at so-called free text queries that simply consist of query terms with no specification on their relative order, importance or where in a document they should be found. The bulk of our study of scoring will be in this latter notion of a query being such a set of terms.   Subsections Parametric and zone indexes Weighted zone scoring Learning weights The optimal weight g Term frequency and weighting Inverse document frequency Tf-idf weighting The vector space model for scoring Dot products Queries as vectors Computing vector scores Variant tf-idf functions Sublinear tf scaling Maximum tf normalization Document and query weighting schemes Pivoted normalized document length References and further reading</text>
    <subsections>
       <section>
         <id>iir_6_1</id>
         <title>Parametric and zone indexes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/parametric-and-zone-indexes-1.html</url>
         <file>parametric-and-zone-indexes-1.html</file>
         <text> Parametric and zone indexes We have thus far viewed a document as a sequence of terms. In fact, most documents have additional structure. Digital documents generally encode, in machine-recognizable form, certain metadata associated with each document. By metadata, we mean specific forms of data about a document, such as its author(s), title and date of publication. This metadata would generally include fields such as the date of creation and the format of the document, as well the author and possibly the title of the document. The possible values of a field should be thought of as finite - for instance, the set of all dates of authorship. Consider queries of the form ``find documents authored by William Shakespeare in 1601, containing the phrase alas poor Yorick''. Query processing then consists as usual of postings intersections, except that we may merge postings from standard inverted as well as parametric indexes . There is one parametric index for each field (say, date of creation); it allows us to select only the documents matching a date specified in the query. Figure 6.1 illustrates the user's view of such a parametric search. Some of the fields may assume ordered values, such as dates; in the example query above, the year 1601 is one such field value. The search engine may support querying ranges on such ordered values; to this end, a structure like a B-tree may be used for the field's dictionary.  Parametric search.In this example we have a collection with fields allowing us to select publications by zones such as Author and fields such as Language. Zones are similar to fields, except the contents of a zone can be arbitrary free text. Whereas a field may take on a relatively small set of values, a zone can be thought of as an arbitrary, unbounded amount of text. For instance, document titles and abstracts are generally treated as zones. We may build a separate inverted index for each zone of a document, to support queries such as ``find documents with merchant in the title and william in the author list and the phrase gentle rain in the body''. This has the effect of building an index that looks like Figure 6.2. Whereas the dictionary for a parametric index comes from a fixed vocabulary (the set of languages, or the set of dates), the dictionary for a zone index must structure whatever vocabulary stems from the text of that zone.   In fact, we can reduce the size of the dictionary by encoding the zone in which a term occurs in the postings. In Figure 6.3 for instance, we show how occurrences of william in the title and author zones of various documents are encoded. Such an encoding is useful when the size of the dictionary is a concern (because we require the dictionary to fit in main memory). But there is another important reason why the encoding of Figure 6.3 is useful: the efficient computation of scores using a technique we will call weighted zone scoring .  Figure 6.3: Zone index in which the zone is encoded in the postings rather than the dictionary.   Subsections Weighted zone scoring Learning weights The optimal weight g</text>
         <subsections>
            <section>
              <id>iir_6_1_1</id>
              <title>Weighted zone scoring</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/weighted-zone-scoring-1.html</url>
              <file>weighted-zone-scoring-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_1_2</id>
              <title>Learning weights</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/learning-weights-1.html</url>
              <file>learning-weights-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_1_3</id>
              <title>The optimal weight g</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-optimal-weight-g-1.html</url>
              <file>the-optimal-weight-g-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_6_2</id>
         <title>Term frequency and weighting</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html</url>
         <file>term-frequency-and-weighting-1.html</file>
         <text> Term frequency and weighting  free text query 1.4 Towards this end, we assign to each term in a document a weight for that term, that depends on the number of occurrences of the term in the document. We would like to compute a score between a query term  and a document , based on the weight of in . The simplest approach is to assign the weight to be equal to the number of occurrences of term  in document . This weighting scheme is referred to as term frequency and is denoted , with the subscripts denoting the term and the document in order. For a document , the set of weights determined by the weights above (or indeed any weighting function that maps the number of occurrences of in to a positive real value) may be viewed as a quantitative digest of that document. In this view of a document, known in the literature as the bag of words model , the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material (in contrast to Boolean retrieval). We only retain information on the number of occurrences of each term. Thus, the document ``Mary is quicker than John'' is, in this view, identical to the document ``John is quicker than Mary''. Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content. We will develop this intuition further in Section 6.3 . Before doing so we first study the question: are all words in a document equally important? Clearly not; in Section 2.2.2 (page ) we looked at the idea of stop words - words that we decide not to index at all, and therefore do not contribute in any way to retrieval and scoring.   Subsections Inverse document frequency Tf-idf weighting</text>
         <subsections>
            <section>
              <id>iir_6_2_1</id>
              <title>Inverse document frequency</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html</url>
              <file>inverse-document-frequency-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_2_2</id>
              <title>Tf-idf weighting</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html</url>
              <file>tf-idf-weighting-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_6_3</id>
         <title>The vector space model for scoring</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html</url>
         <file>the-vector-space-model-for-scoring-1.html</file>
         <text> The vector space model for scoring In Section 6.2 (page ) we developed the notion of a document vector that captures the relative importance of the terms in a document. The representation of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval operations ranging from scoring documents on a query, document classification and document clustering. We first develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2 ) of queries as vectors in the same vector space as the document collection.   Subsections Dot products Queries as vectors Computing vector scores </text>
         <subsections>
            <section>
              <id>iir_6_3_1</id>
              <title>Dot products</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/dot-products-1.html</url>
              <file>dot-products-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_3_2</id>
              <title>Queries as vectors</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/queries-as-vectors-1.html</url>
              <file>queries-as-vectors-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_3_3</id>
              <title>Computing vector scores</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/computing-vector-scores-1.html</url>
              <file>computing-vector-scores-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_6_4</id>
         <title>Variant tf-idf functions</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/variant-tf-idf-functions-1.html</url>
         <file>variant-tf-idf-functions-1.html</file>
         <text> Variant tf-idf functions 11 6.4.3    Subsections Sublinear tf scaling Maximum tf normalization Document and query weighting schemes Pivoted normalized document length </text>
         <subsections>
            <section>
              <id>iir_6_4_1</id>
              <title>Sublinear tf scaling</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html</url>
              <file>sublinear-tf-scaling-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_4_2</id>
              <title>Maximum tf normalization</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html</url>
              <file>maximum-tf-normalization-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_4_3</id>
              <title>Document and query weighting schemes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html</url>
              <file>document-and-query-weighting-schemes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_6_4_4</id>
              <title>Pivoted normalized document length</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html</url>
              <file>pivoted-normalized-document-length-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_6_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-6.html</url>
         <file>references-and-further-reading-6.html</file>
         <text> References and further reading Chapter 7 develops the computational aspects of vector space scoring. Luhn (1957;1958) describes some of the earliest reported applications of term weighting. His paper dwells on the importance of medium frequency terms (terms that are neither too commonplace nor too rare) and may be thought of as anticipating tf-idf and related weighting schemes. Spärck Jones (1972) builds on this intuition through detailed experiments showing the use of inverse document frequency in term weighting. A series of extensions and theoretical justifications of idf are due to Salton and Buckley (1987) Robertson and Jones (1976), Croft and Harper (1979) and Papineni (2001). Robertson maintains a web page (http://www.soi.city.ac.uk/~ser/idf.html) containing the history of idf, including soft copies of early papers that predated electronic versions of journal article. Singhal et al. (1996a) develop pivoted document length normalization. Probabilistic language models (Chapter 11 ) develop weighting techniques that are more nuanced than tf-idf; the reader will find this development in Section 11.4.3 . We observed that by assigning a weight for each term in a document, a document may be viewed as a vector of term weights, one for each term in the collection. The SMART information retrieval system at Cornell (Salton, 1971b) due to Salton and colleagues was perhaps the first to view a document as a vector of weights. The basic computation of cosine scores as described in Section 6.3.3 is due to Zobel and Moffat (2006). The two query evaluation strategies term-at-a-time and document-at-a-time are discussed by Turtle and Flood (1995). The SMART notation for tf-idf term weighting schemes in Figure 6.15 is presented in (Singhal et al., 1996b;1995, Salton and Buckley, 1988). Not all versions of the notation are consistent; we most closely follow (Singhal et al., 1996b). A more detailed and exhaustive notation was developed in Moffat and Zobel (1998), considering a larger palette of schemes for term and document frequency weighting. Beyond the notation, Moffat and Zobel (1998) sought to set up a space of feasible weighting functions through which hill-climbing approaches could be used to begin with weighting schemes that performed well, then make local improvements to identify the best combinations. However, they report that such hill-climbing methods failed to lead to any conclusions on the best weighting schemes. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_7</id>
    <title>Computing scores in a complete search system</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/computing-scores-in-a-complete-search-system-1.html</url>
    <file>computing-scores-in-a-complete-search-system-1.html</file>
    <text> Computing scores in a complete search system Chapter 6 developed the theory underlying term weighting in documents for the purposes of scoring, leading up to vector space models and the basic cosine scoring algorithm of Section 6.3.3 (page ). In this chapter we begin in Section 7.1 with heuristics for speeding up this computation; many of these heuristics achieve their speed at the risk of not finding quite the top documents matching the query. Some of these heuristics generalize beyond cosine scoring. With Section 7.1 in place, we have essentially all the components needed for a complete search engine. We therefore take a step back from cosine scoring, to the more general problem of computing scores in a search engine. In Section 7.2 we outline a complete search engine, including indexes and structures to support not only cosine scoring but also more general ranking factors such as query term proximity. We describe how all of the various pieces fit together in Section 7.2.4 . We conclude this chapter with Section 7.3 , where we discuss how the vector space model for free text queries interacts with common query operators.   Subsections Efficient scoring and ranking Inexact top K document retrieval Index elimination Champion lists Static quality scores and ordering Impact ordering Cluster pruning Components of an information retrieval system Tiered indexes Query-term proximity Designing parsing and scoring functions Putting it all together Vector space scoring and query operator interaction Boolean retrieval Wildcard queries Phrase queries References and further reading</text>
    <subsections>
       <section>
         <id>iir_7_1</id>
         <title>Efficient scoring and ranking</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html</url>
         <file>efficient-scoring-and-ranking-1.html</file>
         <text> Efficient scoring and ranking We begin by recapping the algorithm of Figure 6.14 . For a query such as jealous gossip, two observations are immediate: The unit vector has only two non-zero components. In the absence of any weighting for query terms, these non-zero components are equal - in this case, both equal 0.707. For the purpose of ranking the documents matching this query, we are really interested in the relative (rather than absolute) scores of the documents in the collection. To this end, it suffices to compute the cosine similarity from each document unit vector to (in which all non-zero components of the query vector are set to 1), rather than to the unit vector . For any two documents (34)      6.14  7.1  6.3.3   Figure 7.1: A faster algorithm for vector space scores. Given these scores, the final step before presenting results to a user is to pick out the highest-scoring documents. While one could sort the complete set of scores, a better approach is to use a heap to retrieve only the top documents in order. Where is the number of documents with non-zero cosine scores, constructing such a heap can be performed in comparison steps, following which each of the highest scoring documents can be ``read off'' the heap with comparison steps.   Subsections Inexact top K document retrieval Index elimination Champion lists Static quality scores and ordering Impact ordering Cluster pruning</text>
         <subsections>
            <section>
              <id>iir_7_1_1</id>
              <title>Inexact top K document retrieval</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/inexact-top-k-document-retrieval-1.html</url>
              <file>inexact-top-k-document-retrieval-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_1_2</id>
              <title>Index elimination</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/index-elimination-1.html</url>
              <file>index-elimination-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_1_3</id>
              <title>Champion lists</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/champion-lists-1.html</url>
              <file>champion-lists-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_1_4</id>
              <title>Static quality scores and ordering</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/static-quality-scores-and-ordering-1.html</url>
              <file>static-quality-scores-and-ordering-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_1_5</id>
              <title>Impact ordering</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/impact-ordering-1.html</url>
              <file>impact-ordering-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_1_6</id>
              <title>Cluster pruning</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/cluster-pruning-1.html</url>
              <file>cluster-pruning-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_7_2</id>
         <title>Components of an information retrieval system</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/components-of-an-information-retrieval-system-1.html</url>
         <file>components-of-an-information-retrieval-system-1.html</file>
         <text> Components of an information retrieval system 7.3   Subsections Tiered indexes Query-term proximity Designing parsing and scoring functions Putting it all together </text>
         <subsections>
            <section>
              <id>iir_7_2_1</id>
              <title>Tiered indexes</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/tiered-indexes-1.html</url>
              <file>tiered-indexes-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_2_2</id>
              <title>Query-term proximity</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/query-term-proximity-1.html</url>
              <file>query-term-proximity-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_2_3</id>
              <title>Designing parsing and scoring functions</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/designing-parsing-and-scoring-functions-1.html</url>
              <file>designing-parsing-and-scoring-functions-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_7_2_4</id>
              <title>Putting it all together</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/putting-it-all-together-1.html</url>
              <file>putting-it-all-together-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_7_3</id>
         <title>Vector space scoring and query operator interaction</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-scoring-and-query-operator-interaction-1.html</url>
         <file>vector-space-scoring-and-query-operator-interaction-1.html</file>
         <text> Vector space scoring and query operator interaction Vector space scoring supports so-called free text retrieval, in which a query is specified as a set of words without any query operators connecting them. It allows documents matching the query to be scored and thus ranked, unlike the Boolean, wildcard and phrase queries studied earlier. Classically, the interpretation of such free text queries was that at least one of the query terms be present in any retrieved document. However more recently, web search engines such as Google have popularized the notion that a set of terms typed into their query boxes (thus on the face of it, a free text query) carries the semantics of a conjunctive query that only retrieves documents containing all or most query terms.   Subsections Boolean retrieval Wildcard queries Phrase queries</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_7_4</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html</url>
         <file>references-and-further-reading-7.html</file>
         <text> References and further reading Heuristics for fast query processing with early termination are described by Persin et al. (1996), Anh et al. (2001), Garcia et al. (2004), Anh and Moffat (2006b). Cluster pruning is investigated by Singitham et al. (2004) and by Chierichetti et al. (2007); see also Section 16.6 (page ). Champion lists are described in Persin (1994) and (under the name top docs ) in Brown (1995), and further developed in Long and Suel (2003), Brin and Page (1998). While these heuristics are well-suited to free text queries that can be viewed as vectors, they complicate phrase queries; see Anh and Moffat (2006c) for an index structure that supports both weighted and Boolean/phrase searches. Carmel et al. (2001) Clarke et al. (2000) and Song et al. (2005) treat the use of query term proximity in assessing relevance. Pioneering work on learning of ranking functions was done by Fuhr (1989), Fuhr and Pfeifer (1994), Cooper et al. (1994), Bartell et al. (1998), Bartell (1994) and by Cohen et al. (1998). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_8</id>
    <title>Evaluation in information retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-in-information-retrieval-1.html</url>
    <file>evaluation-in-information-retrieval-1.html</file>
    <text> Evaluation in information retrieval We have seen in the preceding chapters many alternatives in designing an IR system. How do we know which of these techniques are effective in which applications? Should we use stop lists? Should we stem? Should we use inverse document frequency weighting? Information retrieval has developed as a highly empirical discipline, requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections. In this chapter we begin with a discussion of measuring the effectiveness of IR systems (Section 8.1 ) and the test collections that are most often used for this purpose (Section 8.2 ). We then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results (Section 8.3 ). This includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate. We then extend these notions and develop further measures for evaluating ranked retrieval results (Section 8.4 ) and discuss developing reliable and informative test collections (Section 8.5 ). We then step back to introduce the notion of user utility, and how it is approximated by the use of document relevance (Section 8.6 ). The key utility measure is user happiness. Speed of response and the size of the index are factors in user happiness. It seems reasonable to assume that relevance of results is the most important factor: blindingly fast, useless answers do not make a user happy. However, user perceptions do not always coincide with system designers' notions of quality. For example, user happiness commonly depends very strongly on user interface design issues, including the layout, clarity, and responsiveness of the user interface, which are independent of the quality of the results returned. We touch on other measures of the quality of a system, in particular the generation of high-quality result summary snippets, which strongly influence user utility, but are not measured in the basic relevance ranking paradigm (Section 8.7 ).   Subsections Information retrieval system evaluation Standard test collections Evaluation of unranked retrieval sets Evaluation of ranked retrieval results Assessing relevance Critiques and justifications of the concept of relevance A broader perspective: System quality and user utility System issues User utility Refining a deployed system Results snippets References and further reading</text>
    <subsections>
       <section>
         <id>iir_8_1</id>
         <title>Information retrieval system evaluation</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/information-retrieval-system-evaluation-1.html</url>
         <file>information-retrieval-system-evaluation-1.html</file>
         <text> Information retrieval system evaluation To measure ad hoc information retrieval effectiveness in the standard way, we need a test collection consisting of three things: A document collection A test suite of information needs, expressible as queries A set of relevance judgments, standardly a binary assessment of either relevant or nonrelevant for each query-document pair.  relevant nonrelevant   gold standard  ground truth Relevance is assessed relative to an , not a query. For example, an information need might be: Information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine. wine and red and white and heart and attack and effective 8.5.1 Many systems contain various weights (often known as parameters) that can be adjusted to tune system performance. It is wrong to report results on a test collection which were obtained by tuning these parameters to maximize performance on that collection. That is because such tuning overstates the expected performance of the system, because the weights will be set to maximize performance on one particular set of queries rather than for a random sample of queries. In such cases, the correct procedure is to have one or more development test collections , and to tune the parameters on the development test collection. The tester then runs the system with those weights on the test collection and reports the results on that collection as an unbiased estimate of performance. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_8_2</id>
         <title>Standard test collections</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/standard-test-collections-1.html</url>
         <file>standard-test-collections-1.html</file>
         <text> Standard test collections Here is a list of the most standard test collections and evaluation series. We focus particularly on test collections for ad hoc information retrieval system evaluation, but also mention a couple of similar test collections for text classification. The Cranfield collection. This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs. Text Retrieval Conference (TREC) . The U.S. National Institute of Standards and Technology (NIST) has run a large IR test bed evaluation series since 1992. Within this framework, there have been many tracks over a range of different test collections, but the best known test collections are the ones used for the TREC Ad Hoc track during the first 8 TREC evaluations between 1992 and 1999. In total, these test collections comprise 6 CDs containing 1.89 million documents (mainly, but not exclusively, newswire articles) and relevance judgments for 450 information needs, which are called topics and specified in detailed text passages. Individual test collections are defined over different subsets of this data. The early TRECs each consisted of 50 information needs, evaluated over different but overlapping sets of documents. TRECs 6-8 provide 150 information needs over about 528,000 newswire and Foreign Broadcast Information Service articles. This is probably the best subcollection to use in future work, because it is the largest and the topics are more consistent. Because the test document collections are so large, there are no exhaustive relevance judgments. Rather, NIST assessors' relevance judgments are available only for the documents that were among the top returned for some system which was entered in the TREC evaluation for which the information need was developed. In more recent years, NIST has done evaluations on larger document collections, including the 25 million page GOV2 web page collection. From the beginning, the NIST test document collections were orders of magnitude larger than anything available to researchers previously and GOV2 is now the largest Web collection easily available for research purposes. Nevertheless, the size of GOV2 is still more than 2 orders of magnitude smaller than the current size of the document collections indexed by the large web search companies. NII Test Collections for IR Systems ( NTCIR ). The NTCIR project has built various test collections of similar sizes to the TREC collections, focusing on East Asian language and cross-language information retrieval , where queries are made in one language over a document collection containing documents in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-en.html Cross Language Evaluation Forum ( CLEF ). This evaluation series has concentrated on European languages and cross-language information retrieval. See: http://www.clef-campaign.org/ and Reuters-RCV1. For text classification, the most used test collection has been the Reuters-21578 collection of 21578 newswire articles; see Chapter 13 , page 13.6 . More recently, Reuters released the much larger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents; see Chapter 4 , page 4.2 . Its scale and rich annotation makes it a better basis for future research. 20 Newsgroups . This is another widely used text classification collection, collected by Ken Lang. It consists of 1000 articles from each of 20 Usenet newsgroups (the newsgroup name being regarded as the category). After the removal of duplicate articles, as it is usually used, it contains 18941 articles. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_8_3</id>
         <title>Evaluation of unranked retrieval sets</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html</url>
         <file>evaluation-of-unranked-retrieval-sets-1.html</file>
         <text> Evaluation of unranked retrieval sets Given these ingredients, how is system effectiveness measured? The two most frequent and basic measures for information retrieval effectiveness are precision and recall. These are first defined for the simple case where an IR system returns a set of documents for a query. We will see later how to extend these notions to ranked retrieval situations. Precision () is the fraction of retrieved documents that are relevant (36) Recall () is the fraction of relevant documents that are retrieved (37)     (38) (39)   An obvious alternative that may occur to the reader is to judge an information retrieval system by its accuracy , that is, the fraction of its classifications that are correct. In terms of the contingency table above, . This seems plausible, since there are two actual classes, relevant and nonrelevant, and an information retrieval system can be thought of as a two-class classifier which attempts to label them as such (it retrieves the subset of documents which it believes to be relevant). This is precisely the effectiveness measure often used for evaluating machine learning classification problems. There is a good reason why accuracy is not an appropriate measure for information retrieval problems. In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category. A system tuned to maximize accuracy can appear to perform well by simply deeming all documents nonrelevant to all queries. Even if the system is quite good, trying to label some documents as relevant will almost always lead to a high rate of false positives. However, labeling all documents as nonrelevant is completely unsatisfying to an information retrieval system user. Users are always going to want to see some documents, and can be assumed to have a certain tolerance for seeing some false positives providing that they get some useful information. The measures of precision and recall concentrate the evaluation on the return of true positives, asking what percentage of the relevant documents have been found and how many false positives have also been returned. The advantage of having the two numbers for precision and recall is that one is more important than the other in many circumstances. Typical web surfers would like every result on the first page to be relevant (high precision) but have not the slightest interest in knowing let alone looking at every document that is relevant. In contrast, various professional searchers such as paralegals and intelligence analysts are very concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it. Individuals searching their hard disks are also often interested in high recall searches. Nevertheless, the two quantities clearly trade off against one another: you can always get a recall of 1 (but very low precision) by retrieving all documents for all queries! Recall is a non-decreasing function of the number of documents retrieved. On the other hand, in a good system, precision usually decreases as the number of documents retrieved is increased. In general we want to get some amount of recall while tolerating only a certain percentage of false positives. A single measure that trades off precision versus recall is the F measure , which is the weighted harmonic mean of precision and recall: (40)     balanced F measure        (41)       Graph comparing the harmonic mean to other means.The graph shows a slice through the calculation of various means of precision and recall for the fixed recall value of 70%. The harmonic mean is always less than either the arithmetic or geometric mean, and often quite close to the minimum of the two numbers. When the precision is also 70%, all the measures coincide. Why do we use a harmonic mean rather than the simpler average (arithmetic mean)? Recall that we can always get 100% recall by just returning all documents, and therefore we can always get a 50% arithmetic mean by the same process. This strongly suggests that the arithmetic mean is an unsuitable measure to use. In contrast, if we assume that 1 document in 10,000 is relevant to the query, the harmonic mean score of this strategy is 0.02%. The harmonic mean is always less than or equal to the arithmetic mean and the geometric mean. When the values of two numbers differ greatly, the harmonic mean is closer to their minimum than to their arithmetic mean; see Figure 8.1 . Exercises. An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall? The balanced F measure (a.k.a. F) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than ``averaging'' (using the arithmetic mean)? Derive the equivalence between the two formulas for F measure shown in Equation 40, given that . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_8_4</id>
         <title>Evaluation of ranked retrieval results</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html</url>
         <file>evaluation-of-ranked-retrieval-results-1.html</file>
         <text> Evaluation of ranked retrieval results  Figure 8.2: Precision/recall graph. Precision, recall, and the F measure are set-based measures. They are computed using unordered sets of documents. We need to extend these measures (or to define new measures) if we are to evaluate the ranked retrieval results that are now standard with search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top retrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve , such as the one shown in Figure 8.2 . Precision-recall curves have a distinctive saw-tooth shape: if the document retrieved is nonrelevant then recall is the same as for the top documents, but precision has dropped. If it is relevant, then both precision and recall increase, and the curve jags up and to the right. It is often useful to remove these jiggles and the standard way to do this is with an interpolated precision: the interpolated precision at a certain recall level is defined as the highest precision found for any recall level : (42)  The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher). Interpolated precision is shown by a thinner line in Figure 8.2 . With this definition, the interpolated precision at a recall of 0 is well-defined (Exercise 8.4 ).   Recall Interp.   Precision 0.0 1.00 0.1 0.67 0.2 0.63 0.3 0.55 0.4 0.45 0.5 0.41 0.6 0.36 0.7 0.29 0.8 0.13 0.9 0.10 1.0 0.08 Calculation of 11-point Interpolated Average Precision.This is for the precision-recall curve shown in Figure 8.2 .  Examining the entire precision-recall curve is very informative, but there is often a desire to boil this information down to a few numbers, or perhaps even a single number. The traditional way of doing this (used for instance in the first 8 TREC Ad Hoc evaluations) is the 11-point interpolated average precision . For each information need, the interpolated precision is measured at the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. For the precision-recall curve in Figure 8.2 , these 11 values are shown in Table 8.1 . For each recall level, we then calculate the arithmetic mean of the interpolated precision at that recall level for each information need in the test collection. A composite precision-recall curve showing 11 points can then be graphed. Figure 8.3 shows an example graph of such results from a representative good system at TREC 8.  Averaged 11-point precision/recall graph across 50 queries for a representative TREC system.The Mean Average Precision for this system is 0.2553. In recent years, other measures have become more common. Most standard among the TREC community is Mean Average Precision (MAP), which provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. For a single information need, Average Precision is the average of the precision value obtained for the set of top documents existing after each relevant document is retrieved, and this value is then averaged over information needs. That is, if the set of relevant documents for an information need is and is the set of ranked retrieval results from the top result until you get to document , then (43)   Using MAP, fixed recall levels are not chosen, and there is no interpolation. The MAP value for a test collection is the arithmetic mean of average precision values for individual information needs. (This has the effect of weighting each information need equally in the final reported number, even if many documents are relevant to some queries whereas very few are relevant to other queries.) Calculated MAP scores normally vary widely across information needs when measured within a single system, for instance, between 0.1 and 0.7. Indeed, there is normally more agreement in MAP for an individual information need across systems than for MAP scores for different information needs for the same system. This means that a set of test information needs must be large and diverse enough to be representative of system effectiveness across different queries. The above measures factor in precision at all recall levels. For many prominent applications, particularly web search, this may not be germane to users. What matters is rather how many good results there are on the first page or the first three pages. This leads to measuring precision at fixed low levels of retrieved results, such as 10 or 30 documents. This is referred to as ``Precision at '', for example ``Precision at 10''. It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at . An alternative, which alleviates this problem, is R-precision . It requires having a set of known relevant documents , from which we calculate the precision of the top documents returned. (The set may be incomplete, such as when is formed by creating relevance judgments for the pooled top results of particular systems in a set of experiments.) R-precision adjusts for the size of the set of relevant documents: A perfect system could score 1 on this metric for each query, whereas, even a perfect system could only achieve a precision at 20 of 0.4 if there were only 8 documents in the collection relevant to an information need. Averaging this measure across queries thus makes more sense. This measure is harder to explain to naive users than Precision at but easier to explain than MAP. If there are relevant documents for a query, we examine the top results of a system, and find that are relevant, then by definition, not only is the precision (and hence R-precision) , but the recall of this result set is also . Thus, R-precision turns out to be identical to the break-even point , another measure which is sometimes used, defined in terms of this equality relationship holding. Like Precision at , R-precision describes only one point on the precision-recall curve, rather than attempting to summarize effectiveness across the curve, and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal F-measure) or a retrieval level of interest to a particular application (Precision at ). Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on the curve.  Figure 8.4: The ROC curve corresponding to the precision-recall curve in Figure 8.2 . . Another concept sometimes used in evaluation is an ROC curve . (``ROC'' stands for ``Receiver Operating Characteristics'', but knowing that doesn't help most people.) An ROC curve plots the true positive rate or sensitivity against the false positive rate or ( ). Here, sensitivity is just another term for recall. The false positive rate is given by . Figure 8.4 shows the ROC curve corresponding to the precision-recall curve in Figure 8.2 . An ROC curve always goes from the bottom left to the top right of the graph. For a good system, the graph climbs steeply on the left side. For unranked result sets, specificity , given by , was not seen as a very useful notion. Because the set of true negatives is always so large, its value would be almost 1 for all information needs (and, correspondingly, the value of the false positive rate would be almost 0). That is, the ``interesting'' part of Figure 8.2 is , a part which is compressed to a small corner of Figure 8.4 . But an ROC curve could make sense when looking over the full retrieval spectrum, and it provides another way of looking at the data. In many fields, a common aggregate measure is to report the area under the ROC curve, which is the ROC analog of MAP. Precision-recall curves are sometimes loosely referred to as ROC curves. This is understandable, but not accurate. A final approach that has seen increasing adoption, especially when employed with machine learning approaches to ranking svm-ranking is measures of cumulative gain , and in particular normalized discounted cumulative gain ( NDCG ). NDCG is designed for situations of non-binary notions of relevance (cf. Section 8.5.1 ). Like precision at , it is evaluated over some number of top search results. For a set of queries , let be the relevance score assessors gave to document for query . Then, (44)       Exercises. What are the possible values for interpolated precision at a recall level of 0? Must there always be a break-even point between precision and recall? Either show there must be or give a counter-example. What is the relationship between the value of and the break-even point? The Dice coefficient of two sets is a measure of their intersection scaled by their size (giving a value in the range 0 to 1): (45) Show that the balanced F-measure () is equal to the Dice coefficient of the retrieved and relevant document sets. Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result): System 1   R N R N N   N N N R R System 2   N R N N R   R R N N N What is the MAP of each system? Which has a higher MAP? Does this result intuitively make sense? What does it say about what is important in getting a good MAP score? What is the R-precision of each system? (Does it rank the systems the same as MAP?) The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection. R R N N N   N N N R N   R N N N R   N N N N R What is the precision of the system on the top 20? What is the F on the top 20? What is the uninterpolated precision of the system at 25% recall? What is the interpolated precision at 33% recall? Assume that these 20 documents are the complete result set of the system. What is the MAP for the query? Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned. f. What is the largest possible MAP that this system could have? g. What is the smallest possible MAP that this system could have? h. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)-(g). For this example, how large (in absolute terms) can the error for the MAP be by calculating (e) instead of (f) and (g) for this query? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_8_5</id>
         <title>Assessing relevance</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/assessing-relevance-1.html</url>
         <file>assessing-relevance-1.html</file>
         <text> Assessing relevance To properly evaluate a system, your test information needs must be germane to the documents in the test document collection, and appropriate for predicted usage of the system. These information needs are best designed by domain experts. Using random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distribution of information needs. Given information needs and documents, you need to collect relevance assessments. This is a time-consuming and expensive process involving human beings. For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained. For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query. The most standard approach is pooling , where relevance is assessed over a subset of the collection that is formed from the top documents returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process.   Table 8.2: Calculating the kappa statistic.     Judge 2 Relevance     Yes   No Total Judge 1 Yes 300   20 320 Relevance No 10   70 80   Total 310   90 400 Observed proportion of the times the judges agreed Pooled marginals Probability that the two judges agreed by chance Kappa statistic   A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query. Rather, humans and their relevance judgments are quite idiosyncratic and variable. But this is not a problem to be solved: in the final analysis, the success of an IR system depends on how good it is at satisfying the needs of these idiosyncratic humans, one information need at a time. Nevertheless, it is interesting to consider and measure how much agreement between judges there is on relevance judgments. In the social sciences, a common measure for agreement between judges is the kappa statistic . It is designed for categorical judgments and corrects a simple agreement rate for the rate of chance agreement. (46)      marginal  8.2 Interjudge agreement of relevance has been measured within the TREC evaluations and for medical IR collections. Using the above rules of thumb, the level of agreement normally falls in the range of ``fair'' (0.67-0.8). The fact that human agreement on a binary relevance judgment is quite modest is one reason for not requiring more fine-grained relevance labeling from the test set creator. To answer the question of whether IR evaluation results are valid despite the variation of individual assessors' judgments, people have experimented with evaluations taking one or the other of two judges' opinions as the gold standard. The choice can make a considerable absolute difference to reported scores, but has in general been found to have little impact on the relative effectiveness ranking of either different systems or variants of a single system which are being compared for effectiveness.   Subsections Critiques and justifications of the concept of relevance</text>
         <subsections>
            <section>
              <id>iir_8_5_1</id>
              <title>Critiques and justifications of the concept of relevance</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/critiques-and-justifications-of-the-concept-of-relevance-1.html</url>
              <file>critiques-and-justifications-of-the-concept-of-relevance-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_8_6</id>
         <title>A broader perspective: System quality and user utility</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/a-broader-perspective-system-quality-and-user-utility-1.html</url>
         <file>a-broader-perspective-system-quality-and-user-utility-1.html</file>
         <text> A broader perspective: System quality and user utility Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies. These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface. In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility.   Subsections System issues User utility Refining a deployed system </text>
         <subsections>
            <section>
              <id>iir_8_6_1</id>
              <title>System issues</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/system-issues-1.html</url>
              <file>system-issues-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_8_6_2</id>
              <title>User utility</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/user-utility-1.html</url>
              <file>user-utility-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_8_6_3</id>
              <title>Refining a deployed system</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/refining-a-deployed-system-1.html</url>
              <file>refining-a-deployed-system-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_8_7</id>
         <title>Results snippets</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/results-snippets-1.html</url>
         <file>results-snippets-1.html</file>
         <text> Results snippets Having chosen or ranked the documents matching a query, we wish to present a results list that will be informative to the user. In many cases the user will not want to examine all the returned documents and so we want to make the results list informative enough that the user can do a final ranking of the documents for themselves based on relevance to their information need.The standard way of doing this is to provide a snippet , a short summary of the document, which is designed so as to allow the user to decide its relevance. Typically, the snippet consists of the document title and a short summary, which is automatically extracted. The question is how to design the summary so as to maximize its usefulness to the user. The two basic kinds of summaries are static , which are always the same regardless of the query, and dynamic (or query-dependent), which are customized according to the user's information need as deduced from a query. Dynamic summaries attempt to explain why a particular document was retrieved for the query at hand. A static summary is generally comprised of either or both a subset of the document and metadata associated with the document. The simplest form of summary takes the first two sentences or 50 words of a document, or extracts particular zones of a document, such as the title and author. Instead of zones of a document, the summary can instead use metadata associated with the document. This may be an alternative way to provide an author or date, or may include elements which are designed to give a summary, such as the description metadata which can appear in the meta element of a web HTML page. This summary is typically extracted and cached at indexing time, in such a way that it can be retrieved and presented quickly when displaying search results, whereas having to access the actual document content might be a relatively expensive operation. There has been extensive work within natural language processing (NLP) on better ways to do text summarization . Most such work still aims only to choose sentences from the original document to present and concentrates on how to select good sentences. The models typically combine positional factors, favoring the first and last paragraphs of documents and the first and last sentences of paragraphs, with content factors, emphasizing sentences with key terms, which have low document frequency in the collection as a whole, but high frequency and good distribution across the particular document being returned. In sophisticated NLP approaches, the system synthesizes sentences for a summary, either by doing full text generation or by editing and perhaps combining sentences used in the document. For example, it might delete a relative clause or replace a pronoun with the noun phrase that it refers to. This last class of methods remains in the realm of research and is seldom used for search results: it is easier, safer, and often even better to just use sentences from the original document. Dynamic summaries display one or more ``windows'' on the document, aiming to present the pieces that have the most utility to the user in evaluating the document with respect to their information need. Usually these windows contain one or several of the query terms, and so are often referred to as keyword-in-context ( ) snippets, though sometimes they may still be pieces of the text such as the title that are selected for their query-independent information value just as in the case of static summarization. Dynamic summaries are generated in conjunction with scoring. If the query is found as a phrase, occurrences of the phrase in the document will be shown as the summary. If not, windows within the document that contain multiple query terms will be selected. Commonly these windows may just stretch some number of words to the left and right of the query terms. This is a place where NLP techniques can usefully be employed: users prefer snippets that read well because they contain complete phrases.   Dynamic summaries are generally regarded as greatly improving the usability of IR systems, but they present a complication for IR system design. A dynamic summary cannot be precomputed, but, on the other hand, if a system has only a positional index, then it cannot easily reconstruct the context surrounding search engine hits in order to generate such a dynamic summary. This is one reason for using static summaries. The standard solution to this in a world of large and cheap disk drives is to locally cache all the documents at index time (notwithstanding that this approach raises various legal, information security and control issues that are far from resolved) as shown in Figure 7.5 (page ). Then, a system can simply scan a document which is about to appear in a displayed results list to find snippets containing the query words. Beyond simply access to the text, producing a good KWIC snippet requires some care. Given a variety of keyword occurrences in a document, the goal is to choose fragments which are: (i) maximally informative about the discussion of those terms in the document, (ii) self-contained enough to be easy to read, and (iii) short enough to fit within the normally strict constraints on the space available for summaries. Generating snippets must be fast since the system is typically generating many snippets for each query that it handles. Rather than caching an entire document, it is common to cache only a generous but fixed size prefix of the document, such as perhaps 10,000 characters. For most common, short documents, the entire document is thus cached, but huge amounts of local storage will not be wasted on potentially vast documents. Summaries of documents whose length exceeds the prefix size will be based on material in the prefix only, which is in general a useful zone in which to look for a document summary anyway. If a document has been updated since it was last processed by a crawler and indexer, these changes will be neither in the cache nor in the index. In these circumstances, neither the index nor the summary will accurately reflect the current contents of the document, but it is the differences between the summary and the actual document content that will be more glaringly obvious to the end user. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_8_8</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-8.html</url>
         <file>references-and-further-reading-8.html</file>
         <text> References and further reading Definition and implementation of the notion of relevance to a query got off to a rocky start in 1953. Swanson (1988) reports that in an evaluation in that year between two teams, they agreed that 1390 documents were variously relevant to a set of 98 questions, but disagreed on a further 1577 documents, and the disagreements were never resolved. Rigorous formal testing of IR systems was first completed in the Cranfield experiments, beginning in the late 1950s. A retrospective discussion of the Cranfield test collection and experimentation with it can be found in (Cleverdon, 1991). The other seminal series of early IR experiments were those on the SMART system by Gerard Salton and colleagues (Salton, 1971b;1991). The TREC evaluations are described in detail by Voorhees and Harman (2005). Online information is available at http://trec.nist.gov/. Initially, few researchers computed the statistical significance of their experimental results, but the IR community increasingly demands this (Hull, 1993). User studies of IR system effectiveness began more recently (Saracevic and Kantor, 1988;1996). The notions of recall and precision were first used by Kent et al. (1955), although the term precision did not appear until later. The (or, rather its complement ) was introduced by van Rijsbergen (1979). He provides an extensive theoretical discussion, which shows how adopting a principle of decreasing marginal relevance (at some point a user will be unwilling to sacrifice a unit of precision for an added unit of recall) leads to the harmonic mean being the appropriate method for combining precision and recall (and hence to its adoption rather than the minimum or geometric mean). Buckley and Voorhees (2000) compare several evaluation measures, including precision at , MAP, and R-precision, and evaluate the error rate of each measure. was adopted as the official evaluation metric in the TREC HARD track (Allan, 2005). Aslam and Yilmaz (2005) examine its surprisingly close correlation to MAP, which had been noted in earlier studies (Buckley and Voorhees, 2000, Tague-Sutcliffe and Blustein, 1995). A standard program for evaluating IR systems which computes many measures of ranked retrieval effectiveness is Chris Buckley's trec_eval program used in the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/. Kekäläinen and Järvelin (2002) argue for the superiority of graded relevance judgments when dealing with very large document collections, and Järvelin and Kekäläinen (2002) introduce cumulated gain-based methods for IR system evaluation in this context. Sakai (2007) does a study of the stability and sensitivity of evaluation measures based on graded relevance judgments from NTCIR tasks, and concludes that NDCG is best for evaluating document ranking. Schamber et al. (1990) examine the concept of relevance, stressing its multidimensional and context-specific nature, but also arguing that it can be measured effectively. (Voorhees, 2000) is the standard article for examining variation in relevance judgments and their effects on retrieval system scores and ranking for the TREC Ad Hoc task. Voorhees concludes that although the numbers change, the rankings are quite stable. Hersh et al. (1994) present similar analysis for a medical IR collection. In contrast, Kekäläinen (2005) analyze some of the later TRECs, exploring a 4-way relevance judgment and the notion of cumulative gain, arguing that the relevance measure used does substantially affect system rankings. See also Harter (1998). Zobel (1998) studies whether the pooling method used by TREC to collect a subset of documents that will be evaluated for relevance is reliable and fair, and concludes that it is. The and its use for language-related purposes is discussed by Carletta (1996). Many standard sources (e.g., Siegel and Castellan, 1988) present pooled calculation of the expected agreement, but Di Eugenio (2004) argue for preferring the unpooled agreement (though perhaps presenting multiple measures). For further discussion of alternative measures of agreement, which may in fact be better, see Lombard et al. (2002) and Krippendorff (2003). Text summarization has been actively explored for many years. Modern work on sentence selection was initiated by Kupiec et al. (1995). More recent work includes (Barzilay and Elhadad, 1997) and (Jing, 2000), together with a broad selection of work appearing at the yearly DUC conferences and at other NLP venues. Tombros and Sanderson (1998) demonstrate the advantages of dynamic summaries in the IR context. Turpin et al. (2007) address how to generate snippets efficiently. Clickthrough log analysis is studied in (Joachims, 2002b, Joachims et al., 2005). In a series of papers, Hersh, Turpin and colleagues show how improvements in formal retrieval effectiveness, as evaluated in batch experiments, do not always translate into an improved system for users (Hersh et al., 2000b, Turpin and Hersh, 2002, Hersh et al., 2000a;2001, Turpin and Hersh, 2001). User interfaces for IR and human factors such as models of human information seeking and usability testing are outside the scope of what we cover in this book. More information on these topics can be found in other textbooks, including (Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) and (Korfhage, 1997), and collections focused on cognitive aspects (Spink and Cole, 2005). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_9</id>
    <title>Relevance feedback and query expansion</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-query-expansion-1.html</url>
    <file>relevance-feedback-and-query-expansion-1.html</file>
    <text> Relevance feedback and query expansion In most collections, the same concept may be referred to using different words. This issue, known as synonymy , has an impact on the recall of most information retrieval systems. For example, you would want a search for aircraft to match plane (but only for references to an airplane, not a woodworking plane), and for a search on thermodynamics to match references to heat in appropriate discussions. Users often attempt to address this problem themselves by manually refining a query, as was discussed in Section 1.4 ; in this chapter we discuss ways in which a system can help with query refinement, either fully automatically or with the user in the loop. The methods for tackling this problem split into two major classes: global methods and local methods. Global methods are techniques for expanding or reformulating query terms independent of the query and results returned from it, so that changes in the query wording will cause the new query to match other semantically similar terms. Global methods include: Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2 ) Query expansion via automatic thesaurus generation (Section 9.2.3 ) Techniques like spelling correction (discussed in Chapter 3 ) Relevance feedback (Section 9.1 ) Pseudo relevance feedback, also known as Blind relevance feedback (Section 9.1.6 ) (Global) indirect relevance feedback (Section 9.1.7 )   Subsections Relevance feedback and pseudo relevance feedback The Rocchio algorithm for relevance feedback The underlying theory. The Rocchio (1971) algorithm. Probabilistic relevance feedback When does relevance feedback work? Relevance feedback on the web Evaluation of relevance feedback strategies Pseudo relevance feedback Indirect relevance feedback Summary Global methods for query reformulation Vocabulary tools for query reformulation Query expansion Automatic thesaurus generation References and further reading</text>
    <subsections>
       <section>
         <id>iir_9_1</id>
         <title>Relevance feedback and pseudo relevance feedback</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-pseudo-relevance-feedback-1.html</url>
         <file>relevance-feedback-and-pseudo-relevance-feedback-1.html</file>
         <text> Relevance feedback and pseudo relevance feedback The idea of relevance feedback ( ) is to involve the user in the retrieval process so as to improve the final result set. In particular, the user gives feedback on the relevance of documents in an initial set of results. The basic procedure is: The user issues a (short, simple) query. The system returns an initial set of retrieval results. The user marks some returned documents as relevant or nonrelevant. The system computes a better representation of the information need based on the user feedback. The system displays a revised set of retrieval results.  (a) (b) Relevance feedback searching over images.(a) The user views the initial query results for a query of bike, selects the first, third and fourth result in the top row and the fourth result in the bottom row as relevant, and submits this feedback. (b) The users sees the revised result set. Precision is greatly improved. From http://nayana.ece.ucsb.edu/imsearch/imsearch.html(Newsam et al., 2001). Image search provides a good example of relevance feedback. Not only is it easy to see the results at work, but this is a domain where a user can easily have difficulty formulating what they want in words, but can easily indicate relevant or nonrelevant images. After the user enters an initial query for bike on the demonstration system at: http://nayana.ece.ucsb.edu/imsearch/imsearch.html 9.1 9.1 Figure 9.2 shows a textual IR example where the user wishes to find out about new applications of space satellites.     Subsections The Rocchio algorithm for relevance feedback The underlying theory. The Rocchio (1971) algorithm. Probabilistic relevance feedback When does relevance feedback work? Relevance feedback on the web Evaluation of relevance feedback strategies Pseudo relevance feedback Indirect relevance feedback Summary</text>
         <subsections>
            <section>
              <id>iir_9_1_1</id>
              <title>The Rocchio algorithm for relevance feedback</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-rocchio-algorithm-for-relevance-feedback-1.html</url>
              <file>the-rocchio-algorithm-for-relevance-feedback-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_2</id>
              <title>Probabilistic relevance feedback</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-relevance-feedback-1.html</url>
              <file>probabilistic-relevance-feedback-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_3</id>
              <title>When does relevance feedback work?</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/when-does-relevance-feedback-work-1.html</url>
              <file>when-does-relevance-feedback-work-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_4</id>
              <title>Relevance feedback on the web</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-on-the-web-1.html</url>
              <file>relevance-feedback-on-the-web-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_5</id>
              <title>Evaluation of relevance feedback strategies</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-relevance-feedback-strategies-1.html</url>
              <file>evaluation-of-relevance-feedback-strategies-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_6</id>
              <title>Pseudo relevance feedback</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/pseudo-relevance-feedback-1.html</url>
              <file>pseudo-relevance-feedback-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_7</id>
              <title>Indirect relevance feedback</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/indirect-relevance-feedback-1.html</url>
              <file>indirect-relevance-feedback-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_1_8</id>
              <title>Summary</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/summary-1.html</url>
              <file>summary-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_9_2</id>
         <title>Global methods for query reformulation</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/global-methods-for-query-reformulation-1.html</url>
         <file>global-methods-for-query-reformulation-1.html</file>
         <text> Global methods for query reformulation In this section we more briefly discuss three global methods for expanding a query: by simply aiding the user in doing so, by using a manual thesaurus, and through building a thesaurus automatically.   Subsections Vocabulary tools for query reformulation Query expansion Automatic thesaurus generation </text>
         <subsections>
            <section>
              <id>iir_9_2_1</id>
              <title>Vocabulary tools for query reformulation</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/vocabulary-tools-for-query-reformulation-1.html</url>
              <file>vocabulary-tools-for-query-reformulation-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_2_2</id>
              <title>Query expansion</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/query-expansion-1.html</url>
              <file>query-expansion-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_9_2_3</id>
              <title>Automatic thesaurus generation</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/automatic-thesaurus-generation-1.html</url>
              <file>automatic-thesaurus-generation-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_9_3</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-9.html</url>
         <file>references-and-further-reading-9.html</file>
         <text> References and further reading Work in information retrieval quickly confronted the problem of variant expression which meant that the words in a query might not appear in a document, despite it being relevant to the query. An early experiment about 1960 cited by Swanson (1988) found that only 11 out of 23 documents properly indexed under the subject toxicity had any use of a word containing the stem toxi. There is also the issue of translation, of users knowing what terms a document will use. Blair and Maron (1985) conclude that ``it is impossibly difficult for users to predict the exact words, word combinations, and phrases that are used by all (or most) relevant documents and only (or primarily) by those documents''. The main initial papers on relevance feedback using vector space models all appear in Salton (1971b), including the presentation of the Rocchio algorithm (Rocchio, 1971) and the Ide dec-hi variant along with evaluation of several variants (Ide, 1971). Another variant is to regard all documents in the collection apart from those judged relevant as nonrelevant, rather than only ones that are explicitly judged nonrelevant. However, Schütze et al. (1995) and Singhal et al. (1997) show that better results are obtained for routing by using only documents close to the query of interest rather than all documents. Other later work includes Salton and Buckley (1990), Riezler et al. (2007) (a statistical NLP approach to RF) and the recent survey paper Ruthven and Lalmas (2003). The effectiveness of interactive relevance feedback systems is discussed in (Harman, 1992, Buckley et al., 1994b, Salton, 1989). Koenemann and Belkin (1996) do user studies of the effectiveness of relevance feedback. Traditionally Roget's thesaurus has been the best known English language thesaurus (Roget, 1946). In recent computational work, people almost always use WordNet (Fellbaum, 1998), not only because it is free, but also because of its rich link structure. It is available at: http://wordnet.princeton.edu. Qiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus generation. Xu and Croft (1996) explore using both local and global query expansion. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_10</id>
    <title>XML retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/xml-retrieval-1.html</url>
    <file>xml-retrieval-1.html</file>
    <text> XML retrieval Information retrieval systems are often contrasted with relational databases. Traditionally, IR systems have retrieved information from unstructured text - by which we mean ``raw'' text without markup. Databases are designed for querying relational data: sets of records that have values for predefined attributes such as employee number, title and salary. There are fundamental differences between information retrieval and database systems in terms of retrieval model, data structures and query language as shown in Table 10.1 .     RDB search unstructured retrieval structured retrieval objects records unstructured documents trees with text at leaves model relational model vector space & others ? main data structure table inverted index ? queries SQL free text queries ? RDB (relational database) search, unstructured information retrieval and structured information retrieval. There is no consensus yet as to which methods work best for structured retrieval although many researchers believe that XQuery (page 10.5 ) will become the standard for structured queries.  Some highly structured text search problems are most efficiently handled by a relational database, for example, if the employee table contains an attribute for short textual job descriptions and you want to find all employees who are involved with invoicing. In this case, the SQL query: select lastname from employees where job_desc like 'invoic%'; However, many structured data sources containing text are best modeled as structured documents rather than relational data. We call the search over such structured documents structured retrieval . Queries in structured retrieval can be either structured or unstructured, but we will assume in this chapter that the collection consists only of structured documents. Applications of structured retrieval include digital libraries , patent databases , , text in which entities like persons and locations have been tagged (in a process called ) and output from office suites like OpenOffice that save documents as marked up text. In all of these applications, we want to be able to run queries that combine textual criteria with structural criteria. Examples of such queries are give me a full-length article on fast fourier transforms (digital libraries), give me patents whose claims mention RSA public key encryption and that cite US patent 4,405,829 (patents), or give me articles about sightseeing tours of the Vatican and the Coliseum (entity-tagged text). These three queries are structured queries that cannot be answered well by an unranked retrieval system. As we argued in westlaw unranked retrieval models like the Boolean model suffer from low recall. For instance, an unranked system would return a potentially large number of articles that mention the Vatican, the Coliseum and sightseeing tours without ranking the ones that are most relevant for the query first. Most users are also notoriously bad at precisely stating structural constraints. For instance, users may not know for which structured elements the search system supports search. In our example, the user may be unsure whether to issue the query as sightseeing AND (COUNTRY:Vatican OR LANDMARK:Coliseum) , as sightseeing AND (STATE:Vatican OR BUILDING:Coliseum) or in some other form. Users may also be completely unfamiliar with structured search and advanced search interfaces or unwilling to use them. In this chapter, we look at how ranked retrieval methods can be adapted to structured documents to address these problems. We will only look at one standard for encoding structured documents: Extensible Markup Language or XML , which is currently the most widely used such standard. We will not cover the specifics that distinguish XML from other types of markup such as HTML and SGML. But most of what we say in this chapter is applicable to markup languages in general. In the context of information retrieval, we are only interested in XML as a language for encoding text and documents. A perhaps more widespread use of XML is to encode non-text data. For example, we may want to export data in XML format from an enterprise resource planning system and then read them into an analytics program to produce graphs for a presentation. This type of application of XML is called data-centric because numerical and non-text attribute-value data dominate and text is usually a small fraction of the overall data. Most data-centric XML is stored in databases - in contrast to the inverted index-based methods for text-centric XML that we present in this chapter. We call XML retrieval structured retrieval in this chapter. Some researchers prefer the term semistructured retrieval to distinguish XML retrieval from database querying. We have adopted the terminology that is widespread in the XML retrieval community. For instance, the standard way of referring to XML queries is structured queries , not semistructured queries . The term structured retrieval is rarely used for database querying and it always refers to XML retrieval in this book. There is a second type of information retrieval problem that is intermediate between unstructured retrieval and querying a relational database: parametric and zone search, which we discussed in Section 6.1 (page ). In the data model of parametric and zone search, there are parametric fields (relational attributes like date or file-size) and zones - text attributes that each take a chunk of unstructured text as value, e.g., author and title in Figure 6.1 (page ). The data model is flat, that is, there is no nesting of attributes. The number of attributes is small. In contrast, XML documents have the more complex tree structure that we see in Figure 10.2 in which attributes are nested. The number of attributes and nodes is greater than in parametric and zone search. After presenting the basic concepts of XML in Section 10.1 , this chapter first discusses the challenges we face in XML retrieval (Section 10.2 ). Next we describe a vector space model for XML retrieval (Section 10.3 ). Section 10.4 presents INEX, a shared task evaluation that has been held for a number of years and currently is the most important venue for XML retrieval research. We discuss the differences between data-centric and text-centric approaches to XML in Section 10.5 .   Subsections Basic XML concepts Challenges in XML retrieval A vector space model for XML retrieval Evaluation of XML retrieval Text-centric vs. data-centric XML retrieval References and further reading Exercises</text>
    <subsections>
       <section>
         <id>iir_10_1</id>
         <title>Basic XML concepts</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/basic-xml-concepts-1.html</url>
         <file>basic-xml-concepts-1.html</file>
         <text> Basic XML concepts Figure 10.1: An XML document.  XML element  tag  XML attributes 10.1 <scene ...> </scene>  Figure 10.2: The XML document in Figure 10.1 as a simplified DOM object. Figure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of text, e.g., Shakespeare, Macbeth, and Macbeth's castle. The tree's internal nodes encode either the structure of the document (title, act, and scene) or metadata functions (author). The standard for accessing and processing XML documents is the XML Document Object Model or DOM . The DOM represents elements, attributes and text within elements as nodes in a tree. Figure 10.2 is a simplified DOM representation of the XML document in Figure 10.1 .With a DOM API, we can process an XML document by starting at the root element and then descending down the tree from parents to children. XPath is a standard for enumerating paths in an XML document collection. We will also refer to paths as XML contexts or simply contexts in this chapter. Only a small subset of XPath is needed for our purposes. The XPath expression node selects all nodes of that name. Successive elements of a path are separated by slashes, so act/scene selects all scene elements whose parent is an act element. Double slashes indicate that an arbitrary number of elements can intervene on a path: play//scene selects all scene elements occurring in a play element. In Figure 10.2 this set consists of a single scene element, which is accessible via the path play, act, scene from the top. An initial slash starts the path at the root element. /play/title selects the play's title in Figure 10.1 , /play//title selects a set with two members (the play's title and the scene's title), and /scene/title selects no elements. For notational convenience, we allow the final element of a path to be a vocabulary term and separate it from the element path by the symbol #, even though this does not conform to the XPath standard. For example, title#"Macbeth" selects all titles containing the term Macbeth. We also need the concept of schema in this chapter. A schema puts constraints on the structure of allowable XML documents for a particular application. A schema for Shakespeare's plays may stipulate that scenes can only occur as children of acts and that only acts and scenes have the number attribute. Two standards for schemas for XML documents are XML DTD (document type definition) and XML Schema . Users can only write structured queries for an XML retrieval system if they have some minimal knowledge about the schema of the collection.  Figure 10.3: An XML query in NEXI format and its partial representation as a tree. A common format for XML queries is NEXI (Narrowed Extended XPath I). We give an example in Figure 10.3 . We display the query on four lines for typographical convenience, but it is intended to be read as one unit without line breaks. In particular, //section is embedded under //article. The query in Figure 10.3 specifies a search for sections about the summer holidays that are part of articles from 2001 or 2002. As in XPath double slashes indicate that an arbitrary number of elements can intervene on a path. The dot in a clause in square brackets refers to the element the clause modifies. The clause [.//yr = 2001 or .//yr = 2002] modifies //article. Thus, the dot refers to //article in this case. Similarly, the dot in [about(., summer holidays)] refers to the section that the clause modifies. The two yr conditions are relational attribute constraints. Only articles whose yr attribute is 2001 or 2002 (or that contain an element whose yr attribute is 2001 or 2002) are to be considered. The about clause is a ranking constraint: Sections that occur in the right type of article are to be ranked according to how relevant they are to the topic summer holidays.  Figure 10.4: Tree representation of XML documents and queries. We usually handle relational attribute constraints by prefiltering or postfiltering: We simply exclude all elements from the result set that do not meet the relational attribute constraints. In this chapter, we will not address how to do this efficiently and instead focus on the core information retrieval problem in XML retrieval, namely how to rank documents according to the relevance criteria expressed in the about conditions of the NEXI query. If we discard relational attributes, we can represent documents as trees with only one type of node: element nodes. In other words, we remove all attribute nodes from the XML document, such as the number attribute in Figure 10.1 . Figure 10.4 shows a subtree of the document in Figure 10.1 as an element-node tree (labeled ). We can represent queries as trees in the same way. This is a query-by-example approach to query language design because users pose queries by creating objects that satisfy the same formal description as documents. In Figure 10.4 , is a search for books whose titles score highly for the keywords Julius Caesar. is a search for books whose author elements score highly for Julius Caesar and whose title elements score highly for Gallic war. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_2</id>
         <title>Challenges in XML retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/challenges-in-xml-retrieval-1.html</url>
         <file>challenges-in-xml-retrieval-1.html</file>
         <text> Challenges in XML retrieval In this section, we discuss a number of challenges that make structured retrieval more difficult than unstructured retrieval. Recall from page 10 the basic setting we assume in structured retrieval: the collection consists of structured documents and queries are either structured (as in Figure 10.3 ) or unstructured (e.g., summer holidays). The first challenge in structured retrieval is that users want us to return parts of documents (i.e., XML elements), not entire documents as IR systems usually do in unstructured retrieval. If we query Shakespeare's plays for Macbeth's castle, should we return the scene, the act or the entire play in Figure 10.2 ? In this case, the user is probably looking for the scene. On the other hand, an otherwise unspecified search for Macbeth should return the play of this name, not a subunit. One criterion for selecting the most appropriate part of a document is the structured document retrieval principle : Structured document retrieval principle. A system should always retrieve the most specific part of a document answering the query. title#"Macbeth" 10.2 Macbeth Macbeth's castle  Figure 10.5: Partitioning an XML document into non-overlapping indexing units. Parallel to the issue of which parts of a document to return to the user is the issue of which parts of a document to index. In Section 2.1.2 (page ), we discussed the need for a document unit or indexing unit in indexing and retrieval. In unstructured retrieval, it is usually clear what the right document unit is: files on your desktop, email messages, web pages on the web etc. In structured retrieval, there are a number of different approaches to defining the indexing unit. One approach is to group nodes into non-overlapping pseudodocuments as shown in Figure 10.5 . In the example, books, chapters and sections have been designated to be indexing units, but without overlap. For example, the leftmost dashed indexing unit contains only those parts of the tree dominated by book that are not already part of other indexing units. The disadvantage of this approach is that pseudodocuments may not make sense to the user because they are not coherent units. For instance, the leftmost indexing unit in Figure 10.5 merges three disparate elements, the class, author and title elements. We can also use one of the largest elements as the indexing unit, for example, the book element in a collection of books or the play element for Shakespeare's works. We can then postprocess search results to find for each book or play the subelement that is the best hit. For example, the query Macbeth's castle may return the play Macbeth, which we can then postprocess to identify act I, scene vii as the best-matching subelement. Unfortunately, this two-stage retrieval process fails to return the best subelement for many queries because the relevance of a whole book is often not a good predictor of the relevance of small subelements within it. Instead of retrieving large units and identifying subelements (top down), we can also search all leaves, select the most relevant ones and then extend them to larger units in postprocessing (bottom up). For the query Macbeth's castle in Figure 10.1 , we would retrieve the title Macbeth's castle in the first pass and then decide in a postprocessing step whether to return the title, the scene, the act or the play. This approach has a similar problem as the last one: The relevance of a leaf element is often not a good predictor of the relevance of elements it is contained in. The least restrictive approach is to index all elements. This is also problematic. Many XML elements are not meaningful search results, e.g., typographical elements like <b>definitely</b> or an ISBN number which cannot be interpreted without context. Also, indexing all elements means that search results will be highly redundant. For the query Macbeth's castle and the document in Figure 10.1 , we would return all of the play, act, scene and title elements on the path between the root node and Macbeth's castle. The leaf node would then occur four times in the result set, once directly and three times as part of other elements. We call elements that are contained within each other nested . Returning redundant nested elements in a list of returned hits is not very user-friendly. Because of the redundancy caused by nested elements it is common to restrict the set of elements that are eligible to be returned. Restriction strategies include: discard all small elements discard all element types that users do not look at (this requires a working XML retrieval system that logs this information) discard all element types that assessors generally do not judge to be relevant (if relevance assessments are available) only keep element types that a system designer or librarian has deemed to be useful search results  highlighting If the user knows the schema of the collection and is able to specify the desired type of element, then the problem of redundancy is alleviated as few nested elements have the same type. But as we discussed in the introduction, users often don't know what the name of an element in the collection is (Is the Vatican a country or a city?) or they may not know how to compose structured queries at all. A challenge in XML retrieval related to nesting is that we may need to distinguish different contexts of a term when we compute term statistics for ranking, in particular inverse document frequency ( idf ) statistics as defined in Section 6.2.1 (page ). For example, the term Gates under the node author is unrelated to an occurrence under a content node like section if used to refer to the plural of gate. It makes little sense to compute a single document frequency for Gates in this example. One solution is to compute idf for XML-contextterm pairs, e.g., to compute different idf weights for author#"Gates" and section#"Gates". Unfortunately, this scheme will run into sparse data problems - that is, many XML-context pairs occur too rarely to reliably estimate df (see Section 13.2 , page 13.2 , for a discussion of sparseness). A compromise is only to consider the parent node of the term and not the rest of the path from the root to to distinguish contexts. There are still conflations of contexts that are harmful in this scheme. For instance, we do not distinguish names of authors and names of corporations if both have the parent node name. But most important distinctions, like the example contrast author#"Gates" vs. section#"Gates", will be respected.  Figure 10.6: Schema heterogeneity: intervening nodes and mismatched names. In many cases, several different XML schemas occur in a collection since the XML documents in an IR application often come from more than one source. This phenomenon is called schema heterogeneity or schema diversity and presents yet another challenge. As illustrated in Figure 10.6 comparable elements may have different names: creator in vs. author in . In other cases, the structural organization of the schemas may be different: Author names are direct descendants of the node author in , but there are the intervening nodes firstname and lastname in . If we employ strict matching of trees, then will retrieve neither nor although both documents are relevant. Some form of approximate matching of element names in combination with semi-automatic matching of different document structures can help here. Human editing of correspondences of elements in different schemas will usually do better than automatic methods. Schema heterogeneity is one reason for query-document mismatches like and . Another reason is that users often are not familiar with the element names and the structure of the schemas of collections they search as mentioned. This poses a challenge for interface design in XML retrieval. Ideally, the user interface should expose the tree structure of the collection and allow users to specify the elements they are querying. If we take this approach, then designing the query interface in structured retrieval is more complex than a search box for keyword queries in unstructured retrieval. We can also support the user by interpreting all parent-child relationships in queries as descendant relationships with any number of intervening nodes allowed. We call such queries extended queries . The tree in Figure 10.3 and in Figure 10.6 are examples of extended queries. We show edges that are interpreted as descendant relationships as dashed arrows. In , a dashed arrow connects book and Gates. As a pseudo-XPath notation for , we adopt book//#"Gates": a book that somewhere in its structure contains the word Gates where the path from the book node to Gates can be arbitrarily long. The pseudo-XPath notation for the extended query that in addition specifies that Gates occurs in a section of the book is book//section//#"Gates". It is convenient for users to be able to issue such extended queries without having to specify the exact structural configuration in which a query term should occur - either because they do not care about the exact configuration or because they do not know enough about the schema of the collection to be able to specify it.  Figure 10.7: A structural mismatch between two queries and a document. In Figure 10.7 , the user is looking for a chapter entitled FFT (). Suppose there is no such chapter in the collection, but that there are references to books on FFT (). A reference to a book on FFT is not exactly what the user is looking for, but it is better than returning nothing. Extended queries do not help here. The extended query also returns nothing. This is a case where we may want to interpret the structural constraints specified in the query as hints as opposed to as strict conditions. As we will discuss in Section 10.4 , users prefer a relaxed interpretation of structural constraints: Elements that do not meet structural constraints perfectly should be ranked lower, but they should not be omitted from search results. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_3</id>
         <title>A vector space model for XML retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/a-vector-space-model-for-xml-retrieval-1.html</url>
         <file>a-vector-space-model-for-xml-retrieval-1.html</file>
         <text> A vector space model for XML retrieval  Figure 10.8: A mapping of an XML document (left) to a set of lexicalized subtrees (right). To take account of structure in retrieval in Figure 10.4 , we want a book entitled Julius Caesar to be a match for and no match (or a lower weighted match) for . In unstructured retrieval, there would be a single dimension of the vector space for Caesar. In XML retrieval, we must separate the title word Caesar from the author name Caesar. One way of doing this is to have each dimension of the vector space encode a word together with its position within the XML tree. Figure 10.8 illustrates this representation. We first take each text node (which in our setup is always a leaf) and break it into multiple nodes, one for each word. So the leaf node Bill Gates is split into two leaves Bill and Gates. Next we define the dimensions of the vector space to be lexicalized subtrees of documents - subtrees that contain at least one vocabulary term. A subset of these possible lexicalized subtrees is shown in the figure, but there are others - e.g., the subtree corresponding to the whole document with the leaf node Gates removed. We can now represent queries and documents as vectors in this space of lexicalized subtrees and compute matches between them. This means that we can use the vector space formalism from Chapter 6 for XML retrieval. The main difference is that the dimensions of vector space in unstructured retrieval are vocabulary terms whereas they are lexicalized subtrees in XML retrieval. There is a tradeoff between the dimensionality of the space and accuracy of query results. If we trivially restrict dimensions to vocabulary terms, then we have a standard vector space retrieval system that will retrieve many documents that do not match the structure of the query (e.g., Gates in the title as opposed to the author element). If we create a separate dimension for each lexicalized subtree occurring in the collection, the dimensionality of the space becomes too large. A compromise is to index all paths that end in a single vocabulary term, in other words, all XML-contextterm pairs. We call such an XML-contextterm pair a structural term and denote it by : a pair of XML-context and vocabulary term . The document in Figure 10.8 has nine structural terms. Seven are shown (e.g., "Bill" and Author#"Bill") and two are not shown: /Book/Author#"Bill" and /Book/Author#"Gates". The tree with the leaves Bill and Gates is a lexicalized subtree that is not a structural term. We use the previously introduced pseudo-XPath notation for structural terms. As we discussed in the last section users are bad at remembering details about the schema and at constructing queries that comply with the schema. We will therefore interpret all queries as extended queries - that is, there can be an arbitrary number of intervening nodes in the document for any parent-child node pair in the query. For example, we interpret in Figure 10.7 as . But we still prefer documents that match the query structure closely by inserting fewer additional nodes. We ensure that retrieval results respect this preference by computing a weight for each match. A simple measure of the similarity of a path in a query and a path in a document is the following context resemblance function CR: (52)        10.6            The final score for a document is computed as a variant of the cosine measure (Equation 24, page 6.3.1 ), which we call SIMNOMERGE for reasons that will become clear shortly. SIMNOMERGE is defined as follows: (53)          6    10.2  10.7  6.3.1 6.3.1   Figure 10.9: The algorithm for scoring documents with SIMNOMERGE. IM O ERGE 10.9 normalizer 10.9  53  Figure 10.10: Scoring of a query with one structural term in SIMNOMERGE. We give an example of how SIMNOMERGE computes query-document similarities in Figure 10.10 . is one of the structural terms in the query. We successively retrieve all postings lists for structural terms with the same vocabulary term . Three example postings lists are shown. For the first one, we have since the two contexts are identical. The next context has no context resemblance with : and the corresponding postings list is ignored. The context match of with is 0.63>0 and it will be processed. In this example, the highest ranking document is with a similarity of . To simplify the figure, the query weight of is assumed to be 1.0. The query-document similarity function in Figure 10.9 is called SIMNOMERGE because different XML contexts are kept separate for the purpose of weighting. An alternative similarity function is SIMMERGE which relaxes the matching conditions of query and document further in the following three ways. We collect the statistics used for computing and from all contexts that have a non-zero resemblance to (as opposed to just from as in SIMNOMERGE). For instance, for computing the document frequency of the structural term atl#"recognition", we also count occurrences of recognition in XML contexts fm/atl, article//atl etc. We modify Equation 53 by merging all structural terms in the document that have a non-zero context resemblance to a given query structural term. For example, the contexts /play/act/scene/title and /play/title in the document will be merged when matching against the query term /play/title#"Macbeth". The context resemblance function is further relaxed: Contexts have a non-zero resemblance in many cases where the definition of CR in Equation 52 returns 0. 10.6 These three changes alleviate the problem of sparse term statistics discussed in Section 10.2 and increase the robustness of the matching function against poorly posed structural queries. The evaluation of SIMNOMERGE and SIMMERGE in the next section shows that the relaxed matching conditions of SIMMERGE increase the effectiveness of XML retrieval. Exercises. Consider computing df for a structural term as the number of times that the structural term occurs under a particular parent node. Assume the following: the structural term author#"Herbert" occurs once as the child of the node squib; there are 10 squib nodes in the collection; occurs 1000 times as the child of article; there are 1,000,000 article nodes in the collection. The idf weight of then is when occurring as the child of squib and when occurring as the child of article. (i) Explain why this is not an appropriate weighting for . Why should not receive a weight that is three times higher in articles than in squibs? (ii) Suggest a better way of computing idf. Write down all the structural terms occurring in the XML document in Figure 10.8 . How many structural terms does the document in Figure 10.1 yield? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_4</id>
         <title>Evaluation of XML retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-xml-retrieval-1.html</url>
         <file>evaluation-of-xml-retrieval-1.html</file>
         <text> Evaluation of XML retrieval   Table 10.2: INEX 2002 collection statistics. 12,107 number of documents 494 MB size 1995-2002 time of publication of articles 1,532 average number of XML nodes per document 6.9 average depth of a node 30 number of CAS topics 30 number of CO topics    Figure 10.11: Simplified schema of the documents in the INEX collection. The premier venue for research on XML retrieval is the INEX (INitiative for the Evaluation of XML retrieval) program, a collaborative effort that has produced reference collections, sets of queries, and relevance judgments. A yearly INEX meeting is held to present and discuss research results. The INEX 2002 collection consisted of about 12,000 articles from IEEE journals. We give collection statistics in Table 10.2 and show part of the schema of the collection in Figure 10.11 . The IEEE journal collection was expanded in 2005. Since 2006 INEX uses the much larger English Wikipedia as a test collection. The relevance of documents is judged by human assessors using the methodology introduced in Section 8.1 (page ), appropriately modified for structured documents as we will discuss shortly. Two types of information needs or in INEX are content-only or CO topics and content-and-structure (CAS) topics. CO topics are regular keyword queries as in unstructured information retrieval. CAS topics have structural constraints in addition to keywords. We already encountered an example of a CAS topic in Figure 10.3 . The keywords in this case are summer and holidays and the structural constraints specify that the keywords occur in a section that in turn is part of an article and that this article has an embedded year attribute with value 2001 or 2002. Since CAS queries have both structural and content criteria, relevance assessments are more complicated than in unstructured retrieval. INEX 2002 defined component coverage and topical relevance as orthogonal dimensions of relevance. The component coverage dimension evaluates whether the element retrieved is ``structurally'' correct, i.e., neither too low nor too high in the tree. We distinguish four cases: Exact coverage (E). The information sought is the main topic of the component and the component is a meaningful unit of information. Too small (S). The information sought is the main topic of the component, but the component is not a meaningful (self-contained) unit of information. Too large (L). The information sought is present in the component, but is not the main topic. No coverage (N). The information sought is not a topic of the component. The topical relevance dimension also has four levels: highly relevant (3), fairly relevant (2), marginally relevant (1) and nonrelevant (0). Components are judged on both dimensions and the judgments are then combined into a digit-letter code. 2S is a fairly relevant component that is too small and 3E is a highly relevant component that has exact coverage. In theory, there are 16 combinations of coverage and relevance, but many cannot occur. For example, a nonrelevant component cannot have exact coverage, so the combination 3N is not possible. The relevance-coverage combinations are quantized as follows: (54)  8.5.1 8.5.1 Q The number of relevant components in a retrieved set of components can then be computed as: (55)  8 10.6 One flaw of measuring relevance this way is that overlap is not accounted for. We discussed the concept of marginal relevance in the context of unstructured retrieval in Section 8.5.1 (page ). This problem is worse in XML retrieval because of the problem of multiple nested elements occurring in a search result as we discussed on page 10.2 . Much of the recent focus at INEX has been on developing algorithms and evaluation measures that return non-redundant results lists and evaluate them properly. See the references in Section 10.6 .   Table 10.3: INEX 2002 results of the vector space model in Section 10.3 for content-and-structure (CAS) queries and the quantization function Q. algorithm average precision SIMNOMERGE 0.242 SIMMERGE 0.271   Table 10.3 shows two INEX 2002 runs of the vector space system we described in Section 10.3 . The better run is the SIMMERGE run, which incorporates few structural constraints and mostly relies on keyword matching. SIMMERGE's median average precision (where the median is with respect to average precision numbers over topics) is only 0.147. Effectiveness in XML retrieval is often lower than in unstructured retrieval since XML retrieval is harder. Instead of just finding a document, we have to find the subpart of a document that is most relevant to the query. Also, XML retrieval effectiveness - when evaluated as described here - can be lower than unstructured retrieval effectiveness on a standard evaluation because graded judgments lower measured performance. Consider a system that returns a document with graded relevance 0.6 and binary relevance 1 at the top of the retrieved list. Then, interpolated precision at 0.00 recall (cf. page 8.4 ) is 1.0 on a binary evaluation, but can be as low as 0.6 on a graded evaluation.   Table 10.4: A comparison of content-only and full-structure search in INEX 2003/2004.   content only full structure improvement precision at 5 0.2000 0.3265 63.3% precision at 10 0.1820 0.2531 39.1% precision at 20 0.1700 0.1796 5.6% precision at 30 0.1527 0.1531 0.3%   Table 10.3 gives us a sense of the typical performance of XML retrieval, but it does not compare structured with unstructured retrieval. Table 10.4 directly shows the effect of using structure in retrieval. The results are for a language-model-based system (cf. Chapter 12 ) that is evaluated on a subset of CAS topics from INEX 2003 and 2004. The evaluation metric is precision at as defined in Chapter 8 (page 8.4 ). The discretization function used for the evaluation maps highly relevant elements (roughly corresponding to the 3E elements defined for Q) to 1 and all other elements to 0. The content-only system treats queries and documents as unstructured bags of words. The full-structure model ranks elements that satisfy structural constraints higher than elements that do not. For instance, for the query in Figure 10.3 an element that contains the phrase summer holidays in a section will be rated higher than one that contains it in an abstract. The table shows that structure helps increase precision at the top of the results list. There is a large increase of precision at and at . There is almost no improvement at . These results demonstrate the benefits of structured retrieval. Structured retrieval imposes additional constraints on what to return and documents that pass the structural filter are more likely to be relevant. Recall may suffer because some relevant documents will be filtered out, but for precision-oriented tasks structured retrieval is superior. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_5</id>
         <title>Text-centric vs. data-centric XML retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/text-centric-vs-data-centric-xml-retrieval-1.html</url>
         <file>text-centric-vs-data-centric-xml-retrieval-1.html</file>
         <text> Text-centric vs. data-centric XML retrieval  text-centric XML   In contrast, data-centric XML mainly encodes numerical and non-text attribute-value data. When querying data-centric XML, we want to impose exact match conditions in most cases. This puts the emphasis on the structural aspects of XML documents and queries. An example is: Find employees whose salary is the same this month as it was 12 months ago. Text-centric approaches are appropriate for data that are essentially text documents, marked up as XML to capture document structure. This is becoming a de facto standard for publishing text databases since most text documents have some form of interesting structure - paragraphs, sections, footnotes etc. Examples include assembly manuals, issues of journals, Shakespeare's collected works and newswire articles. Data-centric approaches are commonly used for data collections with complex structures that mainly contain non-text data. A text-centric retrieval engine will have a hard time with proteomic data in bioinformatics or with the representation of a city map that (together with street names and other textual descriptions) forms a navigational database. Two other types of queries that are difficult to handle in a text-centric structured retrieval model are joins and ordering constraints. The query for employees with unchanged salary requires a join. The following query imposes an ordering constraint: Retrieve the chapter of the book Introduction to algorithms that follows the chapter Binomial heaps.  Relational databases are better equipped to handle many structural constraints, particularly joins (but ordering is also difficult in a database framework - the tuples of a relation in the relational calculus are not ordered). For this reason, most data-centric XML retrieval systems are extensions of relational databases (see the references in Section 10.6 ). If text fields are short, exact matching meets user needs and retrieval results in form of unordered sets are acceptable, then using a relational database for XML retrieval is appropriate. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_6</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-10.html</url>
         <file>references-and-further-reading-10.html</file>
         <text> References and further reading Harold and Means, 2004 10.1 van Rijsbergen, 1979 10.4 Gövert and Kazai (2003) Fuhr et al., 2003a Fuhr et al. (2003b) Fuhr et al. (2005) Fuhr et al. (2006) Fuhr et al. (2007) Fuhr and Lalmas (2007) 10.4 Kamps et al., 2006 Chu-Carroll et al. (2006) Lalmas and Tombros, 2007 Trotman et al. (2006) The structured document retrieval principle is due to Chiaramella et al. (1996). Figure 10.5 is from (Fuhr and Großjohann, 2004). Rahm and Bernstein (2001) give a survey of automatic schema matching that is applicable to XML. The vector-space based XML retrieval method in Section 10.3 is essentially IBM Haifa's JuruXML system as presented by Mass et al. (2003) and Carmel et al. (2003). Schlieder and Meuss (2002) and Grabs and Schek (2002) describe similar approaches. Carmel et al. (2003) represent queries as XML fragments . The trees that represent XML queries in this chapter are all XML fragments, but XML fragments also permit the operators , and phrase on content nodes. We chose to present the vector space model for XML retrieval because it is simple and a natural extension of the unstructured vector space model in Chapter 6 . But many other unstructured retrieval methods have been applied to XML retrieval with at least as much success as the vector space model. These methods include language models (cf. Chapter 12 , e.g., Kamps et al. (2004), Ogilvie and Callan (2005), List et al. (2005)), systems that use a relational database as a backend (Theobald et al., 2008;2005, Mihajlovic et al., 2005), probabilistic weighting (Lu et al., 2007), and fusion (Larson, 2005). There is currently no consensus as to what the best approach to XML retrieval is. Most early work on XML retrieval accomplished relevance ranking by focusing on individual terms, including their structural contexts, in query and document. As in unstructured information retrieval, there is a trend in more recent work to model relevance ranking as combining evidence from disparate measurements about the query, the document and their match. The combination function can be tuned manually (Sigurbjörnsson et al., 2004, Arvola et al., 2005) or trained using machine learning methods (Vittaut and Gallinari (2006), cf. mls). An active area of XML retrieval research is focused retrieval (Trotman et al., 2007), which aims to avoid returning nested elements that share one or more common subelements (cf. discussion in Section 10.2 , page 10.2 ). There is evidence that users dislike redundancy caused by nested elements (Betsi et al., 2006). Focused retrieval requires evaluation measures that penalize redundant results lists (Kazai and Lalmas, 2006, Lalmas et al., 2007). Trotman and Geva (2006) argue that XML retrieval is a form of passage retrieval . In passage retrieval (Kaszkiel and Zobel, 1997, Salton et al., 1993, Hearst and Plaunt, 1993, Hearst, 1997, Zobel et al., 1995), the retrieval system returns short passages instead of documents in response to a user query. While element boundaries in XML documents are cues for identifying good segment boundaries between passages, the most relevant passage often does not coincide with an XML element. In the last several years, the query format at INEX has been the NEXI standard proposed by Trotman and Sigurbjörnsson (2004). Figure 10.3 is from their paper. O'Keefe and Trotman (2004) give evidence that users cannot reliably distinguish the child and descendant axes. This justifies only permitting descendant axes in NEXI (and XML fragments). These structural constraints were only treated as ``hints'' in recent INEXes. Assessors can judge an element highly relevant even though it violates one of the structural constraints specified in a NEXI query. An alternative to structured query languages like NEXI is a more sophisticated user interface for query formulation (Tannier and Geva, 2005, van Zwol, 2006, Woodley and Geva, 2006). A broad overview of XML retrieval that covers database as well as IR approaches is given by Amer-Yahia and Lalmas (2006) and an extensive reference list on the topic can be found in (Amer-Yahia et al., 2005). Chapter 6 of Grossman and Frieder (2004) is a good introduction to structured text retrieval from a database perspective. The proposed standard for XQuery is available at http://www.w3.org/TR/xquery/ including an extension for full-text queries (Amer-Yahia et al., 2006): http://www.w3.org/TR/xquery-full-text/. Work that has looked at combining the relational database and the unstructured information retrieval approaches includes (Fuhr and Rölleke, 1997), (Navarro and Baeza-Yates, 1997), (Cohen, 1998), and (Chaudhuri et al., 2006). </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_10_7</id>
         <title>Exercises</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/exercises-1.html</url>
         <file>exercises-1.html</file>
         <text> Exercises Exercises. Find a reasonably sized XML document collection (or a collection using a markup language different from XML like HTML) on the web and download it. Jon Bosak's XML edition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or the first 10,000 documents of the Wikipedia are good choices. Create three CAS topics of the type shown in Figure 10.3 that you would expect to do better than analogous CO topics. Explain why an XML retrieval system would be able to exploit the XML structure of the documents to achieve better retrieval results on the topics than an unstructured retrieval system. For the collection and the topics in Exercise 10.7 , (i) are there pairs of elements and , with a subelement of such that both answer one of the topics? Find one case each where (ii) (iii) is the better answer to the query. Implement the (i) SIMMERGE (ii) SIMNOMERGE algorithm in Section 10.3 and run it for the collection and the topics in Exercise 10.7 . (iii) Evaluate the results by assigning binary relevance judgments to the first five documents of the three retrieved lists for each algorithm. Which algorithm performs better? Are all of the elements in Exercise 10.7 appropriate to be returned as hits to a user or are there elements (as in the example <b>definitely</b> on page 10.2 ) that you would exclude? We discussed the tradeoff between accuracy of results and dimensionality of the vector space on page 10.3 . Give an example of an information need that we can answer correctly if we index all lexicalized subtrees, but cannot answer if we only index structural terms. If we index all structural terms, what is the size of the index as a function of text size? If we index all lexicalized subtrees, what is the size of the index as a function of text size? Give an example of a query-document pair for which is larger than 1.0. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_11</id>
    <title>Probabilistic information retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-information-retrieval-1.html</url>
    <file>probabilistic-information-retrieval-1.html</file>
    <text> Probabilistic information retrieval During the discussion of relevance feedback in Section 9.1.2 , we observed that if we have some known relevant and nonrelevant documents, then we can straightforwardly start to estimate the probability of a term appearing in a relevant document , and that this could be the basis of a classifier that decides whether documents are relevant or not. In this chapter, we more systematically introduce this probabilistic approach to IR, which provides a different formal basis for a retrieval model and results in different techniques for setting term weights. Users start with information needs, which they translate into query representations. Similarly, there are documents, which are converted into document representations (the latter differing at least by how text is tokenized, but perhaps containing fundamentally less information, as when a non-positional index is used). Based on these two representations, a system tries to determine how well documents satisfy information needs. In the Boolean or vector space models of IR, matching is done in a formally defined but semantically imprecise calculus of index terms. Given only a query, an IR system has an uncertain understanding of the information need. Given the query and document representations, a system has an uncertain guess of whether a document has content relevant to the information need. Probability theory provides a principled foundation for such reasoning under uncertainty. This chapter provides one answer as to how to exploit this foundation to estimate how likely it is that a document is relevant to an information need. There is more than one possible retrieval model which has a probabilistic basis. Here, we will introduce probability theory and the Probability Ranking Principle (Sections 11.1 -11.2 ), and then concentrate on the Binary Independence Model (Section 11.3 ), which is the original and still most influential probabilistic retrieval model. Finally, we will introduce related but extended methods which use term counts, including the empirically successful Okapi BM25 weighting scheme, and Bayesian Network models for IR (Section 11.4 ). In Chapter 12 , we then present the alternative probabilistic language modeling approach to IR, which has been developed with considerable success in recent years.   Subsections Review of basic probability theory The Probability Ranking Principle The 1/0 loss case The PRP with retrieval costs The Binary Independence Model Deriving a ranking function for query terms Probability estimates in theory Probability estimates in practice Probabilistic approaches to relevance feedback An appraisal and some extensions An appraisal of probabilistic models Tree-structured dependencies between terms Okapi BM25: a non-binary model Bayesian network approaches to IR References and further reading</text>
    <subsections>
       <section>
         <id>iir_11_1</id>
         <title>Review of basic probability theory</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/review-of-basic-probability-theory-1.html</url>
         <file>review-of-basic-probability-theory-1.html</file>
         <text> Review of basic probability theory We hope that the reader has seen a little basic probability theory previously. We will give a very quick review; some references for further reading appear at the end of the chapter. A variable represents an event (a subset of the space of possible outcomes). Equivalently, we can represent the subset via a random variable , which is a function from outcomes to real numbers; the subset is the domain over which the random variable has a particular value. Often we will not know with certainty whether an event is true in the world. We can ask the probability of the event . For two events and , the joint event of both events occurring is described by the joint probability . The conditional probability expresses the probability of event given that event occurred. The fundamental relationship between joint and conditional probabilities is given by the chain rule : (56)  Writing for the complement of an event, we similarly have: (57)   partition rule    (58)  From these we can derive Bayes' Rule for inverting conditional probabilities: (59)    prior probability   posterior probability    likelihood    Finally, it is often useful to talk about the odds of an event, which provide a kind of multiplier for how probabilities change: (60)  </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_11_2</id>
         <title>The Probability Ranking Principle</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-probability-ranking-principle-1.html</url>
         <file>the-probability-ranking-principle-1.html</file>
         <text> The Probability Ranking Principle   Subsections The 1/0 loss case The PRP with retrieval costs </text>
         <subsections>
            <section>
              <id>iir_11_2_1</id>
              <title>The 1/0 loss case</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-10-loss-case-1.html</url>
              <file>the-10-loss-case-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_2_2</id>
              <title>The PRP with retrieval costs</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-prp-with-retrieval-costs-1.html</url>
              <file>the-prp-with-retrieval-costs-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_11_3</id>
         <title>The Binary Independence Model</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html</url>
         <file>the-binary-independence-model-1.html</file>
         <text> The Binary Independence Model The Binary Independence Model (BIM) we present in this section is the model that has traditionally been used with the PRP. It introduces some simple assumptions, which make estimating the probability function practical. Here, ``binary'' is equivalent to Boolean: documents and queries are both represented as binary term incidence vectors. That is, a document is represented by the vector where if term is present in document and if is not present in . With this representation, many possible documents have the same vector representation. Similarly, we represent by the incidence vector (the distinction between and is less central since commonly is in the form of a set of words). ``Independence'' means that terms are modeled as occurring in documents independently. The model recognizes no association between terms. This assumption is far from correct, but it nevertheless often gives satisfactory results in practice; it is the ``naive'' assumption of Naive Bayes models, discussed further in Section 13.4 (page ). Indeed, the Binary Independence Model is exactly the same as the multivariate Bernoulli Naive Bayes model presented in Section 13.3 (page ). In a sense this assumption is equivalent to an assumption of the vector space model, where each term is a dimension that is orthogonal to all other terms. We will first present a model which assumes that the user has a single step information need. As discussed in Chapter 9 , seeing a range of results might let the user refine their information need. Fortunately, as mentioned there, it is straightforward to extend the Binary Independence Model so as to provide a framework for relevance feedback, and we present this model in Section 11.3.4 . To make a probabilistic retrieval strategy precise, we need to estimate how terms in documents contribute to relevance, specifically, we wish to know how term frequency, document frequency, document length, and other statistics that we can compute influence judgments about document relevance, and how they can be reasonably combined to estimate the probability of document relevance. We then order documents by decreasing estimated probability of relevance. We assume here that the relevance of each document is independent of the relevance of other documents. As we noted in Section 8.5.1 (page ), this is incorrect: the assumption is especially harmful in practice if it allows a system to return duplicate or near duplicate documents. Under the BIM, we model the probability that a document is relevant via the probability in terms of term incidence vectors . Then, using Bayes rule, we have: (63) (64)            (65)    Subsections Deriving a ranking function for query terms Probability estimates in theory Probability estimates in practice Probabilistic approaches to relevance feedback</text>
         <subsections>
            <section>
              <id>iir_11_3_1</id>
              <title>Deriving a ranking function for query terms</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/deriving-a-ranking-function-for-query-terms-1.html</url>
              <file>deriving-a-ranking-function-for-query-terms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_3_2</id>
              <title>Probability estimates in theory</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-theory-1.html</url>
              <file>probability-estimates-in-theory-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_3_3</id>
              <title>Probability estimates in practice</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html</url>
              <file>probability-estimates-in-practice-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_3_4</id>
              <title>Probabilistic approaches to relevance feedback</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-approaches-to-relevance-feedback-1.html</url>
              <file>probabilistic-approaches-to-relevance-feedback-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_11_4</id>
         <title>An appraisal and some extensions</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/an-appraisal-and-some-extensions-1.html</url>
         <file>an-appraisal-and-some-extensions-1.html</file>
         <text> An appraisal and some extensions   Subsections An appraisal of probabilistic models Tree-structured dependencies between terms Okapi BM25: a non-binary model Bayesian network approaches to IR </text>
         <subsections>
            <section>
              <id>iir_11_4_1</id>
              <title>An appraisal of probabilistic models</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/an-appraisal-of-probabilistic-models-1.html</url>
              <file>an-appraisal-of-probabilistic-models-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_4_2</id>
              <title>Tree-structured dependencies between terms</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/tree-structured-dependencies-between-terms-1.html</url>
              <file>tree-structured-dependencies-between-terms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_4_3</id>
              <title>Okapi BM25: a non-binary model</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html</url>
              <file>okapi-bm25-a-non-binary-model-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_11_4_4</id>
              <title>Bayesian network approaches to IR</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/bayesian-network-approaches-to-ir-1.html</url>
              <file>bayesian-network-approaches-to-ir-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_11_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-11.html</url>
         <file>references-and-further-reading-11.html</file>
         <text> References and further reading Longer introductions to probability theory can be found in most introductory probability and statistics books, such as (Ross, 2006, Grinstead and Snell, 1997, Rice, 2006). An introduction to Bayesian utility theory can be found in (Ripley, 1996). The probabilistic approach to IR originated in the UK in the 1950s. The first major presentation of a probabilistic model is Maron and Kuhns (1960). Robertson and Jones (1976) introduce the main foundations of the BIM and van Rijsbergen (1979) presents in detail the classic BIM probabilistic model. The idea of the PRP is variously attributed to S. E. Robertson, M. E. Maron and W. S. Cooper (the term ``Probabilistic Ordering Principle'' is used in Robertson and Jones (1976), but PRP dominates in later work). Fuhr (1992) is a more recent presentation of probabilistic IR, which includes coverage of other approaches such as probabilistic logics and Bayesian networks. Crestani et al. (1998) is another survey. Spärck Jones et al. (2000) is the definitive presentation of probabilistic IR experiments by the ``London school'', and Robertson (2005) presents a retrospective on the group's participation in TREC evaluations, including detailed discussion of the Okapi BM25 scoring function and its development. Robertson et al. (2004) extend BM25 to the case of multiple weighted fields. The open-source Indri search engine, which is distributed with the Lemur toolkit (http://www.lemurproject.org/) merges ideas from Bayesian inference networks and statistical language modeling approaches (see Chapter 12 ), in particular preserving the former's support for structured query operators.  </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_12</id>
    <title>Language models for information retrieval</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html</url>
    <file>language-models-for-information-retrieval-1.html</file>
    <text> Language models for information retrieval A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. This approach thus provides a different realization of some of the basic ideas for document ranking which we saw in Section 6.2 (page ). Instead of overtly modeling the probability of relevance of a document to a query , as in the traditional probabilistic approach to IR (Chapter 11 ), the basic language modeling approach instead builds a probabilistic language model from each document , and ranks documents based on the probability of the model generating the query: . In this chapter, we first introduce the concept of language models (Section 12.1 ) and then describe the basic and most commonly used language modeling approach to IR, the Query Likelihood Model (Section 12.2 ). After some comparisons between the language modeling approach and other approaches to IR (Section 12.3 ), we finish by briefly describing various extensions to the language modeling approach (Section 12.4 ).   Subsections Language models Finite automata and language models Types of language models Multinomial distributions over words The query likelihood model Using query likelihood language models in IR Estimating the query generation probability Ponte and Croft's Experiments Language modeling versus other approaches in IR Extended language modeling approaches References and further reading</text>
    <subsections>
       <section>
         <id>iir_12_1</id>
         <title>Language models</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/language-models-1.html</url>
         <file>language-models-1.html</file>
         <text> Language models   Subsections Finite automata and language models Types of language models Multinomial distributions over words </text>
         <subsections>
            <section>
              <id>iir_12_1_1</id>
              <title>Finite automata and language models</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/finite-automata-and-language-models-1.html</url>
              <file>finite-automata-and-language-models-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_12_1_2</id>
              <title>Types of language models</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/types-of-language-models-1.html</url>
              <file>types-of-language-models-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_12_1_3</id>
              <title>Multinomial distributions over words</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/multinomial-distributions-over-words-1.html</url>
              <file>multinomial-distributions-over-words-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_12_2</id>
         <title>The query likelihood model</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-query-likelihood-model-1.html</url>
         <file>the-query-likelihood-model-1.html</file>
         <text> The query likelihood model   Subsections Using query likelihood language models in IR Estimating the query generation probability Ponte and Croft's Experiments </text>
         <subsections>
            <section>
              <id>iir_12_2_1</id>
              <title>Using query likelihood language models in IR</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html</url>
              <file>using-query-likelihood-language-models-in-ir-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_12_2_2</id>
              <title>Estimating the query generation probability</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html</url>
              <file>estimating-the-query-generation-probability-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_12_2_3</id>
              <title>Ponte and Croft's Experiments</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/ponte-and-crofts-experiments-1.html</url>
              <file>ponte-and-crofts-experiments-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_12_3</id>
         <title>Language modeling versus other approaches in IR</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/language-modeling-versus-other-approaches-in-ir-1.html</url>
         <file>language-modeling-versus-other-approaches-in-ir-1.html</file>
         <text> Language modeling versus other approaches in IR The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech and language processing. As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used, and hence the performance of the model. The major issue is estimation of the document model, such as choices of how to smooth it effectively. The model has achieved very good retrieval results. Compared to other probabilistic approaches, such as the BIM from Chapter 11 , the main difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the BIM approach). But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss. The LM approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the tools and methods of language modeling from speech and natural language processing. The resulting model is mathematically precise, conceptually simple, computationally tractable, and intuitively appealing. This seems similar to the situation with XML retrieval (Chapter 10 ): there the approaches that assume queries and documents are objects of the same type are also among the most successful. On the other hand, like all IR models, you can also raise objections to the model. The assumption of equivalence between document and information need representation is unrealistic. Current LM approaches use very simple models of language, usually unigram models. Without an explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences. It also seems necessary to move beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators. Subsequent work in the LM approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch between the query language and the document language. The model has significant relations to traditional tf-idf models. Term frequency is directly represented in tf-idf models, and much recent work has recognized the importance of document length normalization. The effect of doing a mixture of document generation probability with collection generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents. In most concrete realizations, the models share treating terms as if they were independent. On the other hand, the intuitions are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term frequency and document length are used differ. If you are concerned mainly with performance numbers, recent work has shown the LM approach to be very effective in retrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is perhaps still insufficient evidence that its performance so greatly exceeds that of a well-tuned traditional vector space retrieval system as to justify changing an existing implementation. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_12_4</id>
         <title>Extended language modeling approaches</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/extended-language-modeling-approaches-1.html</url>
         <file>extended-language-modeling-approaches-1.html</file>
         <text> Extended language modeling approaches In this section we briefly mention some of the work that extends the basic language modeling approach. There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model generating the query, you can look at the probability of a query language model generating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less text available to estimate a language model based on the query text, and so the model will be worse estimated, and will have to depend more on being smoothed with some other language model. On the other hand, it is easy to see how to incorporate relevance feedback into such a model: you can expand the query with terms taken from relevant documents in the usual way and hence update the language model (Zhai and Lafferty, 2001a). Indeed, with appropriate modeling choices, this approach leads to the BIM model of Chapter 11 . The relevance model of Lavrenko and Croft (2001) is an instance of a document likelihood model, which incorporates pseudo-relevance feedback into a language modeling approach. It achieves very strong empirical results.  Figure 12.5: Three ways of developing the language modeling approach: (a) query likelihood, (b) document likelihood, and (c) model comparison. Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other. Lafferty and Zhai (2001) lay out these three ways of thinking about the problem, which we show in Figure 12.5 , and develop a general risk minimization approach for document retrieval. For instance, one way to model the risk of returning a document as relevant to a query is to use the Kullback-Leibler (KL) divergence between their respective language models: (109)    Manning and Schütze, 1999 Cover and Thomas, 1991 Lafferty and Zhai (2001) Kraaij and Spitters (2003)  Basic LMs do not address issues of alternate expression, that is, synonymy, or any deviation in use of language between queries and documents. Berger and Lafferty (1999) introduce translation models to bridge this query-document gap. A translation model lets you generate query words not in a document by translation to alternate terms with similar meaning. This also provides a basis for performing cross-language IR. We assume that the translation model can be represented by a conditional probability distribution between vocabulary terms. The form of the translation query generation model is then: (110)    Building extended LM approaches remains an active area of research. In general, translation models, relevance feedback models, and model comparison approaches have all been demonstrated to improve performance over the basic query likelihood LM. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_12_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-12.html</url>
         <file>references-and-further-reading-12.html</file>
         <text> References and further reading For more details on the basic concepts of probabilistic language models and techniques for smoothing, see either Manning and Schütze (1999, Chapter 6) or Jurafsky and Martin (2008, Chapter 4). The important initial papers that originated the language modeling approach to IR are: (Berger and Lafferty, 1999, Ponte and Croft, 1998, Miller et al., 1999, Hiemstra, 1998). Other relevant papers can be found in the next several years of SIGIR proceedings. (Croft and Lafferty, 2003) contains a collection of papers from a workshop on language modeling approaches and Hiemstra and Kraaij (2005) review one prominent thread of work on using language modeling approaches for TREC tasks. Zhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR and present detailed empirical comparisons of different smoothing methods. Zaragoza et al. (2003) advocate using full Bayesian predictive distributions rather than MAP point estimates, but while they outperform Bayesian smoothing, they fail to outperform a linear interpolation. Zhai and Lafferty (2002) argue that a two-stage smoothing model with first Bayesian smoothing followed by linear interpolation gives a good model of the task, and performs better and more stably than a single form of smoothing. A nice feature of the LM approach is that it provides a convenient and principled way to put various kinds of prior information into the model; Kraaij et al. (2002) demonstrate this by showing the value of link information as a prior in improving web entry page retrieval performance. As briefly discussed in Chapter 16 (page 16.1 ), Liu and Croft (2004) show some gains by smoothing a document LM with estimates from a cluster of similar documents; Tao et al. (2006) report larger gains by doing document-similarity based smoothing. Hiemstra and Kraaij (2005) present TREC results showing a LM approach beating use of BM25 weights. Recent work has achieved some gains by going beyond the unigram model, providing the higher order models are smoothed with lower order models (Cao et al., 2005, Gao et al., 2004), though the gains to date remain modest. Spärck Jones (2004) presents a critical viewpoint on the rationale for the language modeling approach, but Lafferty and Zhai (2003) argue that a unified account can be given of the probabilistic semantics underlying both the language modeling approach presented in this chapter and the classical probabilistic information retrieval approach of Chapter 11 . The Lemur Toolkit (http://www.lemurproject.org/) provides a flexible open source framework for investigating language modeling approaches to IR. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_13</id>
    <title>Text classification and Naive Bayes</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html</url>
    <file>text-classification-and-naive-bayes-1.html</file>
    <text> Text classification and Naive Bayes Thus far, this book has mainly discussed the process of ad hoc retrieval , where users have transient information needs that they try to address by posing one or more queries to a search engine. However, many users have ongoing information needs. For example, you might need to track developments in multicore computer chips. One way of doing this is to issue the query multicore and computer and chip against an index of recent newswire articles each morning. In this and the following two chapters we examine the question: How can this repetitive task be automated? To this end, many systems support standing queries . A standing query is like any other query except that it is periodically executed on a collection to which new documents are incrementally added over time. If your standing query is just multicore and computer and chip, you will tend to miss many relevant new articles which use other terms such as multicore processors. To achieve good recall, standing queries thus have to be refined over time and can gradually become quite complex. In this example, using a Boolean search engine with stemming, you might end up with a query like (multicore or multi-core) and (chip or processor or microprocessor). To capture the generality and scope of the problem space to which standing queries belong, we now introduce the general notion of a classification problem. Given a set of classes, we seek to determine which class(es) a given object belongs to. In the example, the standing query serves to divide new newswire articles into the two classes: documents about multicore computer chips and documents not about multicore computer chips. We refer to this as two-class classification. Classification using standing queries is also called routing or filtering and will be discussed further in Section 15.3.1 (page ). A class need not be as narrowly focused as the standing query multicore computer chips. Often, a class is a more general subject area like China or coffee. Such more general classes are usually referred to as topics , and the classification task is then called text classification , text categorization , topic classification , or topic spotting . An example for China appears in Figure 13.1 . Standing queries and topics differ in their degree of specificity, but the methods for solving routing, filtering, and text classification are essentially the same. We therefore include routing and filtering under the rubric of text classification in this and the following chapters. The notion of classification is very general and has many applications within and beyond information retrieval (IR). For instance, in computer vision, a classifier may be used to divide images into classes such as landscape, portrait, and neither. We focus here on examples from information retrieval such as:  Several of the preprocessing steps necessary for indexing as discussed in Chapter 2 : detecting a document's encoding (ASCII, Unicode UTF-8 etc; page 2.1.1 ); word segmentation (Is the white space between two letters a word boundary or not? page 24 ) ; truecasing (page 2.2.3 ); and identifying the language of a document (page 2.5 ). The automatic detection of spam pages (which then are not included in the search engine index). The automatic detection of sexually explicit content (which is included in search results only if the user turns an option such as SafeSearch off). Sentiment detection or the automatic classification of a movie or product review as positive or negative. An example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems. Personal email sorting . A user may have folders like talk announcements, electronic bills, email from family and friends, and so on, and may want a classifier to classify each incoming email and automatically move it to the appropriate folder. It is easier to find messages in sorted folders than in a very large inbox. The most common case of this application is a spam folder that holds all suspected spam messages. Topic-specific or vertical search. Vertical search engines restrict searches to a particular topic. For example, the query computer science on a vertical search engine for the topic China will return a list of Chinese computer science departments with higher precision and recall than the query computer science China on a general purpose search engine. This is because the vertical search engine does not include web pages in its index that contain the term china in a different sense (e.g., referring to a hard white ceramic), but does include relevant pages even if they do not explicitly mention the term China. Finally, the ranking function in ad hoc information retrieval can also be based on a document classifier as we will explain in Section 15.4 (page ). This list shows the general importance of classification in IR. Most retrieval systems today contain multiple components that use some form of classifier. The classification task we will use as an example in this book is text classification. A computer is not essential for classification. Many classification tasks have traditionally been solved manually. Books in a library are assigned Library of Congress categories by a librarian. But manual classification is expensive to scale. The multicore computer chips example illustrates one alternative approach: classification by the use of standing queries - which can be thought of as rules - most commonly written by hand. As in our example (multicore or multi-core) and (chip or processor or microprocessor), rules are sometimes equivalent to Boolean expressions. A rule captures a certain combination of keywords that indicates a class. Hand-coded rules have good scaling properties, but creating and maintaining them over time is labor intensive. A technically skilled person (e.g., a domain expert who is good at writing regular expressions) can create rule sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly; however, it can be hard to find someone with this specialized skill. Apart from manual classification and hand-crafted rules, there is a third approach to text classification, namely, machine learning-based text classification. It is the approach that we focus on in the next several chapters. In machine learning, the set of rules or, more generally, the decision criterion of the text classifier, is learned automatically from training data. This approach is also called statistical text classification if the learning method is statistical. In statistical text classification, we require a number of good example documents (or training documents) for each class. The need for manual classification is not eliminated because the training documents come from a person who has labeled them - where labeling refers to the process of annotating each document with its class. But labeling is arguably an easier task than writing rules. Almost anybody can look at a document and decide whether or not it is related to China. Sometimes such labeling is already implicitly part of an existing workflow. For instance, you may go through the news articles returned by a standing query each morning and give relevance feedback (cf. Chapter 9 ) by moving the relevant articles to a special folder like multicore-processors. We begin this chapter with a general introduction to the text classification problem including a formal definition (Section 13.1 ); we then cover Naive Bayes, a particularly simple and effective classification method (Sections 13.2-13.4). All of the classification algorithms we study represent documents in high-dimensional spaces. To improve the efficiency of these algorithms, it is generally desirable to reduce the dimensionality of these spaces; to this end, a technique known as feature selection is commonly applied in text classification as discussed in Section 13.5 . Section 13.6 covers evaluation of text classification. In the following chapters, Chapters 14 15 , we look at two other families of classification methods, vector space classifiers and support vector machines.   Subsections The text classification problem Naive Bayes text classification Relation to multinomial unigram language model The Bernoulli model Properties of Naive Bayes A variant of the multinomial model Feature selection Mutual information Feature selectionChi2 Feature selection Assessing as a feature selection methodAssessing chi-square as a feature selection method Frequency-based feature selection Feature selection for multiple classifiers Comparison of feature selection methods Evaluation of text classification References and further reading</text>
    <subsections>
       <section>
         <id>iir_13_1</id>
         <title>The text classification problem</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html</url>
         <file>the-text-classification-problem-1.html</file>
         <text> The text classification problem In text classification, we are given a description of a document, where is the document space ; and a fixed set of classes . Classes are also called categories or labels . Typically, the document space is some type of high-dimensional space, and the classes are human defined for the needs of an application, as in the examples China and documents that talk about multicore computer chips above. We are given a training set of labeled documents , where . For example: (111)  Using a learning method or learning algorithm , we then wish to learn a classifier or classification function that maps documents to classes:  (112)  This type of learning is called supervised learning because a supervisor (the human who defines the classes and labels training documents) serves as a teacher directing the learning process. We denote the supervised learning method by and write . The learning method takes the training set as input and returns the learned classification function . Most names for learning methods are also used for classifiers . We talk about the Naive Bayes (NB) learning method when we say that ``Naive Bayes is robust,'' meaning that it can be applied to many different learning problems and is unlikely to produce classifiers that fail catastrophically. But when we say that ``Naive Bayes had an error rate of 20%,'' we are describing an experiment in which a particular NB classifier (which was produced by the NB learning method) had a 20% error rate in an application. Figure 13.1 shows an example of text classification from the Reuters-RCV1 collection, introduced in Section 4.2 , page 4.2 . There are six classes (UK, China, ..., sports), each with three training documents. We show a few mnemonic words for each document's content. The training set provides some typical examples for each class, so that we can learn the classification function . Once we have learned , we can apply it to the test set (or test data ), for example, the new document first private Chinese airline whose class is unknown. In Figure 13.1 , the classification function assigns the new document to class China, which is the correct assignment. The classes in text classification often have some interesting structure such as the hierarchy in Figure 13.1 . There are two instances each of region categories, industry categories, and subject area categories. A hierarchy can be an important aid in solving a classification problem; see Section 15.3.2 for further discussion. Until then, we will make the assumption in the text classification chapters that the classes form a set with no subset relationships between them.  Figure 13.1: Classes, training set, and test set in text classification . Definition eqn:gammadef stipulates that a document is a member of exactly one class. This is not the most appropriate model for the hierarchy in Figure 13.1 . For instance, a document about the 2008 Olympics should be a member of two classes: the China class and the sports class. This type of classification problem is referred to as an any-of problem and we will return to it in Section 14.5 (page ). For the time being, we only consider one-of problems where a document is a member of exactly one class. Our goal in text classification is high accuracy on test data or new data - for example, the newswire articles that we will encounter tomorrow morning in the multicore chip example. It is easy to achieve high accuracy on the training set (e.g., we can simply memorize the labels). But high accuracy on the training set in general does not mean that the classifier will work well on new data in an application. When we use the training set to learn a classifier for test data, we make the assumption that training data and test data are similar or from the same distribution. We defer a precise definition of this notion to Section 14.6 (page ). </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_13_2</id>
         <title>Naive Bayes text classification</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</url>
         <file>naive-bayes-text-classification-1.html</file>
         <text> Naive Bayes text classification  multinomial Naive Bayes  multinomial NB        (113)                   In text classification, our goal is to find the best class for the document. The best class in NB classification is the most likely or maximum a posteriori ( MAP ) class : (114)      In Equation 114, many conditional probabilities are multiplied, one for each position . This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable; and the logarithm function is monotonic. Hence, the maximization that is actually done in most implementations of NB is:     (115)   Equation 115 has a simple interpretation. Each conditional parameter is a weight that indicates how good an indicator is for . Similarly, the prior is a weight that indicates the relative frequency of . More frequent classes are more likely to be the correct class than infrequent classes. The sum of log prior and term weights is then a measure of how much evidence there is for the document being in the class, and Equation 115 selects the class for which we have the most evidence. We will initially work with this intuitive interpretation of the multinomial NB model and defer a formal derivation to Section 13.4 . How do we estimate the parameters and ? We first try the maximum likelihood estimate (MLE; probtheory), which is simply the relative frequency and corresponds to the most likely value of each parameter given the training data. For the priors this estimate is:     (116)       We estimate the conditional probability as the relative frequency of term in documents belonging to class : (117)      positional independence assumption      The problem with the MLE estimate is that it is zero for a term-class combination that did not occur in the training data. If the term WTO in the training data only occurred in China documents, then the MLE estimates for the other classes, for example UK, will be zero: (118)  113    sparseness  Figure 13.2: Naive Bayes algorithm (multinomial model): Training and testing. To eliminate zeros, we use add-one or Laplace smoothing, which simply adds one to each count (cf. Section 11.3.2 ): (119)   term class 116 We have now introduced all the elements we need for training and applying an NB classifier. The complete algorithm is described in Figure 13.2 .   Table 13.1: Data for parameter estimation examples.     docID words in document in China?     training set 1 Chinese Beijing Chinese yes       2 Chinese Chinese Shanghai yes       3 Chinese Macao yes       4 Tokyo Japan Chinese no     test set 5 Chinese Chinese Chinese Tokyo Japan ?    Worked example. For the example in Table 13.1 , the multinomial parameters we need to classify the test document are the priors and and the following conditional probabilities:          119 We then get:       End worked example.   Table 13.2: Training and test times for NB.   mode time complexity     training     testing          We use as a notation for here, where is the length of the training collection. This is nonstandard; is not defined for an average. We prefer expressing the time complexity in terms of and because these are the primary statistics used to characterize training collections. The time complexity of APPLYMULTINOMIALNB in Figure 13.2 is . and are the numbers of tokens and types, respectively, in the test document . APPLYMULTINOMIALNB can be modified to be (Exercise 13.6 ). Finally, assuming that the length of test documents is bounded, because for a fixed constant . Table 13.2 summarizes the time complexities. In general, we have , so both training and testing complexity are linear in the time it takes to scan the data. Because we have to look at the data at least once, NB can be said to have optimal time complexity. Its efficiency is one reason why NB is a popular text classification method.   Subsections Relation to multinomial unigram language model</text>
         <subsections>
            <section>
              <id>iir_13_2_1</id>
              <title>Relation to multinomial unigram language model</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/relation-to-multinomial-unigram-language-model-1.html</url>
              <file>relation-to-multinomial-unigram-language-model-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_13_3</id>
         <title>The Bernoulli model</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</url>
         <file>the-bernoulli-model-1.html</file>
         <text> The Bernoulli model There are two different ways we can set up an NB classifier. The model we introduced in the previous section is the multinomial model . It generates one term from the vocabulary in each position of the document, where we assume a generative model that will be discussed in more detail in Section 13.4 (see also page 12.1.1 ). An alternative to the multinomial model is the multivariate Bernoulli model or Bernoulli model . It is equivalent to the binary independence model of Section 11.3 (page ), which generates an indicator for each term of the vocabulary, either indicating presence of the term in the document or indicating absence. Figure 13.3 presents training and testing algorithms for the Bernoulli model. The Bernoulli model has the same time complexity as the multinomial model.   The different generation models imply different estimation strategies and different classification rules. The Bernoulli model estimates as the fraction of documents of class that contain term (Figure 13.3 , TRAINBERNOULLINB, line 8). In contrast, the multinomial model estimates as the fraction of tokens or fraction of positions in documents of class that contain term (Equation 119). When classifying a test document, the Bernoulli model uses binary occurrence information, ignoring the number of occurrences, whereas the multinomial model keeps track of multiple occurrences. As a result, the Bernoulli model typically makes many mistakes when classifying long documents. For example, it may assign an entire book to the class China because of a single occurrence of the term China. The models also differ in how nonoccurring terms are used in classification. They do not affect the classification decision in the multinomial model; but in the Bernoulli model the probability of nonoccurrence is factored in when computing (Figure 13.3 , APPLYBERNOULLINB, Line 7). This is because only the Bernoulli NB model models absence of terms explicitly. Worked example. Applying the Bernoulli model to the example in Table 13.1 , we have the same estimates for the priors as before: , . The conditional probabilities are:      The denominators are and because there are three documents in and one document in and because the constant in Equation 119 is 2 - there are two cases to consider for each term, occurrence and nonoccurrence. The scores of the test document for the two classes are               End worked example. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_13_4</id>
         <title>Properties of Naive Bayes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html</url>
         <file>properties-of-naive-bayes-1.html</file>
         <text> Properties of Naive Bayes 11 12   (121)   (122)   (123)   59 59 122  We can interpret Equation 123 as a description of the generative process we assume in Bayesian text classification. To generate a document, we first choose class with probability (top nodes in and 13.5 ). The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution : (124) (125)        It should now be clearer why we introduced the document space in Equation 112 when we defined the classification problem. A critical step in solving a text classification problem is to choose the document representation. and are two different document representations. In the first case, is the set of all term sequences (or, more precisely, sequences of term tokens). In the second case, is . We cannot use and 125 for text classification directly. For the Bernoulli model, we would have to estimate different parameters, one for each possible combination of values and a class. The number of parameters in the multinomial case has the same order of magnitude.This being a very large quantity, estimating these parameters reliably is infeasible. To reduce the number of parameters, we make the Naive Bayes conditional independence assumption . We assume that attribute values are independent of each other given the class: (126) (127)                    Figure 13.4: The multinomial NB model.  Figure 13.5: The Bernoulli NB model. We illustrate the conditional independence assumption in and 13.5 . The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes. The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing. In reality, the conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. But as we will discuss shortly, NB models perform well despite the conditional independence assumption. Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position in the document. The position of a term in a document by itself does not carry information about the class. Although there is a difference between China sues France and France sues China, the occurrence of China in position 1 versus position 3 of the document is not useful in NB classification because we look at each term separately. The conditional independence assumption commits us to this way of processing the evidence. Also, if we assumed different term distributions for each position , we would have to estimate a different set of parameters for each . The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on. This again causes problems in estimation owing to data sparseness. For these reasons, we make a second independence assumption for the multinomial model, positional independence : The conditional probabilities for a term are the same independent of position in the document. (128)         bag of words 6 6.2 With conditional and positional independence assumptions, we only need to estimate parameters (multinomial model) or (Bernoulli model), one for each term-class combination, rather than a number that is at least exponential in , the size of the vocabulary. The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude. To summarize, we generate a document in the multinomial model (Figure 13.4 ) by first picking a class with where is a random variable taking values from as values. Next we generate term in position with for each of the positions of the document. The all have the same distribution over terms for a given . In the example in Figure 13.4 , we show the generation of , corresponding to the one-sentence document Beijing and Taipei join WTO. For a completely specified document generation model, we would also have to define a distribution over lengths. Without it, the multinomial model is a token generation model rather than a document generation model. We generate a document in the Bernoulli model (Figure 13.5 ) by first picking a class with and then generating a binary indicator for each term of the vocabulary ( ). In the example in Figure 13.5 , we show the generation of , corresponding, again, to the one-sentence document Beijing and Taipei join WTO where we have assumed that and is a stop word.   Table 13.3: Multinomial versus Bernoulli model.     multinomial model Bernoulli model     event model generation of token generation of document     random variable(s) iff occurs at given pos iff occurs in doc     document representation                  parameter estimation     decision rule: maximize     multiple occurrences taken into account ignored     length of docs can handle longer docs works best for short docs     # features can handle more works best with fewer     estimate for term the    We compare the two models in Table 13.3 , including estimation equations and decision rules. Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class. This is hardly ever true for terms in documents. In many cases, the opposite is true. The pairs hong and kong or london and english in Figure 13.7 are examples of highly dependent terms. In addition, the multinomial model makes an assumption of positional independence. The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence. This bag-of-words model discards all information that is communicated by the order of words in natural language sentences. How can NB be a good text classifier when its model of natural language is so oversimplified?   Table 13.4: Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation.     class selected     true probability 0.6 0.4     (Equation 126) 0.00099 0.00001       NB estimate 0.99 0.01    The answer is that even though the probability estimates of NB are of low quality, its classification decisions are surprisingly good. Consider a document with true probabilities and as shown in Table 13.4 . Assume that contains many terms that are positive indicators for and many terms that are negative indicators for . Thus, when using the multinomial model in Equation 126, will be much larger than (0.00099 vs. 0.00001 in the table). After division by 0.001 to get well-formed probabilities for , we end up with one estimate that is close to 1.0 and one that is close to 0.0. This is common: The winning class in NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. But the classification decision is based on which class gets the highest score. It does not matter how accurate the estimates are. Despite the bad estimates, NB estimates a higher probability for and therefore assigns to the correct class in Table 13.4 . Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification. It excels if there are many equally important features that jointly contribute to the classification decision. It is also somewhat robust to noise features (as defined in the next section) and concept drift - the gradual change over time of the concept underlying a class like US president from Bill Clinton to George W. Bush (see Section 13.7 ). Classifiers like kNN knn can be carefully tuned to idiosyncratic properties of a particular time period. This will then hurt them when documents in the following time period have slightly different properties. The Bernoulli model is particularly robust with respect to concept drift. We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms. The most important indicators for a class are less likely to change. Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift. NB's main strength is its efficiency: Training and classification can be accomplished with one pass over the data. Because it combines efficiency with good accuracy it is often used as a baseline in text classification research. It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited.   Table 13.5: A set of documents for which the NB independence assumptions are problematic.   (1) He moved from London, Ontario, to London, England.     (2) He moved from London, England, to London, Ontario.     (3) He moved from England to London, Ontario.    In this book, we discuss NB as a classifier for text. The independence assumptions do not hold for text. However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for data where the independence assumptions do hold.   Subsections A variant of the multinomial model</text>
         <subsections>
            <section>
              <id>iir_13_4_1</id>
              <title>A variant of the multinomial model</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/a-variant-of-the-multinomial-model-1.html</url>
              <file>a-variant-of-the-multinomial-model-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_13_5</id>
         <title>Feature selection</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/feature-selection-1.html</url>
         <file>feature-selection-1.html</file>
         <text> Feature selection Feature selection    noise feature   overfitting  Figure: Basic feature selection algorithm for selecting the best features. We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features). It may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification, but when discussing the bias-variance tradeoff in Section 14.6 (page ), we will see that weaker models are often preferable when limited training data are available. The basic feature selection algorithm is shown in Figure 13.6 . For a given class , we compute a utility measure for each term of the vocabulary and select the terms that have the highest values of . All other terms are discarded and not used in classification. We will introduce three different utility measures in this section: mutual information, ; the test, ; and frequency, . Of the two NB models, the Bernoulli model is particularly sensitive to noise features. A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low. This section mainly addresses feature selection for two-class classification tasks like China versus not-China. Section 13.5.5 briefly discusses optimizations for systems with more than two classes.   Subsections Mutual information Feature selectionChi2 Feature selection Assessing as a feature selection methodAssessing chi-square as a feature selection method Frequency-based feature selection Feature selection for multiple classifiers Comparison of feature selection methods</text>
         <subsections>
            <section>
              <id>iir_13_5_1</id>
              <title>Mutual information</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html</url>
              <file>mutual-information-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_13_5_2</id>
              <title>Feature selectionChi2 Feature selection</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html</url>
              <file>feature-selectionchi2-feature-selection-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_13_5_3</id>
              <title>Frequency-based feature selection</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/frequency-based-feature-selection-1.html</url>
              <file>frequency-based-feature-selection-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_13_5_4</id>
              <title>Feature selection for multiple classifiers</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/feature-selection-for-multiple-classifiers-1.html</url>
              <file>feature-selection-for-multiple-classifiers-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_13_5_5</id>
              <title>Comparison of feature selection methods</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/comparison-of-feature-selection-methods-1.html</url>
              <file>comparison-of-feature-selection-methods-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_13_6</id>
         <title>Evaluation of text classification</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-text-classification-1.html</url>
         <file>evaluation-of-text-classification-1.html</file>
         <text> Evaluation of text classification Historically, the classic Reuters-21578 collection was the main benchmark for text classification evaluation. This is a collection of 21,578 newswire articles, originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text classification system. It is much smaller than and predates the Reuters-RCV1 collection discussed in Chapter 4 (page 4.2 ). The articles are assigned classes from a set of 118 topic categories. A document may be assigned several classes or none, but the commonest case is single assignment (documents with at least one class received an average of 1.24 classes). The standard approach to this any-of problem (Chapter 14 , page 14.5 ) is to learn 118 two-class classifiers, one for each class, where the two-class classifier for class is the classifier for the two classes and its complement .   Table 13.7: The ten largest classes in the Reuters-21578 collection with number of documents in training and test sets.   class # train # test   class # train # test     earn 2877 1087   trade 369 119     acquisitions 1650 179   interest 347 131     money-fx 538 179   ship 197 89     grain 433 149   wheat 212 71     crude 389 189   corn 182 56    For each of these classifiers, we can measure recall, precision, and accuracy. In recent work, people almost invariably use the ModApte split , which includes only documents that were viewed and assessed by a human indexer, and comprises 9,603 training documents and 3,299 test documents. The distribution of documents in classes is very uneven, and some work evaluates systems on only documents in the ten largest classes. They are listed in Table 13.7 . A typical document with topics is shown in Figure 13.9 . In Section 13.1 , we stated as our goal in text classification the minimization of classification error on test data. Classification error is 1.0 minus classification accuracy, the proportion of correct decisions, a measure we introduced in Section 8.3 (page 8.3 ). This measure is appropriate if the percentage of documents in the class is high, perhaps 10% to 20% and higher. But as we discussed in Section 8.3 , accuracy is not a good measure for ``small'' classes because always saying no, a strategy that defeats the purpose of building a classifier, will achieve high accuracy. The always-no classifier is 99% accurate for a class with relative frequency 1%. For small classes, precision, recall and are better measures. We will use effectiveness as a generic term for measures that evaluate the quality of classification decisions, including precision, recall, , and accuracy. Performance refers to the computational efficiency of classification and IR systems in this book. However, many researchers mean effectiveness, not efficiency of text classification when they use the term performance.  Figure 13.9: A sample document from the Reuters-21578 collection. When we process a collection with several two-class classifiers (such as Reuters-21578 with its 118 classes), we often want to compute a single aggregate measure that combines the measures for individual classifiers. There are two methods for doing this. Macroaveraging computes a simple average over classes. Microaveraging pools per-document decisions across classes, and then computes an effectiveness measure on the pooled contingency table. Table 13.8 gives an example. The differences between the two methods can be large. Macroaveraging gives equal weight to each class, whereas microaveraging gives equal weight to each per-document classification decision. Because the measure ignores true negatives and its magnitude is mostly determined by the number of true positives, large classes dominate small classes in microaveraging. In the example, microaveraged precision (0.83) is much closer to the precision of (0.9) than to the precision of (0.5) because is five times larger than . Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection. To get a sense of effectiveness on small classes, you should compute macroaveraged results.   Table 13.8: Macro- and microaveraging. ``Truth'' is the true class and ``call'' the decision of the classifier. In this example, macroaveraged precision is . Microaveraged precision is .   class 1   truth: truth:   yes no call: yes 10 10 call: no 10 970 class 2   truth: truth:   yes no call: yes 90 10 call: no 10 890 pooled table   truth: truth:   yes no call: yes 100 20 call: no 20 1860      Table 13.9: Text classification effectiveness numbers on Reuters-21578 for F (in percent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais et al. (1998) (b: NB, Rocchio, trees, SVM).   (a)   NB Rocchio kNN   SVM       micro-avg-L (90 classes) 80 85 86   89       macro-avg (90 classes) 47 59 60   60             (b)   NB Rocchio kNN trees SVM     earn 96 93 97 98 98       acq 88 65 92 90 94       money-fx 57 47 78 66 75       grain 79 68 82 85 95       crude 80 70 86 85 89       trade 64 65 77 73 76       interest 65 63 74 67 78       ship 85 49 79 74 86       wheat 70 69 77 93 92       corn 65 48 78 92 90     micro-avg (top 10) 82 65 82 88 92       micro-avg-D (118 classes) 75 62 n/a n/a 87    In one-of classification (more-than-two-classes), microaveraged is the same as accuracy (Exercise 13.6 ). Table 13.9 gives microaveraged and macroaveraged effectiveness of Naive Bayes for the ModApte split of Reuters-21578. To give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15 ), one of the most effective classifiers, but also one that is more expensive to train than NB. NB has a microaveraged of 80%, which is 9% less than the SVM (89%), a 10% relative decrease (row ``micro-avg-L (90 classes)''). So there is a surprisingly small effectiveness penalty for its simplicity and efficiency. However, on small classes, some of which only have on the order of ten positive examples in the training set, NB does much worse. Its macroaveraged is 13% below the SVM, a 22% relative decrease (row ``macro-avg (90 classes)'' ). The table also compares NB with the other classifiers we cover in this book: Rocchio and kNN. In addition, we give numbers for decision trees , an important classification method we do not cover. The bottom part of the table shows that there is considerable variation from class to class. For instance, NB beats kNN on ship, but is much worse on money-fx. Comparing parts (a) and (b) of the table, one is struck by the degree to which the cited papers' results differ. This is partly due to the fact that the numbers in (b) are break-even scores (cf. page 8.4 ) averaged over 118 classes, whereas the numbers in (a) are true scores (computed without any knowledge of the test set) averaged over ninety classes. This is unfortunately typical of what happens when comparing different results in text classification: There are often differences in the experimental setup or the evaluation that complicate the interpretation of the results. These and other results have shown that the average effectiveness of NB is uncompetitive with classifiers like SVMs when trained and tested on independent and identically distributed ( i.i.d. ) data, that is, uniform data with all the good properties of statistical sampling. However, these differences may often be invisible or even reverse themselves when working in the real world where, usually, the training sample is drawn from a subset of the data to which the classifier will be applied, the nature of the data drifts over time rather than being stationary (the problem of concept drift we mentioned on page 13.4 ), and there may well be errors in the data (among other problems). Many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than NB. Our conclusion from the results in Table 13.9 is that, although most researchers believe that an SVM is better than kNN and kNN better than NB, the ranking of classifiers ultimately depends on the class, the document collection, and the experimental setup. In text classification, there is always more to know than simply which machine learning algorithm was used, as we further discuss in Section 15.3 (page ). When performing evaluations like the one in Table 13.9 , it is important to maintain a strict separation between the training set and the test set . We can easily make correct classification decisions on the test set by using information we have gleaned from the test set, such as the fact that a particular term is a good predictor in the test set (even though this is not the case in the training set). A more subtle example of using knowledge about the test set is to try a large number of values of a parameter (e.g., the number of selected features) and select the value that is best for the test set. As a rule, accuracy on new data - the type of data we will encounter when we use the classifier in an application - will be much lower than accuracy on a test set that the classifier has been tuned for. We discussed the same problem in ad hoc retrieval in Section 8.1 (page 8.1 ). In a clean statistical text classification experiment, you should never run any program on or even look at the test set while developing a text classification system. Instead, set aside a development set for testing while you develop your method. When such a set serves the primary purpose of finding a good value for a parameter, for example, the number of selected features, then it is also called held-out data . Train the classifier on the rest of the training set with different parameter values, and then select the value that gives best results on the held-out part of the training set. Ideally, at the very end, when all parameters have been set and the method is fully specified, you run one final experiment on the test set and publish the results. Because no information about the test set was used in developing the classifier, the results of this experiment should be indicative of actual performance in practice. This ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a period of several years. But it is nevertheless highly important to not look at the test data and to run systems on it as sparingly as possible. Beginners often violate this rule, and their results lose validity because they have implicitly tuned their system to the test data simply by running many variant systems and keeping the tweaks to the system that worked best on the test set. Exercises. Assume a situation where every document in the test collection has been assigned exactly one class, and that a classifier also assigns exactly one class to each document. This setup is called one-of classification more-than-two-classes. Show that in one-of classification (i) the total number of false positive decisions equals the total number of false negative decisions and (ii) microaveraged and accuracy are identical. The class priors in Figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class. Why? The function APPLYMULTINOMIALNB in Figure 13.2 has time complexity . How would you modify the function so that its time complexity is ? Table 13.10: Data for parameter estimation exercise.     docID words in document in China?     training set 1 Taipei Taiwan yes       2 Macao Taiwan Shanghai yes       3 Japan Sapporo no       4 Sapporo Osaka Taiwan no     test set 5 Taiwan Taiwan Sapporo ?   Based on the data in Table 13.10 , (i) estimate a multinomial Naive Bayes classifier, (ii) apply the classifier to the test document, (iii) estimate a Bernoulli NB classifier, (iv) apply the classifier to the test document. You need not estimate parameters that you don't need for classifying the test document. Your task is to classify words as English or not English. Words are generated by a source with the following distribution:   event word English? probability     1 ozb no 4/9     2 uzu no 4/9     3 zoo yes 1/18     4 bun yes 1/18   (i) Compute the parameters (priors and conditionals) of a multinomial NB classifier that uses the letters b, n, o, u, and z as features. Assume a training set that reflects the probability distribution of the source perfectly. Make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text classification. Compute parameters using smoothing, in which computed-zero probabilities are smoothed into probability 0.01, and computed-nonzero probabilities are untouched. (This simplistic smoothing may cause . Solutions are not required to correct this.) (ii) How does the classifier classify the word zoo? (iii) Classify the word zoo using a multinomial classifier as in part (i), but do not make the assumption of positional independence. That is, estimate separate parameters for each position in a word. You only need to compute the parameters you need for classifying zoo. What are the values of and if term and class are completely independent? What are the values if they are completely dependent? The feature selection method in Equation 130 is most appropriate for the Bernoulli model. Why? How could one modify it for the multinomial model? Features can also be selected according to information gain (IG), which is defined as: (138) where is entropy, is the training set, and , and are the subset of with term , and the subset of without term , respectively. is the class distribution in (sub)collection , e.g., if a quarter of the documents in are in class . Show that mutual information and information gain are equivalent. Show that the two formulas ( and 137 ) are equivalent. In the example on page 13.5.2 we have . Show that this holds in general. and mutual information do not distinguish between positively and negatively correlated features. Because most good text classification features are positively correlated (i.e., they occur more often in than in ), one may want to explicitly rule out the selection of negative indicators. How would you do this? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_13_7</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-13.html</url>
         <file>references-and-further-reading-13.html</file>
         <text> References and further reading General introductions to statistical classification and machine learning can be found in (Hastie et al., 2001), (Mitchell, 1997), and (Duda et al., 2000), including many important methods (e.g., decision trees and boosting ) that we do not cover. A comprehensive review of text classification methods and results is (Sebastiani, 2002). Manning and Schütze (1999, Chapter 16) give an accessible introduction to text classification with coverage of decision trees, perceptrons and maximum entropy models. More information on the superlinear time complexity of learning methods that are more accurate than Naive Bayes can be found in (Perkins et al., 2003) and (Joachims, 2006a). Maron and Kuhns (1960) described one of the first NB text classifiers. Lewis (1998) focuses on the history of NB classification. Bernoulli and multinomial models and their accuracy for different collections are discussed by McCallum and Nigam (1998). Eyheramendy et al. (2003) present additional NB models. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu (2001) analyze why NB performs well although its probability estimates are poor. The first paper also discusses NB's optimality when the independence assumptions are true of the data. Pavlov et al. (2004) propose a modified document representation that partially addresses the inappropriateness of the independence assumptions. Bennett (2000) attributes the tendency of NB probability estimates to be close to either 0 or 1 to the effect of document length. Ng and Jordan (2001) show that NB is sometimes (although rarely) superior to discriminative methods because it more quickly reaches its optimal error rate. The basic NB model presented in this chapter can be tuned for better effectiveness (Rennie et al. 2003;Kocz and Yih 2007). The problem of concept drift and other reasons why state-of-the-art classifiers do not always excel in practice are discussed by Forman (2006) and Hand (2006). Early uses of mutual information and for feature selection in text classification are Lewis and Ringuette (1994) and Schütze et al. (1995), respectively. Yang and Pedersen (1997) review feature selection methods and their impact on classification effectiveness. They find that pointwise mutual information is not competitive with other methods. Yang and Pedersen refer to expected mutual information (Equation 130) as information gain (see Exercise 13.6 , page 13.6 ). (Snedecor and Cochran, 1989) is a good reference for the test in statistics, including the Yates' correction for continuity for tables. Dunning (1993) discusses problems of the test when counts are small. Nongreedy feature selection techniques are described by Hastie et al. (2001). Cohen (1995) discusses the pitfalls of using multiple significance tests and methods to avoid them. Forman (2004) evaluates different methods for feature selection for multiple classifiers. David D. Lewis defines the ModApte split at www.daviddlewis.com/resources/testcollections/reuters21578/readme.txtbased on Apté et al. (1994). Lewis (1995) describes utility measures for the evaluation of text classification systems. Yang and Liu (1999) employ significance tests in the evaluation of text classification methods. Lewis et al. (2004) find that SVMs (Chapter 15 ) perform better on Reuters-RCV1 than kNN and Rocchio (Chapter 14 ). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_14</id>
    <title>Vector space classification</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html</url>
    <file>vector-space-classification-1.html</file>
    <text> Vector space classification The document representation in Naive Bayes is a sequence of terms or a binary vector . In this chapter we adopt a different representation for text classification, the vector space model, developed in Chapter 6 . It represents each document as a vector with one real-valued component, usually a tf-idf weight, for each term. Thus, the document space , the domain of the classification function , is . This chapter introduces a number of classification methods that operate on real-valued vectors. The basic hypothesis in using the vector space model for classification is the contiguity hypothesis . Contiguity hypothesis. Documents in the same class form a contiguous region and regions of different classes do not overlap. 13 14.1  Figure 14.1: Vector space classification into three classes. Whether or not a set of documents is mapped into a contiguous region depends on the particular choices we make for the document representation: type of weighting, stop list etc. To see that the document representation is crucial, consider the two classes written by a group vs. written by a single person. Frequent occurrence of the first person pronoun I is evidence for the single-person class. But that information is likely deleted from the document representation if we use a stop list. If the document representation chosen is unfavorable, the contiguity hypothesis will not hold and successful vector space classification is not possible. The same considerations that led us to prefer weighted representations, in particular length-normalized tf-idf representations, in Chapters 6 7 also apply here. For example, a term with 5 occurrences in a document should get a higher weight than a term with one occurrence, but a weight 5 times larger would give too much emphasis to the term. Unweighted and unnormalized counts should not be used in vector space classification. We introduce two vector space classification methods in this chapter, Rocchio and kNN. Rocchio classification (Section 14.2 ) divides the vector space into regions centered on centroids or prototypes , one for each class, computed as the center of mass of all documents in the class. Rocchio classification is simple and efficient, but inaccurate if classes are not approximately spheres with similar radii. kNN or nearest neighbor classification (Section 14.3 ) assigns the majority class of the nearest neighbors to a test document. kNN requires no explicit training and can use the unprocessed training set directly in classification. It is less efficient than other classification methods in classifying documents. If the training set is large, then kNN can handle non-spherical and other complex classes better than Rocchio. A large number of text classifiers can be viewed as linear classifiers - classifiers that classify based on a simple linear combination of the features (Section 14.4 ). Such classifiers partition the space of features into regions separated by linear decision hyperplanes , in a manner to be detailed below. Because of the bias-variance tradeoff (Section 14.6 ) more complex nonlinear models are not systematically better than linear models. Nonlinear models have more parameters to fit on a limited amount of training data and are more likely to make mistakes for small and noisy data sets. When applying two-class classifiers to problems with more than two classes, there are one-of tasks - a document must be assigned to exactly one of several mutually exclusive classes - and any-of tasks - a document can be assigned to any number of classes as we will explain in Section 14.5 . Two-class classifiers solve any-of problems and can be combined to solve one-of problems.   Subsections Document representations and measures of relatedness in vector spaces Rocchio classification k nearest neighbor Time complexity and optimality of kNN Linear versus nonlinear classifiers Classification with more than two classes The bias-variance tradeoff References and further reading Exercises</text>
    <subsections>
       <section>
         <id>iir_14_1</id>
         <title>Document representations and measures of relatedness in vector spaces</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/document-representations-and-measures-of-relatedness-in-vector-spaces-1.html</url>
         <file>document-representations-and-measures-of-relatedness-in-vector-spaces-1.html</file>
         <text> Document representations and measures of relatedness in vector spaces   As in Chapter 6 , we represent documents as vectors in in this chapter. To illustrate properties of document vectors in vector classification, we will render these vectors as points in a plane as in the example in Figure 14.1 . In reality, document vectors are length-normalized unit vectors that point to the surface of a hypersphere. We can view the 2D planes in our figures as projections onto a plane of the surface of a (hyper-)sphere as shown in Figure 14.2 . Distances on the surface of the sphere and on the projection plane are approximately the same as long as we restrict ourselves to small areas of the surface and choose an appropriate projection (Exercise 14.1 ). Decisions of many vector space classifiers are based on a notion of distance, e.g., when computing the nearest neighbors in kNN classification. We will use Euclidean distance in this chapter as the underlying distance measure. We observed earlier (Exercise 6.4.4 , page ) that there is a direct correspondence between cosine similarity and Euclidean distance for length-normalized vectors. In vector space classification, it rarely matters whether the relatedness of two documents is expressed in terms of similarity or distance. However, in addition to documents, centroids or averages of vectors also play an important role in vector space classification. Centroids are not length-normalized. For unnormalized vectors, dot product, cosine similarity and Euclidean distance all have different behavior in general (Exercise 14.8 ). We will be mostly concerned with small local regions when computing the similarity between a document and a centroid, and the smaller the region the more similar the behavior of the three measures is. Exercises. For small areas, distances on the surface of the hypersphere are approximated well by distances on its projection (Figure 14.2 ) because for small angles. For what size angle is the distortion (i) 1.01, (ii) 1.05 and (iii) 1.1?</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_2</id>
         <title>Rocchio classification</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/rocchio-classification-1.html</url>
         <file>rocchio-classification-1.html</file>
         <text> Rocchio classification 14.1  decision boundaries  Figure 14.3: Rocchio classification. Perhaps the best-known way of computing good class boundaries is Rocchio classification , which uses centroids to define the boundaries. The centroid of a class is computed as the vector average or center of mass of its members:     (139)          25 6.3.1 14.3 The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids. For example, , , and in the figure. This set of points is always a line. The generalization of a line in -dimensional space is a hyperplane, which we define as the set of points that satisfy:     (140)      normal vector     Thus, the boundaries of class regions in Rocchio classification are hyperplanes. The classification rule in Rocchio is to classify a point in accordance with the region it falls into. Equivalently, we determine the centroid that the point is closest to and then assign it to . As an example, consider the star in Figure 14.3 . It is located in the China region of the space and Rocchio therefore assigns it to China. We show the Rocchio algorithm in pseudocode in Figure 14.4 .   Table 14.1: Vectors and class centroids for the data in Table 13.1 .   term weights vector Chinese Japan Tokyo Macao Beijing Shanghai 0 0 0 0 1.0 0 0 0 0 0 0 1.0 0 0 0 1.0 0 0 0 0.71 0.71 0 0 0 0 0.71 0.71 0 0 0 0 0 0 0.33 0.33 0.33 0 0.71 0.71 0 0 0   Worked example. Table 14.1 shows the tf-idf vector representations of the five documents in Table 13.1 (page 13.1 ), using the formula if (Equation 29, page 6.4.1 ). The two class centroids are and . The distances of the test document from the centroids are and . Thus, Rocchio assigns to . The separating hyperplane in this case has the following parameters:     14.8            End worked example. The assignment criterion in Figure 14.4 is Euclidean distance (APPLYROCCHIO, line 1). An alternative is cosine similarity: (141)  14.1  16.4 16.4  Figure 14.4: Rocchio classification: Training and testing. Rocchio classification is a form of Rocchio relevance feedback (Section 9.1.1 , page 9.1.1 ). The average of the relevant documents, corresponding to the most important component of the Rocchio vector in relevance feedback (Equation 49, page 49 ), is the centroid of the ``class'' of relevant documents. We omit the query component of the Rocchio formula in Rocchio classification since there is no query in text classification. Rocchio classification can be applied to classes whereas Rocchio relevance feedback is designed to distinguish only two classes, relevant and nonrelevant. In addition to respecting contiguity, the classes in Rocchio classification must be approximate spheres with similar radii. In Figure 14.3 , the solid square just below the boundary between UK and Kenya is a better fit for the class UK since UK is more scattered than Kenya. But Rocchio assigns it to Kenya because it ignores details of the distribution of points in a class and only uses distance from the centroid for classification.   The assumption of sphericity also does not hold in Figure 14.5 . We cannot represent the ``a'' class well with a single prototype because it has two clusters. Rocchio often misclassifies this type of multimodal class . A text classification example for multimodality is a country like Burma, which changed its name to Myanmar in 1989. The two clusters before and after the name change need not be close to each other in space. We also encountered the problem of multimodality in relevance feedback (Section 9.1.2 , page 9.1.3 ). Two-class classification is another case where classes are rarely distributed like spheres with similar radii. Most two-class classifiers distinguish between a class like China that occupies a small region of the space and its widely scattered complement. Assuming equal radii will result in a large number of false positives. Most two-class classification problems therefore require a modified decision rule of the form: (142)        mode time complexity training testing Training and test times for Rocchio classification. is the average number of tokens per document. and are the numbers of tokens and types, respectively, in the test document. Computing Euclidean distance between the class centroids and a document is .  Table 14.2 gives the time complexity of Rocchio classification. Adding all documents to their respective (unnormalized) centroid is (as opposed to ) since we need only consider non-zero entries. Dividing each vector sum by the size of its class to compute the centroid is . Overall, training time is linear in the size of the collection (cf. Exercise 13.2.1 ). Thus, Rocchio classification and Naive Bayes have the same linear training time complexity. In the next section, we will introduce another vector space classification method, kNN, that deals better with classes that have non-spherical, disconnected or other irregular shapes.   Exercises. Show that Rocchio classification can assign a label to a document that is different from its training set label. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_3</id>
         <title>k nearest neighbor</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/k-nearest-neighbor-1.html</url>
         <file>k-nearest-neighbor-1.html</file>
         <text> k nearest neighbor Unlike Rocchio, nearest neighbor or kNN classification determines the decision boundary locally. For 1NN we assign each document to the class of its closest neighbor. For kNN we assign each document to the majority class of its closest neighbors where is a parameter. The rationale of kNN classification is that, based on the contiguity hypothesis, we expect a test document to have the same label as the training documents located in the local region surrounding . Decision boundaries in 1NN are concatenated segments of the Voronoi tessellation as shown in Figure 14.6 . The Voronoi tessellation of a set of objects decomposes space into Voronoi cells, where each object's cell consists of all points that are closer to the object than to other objects. In our case, the objects are documents. The Voronoi tessellation then partitions the plane into convex polygons, each containing its corresponding document (and no other) as shown in Figure 14.6 , where a convex polygon is a convex region in 2-dimensional space bounded by lines. For general in kNN, consider the region in the space for which the set of nearest neighbors is the same. This again is a convex polygon and the space is partitioned into convex polygons , within each of which the set of nearest neighbors is invariant (Exercise 14.8 ). 1NN is not very robust. The classification decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical. kNN for is more robust. It assigns documents to the majority class of their closest neighbors, with ties broken randomly. There is a probabilistic version of this kNN classification algorithm. We can estimate the probability of membership in class as the proportion of the nearest neighbors in . Figure 14.6 gives an example for . Probability estimates for class membership of the star are , , and . The 3nn estimate ( ) and the 1nn estimate ( ) differ with 3nn preferring the X class and 1nn preferring the circle class . The parameter in kNN is often chosen based on experience or knowledge about the classification problem at hand. It is desirable for to be odd to make ties less likely. and are common choices, but much larger values between 50 and 100 are also used. An alternative way of setting the parameter is to select the that gives best results on a held-out portion of the training set.   We can also weight the ``votes'' of the nearest neighbors by their cosine similarity. In this scheme, a class's score is computed as: (143)         Figure 14.7 summarizes the kNN algorithm. Worked example. The distances of the test document from the four training documents in Table 14.1 are and . 's nearest neighbor is therefore and 1NN assigns to 's class, . End worked example.   Subsections Time complexity and optimality of kNN</text>
         <subsections>
            <section>
              <id>iir_14_3_1</id>
              <title>Time complexity and optimality of kNN</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-and-optimality-of-knn-1.html</url>
              <file>time-complexity-and-optimality-of-knn-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_14_4</id>
         <title>Linear versus nonlinear classifiers</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/linear-versus-nonlinear-classifiers-1.html</url>
         <file>linear-versus-nonlinear-classifiers-1.html</file>
         <text> Linear versus nonlinear classifiers In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers. To simplify the discussion, we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class membership by comparing a linear combination of the features to a threshold.  Figure 14.8: There are an infinite number of hyperplanes that separate two linearly separable classes. In two dimensions, a linear classifier is a line. Five examples are shown in Figure 14.8 . These lines have the functional form . The classification rule of a linear classifier is to assign a document to if and to if . Here, is the two-dimensional vector representation of the document and is the parameter vector that defines (together with ) the decision boundary. An alternative geometric interpretation of a linear classifier is provided in Figure 15.7 (page ). We can generalize this 2D linear classifier to higher dimensions by defining a hyperplane as we did in Equation 140, repeated here as Equation 144: (144)        decision hyperplane  Figure 14.9: Linear classification algorithm. The corresponding algorithm for linear classification in dimensions is shown in Figure 14.9 . Linear classification at first seems trivial given the simplicity of this algorithm. However, the difficulty is in training the linear classifier, that is, in determining the parameters and based on the training set. In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data. We now show that Rocchio and Naive Bayes are linear classifiers. To see this for Rocchio, observe that a vector is on the decision boundary if it has equal distance to the two class centroids:     (145)     14.8 We can derive the linearity of Naive Bayes from its decision rule, which chooses the category with the largest (Figure 13.2 , page 13.2 ) where: (146)         (147)   We choose class if the odds are greater than 1 or, equivalently, if the log odds are greater than 0. It is easy to see that Equation 147 is an instance of Equation 144 for , number of occurrences of in , and . Here, the index , , refers to terms of the vocabulary (not to positions in as does; cf. variantmultinomial) and and are -dimensional vectors. So in log space, Naive Bayes is a linear classifier.   prime 0.70 0 1 dlrs -0.71 1 1 rate 0.67 1 0 world -0.35 1 0 interest 0.63 0 0 sees -0.33 0 0 rates 0.60 0 0 year -0.25 0 0 discount 0.46 1 0 group -0.24 0 0 bundesbank 0.43 0 0 dlr -0.24 0 0 A linear classifier. The dimensions and parameters of a linear classifier for the class interest (as in interest rate) in Reuters-21578. The threshold is . Terms like dlr and world have negative weights because they are indicators for the competing class currency.  Worked example. Table 14.4 defines a linear classifier for the category interest in Reuters-21578 (see Section 13.6 , page 13.6 ). We assign document ``rate discount dlrs world'' to interest since . We assign ``prime dlrs'' to the complement class (not in interest) since . For simplicity, we assume a simple binary vector representation in this example: 1 for occurring terms, 0 for non-occurring terms. End worked example.  A linear problem with noise. In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese-English web pages are squares. The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows). Figure 14.10 is a graphical example of a linear problem, which we define to mean that the underlying distributions and of the two classes are separated by a line. We call this separating line the class boundary . It is the ``true'' boundary of the two classes and we distinguish it from the decision boundary that the learning method computes to approximate the class boundary. As is typical in text classification, there are some noise documents in Figure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes. In Section 13.5 (page 13.5 ), we defined a noise feature as a misleading feature that, when included in the document representation, on average increases the classification error. Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error. Intuitively, the underlying distribution partitions the representation space into areas with mostly homogeneous class assignments. A document that does not conform with the dominant class in its area is a noise document. Noise documents are one reason why training a linear classifier is hard. If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data. More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading. If there exists a hyperplane that perfectly separates the two classes, then we call the two classes linearly separable . In fact, if linear separability holds, then there is an infinite number of linear separators (Exercise 14.4 ) as illustrated by Figure 14.8 , where the number of possible separating hyperplanes is infinite. Figure 14.8 illustrates another challenge in training a linear classifier. If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data. In general, some of these hyperplanes will do well on new data, some will not.  Figure 14.11: A nonlinear problem. An example of a nonlinear classifier is kNN. The nonlinearity of kNN is intuitively clear when looking at examples like Figure 14.6 . The decision boundaries of kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions. Figure 14.11 is another example of a nonlinear problem: there is no good linear separator between the distributions and because of the circular ``enclave'' in the upper left part of the graph. Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough. If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier. Exercises. Prove that the number of linear separators of two classes is either infinite or zero.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_5</id>
         <title>Classification with more than two classes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/classification-with-more-than-two-classes-1.html</url>
         <file>classification-with-more-than-two-classes-1.html</file>
         <text> Classification with more than two classes  Classification for classes that are not mutually exclusive is called any-of , multilabel , or multivalue classification . In this case, a document can belong to several classes simultaneously, or to a single class, or to none of the classes. A decision on one class leaves all options open for the others. It is sometimes said that the classes are independent of each other, but this is misleading since the classes are rarely statistically independent in the sense defined on page 13.5.2 . In terms of the formal definition of the classification problem in Equation 112 (page 112 ), we learn different classifiers in any-of classification, each returning either or : . Solving an any-of classification task with linear classifiers is straightforward: Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. The decision of one classifier has no influence on the decisions of the other classifiers. The second type of classification with more than two classes is one-of classification . Here, the classes are mutually exclusive. Each document must belong to exactly one of the classes. One-of classification is also called multinomial , polytomous , multiclass , or single-label classification . Formally, there is a single classification function in one-of classification whose range is , i.e., . kNN is a (nonlinear) one-of classifier. True one-of problems are less common in text classification than any-of problems. With classes like UK, China, poultry, or coffee, a document can be relevant to many topics simultaneously - as when the prime minister of the UK visits China to talk about the coffee and poultry trade. Nevertheless, we will often make a one-of assumption, as we did in Figure 14.1 , even if classes are not really mutually exclusive. For the classification problem of identifying the language of a document, the one-of assumption is a good approximation as most text is written in only one language. In such cases, imposing a one-of constraint can increase the classifier's effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated.  Figure 14.12: hyperplanes do not divide space into disjoint regions.    14.12   Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). Given the test document, apply each classifier separately. Assign the document to the class with the maximum score, the maximum confidence value, or the maximum probability.     assigned class money-fx trade interest wheat corn grain true class               money-fx   95 0 10 0 0 0 trade   1 1 90 0 1 0 interest   13 0 0 0 0 0 wheat   0 0 1 34 3 7 corn   1 0 2 13 26 5 grain   0 0 2 14 5 10 A confusion matrix for Reuters-21578.For example, 14 documents from grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006).  An important tool for analyzing the performance of a classifier for classes is the confusion matrix . The confusion matrix shows for each pair of classes , how many documents from were incorrectly assigned to . In Table 14.5 , the classifier manages to distinguish the three financial classes money-fx, trade, and interest from the three agricultural classes wheat, corn, and grain, but makes many errors within these two groups. The confusion matrix can help pinpoint opportunities for improving the accuracy of the system. For example, to address the second largest error in Table 14.5 (14 in the row grain), one could attempt to introduce features that distinguish wheat documents from grain documents. Exercises. Create a training set of 300 documents, 100 each from three different languages (e.g., English, French, Spanish). Create a test set by the same procedure, but also add 100 documents from a fourth language. Train (i) a one-of classifier (ii) an any-of classifier on this training set and evaluate it on the test set. (iii) Are there any interesting differences in how the two classifiers behave on this task? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_6</id>
         <title>The bias-variance tradeoff</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-bias-variance-tradeoff-1.html</url>
         <file>the-bias-variance-tradeoff-1.html</file>
         <text> The bias-variance tradeoff Nonlinear classifiers are more powerful than linear classifiers. For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier. Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification? To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning. The tradeoff helps explain why there is no universally optimal learning method. Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem. Throughout this section, we use linear and nonlinear classifiers as prototypical examples of ``less powerful'' and ``more powerful'' learning, respectively. This is a simplification for a number of reasons. First, many nonlinear models subsume linear models as a special case. For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier. Second, there are nonlinear models that are less complex than linear models. For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier. Third, the complexity of learning is not really a property of the classifier because there are many aspects of learning (such as feature selection, cf. feature, regularization, and constraints such as margin maximization in Chapter 15 ) that make a learning method either more powerful or less powerful without affecting the type of classifier that is the final result of learning - regardless of whether that classifier is linear or nonlinear. We refer the reader to the publications listed in Section 14.7 for a treatment of the bias-variance tradeoff that takes into account these complexities. In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification. We first need to state our objective in text classification more precisely. In Section 13.1 (page ), we said that we want to minimize classification error on the test set. The implicit assumption was that training documents and test documents are generated according to the same underlying distribution. We will denote this distribution where is the document and its label or class. graphclassmodelbernoulligraph were examples of generative models that decompose into the product of and . typicallineartypicalnonlinear depict generative models for with and . In this section, instead of using the number of correctly classified test documents (or, equivalently, the error rate on test documents) as evaluation measure, we adopt an evaluation measure that addresses the inherent uncertainty of labeling. In many text classification problems, a given document representation can arise from documents belonging to different classes. This is because documents from different classes can be mapped to the same document representation. For example, the one-sentence documents China sues France and France sues China are mapped to the same document representation in a bag of words model. But only the latter document is relevant to the class legal actions brought by France (which might be defined, for example, as a standing query by an international trade lawyer). To simplify the calculations in this section, we do not count the number of errors on the test set when evaluating a classifier, but instead look at how well the classifier estimates the conditional probability of a document being in a class. In the above example, we might have . Our goal in text classification then is to find a classifier such that, averaged over documents , is as close as possible to the true probability . We measure this using mean squared error:     (148)      We define a classifier to be optimal for a distribution if it minimizes . Minimizing MSE is a desideratum for classifiers. We also need a criterion for learning methods. Recall that we defined a learning method as a function that takes a labeled training set as input and returns a classifier . For learning methods, we adopt as our goal to find a that, averaged over training sets, learns classifiers with minimal MSE. We can formalize this as minimizing learning error :     (149)      We can use learning error as a criterion for selecting a learning method in statistical text classification. A learning method is optimal for a distribution if it minimizes the learning error.  (150)   (151)     (152)   (153)     (154)   (155)       (156) (157)   (158)     (159)   157   150   Writing for for better readability, we can transform Equation 149 as follows: (160)   (161)   (162) (163) (164)   162 157 14.6       Bias is the squared difference between , the true conditional probability of being in , and , the prediction of the learned classifier, averaged over training sets. Bias is large if the learning method produces classifiers that are consistently wrong. Bias is small if (i) the classifiers are consistently right or (ii) different training sets cause errors on different documents or (iii) different training sets cause positive and negative errors on the same documents, but that average out to close to 0. If one of these three conditions holds, then , the expectation over all training sets, is close to . Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary, a linear hyperplane. If the generative model has a complex nonlinear class boundary, the bias term in Equation 162 will be high because a large number of points will be consistently misclassified. For example, the circular enclave in Figure 14.11 does not fit a linear model and will be misclassified consistently by linear classifiers. We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier. If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method. But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average. Nonlinear methods like kNN have low bias. We can see in Figure 14.6 that the decision boundaries of kNN are variable - depending on the distribution of documents in the training set, learned decision boundaries can vary greatly. As a result, each document has a chance of being classified correctly for some training sets. The average prediction is therefore closer to and bias is smaller than for a linear learning method. Variance is the variation of the prediction of learned classifiers: the average squared difference between and its average . Variance is large if different training sets give rise to very different classifiers . It is small if the training set has a minor effect on the classification decisions makes, be they correct or incorrect. Variance measures how inconsistent the decisions are, not whether they are correct or incorrect. Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes. The decision lines produced by linear learning methods in and 14.11 will deviate slightly from the main class boundaries, depending on the training set, but the class assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected. The circular enclave in Figure 14.11 will be consistently misclassified. Nonlinear methods like kNN have high variance. It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes. It is therefore sensitive to noise documents of the sort depicted in Figure 14.10 . As a result the variance term in Equation 162 is large for kNN: Test documents are sometimes misclassified - if they happen to be close to a noise document in the training set - and sometimes correctly classified - if there are no noise documents in the training set near them. This results in high variation from training set to training set. High-variance learning methods are prone to overfitting the training data. The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution . In overfitting, the learning method also learns from noise. Overfitting increases MSE and frequently is a problem for high-variance learning methods. We can also think of variance as the model complexity or, equivalently, memory capacity of the learning method - how detailed a characterization of the training set it can remember and then apply to new data. This capacity corresponds to the number of independent parameters available to fit the training set. Each kNN neighborhood makes an independent classification decision. The parameter in this case is the estimate from Figure 14.7 . Thus, kNN's capacity is only limited by the size of the training set. It can memorize arbitrarily large training sets. In contrast, the number of parameters of Rocchio is fixed - parameters per dimension, one for each centroid - and independent of the size of the training set. The Rocchio classifier (in form of the centroids defining it) cannot ``remember'' fine-grained details of the distribution of the documents in the training set. According to Equation 149, our goal in selecting a learning method is to minimize learning error. The fundamental insight captured by Equation 162, which we can succinctly state as: learning-error = bias + variance, is that the learning error has two components, bias and variance, which in general cannot be minimized simultaneously. When comparing two learning methods and , in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance. The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias). Instead, we have to weigh the respective merits of bias and variance in our application and choose accordingly. This tradeoff is called the bias-variance tradeoff . Figure 14.10 provides an illustration, which is somewhat contrived, but will be useful as an example for the tradeoff. Some Chinese text contains English words written in the Roman alphabet like CPU, ONLINE, and GPS. Consider the task of distinguishing Chinese-only web pages from mixed Chinese-English web pages. A search engine might offer Chinese users without knowledge of English (but who understand loanwords like CPU) the option of filtering out mixed pages. We use two features for this classification task: number of Roman alphabet characters and number of Chinese characters on the web page. As stated earlier, the distribution ) of the generative model generates most mixed (respectively, Chinese) documents above (respectively, below) the short-dashed line, but there are a few noise documents. In Figure 14.10 , we see three classifiers: One-feature classifier. Shown as a dotted horizontal line. This classifier uses only one feature, the number of Roman alphabet characters. Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents). So a learning method producing this type of classifier has low variance. But its bias is high since it will consistently misclassify squares in the lower left corner and ``solid circle'' documents with more than 50 Roman characters. Linear classifier. Shown as a dashed line with long dashes. Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified. The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets. Thus, very few documents (documents close to the class boundary) will be inconsistently classified. ``Fit-training-set-perfectly'' classifier. Shown as a solid line. Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set. This method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test set right. But the variance of this learning method is high. Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified - something that a linear learning method is unlikely to do. It is perhaps surprising that so many of the best-known text classification algorithms are linear. Some of these methods, in particular linear SVMs, regularized logistic regression and regularized linear regression, are among the most effective known methods. The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.8 ). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_7</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-14.html</url>
         <file>references-and-further-reading-14.html</file>
         <text> References and further reading As discussed in Chapter 9 , Rocchio relevance feedback is due to Rocchio (1971). Joachims (1997) presents a probabilistic analysis of the method. Rocchio classification was widely used as a classification method in in the 1990s (Buckley et al., 1994b;a, Voorhees and Harman, 2005). Initially, it was used as a form of routing . Routing merely ranks documents according to relevance to a class without assigning them. Early work on filtering , a true classification approach that makes an assignment decision on each document, was published by Ittner et al. (1995) and Schapire et al. (1998). The definition of routing we use here should not be confused with another sense. Routing can also refer to the electronic distribution of documents to subscribers, the so-called push model of document distribution. In a pull model , each transfer of a document to the user is initiated by the user - for example, by means of search or by selecting it from a list of documents on a news aggregation website. Some authors restrict the name Roccchio classification to two-class problems and use the terms cluster-based (Iwayama and Tokunaga, 1995) and centroid-based classification (Han and Karypis, 2000, Tan and Cheng, 2007) for Rocchio classification with . A more detailed treatment of kNN can be found in (Hastie et al., 2001), including methods for tuning the parameter . An example of an approximate fast kNN algorithm is locality-based hashing (Andoni et al., 2006). Kleinberg (1997) presents an approximate kNN algorithm (where is the dimensionality of the space and the number of data points), but at the cost of exponential storage requirements: . Indyk (2004) surveys nearest neighbor methods in high-dimensional spaces. Early work on kNN in text classification was motivated by the availability of massively parallel hardware architectures (Creecy et al., 1992). Yang (1994) uses an inverted index to speed up kNN classification. The optimality result for 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart (1967). The effectiveness of Rocchio classification and kNN is highly dependent on careful parameter tuning (in particular, the parameters for Rocchio on page 14.2 and for kNN), feature engineering svm-text and feature selection feature. Buckley and Salton (1995), Yang and Kisiel (2003), Schapire et al. (1998) and Moschitti (2003) address these issues for Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al. (2000) compare feature selection methods for kNN. The bias-variance tradeoff was introduced by Geman et al. (1992). The derivation in Section 14.6 is for , but the tradeoff applies to many loss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995) and Lewis et al. (1996) discuss linear classifiers for text and Hastie et al. (2001) linear classifiers in general. Readers interested in the algorithms mentioned, but not described in this chapter may wish to consult Bishop (2006) for neural networks, Hastie et al. (2001) for linear and logistic regression, and Minsky and Papert (1988) for the perceptron algorithm . Anagnostopoulos et al. (2006) show that an inverted index can be used for highly efficient document classification with any linear classifier, provided that the classifier is still effective when trained on a modest number of features via feature selection. We have only presented the simplest method for combining two-class classifiers into a one-of classifier. Another important method is the use of error-correcting codes, where a vector of decisions of different two-class classifiers is constructed for each document. A test document's decision vector is then ``corrected'' based on the distribution of decision vectors in the training set, a procedure that incorporates information from all two-class classifiers and their correlations into the final classification decision (Dietterich and Bakiri, 1995). Ghamrawi and McCallum (2005) also exploit dependencies between classes in any-of classification. Allwein et al. (2000) propose a general framework for combining two-class classifiers. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_14_8</id>
         <title>Exercises</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/exercises-2.html</url>
         <file>exercises-2.html</file>
         <text> Exercises   Exercises. In Figure 14.13 , which of the three vectors , , and is (i) most similar to according to dot product similarity, (ii) most similar to according to cosine similarity, (iii) closest to according to Euclidean distance? Download Reuters-21578 and train and test Rocchio and kNN classifiers for the classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Use the ModApte split. You may want to use one of a number of software packages that implement Rocchio classification and kNN classification, for example, the Bow toolkit (McCallum, 1996). Download 20 Newgroups (page 8.2 ) and train and test Rocchio and kNN classifiers for its 20 classes. Show that the decision boundaries in Rocchio classification are, as in kNN, given by the Voronoi tessellation. Computing the distance between a dense centroid and a sparse vector is for a naive implementation that iterates over all dimensions. Based on the equality and assuming that has been precomputed, write down an algorithm that is instead, where is the number of distinct terms in the test document. Prove that the region of the plane consisting of all points with the same nearest neighbors is a convex polygon. Design an algorithm that performs an efficient 1NN search in 1 dimension (where efficiency is with respect to the number of documents ). What is the time complexity of the algorithm? Design an algorithm that performs an efficient 1NN search in 2 dimensions with at most polynomial (in ) preprocessing time. Can one design an exact efficient algorithm for 1NN for very large along the ideas you used to solve the last exercise? Show that Equation 145 defines a hyperplane with and . Figure 14.14: A simple non-separable set of points. We can easily construct non-separable data sets in high dimensions by embedding a non-separable set like the one shown in Figure 14.14 . Consider embedding Figure 14.14 in 3D and then perturbing the 4 points slightly (i.e., moving them a small distance in a random direction). Why would you expect the resulting configuration to be linearly separable? How likely is then a non-separable set of points in -dimensional space? Assuming two classes, show that the percentage of non-separable assignments of the vertices of a hypercube decreases with dimensionality for . For example, for the proportion of non-separable assignments is 0, for , it is . One of the two non-separable cases for is shown in Figure 14.14 , the other is its mirror image. Solve the exercise either analytically or by simulation. Although we point out the similarities of Naive Bayes with linear vector space classifiers, it does not make sense to represent count vectors (the document representations in NB) in a continuous vector space. There is however a formalization of NB that is analogous to Rocchio. Show that NB assigns a document to the class (represented as a parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4 , page 12.4 ) to the document (represented as a count vector as in Section 13.4.1 (page ), normalized to sum to 1) is smallest. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_15</id>
    <title>Support vector machines and machine learning on documents</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-and-machine-learning-on-documents-1.html</url>
    <file>support-vector-machines-and-machine-learning-on-documents-1.html</file>
    <text> Support vector machines and machine learning on documents Improving classifier effectiveness has been an area of intensive machine-learning research over the last two decades, and this work has led to a new generation of state-of-the-art classifiers, such as support vector machines, boosted decision trees, regularized logistic regression, neural networks, and random forests. Many of these methods, including support vector machines (SVMs), the main topic of this chapter, have been applied with success to information retrieval problems, particularly text classification. An SVM is a kind of large-margin classifier: it is a vector space based machine learning method where the goal is to find a decision boundary between two classes that is maximally far from any point in the training data (possibly discounting some points as outliers or noise). We will initially motivate and develop SVMs for the case of two-class data sets that are separable by a linear classifier (Section 15.1 ), and then extend the model in Section 15.2 to non-separable data, multi-class problems, and nonlinear models, and also present some additional discussion of SVM performance. The chapter then moves to consider the practical deployment of text classifiers in Section 15.3 : what sorts of classifiers are appropriate when, and how can you exploit domain-specific text features in classification? Finally, we will consider how the machine learning technology that we have been building for text classification can be applied back to the problem of learning how to rank documents in ad hoc retrieval (Section 15.4 ). While several machine learning methods have been applied to this task, use of SVMs has been prominent. Support vector machines are not necessarily better than other machine learning methods (except perhaps in situations with little training data), but they perform at the state-of-the-art level and have much current theoretical and empirical appeal.   Subsections Support vector machines: The linearly separable case Extensions to the SVM model Soft margin classification Multiclass SVMs Nonlinear SVMs Experimental results Issues in the classification of text documents Choosing what kind of classifier to use Improving classifier performance Large and difficult category taxonomies Features for text Document zones in text classification Upweighting document zones. Separate feature spaces for document zones. Connections to text summarization. Machine learning methods in ad hoc information retrieval A simple example of machine-learned scoring Result ranking by machine learning References and further reading</text>
    <subsections>
       <section>
         <id>iir_15_1</id>
         <title>Support vector machines: The linearly separable case</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html</url>
         <file>support-vector-machines-the-linearly-separable-case-1.html</file>
         <text> Support vector machines: The linearly separable case  Figure 15.1: The support vectors are the 5 points right up against the margin of the classifier. For two-class, separable training data sets, such as the one in Figure 14.8 (page ), there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the perceptron algorithm (see references in vclassfurther) find just any linear separator, others, like Naive Bayes, search for the best linear separator according to some criterion. The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision surface to the closest data point determines the margin of the classifier. This method of construction necessarily means that the decision function for an SVM is fully specified by a (usually small) subset of the data which defines the position of the separator. These points are referred to as the support vectors (in a vector space, a point can be thought of as a vector between the origin and that point). Figure 15.1 shows the margin and support vectors for a sample problem. Other data points play no part in determining the decision surface that is chosen.  An intuition for large-margin classification.Insisting on a large margin reduces the capacity of the model: the range of angles at which the fat decision surface can be placed is smaller than for a decision hyperplane (cf. vclassline). Maximizing the margin seems good because points near the decision surface represent very uncertain classification decisions: there is almost a 50% chance of the classifier deciding either way. A classifier with a large margin makes no low certainty classification decisions. This gives you a classification safety margin: a slight error in measurement or a slight document variation will not cause a misclassification. Another intuition motivating SVMs is shown in Figure 15.2 . By construction, an SVM classifier insists on a large margin around the decision boundary. Compared to a decision hyperplane, if you have to place a fat separator between classes, you have fewer choices of where it can be put. As a result of this, the memory capacity of the model has been decreased, and hence we expect that its ability to correctly generalize to test data is increased (cf. the discussion of the bias-variance tradeoff in Chapter 14 , page 14.6 ). Let us formalize an SVM with algebra. A decision hyperplane (page 14.4 ) can be defined by an intercept term and a decision hyperplane normal vector which is perpendicular to the hyperplane. This vector is commonly referred to in the machine learning literature as the weight vector . To choose among all the hyperplanes that are perpendicular to the normal vector, we specify the intercept term . Because the hyperplane is perpendicular to the normal vector, all points on the hyperplane satisfy . Now suppose that we have a set of training data points , where each member is a pair of a point and a class label corresponding to it.For SVMs, the two data classes are always named and (rather than 1 and 0), and the intercept term is always explicitly represented as (rather than being folded into the weight vector by adding an extra always-on feature). The math works out much more cleanly if you do things this way, as we will see almost immediately in the definition of functional margin. The linear classifier is then: (165)    We are confident in the classification of a point if it is far away from the decision boundary. For a given data set and decision hyperplane, we define the functional margin of the example with respect to a hyperplane as the quantity . The functional margin of a data set with respect to a decision surface is then twice the functional margin of any of the points in the data set with minimal functional margin (the factor of 2 comes from measuring across the whole width of the margin, as in Figure 15.3 ). However, there is a problem with using this definition as is: the value is underconstrained, because we can always make the functional margin as big as we wish by simply scaling up and . For example, if we replace by and by then the functional margin is five times as large. This suggests that we need to place some constraint on the size of the vector. To get a sense of how to do that, let us look at the actual geometry.  Figure 15.3: The geometric margin of a point () and a decision boundary (). What is the Euclidean distance from a point to the decision boundary? In Figure 15.3 , we denote by this distance. We know that the shortest distance between a point and a hyperplane is perpendicular to the plane, and hence, parallel to . A unit vector in this direction is . The dotted line in the diagram is then a translation of the vector . Let us label the point on the hyperplane closest to as . Then: (166)       (167)     (168)   geometric margin  168 15.2       6  Since we can scale the functional margin as we please, for convenience in solving large SVMs, let us choose to require that the functional margin of all data points is at least 1 and that it is equal to 1 for at least one data vector. That is, for all items in the data: (169)      is maximized For all ,      We are now optimizing a quadratic function subject to linear constraints. Quadratic optimization problems are a standard, well-known class of mathematical optimization problems, and many algorithms exist for solving them. We could in principle build our SVM using standard quadratic programming (QP) libraries, but there has been much recent research in this area aiming to exploit the structure of the kind of QP that emerges from an SVM. As a result, there are more intricate but much faster and more scalable libraries available especially for building SVMs, which almost everyone uses to build models. We will not present the details of such algorithms here. However, it will be helpful to what follows to understand the shape of the solution of such an optimization problem. The solution involves constructing a dual problem where a Lagrange multiplier is associated with each constraint in the primal problem: The solution is then of the form: In the solution, most of the are zero. Each non-zero indicates that the corresponding is a support vector. The classification function is then: (170)  dot product     To recap, we start with a training data set. The data set uniquely defines the best separating hyperplane, and we feed the data through a quadratic optimization procedure to find this plane. Given a new point to classify, the classification function in either Equation 165 or Equation 170 is computing the projection of the point onto the hyperplane normal. The sign of this function determines the class to assign to the point. If the point is within the margin of the classifier (or another confidence threshold that we might have determined to minimize classification mistakes) then the classifier can return ``don't know'' rather than one of the two classes. The value of may also be transformed into a probability of classification; fitting a sigmoid to transform the values is standard (Platt, 2000). Also, since the margin is constant, if the model includes dimensions from various sources, careful rescaling of some dimensions may be required. However, this is not a problem if our documents (points) are on the unit hypersphere.  Figure 15.4: A tiny 3 data point training set for an SVM. Worked example. Consider building an SVM over the (very little) data set shown in Figure 15.4 . Working geometrically, for an example like this, the maximum margin weight vector will be parallel to the shortest line connecting points of the two classes, that is, the line between and , giving a weight vector of . The optimal decision surface is orthogonal to that line and intersects it at the halfway point. Therefore, it passes through . So, the SVM decision boundary is: (171)  Working algebraically, with the standard constraint that , we seek to minimize . This happens when this constraint is satisfied with equality by the two support vectors. Further we know that the solution is for some . So we have that:         The margin is . This answer can be confirmed geometrically by examining Figure 15.4 . End worked example. Exercises. What is the minimum number of support vectors that there can be for a data set (which contains instances of each class)? The basis of being able to use kernels in SVMs (see Section 15.2.3 ) is that the classification function can be written in the form of Equation 170 (where, for large problems, most are 0). Show explicitly how the classification function could be written in this form for the data set from small-svm-eg. That is, write as a function where the data points appear and the only variable is . Install an SVM package such as SVMlight (http://svmlight.joachims.org/), and build an SVM for the data set discussed in small-svm-eg. Confirm that the program gives the same solution as the text. For SVMlight, or another package that accepts the same training data format, the training file would be: 1 1:2 2:3 1 1:2 2:0 1 1:1 2:1 The training command for SVMlight is then: svm_learn -c 1 -a alphas.dat train.dat model.dat The -c 1 option is needed to turn off use of the slack variables that we discuss in Section 15.2.1 . Check that the norm of the weight vector agrees with what we found in small-svm-eg. Examine the file alphas.dat which contains the values, and check that they agree with your answers in Exercise 15.1 . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_15_2</id>
         <title>Extensions to the SVM model</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/extensions-to-the-svm-model-1.html</url>
         <file>extensions-to-the-svm-model-1.html</file>
         <text> Extensions to the SVM model   Subsections Soft margin classification Multiclass SVMs Nonlinear SVMs Experimental results </text>
         <subsections>
            <section>
              <id>iir_15_2_1</id>
              <title>Soft margin classification</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/soft-margin-classification-1.html</url>
              <file>soft-margin-classification-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_15_2_2</id>
              <title>Multiclass SVMs</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html</url>
              <file>multiclass-svms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_15_2_3</id>
              <title>Nonlinear SVMs</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/nonlinear-svms-1.html</url>
              <file>nonlinear-svms-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_15_2_4</id>
              <title>Experimental results</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/experimental-results-1.html</url>
              <file>experimental-results-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_15_3</id>
         <title>Issues in the classification of text documents</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/issues-in-the-classification-of-text-documents-1.html</url>
         <file>issues-in-the-classification-of-text-documents-1.html</file>
         <text> Issues in the classification of text documents There are lots of applications of text classification in the commercial world; email spam filtering is perhaps now the most ubiquitous. Jackson and Moulinier (2002) write: ``There is no question concerning the commercial value of being able to classify documents automatically by content. There are myriad potential applications of such a capability for corporate Intranets, government departments, and Internet publishers.'' Most of our discussion of classification has focused on introducing various machine learning methods rather than discussing particular features of text documents relevant to classification. This bias is appropriate for a textbook, but is misplaced for an application developer. It is frequently the case that greater performance gains can be achieved from exploiting domain-specific text features than from changing from one machine learning method to another. Jackson and Moulinier (2002) suggest that ``Understanding the data is one of the keys to successful categorization, yet this is an area in which most categorization tool vendors are extremely weak. Many of the `one size fits all' tools on the market have not been tested on a wide range of content types.'' In this section we wish to step back a little and consider the applications of text classification, the space of possible solutions, and the utility of application-specific heuristics.   Subsections Choosing what kind of classifier to use Improving classifier performance Large and difficult category taxonomies Features for text Document zones in text classification Upweighting document zones. Separate feature spaces for document zones. Connections to text summarization.</text>
         <subsections>
            <section>
              <id>iir_15_3_1</id>
              <title>Choosing what kind of classifier to use</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/choosing-what-kind-of-classifier-to-use-1.html</url>
              <file>choosing-what-kind-of-classifier-to-use-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_15_3_2</id>
              <title>Improving classifier performance</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/improving-classifier-performance-1.html</url>
              <file>improving-classifier-performance-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_15_4</id>
         <title>Machine learning methods in ad hoc information retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/machine-learning-methods-in-ad-hoc-information-retrieval-1.html</url>
         <file>machine-learning-methods-in-ad-hoc-information-retrieval-1.html</file>
         <text> Machine learning methods in ad hoc information retrieval Rather than coming up with term and document weighting functions by hand, as we primarily did in Chapter 6 , we can view different sources of relevance signal (cosine score, title match, etc.) as features in a learning problem. A classifier that has been fed examples of relevant and nonrelevant documents for each of a set of queries can then figure out the relative weights of these signals. If we configure the problem so that there are pairs of a document and a query which are assigned a relevance judgment of relevant or nonrelevant, then we can think of this problem too as a text classification problem. Taking such a classification approach is not necessarily best, and we present an alternative in Section 15.4.2 . Nevertheless, given the material we have covered, the simplest place to start is to approach this problem as a classification problem, by ordering the documents according to the confidence of a two-class classifier in its relevance decision. And this move is not purely pedagogical; exactly this approach is sometimes used in practice.   Subsections A simple example of machine-learned scoring Result ranking by machine learning</text>
         <subsections>
            <section>
              <id>iir_15_4_1</id>
              <title>A simple example of machine-learned scoring</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/a-simple-example-of-machine-learned-scoring-1.html</url>
              <file>a-simple-example-of-machine-learned-scoring-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_15_4_2</id>
              <title>Result ranking by machine learning</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/result-ranking-by-machine-learning-1.html</url>
              <file>result-ranking-by-machine-learning-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_15_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html</url>
         <file>references-and-further-reading-15.html</file>
         <text> References and further reading The somewhat quirky name support vector machine originates in the neural networks literature, where learning algorithms were thought of as architectures, and often referred to as ``machines''. The distinctive element of this model is that the decision boundary to use is completely decided (``supported'') by a few training data points, the support vectors. For a more detailed presentation of SVMs, a good, well-known article-length introduction is (Burges, 1998). Chen et al. (2005) introduce the more recent -SVM, which provides an alternative parameterization for dealing with inseparable problems, whereby rather than specifying a penalty , you specify a parameter which bounds the number of examples which can appear on the wrong side of the decision surface. There are now also several books dedicated to SVMs, large margin learning, and kernels: (Cristianini and Shawe-Taylor, 2000) and (Schölkopf and Smola, 2001) are more mathematically oriented, while (Shawe-Taylor and Cristianini, 2004) aims to be more practical. For the foundations by their originator, see (Vapnik, 1998). Some recent, more general books on statistical learning, such as (Hastie et al., 2001) also give thorough coverage of SVMs. The construction of multiclass SVMs is discussed in (Weston and Watkins, 1999), (Crammer and Singer, 2001), and (Tsochantaridis et al., 2005). The last reference provides an introduction to the general framework of structural SVMs. The kernel trick was first presented in (Aizerman et al., 1964). For more about string kernels and other kernels for structured data, see (Lodhi et al., 2002) and (Gaertner et al., 2002). The Advances in Neural Information Processing (NIPS) conferences have become the premier venue for theoretical machine learning work, such as on SVMs. Other venues such as SIGIR are much stronger on experimental methodology and using text-specific features to improve classifier effectiveness. A recent comparison of most current machine learning classifiers (though on problems rather different from typical text problems) can be found in (Caruana and Niculescu-Mizil, 2006). (Li and Yang, 2003), discussed in Section 13.6 , is the most recent comparative evaluation of machine learning classifiers on text classification. Older examinations of classifiers on text problems can be found in (Yang and Liu, 1999, Dumais et al., 1998, Yang, 1999). Joachims (2002a) presents his work on SVMs applied to text problems in detail. Zhang and Oles (2001) present an insightful comparison of Naive Bayes, regularized logistic regression and SVM classifiers. Joachims (1999) discusses methods of making SVM learning practical over large text data sets. Joachims (2006a) improves on this work. A number of approaches to hierarchical classification have been developed in order to deal with the common situation where the classes to be assigned have a natural hierarchical organization (Weigend et al., 1999, Dumais and Chen, 2000, Koller and Sahami, 1997, McCallum et al., 1998). In a recent large study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005) conclude that hierarchical classification noticeably if still modestly outperforms flat classification. Classifier effectiveness remains limited by the very small number of training documents for many classes. For a more general approach that can be applied to modeling relations between classes, which may be arbitrary rather than simply the case of a hierarchy, see Tsochantaridis et al. (2005). Moschitti and Basili (2004) investigate the use of complex nominals, proper nouns and word senses as features in text classification. Dietterich (2002) overviews ensemble methods for classifier combination, while Schapire (2003) focuses particularly on boosting, which is applied to text classification in (Schapire and Singer, 2000). Chapelle et al. (2006) present an introduction to work in semi-supervised methods, including in particular chapters on using EM for semi-supervised text classification (Nigam et al., 2006) and on transductive SVMs (Joachims, 2006b). Sindhwani and Keerthi (2006) present a more efficient implementation of a transductive SVM for large data sets. Tong and Koller (2001) explore active learning with SVMs for text classification; Baldridge and Osborne (2004) point out that examples selected for annotation with one classifier in an active learning context may be no better than random examples when used with another classifier. Machine learning approaches to ranking for ad hoc retrieval were pioneered in (Wong et al., 1988), (Fuhr, 1992), and (Gey, 1994). But limited training data and poor machine learning techniques meant that these pieces of work achieved only middling results, and hence they only had limited impact at the time. Taylor et al. (2006) study using machine learning to tune the parameters of the BM25 family of ranking functions okapi-bm25 so as to maximize NDCG (Section 8.4 , page 8.4 ). Machine learning approaches to ordinal regression appear in (Herbrich et al., 2000) and (Burges et al., 2005), and are applied to clickstream data in (Joachims, 2002b). Cao et al. (2006) study how to make this approach effective in IR, and Qin et al. (2007) suggest an extension involving using multiple hyperplanes. Yue et al. (2007) study how to do ranking with a structural SVM approach, and in particular show how this construction can be effectively used to directly optimize for MAP ranked-evaluation, rather than using surrogate measures like accuracy or area under the ROC curve. Geng et al. (2007) study feature selection for the ranking problem. Other approaches to learning to rank have also been shown to be effective for web search, such as (Richardson et al., 2006, Burges et al., 2005). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_16</id>
    <title>Flat clustering</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html</url>
    <file>flat-clustering-1.html</file>
    <text> Flat clustering Clustering algorithms group a set of documents into subsets or clusters . The algorithms' goal is to create clusters that are coherent internally, but clearly different from each other. In other words, documents within a cluster should be as similar as possible; and documents in one cluster should be as dissimilar as possible from documents in other clusters.  Figure 16.1: An example of a data set with a clear cluster structure. Clustering is the most common form of unsupervised learning . No supervision means that there is no human expert who has assigned documents to classes. In clustering, it is the distribution and makeup of the data that will determine cluster membership. A simple example is Figure 16.1 . It is visually clear that there are three distinct clusters of points. This chapter and Chapter 17 introduce algorithms that find such clusters in an unsupervised fashion. The difference between clustering and classification may not seem great at first. After all, in both cases we have a partition of a set of documents into groups. But as we will see the two problems are fundamentally different. Classification is a form of supervised learning (Chapter 13 , page 13.1 ): our goal is to replicate a categorical distinction that a human supervisor imposes on the data. In unsupervised learning, of which clustering is the most important example, we have no such teacher to guide us. The key input to a clustering algorithm is the distance measure. In Figure 16.1 , the distance measure is distance in the 2D plane. This measure suggests three different clusters in the figure. In document clustering, the distance measure is often also Euclidean distance. Different distance measures give rise to different clusterings. Thus, the distance measure is an important means by which we can influence the outcome of clustering. Flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters and will be covered in Chapter 17 . Chapter 17 also addresses the difficult problem of labeling clusters automatically. A second important distinction can be made between hard and soft clustering algorithms. Hard clustering computes a hard assignment - each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft - a document's assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Latent semantic indexing, a form of dimensionality reduction, is a soft clustering algorithm (Chapter 18 , page 18.4 ). This chapter motivates the use of clustering in information retrieval by introducing a number of applications (Section 16.1 ), defines the problem we are trying to solve in clustering (Section 16.2 ) and discusses measures for evaluating cluster quality (Section 16.3 ). It then describes two flat clustering algorithms, -means (Section 16.4 ), a hard clustering algorithm, and the Expectation-Maximization (or EM) algorithm (Section 16.5 ), a soft clustering algorithm. -means is perhaps the most widely used flat clustering algorithm due to its simplicity and efficiency. The EM algorithm is a generalization of -means and can be applied to a large variety of document representations and distributions.   Subsections Clustering in information retrieval Problem statement A note on terminology. Cardinality - the number of clusters Evaluation of clustering K-means Cluster cardinality in K-means Model-based clustering References and further reading Exercises</text>
    <subsections>
       <section>
         <id>iir_16_1</id>
         <title>Clustering in information retrieval</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/clustering-in-information-retrieval-1.html</url>
         <file>clustering-in-information-retrieval-1.html</file>
         <text> Clustering in information retrieval  cluster hypothesis Cluster hypothesis. Documents in the same cluster behave similarly with respect to relevance to information needs. 14 14   Table 16.1: Some applications of clustering in information retrieval. Application What is Benefit Example   clustered?     Search result clustering search results more effective information presentation to user Figure 16.2 Scatter-Gather (subsets of) collection alternative user interface: ``search without typing'' Figure 16.3 Collection clustering collection effective information presentation for exploratory browsing McKeown et al. (2002), http://news.google.com Language modeling collection increased precision and/or recall Liu and Croft (2004) Cluster-based retrieval collection higher efficiency: faster search Salton (1971a)   Table 16.1 shows some of the main applications of clustering in information retrieval. They differ in the set of documents that they cluster - search results, collection or subsets of the collection - and the aspect of an information retrieval system they try to improve - user experience, user interface, effectiveness or efficiency of the search system. But they are all based on the basic assumption stated by the cluster hypothesis.  Clustering of search results to improve recall. None of the top hits cover the animal sense of jaguar, but users can easily access it by clicking on the cat cluster in the Clustered Results panel on the left (third arrow from the top). The first application mentioned in Table 16.1 is search result clustering where by search results we mean the documents that were returned in response to a query. The default presentation of search results in information retrieval is a simple list. Users scan the list from top to bottom until they have found the information they are looking for. Instead, search result clustering clusters the search results, so that similar documents appear together. It is often easier to scan a few coherent groups than many individual documents. This is particularly useful if a search term has different word senses. The example in Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the animal and an Apple operating system. The Clustered Results panel returned by the Vivísimo search engine (http://vivisimo.com) can be a more effective user interface for understanding what is in the search results than a simple list of documents.  An example of a user session in Scatter-Gather. A collection of New York Times news stories is clustered (``scattered'') into eight clusters (top row). The user manually gathers three of these into a smaller collection International Stories and performs another scattering operation. This process repeats until a small cluster with relevant documents is found (e.g., Trinidad). A better user interface is also the goal of Scatter-Gather , the second application in Table 16.1 . Scatter-Gather clusters the whole collection to get groups of documents that the user can select or gather. The selected groups are merged and the resulting set is again clustered. This process is repeated until a cluster of interest is found. An example is shown in Figure 16.3 . Automatically generated clusters like those in Figure 16.3 are not as neatly organized as a manually constructed hierarchical tree like the Open Directory at http://dmoz.org. Also, finding descriptive labels for clusters automatically is a difficult problem (Section 17.7 , page 17.7 ). But cluster-based navigation is an interesting alternative to keyword searching, the standard information retrieval paradigm. This is especially true in scenarios where users prefer browsing over searching because they are unsure about which search terms to use. As an alternative to the user-mediated iterative clustering in Scatter-Gather, we can also compute a static hierarchical clustering of a collection that is not influenced by user interactions (``Collection clustering'' in Table 16.1 ). Google News and its precursor, the Columbia NewsBlaster system, are examples of this approach. In the case of news, we need to frequently recompute the clustering to make sure that users can access the latest breaking stories. Clustering is well suited for access to a collection of news stories since news reading is not really search, but rather a process of selecting a subset of stories about recent events. The fourth application of clustering exploits the cluster hypothesis directly for improving search results, based on a clustering of the entire collection. We use a standard inverted index to identify an initial set of documents that match the query, but we then add other documents from the same clusters even if they have low similarity to the query. For example, if the query is car and several car documents are taken from a cluster of automobile documents, then we can add documents from this cluster that use terms other than car (automobile, vehicle etc). This can increase recall since a group of documents with high mutual similarity is often relevant as a whole. More recently this idea has been used for language modeling. Equation 102 , page 102 , showed that to avoid sparse data problems in the language modeling approach to IR, the model of document can be interpolated with a collection model. But the collection contains many documents with terms untypical of . By replacing the collection model with a model derived from 's cluster, we get more accurate estimates of the occurrence probabilities of terms in . Clustering can also speed up search. As we saw in Section 6.3.2 ( page 6.3.2 ) search in the vector space model amounts to finding the nearest neighbors to the query. The inverted index supports fast nearest-neighbor search for the standard IR setting. However, sometimes we may not be able to use an inverted index efficiently, e.g., in latent semantic indexing (Chapter 18 ). In such cases, we could compute the similarity of the query to every document, but this is slow. The cluster hypothesis offers an alternative: Find the clusters that are closest to the query and only consider documents from these clusters. Within this much smaller set, we can compute similarities exhaustively and rank documents in the usual way. Since there are many fewer clusters than documents, finding the closest cluster is fast; and since the documents matching a query are all similar to each other, they tend to be in the same clusters. While this algorithm is inexact, the expected decrease in search quality is small. This is essentially the application of clustering that was covered in Section 7.1.6 (page 7.1.6 ). Exercises. Define two documents as similar if they have at least two proper names like Clinton or Sarkozy in common. Give an example of an information need and two documents, for which the cluster hypothesis does not hold for this notion of similarity. Make up a simple one-dimensional example (i.e. points on a line) with two clusters where the inexactness of cluster-based retrieval shows up. In your example, retrieving clusters close to the query should do worse than direct nearest neighbor search.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_16_2</id>
         <title>Problem statement</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/problem-statement-1.html</url>
         <file>problem-statement-1.html</file>
         <text> Problem statement     objective function    The objective function is often defined in terms of similarity or distance between documents. Below, we will see that the objective in -means clustering is to minimize the average distance between documents and their centroids or, equivalently, to maximize the similarity between documents and their centroids. The discussion of similarity measures and distance metrics in Chapter 14 (page 14.1 ) also applies to this chapter. As in Chapter 14 , we use both similarity and distance to talk about relatedness between documents. For documents, the type of similarity we want is usually topic similarity or high values on the same dimensions in the vector space model. For example, documents about China have high values on dimensions like Chinese, Beijing, and Mao whereas documents about the UK tend to have high values for London, Britain and Queen. We approximate topic similarity with cosine similarity or Euclidean distance in vector space (Chapter 6 ). If we intend to capture similarity of a type other than topic, for example, similarity of language, then a different representation may be appropriate. When computing topic similarity, stop words can be safely ignored, but they are important cues for separating clusters of English (in which the occurs frequently and la infrequently) and French documents (in which the occurs infrequently and la frequently).   Subsections A note on terminology. Cardinality - the number of clusters</text>
         <subsections>
            <section>
              <id>iir_16_2_1</id>
              <title>Cardinality - the number of clusters</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/cardinality---the-number-of-clusters-1.html</url>
              <file>cardinality---the-number-of-clusters-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_16_3</id>
         <title>Evaluation of clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</url>
         <file>evaluation-of-clustering-1.html</file>
         <text> Evaluation of clustering Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low inter-cluster similarity (documents from different clusters are dissimilar). This is an internal criterion for the quality of a clustering. But good scores on an internal criterion do not necessarily translate into good effectiveness in an application. An alternative to internal criteria is direct evaluation in the application of interest. For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms. This is the most direct evaluation, but it is expensive, especially if large user studies are necessary. As a surrogate for user judgments, we can use a set of classes in an evaluation benchmark or gold standard (see Section 8.5 , page 8.5 , and Section 13.6 , page 13.6 ). The gold standard is ideally produced by human judges with a good level of inter-judge agreement (see Chapter 8 , page 8.1 ). We can then compute an external criterion that evaluates how well the clustering matches the gold standard classes. For example, we may want to say that the optimal clustering of the search results for jaguar in Figure 16.2 consists of three classes corresponding to the three senses car, animal, and operating system. In this type of evaluation, we only use the partition provided by the gold standard, not the class labels. This section introduces four external criteria of clustering quality. Purity is a simple and transparent evaluation measure. Normalized mutual information can be information-theoretically interpreted. The Rand index penalizes both false positive and false negative decisions during clustering. The F measure in addition supports differential weighting of these two types of errors.   To compute purity , each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by . Formally: (182)          182 We present an example of how to compute purity in Figure 16.4 . Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1 . Purity is compared with the other three measures discussed in this chapter in Table 16.2 .   Table 16.2: The four external evaluation measures applied to the clustering in Figure 16.4 .   purity NMI RI lower bound 0.0 0.0 0.0 0.0 maximum 1 1 1 1 value for Figure 16.4 0.71 0.36 0.68 0.46   High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. A measure that allows us to make this tradeoff is normalized mutual information or NMI : (183)    13 13.5.1  (184)   (185)          185 184 is entropy as defined in Chapter 5 (page 5.3.2 ): (186)   (187)   in Equation 184 measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are. The minimum of is 0 if the clustering is random with respect to class membership. In that case, knowing that a document is in a particular cluster does not give us any new information about what its class might be. Maximum mutual information is reached for a clustering that perfectly recreates the classes - but also if clusters in are further subdivided into smaller clusters (Exercise 16.7 ). In particular, a clustering with one-document clusters has maximum MI. So MI has the same problem as purity: it does not penalize large cardinalities and thus does not formalize our bias that, other things being equal, fewer clusters are better. The normalization by the denominator in Equation 183 fixes this problem since entropy tends to increase with the number of clusters. For example, reaches its maximum for , which ensures that NMI is low for . Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters. The particular form of the denominator is chosen because is a tight upper bound on (Exercise 16.7 ). Thus, NMI is always a number between 0 and 1. An alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions, one for each of the pairs of documents in the collection. We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters. The Rand index ( ) measures the percentage of decisions that are correct. That is, it is simply accuracy (Section 8.3 , page 8.3 ).     As an example, we compute RI for Figure 16.4 . We first compute . The three clusters contain 6, 6, and 5 points, respectively, so the total number of ``positives'' or pairs of documents that are in the same cluster is: (188)    (189)   and are computed similarly, resulting in the following contingency table:   Same cluster Different clusters Same class Different classes   The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the F measure measuresperf to penalize false negatives more strongly than false positives by selecting a value , thus giving more weight to recall.            Exercises. Replace every point in Figure 16.4 with two identical copies of in the same class. (i) Is it less difficult, equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in Figure 16.4 ? (ii) Compute purity, NMI, RI, and for the clustering with 34 points. Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_16_4</id>
         <title>K-means</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html</url>
         <file>k-means-1.html</file>
         <text> K-means  6 6.4.4  centroid    (190)  The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way. We used centroids for Rocchio classification in Chapter 14 (page 14.2 ). They play a similar role here. The ideal cluster in -means is a sphere with the centroid as its center of gravity. Ideally, the clusters should not overlap. Our desiderata for classes in Rocchio classification were the same. The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster. A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS , the squared distance of each vector from its centroid summed over all vectors:          (191)    objective function          seeds 16.5 16.6  17.2 17.2 We can apply one of the following termination conditions. A fixed number of iterations has been completed. This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations. Assignment of documents to clusters (the partitioning function ) does not change between iterations. Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long. Centroids do not change between iterations. This is equivalent to not changing (Exercise 16.4.1 ). Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. In practice, we need to combine it with a bound on the number of iterations to guarantee termination. Terminate when the decrease in RSS falls below a threshold . For small , this indicates that we are close to convergence. Again, we need to combine it with a bound on the number of iterations to prevent very long runtimes. We now show that -means converges by proving that monotonically decreases in each iteration. We will use decrease in the meaning decrease or does not change in this section. First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to decreases. Second, it decreases in the recomputation step because the new centroid is the vector for which reaches its minimum. (192) (193)           (194)      Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum. Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids. Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost.   While this proves the convergence of -means, there is unfortunately no guarantee that a global minimum in the objective function will be reached. This is a particular problem if a document set contains many outliers , documents that are far from any other documents and therefore do not fit well into any cluster. Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations. Thus, we end up with a singleton cluster (a cluster with only one document) even though there is probably a clustering with lower RSS. Figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds. Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise 16.7 ). Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering. Since deterministic hierarchical clustering methods are more predictable than -means, a hierarchical clustering of a small random sample of size (e.g., for or ) often provides good seeds (see the description of the Buckshot algorithm, Chapter 17 , page 17.8 ). Other initialization methods compute seeds that are not selected from the vectors to be clustered. A robust method that works well for a large variety of document distributions is to select (e.g., ) random vectors for each cluster and use their centroid as the seed for this cluster. See Section 16.6 for more sophisticated initializations. What is the time complexity of -means? Most of the time is spent on computing vector distances. One such operation costs . The reassignment step computes distances, so its overall complexity is . In the recomputation step, each vector gets added to a centroid once, so the complexity of this step is . For a fixed number of iterations , the overall complexity is therefore . Thus, -means is linear in all relevant factors: iterations, number of clusters, number of vectors and dimensionality of the space. This means that -means is more efficient than the hierarchical algorithms in Chapter 17 . We had to fix the number of iterations , which can be tricky in practice. But in most cases, -means quickly reaches either complete convergence or a clustering that is close to convergence. In the latter case, a few documents would switch membership if further iterations were computed, but this has a small effect on the overall quality of the clustering. There is one subtlety in the preceding argument. Even a linear algorithm can be quite slow if one of the arguments of is large, and usually is large. High dimensionality is not a problem for computing the distance between two documents. Their vectors are sparse, so that only a small fraction of the theoretically possible componentwise differences need to be computed. Centroids, however, are dense since they pool all terms that occur in any of the documents of their clusters. As a result, distance computations are time consuming in a naive implementation of -means. However, there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities. Truncating centroids to the most significant terms (e.g., ) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in Section 16.6 ). The same efficiency problem is addressed by K-medoids , a variant of -means that computes medoids instead of centroids as cluster centers. We define the medoid of a cluster as the document vector that is closest to the centroid. Since medoids are sparse document vectors, distance computations are fast.  Estimated minimal residual sum of squares as a function of the number of clusters in -means. In this clustering of 1203 Reuters-RCV1 documents, there are two points where the curve flattens: at 4 clusters and at 9 clusters. The documents were selected from the categories China, Germany, Russia and Sports, so the clustering is closest to the Reuters classification.   Subsections Cluster cardinality in K-means</text>
         <subsections>
            <section>
              <id>iir_16_4_1</id>
              <title>Cluster cardinality in K-means</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/cluster-cardinality-in-k-means-1.html</url>
              <file>cluster-cardinality-in-k-means-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_16_5</id>
         <title>Model-based clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/model-based-clustering-1.html</url>
         <file>model-based-clustering-1.html</file>
         <text> Model-based clustering In this section, we describe a generalization of -means, the EM algorithm. It can be applied to a larger variety of document representations and distributions than -means. In -means, we attempt to find centroids that are good representatives. We can view the set of centroids as a model that generates the data. Generating a document in this model consists of first picking a centroid at random and then adding some noise. If the noise is normally distributed, this procedure will result in clusters of spherical shape. Model-based clustering assumes that the data were generated by a model and tries to recover the original model from the data. The model that we recover from the data then defines clusters and an assignment of documents to clusters. A commonly used criterion for estimating the model parameters is maximum likelihood. In -means, the quantity is proportional to the likelihood that a particular model (i.e., a set of centroids) generated the data. For -means, maximum likelihood and minimal RSS are equivalent criteria. We denote the model parameters by . In -means, . More generally, the maximum likelihood criterion is to select the parameters that maximize the log-likelihood of generating the data : (198)    This is the same approach we took in Chapter 12 (page 12.1.1 ) for language modeling and in Section 13.1 (page 13.4 ) for text classification. In text classification, we chose the class that maximizes the likelihood of generating a particular document. Here, we choose the clustering that maximizes the likelihood of generating a given set of documents. Once we have , we can compute an assignment probability for each document-cluster pair. This set of assignment probabilities defines a soft clustering. An example of a soft assignment is that a document about Chinese cars may have a fractional membership of 0.5 in each of the two clusters China and automobiles, reflecting the fact that both topics are pertinent. A hard clustering like -means cannot model this simultaneous relevance to two topics. Model-based clustering provides a framework for incorporating our knowledge about a domain. -means and the hierarchical algorithms in Chapter 17 make fairly rigid assumptions about the data. For example, clusters in -means are assumed to be spheres. Model-based clustering offers more flexibility. The clustering model can be adapted to what we know about the underlying distribution of the data, be it Bernoulli (as in the example in Table 16.3 ), Gaussian with non-spherical variance (another model that is important in document clustering) or a member of a different family. A commonly used algorithm for model-based clustering is the Expectation-Maximization algorithm or EM algorithm . EM clustering is an iterative algorithm that maximizes . EM can be applied to many different types of probabilistic modeling. We will work with a mixture of multivariate Bernoulli distributions here, the distribution we know from Section 11.3 (page 11.3 ) and Section 13.3 (page 13.3 ):     (199)               The mixture model then is:     (200)       How do we use EM to infer the parameters of the clustering from the data? That is, how do we choose parameters that maximize ? EM is similar to -means in that it alternates between an expectation step , corresponding to reassignment, and a maximization step , corresponding to recomputation of the parameters of the model. The parameters of -means are the centroids, the parameters of the instance of EM in this section are the and . The maximization step recomputes the conditional parameters and the priors as follows:      (201)        13.3 13.3 The expectation step computes the soft assignment of documents to clusters given the current parameters and :      (202)   200   13.3   (a) docID document text docID document text   1 hot chocolate cocoa beans 7 sweet sugar   2 cocoa ghana africa 8 sugar cane brazil   3 beans harvest ghana 9 sweet sugar beet   4 cocoa butter 10 sweet cake icing   5 butter truffles 11 cake black forest   6 sweet chocolate     (b) Parameter Iteration of clustering     0 1 2 3 4 5 15 25     0.50 0.45 0.53 0.57 0.58 0.54 0.45     1.00 1.00 1.00 1.00 1.00 1.00 1.00     0.50 0.79 0.99 1.00 1.00 1.00 1.00     0.50 0.84 1.00 1.00 1.00 1.00 1.00     0.50 0.75 0.94 1.00 1.00 1.00 1.00     0.50 0.52 0.66 0.91 1.00 1.00 1.00   1.00 1.00 1.00 1.00 1.00 1.00 0.83 0.00   0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.00 0.00 0.00 0.00 0.00 0.00 0.00     0.50 0.40 0.14 0.01 0.00 0.00 0.00     0.50 0.57 0.58 0.41 0.07 0.00 0.00     0.000 0.100 0.134 0.158 0.158 0.169 0.200     0.000 0.083 0.042 0.001 0.000 0.000 0.000     0.000 0.000 0.000 0.000 0.000 0.000 0.000     0.000 0.167 0.195 0.213 0.214 0.196 0.167     0.000 0.400 0.432 0.465 0.474 0.508 0.600     0.000 0.167 0.090 0.014 0.001 0.000 0.000     0.000 0.000 0.000 0.000 0.000 0.000 0.000     1.000 0.500 0.585 0.640 0.642 0.589 0.500     1.000 0.300 0.238 0.180 0.159 0.153 0.000     1.000 0.417 0.507 0.610 0.640 0.608 0.667 The EM clustering algorithm.The table shows a set of documents (a) and parameter values for selected iterations during EM clustering (b). Parameters shown are prior , soft assignment scores (both omitted for cluster 2), and lexical parameters for a few terms. The authors initially assigned document 6 to cluster 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For smoothing, the in Equation 201 were replaced with where .  We clustered a set of 11 documents into two clusters using EM in Table 16.3 . After convergence in iteration 25, the first 5 documents are assigned to cluster 1 ( ) and the last 6 to cluster 2 (). Somewhat atypically, the final assignment is a hard assignment here. EM usually converges to a soft assignment. In iteration 25, the prior for cluster 1 is because 5 of the 11 documents are in cluster 1. Some terms are quickly associated with one cluster because the initial assignment can ``spread'' to them unambiguously. For example, membership in cluster 2 spreads from document 7 to document 8 in the first iteration because they share sugar ( in iteration 1). For parameters of terms occurring in ambiguous contexts, convergence takes longer. Seed documents 6 and 7 both contain sweet. As a result, it takes 25 iterations for the term to be unambiguously associated with cluster 2. ( in iteration 25.) Finding good seeds is even more critical for EM than for -means. EM is prone to get stuck in local optima if the seeds are not chosen well. This is a general problem that also occurs in other applications of EM.Therefore, as with -means, the initial assignment of documents to clusters is often computed by a different algorithm. For example, a hard -means clustering may provide the initial assignment, which EM can then ``soften up.'' Exercises. We saw above that the time complexity of -means is . What is the time complexity of EM?</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_16_6</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-16.html</url>
         <file>references-and-further-reading-16.html</file>
         <text> References and further reading Berkhin (2006b) gives a general up-to-date survey of clustering methods with special attention to scalability. The classic reference for clustering in pattern recognition, covering both -means and EM, is (Duda et al., 2000). Rasmussen (1992) introduces clustering from an information retrieval perspective. Anderberg (1973) provides a general introduction to clustering for applications. In addition to Euclidean distance and cosine similarity , Kullback-Leibler divergence is often used in clustering as a measure of how (dis)similar documents and clusters are (Xu and Croft, 1999, Muresan and Harper, 2004, Kurland and Lee, 2004). The cluster hypothesis is due to Jardine and van Rijsbergen (1971) who state it as follows: Associations between documents convey information about the relevance of documents to requests. Croft (1978), Can and Ozkarahan (1990), Voorhees (1985a), Salton (1975), Cacheda et al. (2003), Salton (1971a), Singitham et al. (2004), Can et al. (2004) and Altingövde et al. (2008) investigate the efficiency and effectiveness of cluster-based retrieval. While some of these studies show improvements in effectiveness, efficiency or both, there is no consensus that cluster-based retrieval works well consistently across scenarios. Cluster-based language modeling was pioneered by Liu and Croft (2004). There is good evidence that clustering of search results improves user experience and search result quality (Hearst and Pedersen, 1996, Zamir and Etzioni, 1999, Käki, 2005, Toda and Kataoka, 2005, Tombros et al., 2002), although not as much as search result structuring based on carefully edited category hierarchies (Hearst, 2006). The Scatter-Gather interface for browsing collections was presented by Cutting et al. (1992). A theoretical framework for analyzing the properties of Scatter/Gather and other information seeking user interfaces is presented by Pirolli (2007). Schütze and Silverstein (1997) evaluate LSI (Chapter 18 ) and truncated representations of centroids for efficient -means clustering. The Columbia NewsBlaster system (McKeown et al., 2002), a forerunner to the now much more famous and refined Google News (http://news.google.com), used hierarchical clustering (Chapter 17 ) to give two levels of news topic granularity. See Hatzivassiloglou et al. (2000) for details, and Chen and Lin (2000) and Radev et al. (2001) for related systems. Other applications of clustering in information retrieval are duplicate detection (Yang and Callan (2006), shingling), novelty detection (see references in hclstfurther) and metadata discovery on the semantic web (Alonso et al., 2006). The discussion of external evaluation measures is partially based on Strehl (2002). Dom (2002) proposes a measure that is better motivated theoretically than NMI. is the number of bits needed to transmit class memberships assuming cluster memberships are known. The Rand index is due to Rand (1971). Hubert and Arabie (1985) propose an adjusted that ranges between and 1 and is 0 if there is only chance agreement between clusters and classes (similar to in Chapter 8 , page 8.2 ). Basu et al. (2004) argue that the three evaluation measures NMI, Rand index and F measure give very similar results. Stein et al. (2003) propose expected edge density as an internal measure and give evidence that it is a good predictor of the quality of a clustering. Kleinberg (2002) and Meila (2005) present axiomatic frameworks for comparing clusterings. Authors that are often credited with the invention of the -means algorithm include Lloyd (1982) (first distributed in 1957), Ball (1965), MacQueen (1967), and Hartigan and Wong (1979). Arthur and Vassilvitskii (2006) investigate the worst-case complexity of -means. Bradley and Fayyad (1998), Pelleg and Moore (1999) and Davidson and Satyanarayana (2003) investigate the convergence properties of -means empirically and how it depends on initial seed selection. Dhillon and Modha (2001) compare -means clusters with SVD -based clusters (Chapter 18 ). The K-medoid algorithm was presented by Kaufman and Rousseeuw (1990). The EM algorithm was originally introduced by Dempster et al. (1977). An in-depth treatment of EM is (McLachlan and Krishnan, 1996). See Section 18.5 (page ) for publications on latent analysis, which can also be viewed as soft clustering. AIC is due to Akaike (1974) (see also Burnham and Anderson (2002)). An alternative to AIC is BIC, which can be motivated as a Bayesian model selection procedure (Schwarz, 1978). Fraley and Raftery (1998) show how to choose an optimal number of clusters based on BIC. An application of BIC to -means is (Pelleg and Moore, 2000). Hamerly and Elkan (2003) propose an alternative to BIC that performs better in their experiments. Another influential Bayesian approach for determining the number of clusters (simultaneously with cluster assignment) is described by Cheeseman and Stutz (1996). Two methods for determining cardinality without external criteria are presented by Tibshirani et al. (2001). We only have space here for classical completely unsupervised clustering. An important current topic of research is how to use prior knowledge to guide clustering (e.g., Ji and Xu (2006)) and how to incorporate interactive feedback during clustering (e.g., Huang and Mitchell (2006)). Fayyad et al. (1998) propose an initialization for EM clustering. For algorithms that can cluster very large data sets in one scan through the data see Bradley et al. (1998). The applications in Table 16.1 all cluster documents. Other information retrieval applications cluster words (e.g., Crouch, 1988), contexts of words (e.g., Schütze and Pedersen, 1995) or words and documents simultaneously (e.g., Tishby and Slonim, 2000, Zha et al., 2001, Dhillon, 2001). Simultaneous clustering of words and documents is an example of co-clustering or biclustering . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_16_7</id>
         <title>Exercises</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/exercises-3.html</url>
         <file>exercises-3.html</file>
         <text> Exercises Exercises. Let be a clustering that exactly reproduces a class structure and a clustering that further subdivides some clusters in . Show that . Show that . Mutual information is symmetric in the sense that its value does not change if the roles of clusters and classes are switched: . Which of the other three evaluation measures are symmetric in this sense? Compute RSS for the two clusterings in Figure 16.7 . (i) Give an example of a set of points and three initial centroids (which need not be members of the set of points) for which 3-means converges to a clustering with an empty cluster. (ii) Can a clustering with an empty cluster be the global optimum with respect to RSS? Download Reuters-21578. Discard documents that do not occur in one of the 10 classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Discard documents that occur in two of these 10 classes. (i) Compute a -means clustering of this subset into 10 clusters. There are a number of software packages that implement -means, such as WEKA (Witten and Frank, 2005) and R (R Development Core Team, 2005). (ii) Compute purity, normalized mutual information, and RI for the clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Table 14.5 , page 14.5 ) for the 10 classes and 10 clusters. Identify classes that give rise to false positives and false negatives. Prove that is monotonically decreasing in . There is a soft version of -means that computes the fractional membership of a document in a cluster as a monotonically decreasing function of the distance from its centroid, e.g., as . Modify reassignment and recomputation steps of hard -means for this soft version. In the last iteration in Table 16.3 , document 6 is in cluster 2 even though it was the initial seed for cluster 1. Why does the document change membership? The values of the parameters in iteration 25 in Table 16.3 are rounded. What are the exact values that EM will converge to? Perform a -means clustering for the documents in Table 16.3 . After how many iterations does -means converge? Compare the result with the EM clustering in Table 16.3 and discuss the differences. Modify the expectation and maximization steps of EM for a Gaussian mixture. The maximization step computes the maximum likelihood parameter estimates , , and for each of the clusters. The expectation step computes for each vector a soft assignment to clusters (Gaussians) based on their current parameters. Write down the equations for Gaussian mixtures corresponding to and 202 . Show that -means can be viewed as the limiting case of EM for Gaussian mixtures if variance is very small and all covariances are 0. The within-point scatter of a clustering is defined as . Show that minimizing RSS and minimizing within-point scatter are equivalent. Derive an AIC criterion for the multivariate Bernoulli mixture model from Equation 196. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_17</id>
    <title>Hierarchical clustering</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html</url>
    <file>hierarchical-clustering-1.html</file>
    <text> Hierarchical clustering Flat clustering is efficient and conceptually simple, but as we saw in Chapter 16 it has a number of drawbacks. The algorithms introduced in Chapter 16 return a flat unstructured set of clusters, require a prespecified number of clusters as input and are nondeterministic. Hierarchical clustering (or hierarchic clustering ) outputs a hierarchy, a structure that is more informative than the unstructured set of clusters returned by flat clustering.Hierarchical clustering does not require us to prespecify the number of clusters and most hierarchical algorithms that have been used in IR are deterministic. These advantages of hierarchical clustering come at the cost of lower efficiency. The most common hierarchical clustering algorithms have a complexity that is at least quadratic in the number of documents compared to the linear complexity of -means and EM (cf. Section 16.4 , page 16.4 ). This chapter first introduces agglomerative hierarchical clustering (Section 17.1 ) and presents four different agglomerative algorithms, in Sections 17.2 -17.4 , which differ in the similarity measures they employ: single-link, complete-link, group-average, and centroid similarity. We then discuss the optimality conditions of hierarchical clustering in Section 17.5 . Section 17.6 introduces top-down (or divisive) hierarchical clustering. Section 17.7 looks at labeling clusters automatically, a problem that must be solved whenever humans interact with the output of clustering. We discuss implementation issues in Section 17.8 . Section 17.9 provides pointers to further reading, including references to soft hierarchical clustering, which we do not cover in this book. There are few differences between the applications of flat and hierarchical clustering in information retrieval. In particular, hierarchical clustering is appropriate for any of the applications shown in Table 16.1 (page 16.1 ; see also Section 16.6 , page 16.6 ). In fact, the example we gave for collection clustering is hierarchical. In general, we select flat clustering when efficiency is important and hierarchical clustering when one of the potential problems of flat clustering (not enough structure, predetermined number of clusters, non-determinism) is a concern. In addition, many researchers believe that hierarchical clustering produces better clusters than flat clustering. However, there is no consensus on this issue (see references in Section 17.9 ).   Subsections Hierarchical agglomerative clustering Single-link and complete-link clustering Time complexity of HAC Group-average agglomerative clustering Centroid clustering Optimality of HAC Divisive clustering Cluster labeling Implementation notes References and further reading Exercises</text>
    <subsections>
       <section>
         <id>iir_17_1</id>
         <title>Hierarchical agglomerative clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html</url>
         <file>hierarchical-agglomerative-clustering-1.html</file>
         <text> Hierarchical agglomerative clustering  agglomerate  hierarchical agglomerative clustering  HAC 17.6  A dendrogram of a single-link clustering of 30 documents from Reuters-RCV1. Two possible cuts of the dendrogram are shown: at 0.4 into 24 clusters and at 0.1 into 12 clusters. Before looking at specific similarity measures used in HAC in Sections 17.2 -17.4 , we first introduce a method for depicting hierarchical clusterings graphically, discuss a few key properties of HACs and present a simple algorithm for computing an HAC. An HAC clustering is typically visualized as a dendrogram as shown in Figure 17.1 . Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where documents are viewed as singleton clusters. We call this similarity the combination similarity of the merged cluster. For example, the combination similarity of the cluster consisting of Lloyd's CEO questioned and Lloyd's chief / U.S. grilling in Figure 17.1 is . We define the combination similarity of a singleton cluster as its document's self-similarity (which is 1.0 for cosine similarity). By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. For example, we see that the two documents entitled War hero Colin Powell were merged first in Figure 17.1 and that the last merge added Ag trade reform to a cluster consisting of the other 29 documents. A fundamental assumption in HAC is that the merge operation is monotonic . Monotonic means that if are the combination similarities of the successive merges of an HAC, then holds. A non-monotonic hierarchical clustering contains at least one inversion and contradicts the fundamental assumption that we chose the best merge available at each step. We will see an example of an inversion in Figure 17.12 . Hierarchical clustering does not require a prespecified number of clusters. However, in some applications we want a partition of disjoint clusters just as in flat clustering. In those cases, the hierarchy needs to be cut at some point. A number of criteria can be used to determine the cutting point: Cut at a prespecified level of similarity. For example, we cut the dendrogram at 0.4 if we want clusters with a minimum combination similarity of 0.4. In Figure 17.1 , cutting the diagram at yields 24 clusters (grouping only documents with high similarity together) and cutting it at yields 12 clusters (one large financial news cluster and 11 smaller clusters). Cut the dendrogram where the gap between two successive combination similarities is largest. Such large gaps arguably indicate ``natural'' clusterings. Adding one more cluster decreases the quality of the clustering significantly, so cutting before this steep decrease occurs is desirable. This strategy is analogous to looking for the knee in the -means graph in Figure 16.8 (page 16.8 ). Apply Equation 195 (page 16.4.1 ): where refers to the cut of the hierarchy that results in clusters, RSS is the residual sum of squares and is a penalty for each additional cluster. Instead of RSS, another measure of distortion can be used. As in flat clustering, we can also prespecify the number of clusters and select the cutting point that produces clusters.  Figure 17.2: A simple, but inefficient HAC algorithm.   A simple, naive HAC algorithm is shown in Figure 17.2 . We first compute the similarity matrix . The algorithm then executes steps of merging the currently most similar clusters. In each iteration, the two most similar clusters are merged and the rows and columns of the merged cluster in are updated.The clustering is stored as a list of merges in . indicates which clusters are still available to be merged. The function SIM computes the similarity of cluster with the merge of clusters and . For some HAC algorithms, SIM is simply a function of and , for example, the maximum of these two values for single-link. We will now refine this algorithm for the different similarity measures of single-link and complete-link clustering (Section 17.2 ) and group-average and centroid clustering ( and 17.4 ). The merge criteria of these four variants of HAC are shown in Figure 17.3 .   </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_2</id>
         <title>Single-link and complete-link clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/single-link-and-complete-link-clustering-1.html</url>
         <file>single-link-and-complete-link-clustering-1.html</file>
         <text> Single-link and complete-link clustering In single-link clustering or single-linkage clustering , the similarity of two clusters is the similarity of their most similar members (see Figure 17.3 , (a)). This single-link merge criterion is local. We pay attention solely to the area where the two clusters come closest to each other. Other, more distant parts of the cluster and the clusters' overall structure are not taken into account. In complete-link clustering or complete-linkage clustering , the similarity of two clusters is the similarity of their most dissimilar members (see Figure 17.3 , (b)). This is equivalent to choosing the cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions. This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering.  A dendrogram of a complete-link clustering.The same 30 documents were clustered with single-link clustering in Figure 17.1 . Figure 17.4 depicts a single-link and a complete-link clustering of eight documents. The first four steps, each producing a cluster consisting of a pair of two documents, are identical. Then single-link clustering joins the upper two pairs (and after that the lower two pairs) because on the maximum-similarity definition of cluster similarity, those two clusters are closest. Complete-link clustering joins the left two pairs (and then the right two pairs) because those are the closest pairs according to the minimum-similarity definition of cluster similarity. Figure 17.1 is an example of a single-link clustering of a set of documents and Figure 17.5 is the complete-link clustering of the same set. When cutting the last merge in Figure 17.5 , we obtain two clusters of similar size (documents 1-16, from NYSE closing averages to Lloyd's chief / U.S. grilling, and documents 17-30, from Ohio Blue Cross to Clinton signs law). There is no cut of the dendrogram in Figure 17.1 that would give us an equally balanced clustering. Both single-link and complete-link clustering have graph-theoretic interpretations. Define to be the combination similarity of the two clusters merged in step , and the graph that links all data points with a similarity of at least . Then the clusters after step in single-link clustering are the connected components of and the clusters after step in complete-link clustering are maximal cliques of . A connected component is a maximal set of connected points such that there is a path connecting each pair. A clique is a set of points that are completely linked with each other. These graph-theoretic interpretations motivate the terms single-link and complete-link clustering. Single-link clusters at step are maximal sets of points that are linked via at least one link (a single link) of similarity ; complete-link clusters at step are maximal sets of points that are completely linked with each other via links of similarity .   Single-link and complete-link clustering reduce the assessment of cluster quality to a single similarity between a pair of documents: the two most similar documents in single-link clustering and the two most dissimilar documents in complete-link clustering. A measurement based on one pair cannot fully reflect the distribution of documents in a cluster. It is therefore not surprising that both algorithms often produce undesirable clusters. Single-link clustering can produce straggling clusters as shown in Figure 17.6 . Since the merge criterion is strictly local, a chain of points can be extended for long distances without regard to the overall shape of the emerging cluster. This effect is called chaining . The chaining effect is also apparent in Figure 17.1 . The last eleven merges of the single-link clustering (those above the line) add on single documents or pairs of documents, corresponding to a chain. The complete-link clustering in Figure 17.5 avoids this problem. Documents are split into two groups of roughly equal size when we cut the dendrogram at the last merge. In general, this is a more useful organization of the data than a clustering with chains.   However, complete-link clustering suffers from a different problem. It pays too much attention to outliers, points that do not fit well into the global structure of the cluster. In the example in Figure 17.7 the four documents are split because of the outlier at the left edge (Exercise 17.2.1 ). Complete-link clustering does not find the most intuitive cluster structure in this example.   Subsections Time complexity of HAC</text>
         <subsections>
            <section>
              <id>iir_17_2_1</id>
              <title>Time complexity of HAC</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-of-hac-1.html</url>
              <file>time-complexity-of-hac-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_17_3</id>
         <title>Group-average agglomerative clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/group-average-agglomerative-clustering-1.html</url>
         <file>group-average-agglomerative-clustering-1.html</file>
         <text> Group-average agglomerative clustering Group-average agglomerative clustering  GAAC 17.3 all  group-average clustering  average-link clustering SIM-GA  (203)         The motivation for GAAC is that our goal in selecting two clusters and as the next merge in HAC is that the resulting merge cluster should be coherent. To judge the coherence of , we need to look at all document-document similarities within , including those that occur within and those that occur within . We can compute the measure SIM-GA efficiently because the sum of individual vector similarities is equal to the similarities of their sums:     (204)    (205)         SIM FFICIENT 17.8 205 Equation 204 relies on the distributivity of the dot product with respect to vector addition. Since this is crucial for the efficient computation of a GAAC clustering, the method cannot be easily applied to representations of documents that are not real-valued vectors. Also, Equation 204 only holds for the dot product. While many algorithms introduced in this book have near-equivalent descriptions in terms of dot product, cosine similarity and Euclidean distance (cf. simdisfigs), Equation 204 can only be expressed using the dot product. This is a fundamental difference between single-link/complete-link clustering and GAAC. The first two only require a square matrix of similarities as input and do not care how these similarities were computed. To summarize, GAAC requires (i) documents represented as vectors, (ii) length normalization of vectors, so that self-similarities are 1.0, and (iii) the dot product as the measure of similarity between vectors and sums of vectors. The merge algorithms for GAAC and complete-link clustering are the same except that we use Equation 205 as similarity function in Figure 17.8 . Therefore, the overall time complexity of GAAC is the same as for complete-link clustering: . Like complete-link clustering, GAAC is not best-merge persistent (Exercise 17.10 ). This means that there is no algorithm for GAAC that would be analogous to the algorithm for single-link in Figure 17.9 . We can also define group-average similarity as including self-similarities:     (206)    139 139   Self-similarities are always equal to 1.0, the maximum possible value for length-normalized vectors. The proportion of self-similarities in Equation 206 is for a cluster of size . This gives an unfair advantage to small clusters since they will have proportionally more self-similarities. For two documents , with a similarity , we have . In contrast, . This similarity of two documents is the same as in single-link, complete-link and centroid clustering. We prefer the definition in Equation 205, which excludes self-similarities from the average, because we do not want to penalize large clusters for their smaller proportion of self-similarities and because we want a consistent similarity value for document pairs in all four HAC algorithms. Exercises. Apply group-average clustering to the points in and 17.7 . Map them onto the surface of the unit sphere in a three-dimensional space to get length-normalized vectors. Is the group-average clustering different from the single-link and complete-link clusterings? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_4</id>
         <title>Centroid clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/centroid-clustering-1.html</url>
         <file>centroid-clustering-1.html</file>
         <text> Centroid clustering   (207)   (208)   (209)   207 209 different 17.3 17.3 Figure 17.11 shows the first three steps of a centroid clustering. The first two iterations form the clusters with centroid and with centroid because the pairs and have the highest centroid similarities. In the third iteration, the highest centroid similarity is between and producing the cluster with centroid . Like GAAC, centroid clustering is not best-merge persistent and therefore (Exercise 17.10 ).   In contrast to the other three HAC algorithms, centroid clustering is not monotonic. So-called inversions can occur: Similarity can increase during clustering as in the example in Figure 17.12 , where we define similarity as negative distance. In the first merge, the similarity of and is . In the second merge, the similarity of the centroid of and (the circle) and is . This is an example of an inversion: similarity increases in this sequence of two clustering steps. In a monotonic HAC algorithm, similarity is monotonically decreasing from iteration to iteration. Increasing similarity in a series of HAC clustering steps contradicts the fundamental assumption that small clusters are more coherent than large clusters. An inversion in a dendrogram shows up as a horizontal merge line that is lower than the previous merge line. All merge lines in and 17.5 are higher than their predecessors because single-link and complete-link clustering are monotonic clustering algorithms. Despite its non-monotonicity, centroid clustering is often used because its similarity measure - the similarity of two centroids - is conceptually simpler than the average of all pairwise similarities in GAAC. Figure 17.11 is all one needs to understand centroid clustering. There is no equally simple graph that would explain how GAAC works. Exercises. For a fixed set of documents there are up to distinct similarities between clusters in single-link and complete-link clustering. How many distinct cluster similarities are there in GAAC and centroid clustering? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_5</id>
         <title>Optimality of HAC</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/optimality-of-hac-1.html</url>
         <file>optimality-of-hac-1.html</file>
         <text> Optimality of HAC To state the optimality conditions of hierarchical clustering precisely, we first define the combination similarity COMB-SIM of a clustering as the smallest combination similarity of any of its clusters: (210)       17.1 We then define to be optimal if all clusterings with clusters, , have lower combination similarities: (211)  Figure 17.12 shows that centroid clustering is not optimal. The clustering (for ) has combination similarity and (for ) has combination similarity -3.46. So the clustering produced in the first merge is not optimal since there is a clustering with fewer clusters ( ) that has higher combination similarity. Centroid clustering is not optimal because inversions can occur. The above definition of optimality would be of limited use if it was only applicable to a clustering together with its merge history. However, we can show (Exercise 17.5 ) that for the three non-inversion algorithms can be read off from the cluster without knowing its history. These direct definitions of combination similarity are as follows. single-link The combination similarity of a cluster is the smallest similarity of any bipartition of the cluster, where the similarity of a bipartition is the largest similarity between any two documents from the two parts: (212) where each is a bipartition of . complete-link The combination similarity of a cluster is the smallest similarity of any two points in : . GAAC The combination similarity of a cluster is the average of all pairwise similarities in (where self-similarities are not included in the average): Equation 205. We can now prove the optimality of single-link clustering by induction over the number of clusters . We will give a proof for the case where no two pairs of documents have the same similarity, but it can easily be extended to the case with ties. The inductive basis of the proof is that a clustering with clusters has combination similarity 1.0, which is the largest value possible. The induction hypothesis is that a single-link clustering with clusters is optimal: for all . Assume for contradiction that the clustering we obtain by merging the two most similar clusters in is not optimal and that instead a different sequence of merges leads to the optimal clustering with clusters. We can write the assumption that is optimal and that is not as . Case 1: The two documents linked by are in the same cluster in . They can only be in the same cluster if a merge with similarity smaller than has occurred in the merge sequence producing . This implies . Thus, . Contradiction. Case 2: The two documents linked by are not in the same cluster in . But , so the single-link merging rule should have merged these two clusters when processing . Contradiction. Thus, is optimal. In contrast to single-link clustering, complete-link clustering and GAAC are not optimal as this example shows:  Both algorithms merge the two points with distance 1 ( and ) first and thus cannot find the two-cluster clustering . But is optimal on the optimality criteria of complete-link clustering and GAAC. However, the merge criteria of complete-link clustering and GAAC approximate the desideratum of approximate sphericity better than the merge criterion of single-link clustering. In many applications, we want spherical clusters. Thus, even though single-link clustering may seem preferable at first because of its optimality, it is optimal with respect to the wrong criterion in many document clustering applications.   Table 17.1: Comparison of HAC algorithms. method combination similarity time compl. optimal? comment single-link max inter-similarity of any 2 docs yes chaining effect complete-link min inter-similarity of any 2 docs no sensitive to outliers group-average average of all sims no best choice for most applications centroid average inter-similarity no inversions can occur   Table 17.1 summarizes the properties of the four HAC algorithms introduced in this chapter. We recommend GAAC for document clustering because it is generally the method that produces the clustering with the best properties for applications. It does not suffer from chaining, from sensitivity to outliers and from inversions. There are two exceptions to this recommendation. First, for non-vector representations, GAAC is not applicable and clustering should typically be performed with the complete-link method. Second, in some applications the purpose of clustering is not to create a complete hierarchy or exhaustive partition of the entire document set. For instance, first story detection or novelty detection is the task of detecting the first occurrence of an event in a stream of news stories. One approach to this task is to find a tight cluster within the documents that were sent across the wire in a short period of time and are dissimilar from all previous documents. For example, the documents sent over the wire in the minutes after the World Trade Center attack on September 11, 2001 form such a cluster. Variations of single-link clustering can do well on this task since it is the structure of small parts of the vector space - and not global structure - that is important in this case. Similarly, we will describe an approach to duplicate detection on the web in Section 19.6 (page 19.6 ) where single-link clustering is used in the guise of the union-find algorithm . Again, the decision whether a group of documents are duplicates of each other is not influenced by documents that are located far away and single-link clustering is a good choice for duplicate detection. Exercises. Show the equivalence of the two definitions of combination similarity: the process definition on page 17.1 and the static definition on page 17.5 . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_6</id>
         <title>Divisive clustering</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/divisive-clustering-1.html</url>
         <file>divisive-clustering-1.html</file>
         <text> Divisive clustering  top-down clustering  divisive clustering Top-down clustering is conceptually more complex than bottom-up clustering since we need a second, flat clustering algorithm as a ``subroutine''. It has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves. For a fixed number of top levels, using an efficient flat algorithm like -means, top-down algorithms are linear in the number of documents and clusters. So they run much faster than HAC algorithms, which are at least quadratic. There is evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms in some circumstances. See the references on bisecting -means in Section 17.9 . Bottom-up methods make clustering decisions based on local patterns without initially taking into account the global distribution. These early decisions cannot be undone. Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_7</id>
         <title>Cluster labeling</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html</url>
         <file>cluster-labeling-1.html</file>
         <text> Cluster labeling In many applications of flat clustering and hierarchical clustering, particularly in analysis tasks and in user interfaces (see applications in Table 16.1 , page 16.1 ), human users interact with clusters. In such settings, we must label clusters, so that users can see what a cluster is about. Differential cluster labeling selects cluster labels by comparing the distribution of terms in one cluster with that of other clusters. The feature selection methods we introduced in Section 13.5 (page ) can all be used for differential cluster labeling. In particular, mutual information (MI) (Section 13.5.1 , page 13.5.1 ) or, equivalently, information gain and the -test (Section 13.5.2 , page 13.5.2 ) will identify cluster labels that characterize one cluster in contrast to other clusters. A combination of a differential test with a penalty for rare terms often gives the best labeling results because rare terms are not necessarily representative of the cluster as a whole.       labeling method   # docs centroid mutual information title 4 622 oil plant mexico production crude power 000 refinery gas bpd plant oil production barrels crude bpd mexico dolly capacity petroleum MEXICO: Hurricane Dolly heads for Mexico coast 9 1017 police security russian people military peace killed told grozny court police killed military security peace told troops forces rebels people RUSSIA: Russia's Lebed meets rebel chief in Chechnya 10 1259 00 000 tonnes traders futures wheat prices cents september tonne delivery traders futures tonne tonnes desk wheat prices 000 00 USA: Export Business - Grain/oilseeds complex Automatically computed cluster labels.This is for three of ten clusters (4, 9, and 10) in a -means clustering of the first 10,000 documents in Reuters-RCV1. The last three columns show cluster summaries computed by three labeling methods: most highly weighted terms in centroid (centroid), mutual information, and the title of the document closest to the centroid of the cluster (title). Terms selected by only one of the first two methods are in bold.  We apply three labeling methods to a -means clustering in Table 17.2 . In this example, there is almost no difference between MI and . We therefore omit the latter. Cluster-internal labeling computes a label that solely depends on the cluster itself, not on other clusters. Labeling a cluster with the title of the document closest to the centroid is one cluster-internal method. Titles are easier to read than a list of terms. A full title can also contain important context that didn't make it into the top 10 terms selected by MI. On the web, anchor text can play a role similar to a title since the anchor text pointing to a page can serve as a concise summary of its contents. In Table 17.2 , the title for cluster 9 suggests that many of its documents are about the Chechnya conflict, a fact the MI terms do not reveal. However, a single document is unlikely to be representative of all documents in a cluster. An example is cluster 4, whose selected title is misleading. The main topic of the cluster is oil. Articles about hurricane Dolly only ended up in this cluster because of its effect on oil prices. We can also use a list of terms with high weights in the centroid of the cluster as a label. Such highly weighted terms (or, even better, phrases, especially noun phrases) are often more representative of the cluster than a few titles can be, even if they are not filtered for distinctiveness as in the differential methods. However, a list of phrases takes more time to digest for users than a well crafted title. Cluster-internal methods are efficient, but they fail to distinguish terms that are frequent in the collection as a whole from those that are frequent only in the cluster. Terms like year or Tuesday may be among the most frequent in a cluster, but they are not helpful in understanding the contents of a cluster with a specific topic like oil. In Table 17.2 , the centroid method selects a few more uninformative terms (000, court, cents, september) than MI (forces, desk), but most of the terms selected by either method are good descriptors. We get a good sense of the documents in a cluster from scanning the selected terms. For hierarchical clustering, additional complications arise in cluster labeling. Not only do we need to distinguish an internal node in the tree from its siblings, but also from its parent and its children. Documents in child nodes are by definition also members of their parent node, so we cannot use a naive differential method to find labels that distinguish the parent from its children. However, more complex criteria, based on a combination of overall collection frequency and prevalence in a given cluster, can determine whether a term is a more informative label for a child node or a parent node (see Section 17.9 ). </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_8</id>
         <title>Implementation notes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/implementation-notes-1.html</url>
         <file>implementation-notes-1.html</file>
         <text> Implementation notes In low dimensions, more aggressive optimizations are possible that make the computation of most pairwise similarities unnecessary (Exercise 17.10 ). However, no such algorithms are known in higher dimensions. We encountered the same problem in kNN classification (see Section 14.7 , page 14.7 ). When using GAAC on a large document set in high dimensions, we have to take care to avoid dense centroids. For dense centroids, clustering can take time where is the size of the vocabulary, whereas complete-link clustering is where is the average size of the vocabulary of a document. So for large vocabularies complete-link clustering can be more efficient than an unoptimized implementation of GAAC. We discussed this problem in the context of -means clustering in Chapter 16 (page 16.4 ) and suggested two solutions: truncating centroids (keeping only highly weighted terms) and representing clusters by means of sparse medoids instead of dense centroids. These optimizations can also be applied to GAAC and centroid clustering. Even with these optimizations, HAC algorithms are all or and therefore infeasible for large sets of 1,000,000 or more documents. For such large sets, HAC can only be used in combination with a flat clustering algorithm like -means. Recall that -means requires a set of seeds as initialization (Figure 16.5 , page 16.5 ). If these seeds are badly chosen, then the resulting clustering will be of poor quality. We can employ an HAC algorithm to compute seeds of high quality. If the HAC algorithm is applied to a document subset of size , then the overall runtime of -means cum HAC seed generation is . This is because the application of a quadratic algorithm to a sample of size has an overall complexity of . An appropriate adjustment can be made for an algorithm to guarantee linearity. This algorithm is referred to as the Buckshot algorithm . It combines the determinism and higher reliability of HAC with the efficiency of -means. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_9</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-17.html</url>
         <file>references-and-further-reading-17.html</file>
         <text> References and further reading An excellent general review of clustering is (Jain et al., 1999). Early references for specific HAC algorithms are (King, 1967) (single-link), (Sneath and Sokal, 1973) (complete-link, GAAC) and (Lance and Williams, 1967) (discussing a large variety of hierarchical clustering algorithms). The single-link algorithm in Figure 17.9 is similar to Kruskal's algorithm for constructing a minimum spanning tree. A graph-theoretical proof of the correctness of Kruskal's algorithm (which is analogous to the proof in Section 17.5 ) is provided by Cormen et al. (1990, Theorem 23.1). See Exercise 17.10 for the connection between minimum spanning trees and single-link clusterings. It is often claimed that hierarchical clustering algorithms produce better clusterings than flat algorithms (Jain and Dubes (1988, p. 140), Cutting et al. (1992), Larsen and Aone (1999)) although more recently there have been experimental results suggesting the opposite (Zhao and Karypis, 2002). Even without a consensus on average behavior, there is no doubt that results of EM and -means are highly variable since they will often converge to a local optimum of poor quality. The HAC algorithms we have presented here are deterministic and thus more predictable. The complexity of complete-link, group-average and centroid clustering is sometimes given as (Day and Edelsbrunner, 1984, Murtagh, 1983, Voorhees, 1985b) because a document similarity computation is an order of magnitude more expensive than a simple comparison, the main operation executed in the merging steps after the similarity matrix has been computed. The centroid algorithm described here is due to Voorhees (1985b). Voorhees recommends complete-link and centroid clustering over single-link for a retrieval application. The Buckshot algorithm was originally published by Cutting et al. (1993). Allan et al. (1998) apply single-link clustering to first story detection . An important HAC technique not discussed here is Ward's method (El-Hamdouchi and Willett, 1986, Ward Jr., 1963), also called minimum variance clustering . In each step, it selects the merge with the smallest RSS (Chapter 16 , page 191 ). The merge criterion in Ward's method (a function of all individual distances from the centroid) is closely related to the merge criterion in GAAC (a function of all individual similarities to the centroid). Despite its importance for making the results of clustering useful, comparatively little work has been done on labeling clusters. Popescul and Ungar (2000) obtain good results with a combination of and collection frequency of a term. Glover et al. (2002b) use information gain for labeling clusters of web pages. Stein and zu Eissen's approach is ontology-based (2004). The more complex problem of labeling nodes in a hierarchy (which requires distinguishing more general labels for parents from more specific labels for children) is tackled by Glover et al. (2002a) and Treeratpituk and Callan (2006). Some clustering algorithms attempt to find a set of labels first and then build (often overlapping) clusters around the labels, thereby avoiding the problem of labeling altogether (Osinski and Weiss, 2005, Zamir and Etzioni, 1999, Käki, 2005). We know of no comprehensive study that compares the quality of such ``label-based'' clustering to the clustering algorithms discussed in this chapter and in Chapter 16 . In principle, work on multi-document summarization (McKeown and Radev, 1995) is also applicable to cluster labeling, but multi-document summaries are usually longer than the short text fragments needed when labeling clusters (cf. snippets). Presenting clusters in a way that users can understand is a UI problem. We recommend reading (Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) for an introduction to user interfaces in IR. An example of an efficient divisive algorithm is bisecting -means (Steinbach et al., 2000). Spectral clustering algorithms (Kannan et al., 2000, Dhillon, 2001, Zha et al., 2001, Ng et al., 2001a), including principal direction divisive partitioning (PDDP) (whose bisecting decisions are based on SVD , see Chapter 18 ) (Boley, 1998, Savaresi and Boley, 2004), are computationally more expensive than bisecting -means, but have the advantage of being deterministic. Unlike -means and EM, most hierarchical clustering algorithms do not have a probabilistic interpretation. Model-based hierarchical clustering (Kamvar et al., 2002, Vaithyanathan and Dom, 2000, Castro et al., 2004) is an exception. The evaluation methodology described in Section 16.3 (page 16.3 ) is also applicable to hierarchical clustering. Specialized evaluation measures for hierarchies are discussed by Fowlkes and Mallows (1983), Larsen and Aone (1999) and Sahoo et al. (2006). The R environment (R Development Core Team, 2005) offers good support for hierarchical clustering. The R function hclust implements single-link, complete-link, group-average, and centroid clustering; and Ward's method. Another option provided is median clustering which represents each cluster by its medoid (cf. k-medoids in Chapter 16 , page 16.4 ). Support for clustering vectors in high-dimensional spaces is provided by the software package CLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto). </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_17_10</id>
         <title>Exercises</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/exercises-4.html</url>
         <file>exercises-4.html</file>
         <text> Exercises Exercises. A single-link clustering can also be computed from the minimum spanning tree of a graph. The minimum spanning tree connects the vertices of a graph at the smallest possible cost, where cost is defined as the sum over all edges of the graph. In our case the cost of an edge is the distance between two documents. Show that if are the costs of the edges of a minimum spanning tree, then these edges correspond to the merges in constructing a single-link clustering. Show that single-link clustering is best-merge persistent and that GAAC and centroid clustering are not best-merge persistent. Consider running 2-means clustering on a collection with documents from two different languages. What result would you expect? Would you expect the same result when running an HAC algorithm? Download Reuters-21578. Keep only documents that are in the classes crude, interest, and grain. Discard documents that are members of more than one of these three classes. Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid clustering of the documents. (v) Cut each dendrogram at the second branch from the top to obtain clusters. Compute the Rand index for each of the 4 clusterings. Which clustering method performs best? Suppose a run of HAC finds the clustering with to have the highest value on some prechosen goodness measure of clustering. Have we found the highest-value clustering among all clusterings with ? Consider the task of producing a single-link clustering of points on a line: Show that we only need to compute a total of about similarities. What is the overall complexity of single-link clustering for a set of points on a line? Prove that single-link, complete-link, and group-average clustering are monotonic in the sense defined on page 17.1 . For points, there are different flat clusterings into clusters (Section 16.2 , page 16.2.1 ). What is the number of different hierarchical clusterings (or dendrograms) of documents? Are there more flat clusterings or more hierarchical clusterings for given and ? </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_18</id>
    <title>Matrix decompositions and latent semantic indexing</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-and-latent-semantic-indexing-1.html</url>
    <file>matrix-decompositions-and-latent-semantic-indexing-1.html</file>
    <text> Matrix decompositions and latent semantic indexing On page 6.3.1 we introduced the notion of a term-document matrix: an matrix , each of whose rows represents a term and each of whose columns represents a document in the collection. Even for a collection of modest size, the term-document matrix is likely to have several tens of thousands of rows and columns. In Section 18.1.1 we first develop a class of operations from linear algebra, known as matrix decomposition. In Section 18.2 we use a special form of matrix decomposition to construct a low-rank approximation to the term-document matrix. In Section 18.3 we examine the application of such low-rank approximations to indexing and retrieving documents, a technique referred to as latent semantic indexing. While latent semantic indexing has not been established as a significant force in scoring and ranking for information retrieval, it remains an intriguing approach to clustering in a number of domains including for collections of text documents (Section 16.6 , page 16.6 ). Understanding its full potential remains an area of active research. Readers who do not require a refresher on linear algebra may skip Section 18.1 , although Example 18.1 is especially recommended as it highlights a property of eigenvalues that we exploit later in the chapter.   Subsections Linear algebra review Matrix decompositions Term-document matrices and singular value decompositions Low-rank approximations Latent semantic indexing References and further reading</text>
    <subsections>
       <section>
         <id>iir_18_1</id>
         <title>Linear algebra review</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/linear-algebra-review-1.html</url>
         <file>linear-algebra-review-1.html</file>
         <text> Linear algebra review   The rank of a matrix is the number of linearly independent rows (or columns) in it; thus, . A square matrix all of whose off-diagonal entries are zero is called a diagonal matrix; its rank is equal to the number of non-zero diagonal entries. If all diagonal entries of such a diagonal matrix are , it is called the identity matrix of dimension and represented by . For a square matrix and a vector that is not all zeros, the values of satisfying (213)   eigenvalues    213  right eigenvector principal eigenvector. left eigenvectors     (214)    The eigenvalues of a matrix are found by solving the characteristic equation, which is obtained by rewriting Equation 213 in the form . The eigenvalues of are then the solutions of , where denotes the determinant of a square matrix . The equation is an th order polynomial equation in and can have at most roots, which are the eigenvalues of . These eigenvalues can in general be complex, even if all entries of are real. We now examine some further properties of eigenvalues and eigenvectors, to set up the central idea of singular value decompositions in Section 18.2 below. First, we look at the relationship between matrix-vector multiplication and eigenvalues. Worked example. Consider the matrix (215)      (216)       (217)     (218)   (219)   (220)   (221)   End worked example. Example 18.1 shows that even though is an arbitrary vector, the effect of multiplication by is determined by the eigenvalues and eigenvectors of . Furthermore, it is intuitively apparent from Equation 221 that the product is relatively unaffected by terms arising from the small eigenvalues of ; in our example, since , the contribution of the third term on the right hand side of Equation 221 is small. In fact, if we were to completely ignore the contribution in Equation 221 from the third eigenvector corresponding to , then the product would be computed to be rather than the correct product which is ; these two vectors are relatively close to each other by any of various metrics one could apply (such as the length of their vector difference). This suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector product is small. We will carry forward this intuition when studying matrix decompositions and low-rank approximations in Section 18.2 . Before doing so, we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us. For a symmetric matrix , the eigenvectors corresponding to distinct eigenvalues are orthogonal. Further, if is both real and symmetric, the eigenvalues are all real. Worked example. Consider the real, symmetric matrix (222)        End worked example.   Subsections Matrix decompositions</text>
         <subsections>
            <section>
              <id>iir_18_1_1</id>
              <title>Matrix decompositions</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-1.html</url>
              <file>matrix-decompositions-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_18_2</id>
         <title>Term-document matrices and singular value decompositions</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/term-document-matrices-and-singular-value-decompositions-1.html</url>
         <file>term-document-matrices-and-singular-value-decompositions-1.html</file>
         <text> Term-document matrices and singular value decompositions      singular value decomposition 18.3  18.2  18.1.1          Theorem. Let be the rank of the matrix . Then, there is a singular-value decomposition ( SVD for short) of of the form (232)  The eigenvalues of are the same as the eigenvalues of ; For , let , with . Then the matrix is composed by setting for , and zero otherwise. End theorem. The values are referred to as the singular values of . It is instructive to examine the relationship of Theorem 18.2 to Theorem 18.1.1; we do this rather than derive the general proof of Theorem 18.2, which is beyond the scope of this book. By multiplying Equation 232 by its transposed version, we have (233)  Note now that in Equation 233, the left-hand side is a square symmetric matrix real-valued matrix, and the right-hand side represents its symmetric diagonal decomposition as in Theorem 18.1.1. What does the left-hand side represent? It is a square matrix with a row and a column corresponding to each of the terms. The entry in the matrix is a measure of the overlap between the th and th terms, based on their co-occurrence in documents. The precise mathematical meaning depends on the manner in which is constructed based on term weighting. Consider the case where is the term-document incidence matrix of page 1.1 , illustrated in Figure 1.1 . Then the entry in is the number of documents in which both term and term occur.   When writing down the numerical values of the SVD, it is conventional to represent as an matrix with the singular values on the diagonals, since all its entries outside this sub-matrix are zeros. Accordingly, it is conventional to omit the rightmost columns of corresponding to these omitted rows of ; likewise the rightmost columns of are omitted since they correspond in to the rows that will be multiplied by the columns of zeros in . This written form of the SVD is sometimes known as the reduced SVD or truncated SVD and we will encounter it again in Exercise 18.3 . Henceforth, our numerical examples and exercises will use this reduced form. Worked example. We now illustrate the singular-value decomposition of a matrix of rank 2; the singular values are and .  (234)  End worked example. As with the matrix decompositions defined in Section 18.1.1 , the singular value decomposition of a matrix can be computed by a variety of algorithms, many of which have been publicly available software implementations; pointers to these are given in Section 18.5 . Exercises. Let (235) be the term-document incidence matrix for a collection. Compute the co-occurrence matrix . What is the interpretation of the diagonal entries of when is a term-document incidence matrix? Verify that the SVD of the matrix in Equation 235 is (236) by verifying all of the properties in the statement of Theorem 18.2. Suppose that is a binary term-document incidence matrix. What do the entries of represent? Let (237) be a term-document matrix whose entries are term frequencies; thus term 1 occurs 2 times in document 2 and once in document 3. Compute ; observe that its entries are largest where two terms have their most frequent occurrences together in the same document.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_18_3</id>
         <title>Low-rank approximations</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/low-rank-approximations-1.html</url>
         <file>low-rank-approximations-1.html</file>
         <text> Low-rank approximations We next state a matrix approximation problem that at first seems to have little to do with information retrieval. We describe a solution to this matrix problem using singular-value decompositions, then develop its application to information retrieval. Given an matrix and a positive integer , we wish to find an matrix of rank at most , so as to minimize the Frobenius norm of the matrix difference , defined to be (238)               low-rank approximation The singular value decomposition can be used to solve the low-rank matrix approximation problem. We then derive from it an application to approximating term-document matrices. We invoke the following three-step procedure to this end: Given , construct its SVD in the form shown in (232); thus, . Derive from the matrix formed by replacing by zeros the smallest singular values on the diagonal of . Compute and output as the rank- approximation to .     18.1   Theorem. (239)  End theorem. Recalling that the singular values are in decreasing order , we learn from Theorem 18.3 that is the best rank- approximation to , incurring an error (measured by the Frobenius norm of ) equal to . Thus the larger is, the smaller this error (and in particular, for , the error is zero since ; provided , then and thus ).   To derive further insight into why the process of truncating the smallest singular values in helps generate a rank- approximation of low error, we examine the form of : (240)   (241)   (242)              Exercises. Compute a rank 1 approximation to the matrix in Example 235, using the SVD as in Exercise 236. What is the Frobenius norm of the error of this approximation? Consider now the computation in Exercise 18.3 . Following the schematic in Figure 18.2 , notice that for a rank 1 approximation we have being a scalar. Denote by the first column of and by the first column of . Show that the rank-1 approximation to can then be written as . reduced can be generalized to rank approximations: we let and denote the ``reduced'' matrices formed by retaining only the first columns of and , respectively. Thus is an matrix while is a matrix. Then, we have (243) where is the square submatrix of with the singular values on the diagonal. The primary advantage of using (243) is to eliminate a lot of redundant columns of zeros in and , thereby explicitly eliminating multiplication by columns that do not affect the low-rank approximation; this version of the SVD is sometimes known as the reduced SVD or truncated SVD and is a computationally simpler representation from which to compute the low rank approximation. For the matrix in Example 18.2, write down both and . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_18_4</id>
         <title>Latent semantic indexing</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html</url>
         <file>latent-semantic-indexing-1.html</file>
         <text> Latent semantic indexing    latent semantic indexing But first, we motivate such an approximation. Recall the vector space representation of documents and queries introduced in Section 6.3 (page ). This vector space representation enjoys a number of advantages including the uniform treatment of queries and documents as vectors, the induced score computation based on cosine similarity, the ability to weight different terms differently, and its extension beyond document retrieval to such applications as clustering and classification. The vector space representation suffers, however, from its inability to cope with two classic problems arising in natural languages: synonymy and polysemy. Synonymy refers to a case where two different words (say car and automobile) have the same meaning. Because the vector space representation fails to capture the relationship between synonymous terms such as car and automobile - according each a separate dimension in the vector space. Consequently the computed similarity between a query (say, car) and a document containing both car and automobile underestimates the true similarity that a user would perceive. Polysemy on the other hand refers to the case where a term such as charge has multiple meanings, so that the computed similarity overestimates the similarity that a user would perceive. Could we use the co-occurrences of terms (whether, for instance, charge occurs in a document containing steed versus in a document containing electron) to capture the latent semantic associations of terms and alleviate these problems? Even for a collection of modest size, the term-document matrix is likely to have several tens of thousand of rows and columns, and a rank in the tens of thousands as well. In latent semantic indexing (sometimes referred to as latent semantic analysis (LSA) ), we use the SVD to construct a low-rank approximation to the term-document matrix, for a value of that is far smaller than the original rank of . In the experimental work cited later in this section, is generally chosen to be in the low hundreds. We thus map each row/column (respectively corresponding to a term/document) to a -dimensional space; this space is defined by the principal eigenvectors (corresponding to the largest eigenvalues) of and . Note that the matrix is itself still an matrix, irrespective of . Next, we use the new -dimensional LSI representation as we did the original representation - to compute similarities between vectors. A query vector is mapped into its representation in the LSI space by the transformation (244)  6.3.1  244  244 The fidelity of the approximation of to leads us to hope that the relative values of cosine similarities are preserved: if a query is close to a document in the original space, it remains relatively close in the -dimensional space. But this in itself is not sufficiently interesting, especially given that the sparse query vector turns into a dense query vector in the low-dimensional space. This has a significant computational cost, when compared with the cost of processing in its native form. Worked example. Consider the term-document matrix         ship 1 0 1 0 0 0     boat 0 1 0 0 0 0     ocean 1 1 0 0 0 0     voyage 1 0 0 1 1 0     trip 0 0 0 1 0 1   Its singular value decomposition is the product of three matrices as below. First we have which in this example is:     1 2 3 4 5     ship     boat 0.00 0.73     ocean 0.00     voyage 0.35 0.15 0.16     trip 0.65 0.58   When applying the SVD to a term-document matrix, is known as the SVD term matrix. The singular values are 2.16 0.00 0.00 0.00 0.00 0.00 1.59 0.00 0.00 0.00 0.00 0.00 1.28 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.39 Finally we have , which in the context of a term-document matrix is known as the SVD document matrix:         1     2 0.63 0.22 0.41     3 0.28 0.45 0.12     4 0.00 0.00 0.58 0.00 0.58     5 0.29 0.63 0.19 0.41   By ``zeroing out'' all but the two largest singular values of , we obtain 2.16 0.00 0.00 0.00 0.00 0.00 1.59 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 From this, we compute         1     2 1.00 0.35 0.65     3 0.00 0.00 0.00 0.00 0.00 0.00     4 0.00 0.00 0.00 0.00 0.00 0.00     5 0.00 0.00 0.00 0.00 0.00 0.00   Notice that the low-rank approximation, unlike the original matrix , can have negative entries. End worked example. Examination of and in Example 18.4 shows that the last 3 rows of each of these matrices are populated entirely by zeros. This suggests that the SVD product in Equation 241 can be carried out with only two rows in the representations of and ; we may then replace these matrices by their truncated versions and . For instance, the truncated SVD document matrix in this example is:         1     2 1.00 0.35 0.65   Figure 18.3 illustrates the documents in in two dimensions. Note also that is dense relative to .  Figure 18.3: The documents of Example 18.4 reduced to two dimensions in . We may in general view the low-rank approximation of by as a constrained optimization problem: subject to the constraint that have rank at most , we seek a representation of the terms and documents comprising with low Frobenius norm for the error . When forced to squeeze the terms/documents down to a -dimensional space, the SVD should bring together terms with similar co-occurrences. This intuition suggests, then, that not only should retrieval quality not suffer too much from the dimension reduction, but in fact may improve. Dumais (1993) and Dumais (1995) conducted experiments with LSI on TREC documents and tasks, using the commonly-used Lanczos algorithm to compute the SVD. At the time of their work in the early 1990's, the LSI computation on tens of thousands of documents took approximately a day on one machine. On these experiments, they achieved precision at or above that of the median TREC participant. On about 20% of TREC topics their system was the top scorer, and reportedly slightly better on average than standard vector spaces for LSI at about 350 dimensions. Here are some conclusions on LSI first suggested by their work, and subsequently verified by many other experiments.  The computational cost of the SVD is significant; at the time of this writing, we know of no successful experiment with over one million documents. This has been the biggest obstacle to the widespread adoption to LSI. One approach to this obstacle is to build the LSI representation on a randomly sampled subset of the documents in the collection, following which the remaining documents are ``folded in'' as detailed with Equation 244. As we reduce , recall tends to increase, as expected. Most surprisingly, a value of in the low hundreds can actually increase precision on some query benchmarks. This appears to suggest that for a suitable value of , LSI addresses some of the challenges of synonymy. LSI works best in applications where there is little overlap between queries and documents. The experiments also documented some modes where LSI failed to match the effectiveness of more traditional indexes and score computations. Most notably (and perhaps obviously), LSI shares two basic drawbacks of vector space retrieval: there is no good way of expressing negations (find documents that contain german but not shepherd), and no way of enforcing Boolean conditions. LSI can be viewed as soft clustering by interpreting each dimension of the reduced space as a cluster and the value that a document has on that dimension as its fractional membership in that cluster. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_18_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-18.html</url>
         <file>references-and-further-reading-18.html</file>
         <text> References and further reading Strang (1986) provides an excellent introductory overview of matrix decompositions including the singular value decomposition. Theorem 18.3 is due to Eckart and Young (1936). The connection between information retrieval and low-rank approximations of the term-document matrix was introduced in Deerwester et al. (1990), with a subsequent survey of results in Berry et al. (1995). Dumais (1993) and Dumais (1995) describe experiments on TREC benchmarks giving evidence that at least on some benchmarks, LSI can produce better precision and recall than standard vector-space retrieval. http://www.cs.utk.edu/~berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.htmloffer comprehensive pointers to the literature and software of LSI. Schütze and Silverstein (1997) evaluate LSI and truncated representations of centroids for efficient -means clustering (Section 16.4 ). Bast and Majumdar (2005) detail the role of the reduced dimension in LSI and how different pairs of terms get coalesced together at differing values of . Applications of LSI to cross-language information retrieval (where documents in two or more different languages are indexed, and a query posed in one language is expected to retrieve documents in other languages) are developed in Berry and Young (1995) and Littman et al. (1998). LSI (referred to as LSA in more general settings) has been applied to host of other problems in computer science ranging from memory modeling to computer vision. Hofmann (1999a;b) provides an initial probabilistic extension of the basic latent semantic indexing technique. A more satisfactory formal basis for a probabilistic latent variable model for dimensionality reduction is the Latent Dirichlet Allocation ( LDA ) model (Blei et al., 2003), which is generative and assigns probabilities to documents outside of the training set. This model is extended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft (2006) present the first large scale evaluation of LDA, finding it to significantly outperform the query likelihood model of Section 12.2 (page ), but to not perform quite as well as the relevance model mentioned in Section 12.4 (page ) - but the latter does additional per-query processing unlike LDA. Teh et al. (2006) generalize further by presenting Hierarchical Dirichlet Processes , a probabilistic model which allows a group (for us, a document) to be drawn from an infinite mixture of latent topics, while still allowing these topics to be shared across documents. Exercises. Assume you have a set of documents each of which is in either English or in Spanish. The collection is given in Figure 18.4 . Figure: Documents for Exercise 18.5. Figure 18.5 gives a glossary relating the Spanish and English words above for your own information. This glossary is NOT available to the retrieval system: Figure 18.5: Glossary for Exercise 18.5. Construct the appropriate term-document matrix to use for a collection consisting of these documents. For simplicity, use raw term frequencies rather than normalized tf-idf weights. Make sure to clearly label the dimensions of your matrix. Write down the matrices and and from these derive the rank 2 approximation . State succinctly what the entry in the matrix represents. State succinctly what the entry in the matrix represents, and why it differs from that in . </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_19</id>
    <title>Web search basics</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/web-search-basics-1.html</url>
    <file>web-search-basics-1.html</file>
    <text> Web search basics 19.1 19.4 19.5 19.6   Subsections Background and history Web characteristics The web graph Spam Advertising as the economic model The search user experience User query needs Index size and estimation Near-duplicates and shingling References and further reading </text>
    <subsections>
       <section>
         <id>iir_19_1</id>
         <title>Background and history</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/background-and-history-1.html</url>
         <file>background-and-history-1.html</file>
         <text> Background and history The invention of hypertext, envisioned by Vannevar Bush in the 1940's and first realized in working systems in the 1970's, significantly precedes the formation of the World Wide Web (which we will simply refer to as the Web), in the 1990's. Web usage has shown tremendous growth to the point where it now claims a good fraction of humanity as participants, by relying on a simple, open client-server design: (1) the server communicates with the client via a protocol (the http or hypertext transfer protocol) that is lightweight and simple, asynchronously carrying a variety of payloads (text, images and - over time - richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language); (2) the client - generally a browser, an application within a graphical user environment - can ignore what it does not understand. Each of these seemingly innocuous features has contributed enormously to the growth of the Web, so it is worthwhile to examine them further. The basic operation is as follows: a client (such as a browser) sends an http request to a web server. The browser specifies a URL (for Uniform Resource Locator) such as http://www.stanford.edu/home/atoz/contact.html. In this example URL, the string http refers to the protocol to be used for transmitting the data. The string www.stanford.edu is known as the domain and specifies the root of a hierarchy of web pages (typically mirroring a filesystem hierarchy underlying the web server). In this example, /home/atoz/contact.html is a path in this hierarchy with a file contact.html that contains the information to be returned by the web server at www.stanford.edu in response to this request. The HTML-encoded file contact.html holds the hyperlinks and the content (in this instance, contact information for Stanford University), as well as formatting rules for rendering this content in a browser. Such an http request thus allows us to fetch the content of a page, something that will prove to be useful to us for crawling and indexing documents (Chapter 20 ). The designers of the first browsers made it easy to view the HTML markup tags on the content of a URL. This simple convenience allowed new users to create their own HTML content without extensive training or experience; rather, they learned from example content that they liked. As they did so, a second feature of browsers supported the rapid proliferation of web content creation and usage: browsers ignored what they did not understand. This did not, as one might fear, lead to the creation of numerous incompatible dialects of HTML. What it did promote was amateur content creators who could freely experiment with and learn from their newly created web pages without fear that a simple syntax error would ``bring the system down.'' Publishing on the Web became a mass activity that was not limited to a few trained programmers, but rather open to tens and eventually hundreds of millions of individuals. For most users and for most information needs, the Web quickly became the best way to supply and consume information on everything from rare ailments to subway schedules. The mass publishing of information on the Web is essentially useless unless this wealth of information can be discovered and consumed by other users. Early attempts at making web information ``discoverable'' fell into two broad categories: (1) full-text index search engines such as Altavista, Excite and Infoseek and (2) taxonomies populated with web pages in categories, such as Yahoo! The former presented the user with a keyword search interface supported by inverted indexes and ranking mechanisms building on those introduced in earlier chapters. The latter allowed the user to browse through a hierarchical tree of category labels. While this is at first blush a convenient and intuitive metaphor for finding web pages, it has a number of drawbacks: first, accurately classifying web pages into taxonomy tree nodes is for the most part a manual editorial process, which is difficult to scale with the size of the Web. Arguably, we only need to have ``high-quality'' web pages in the taxonomy, with only the best web pages for each category. However, just discovering these and classifying them accurately and consistently into the taxonomy entails significant human effort. Furthermore, in order for a user to effectively discover web pages classified into the nodes of the taxonomy tree, the user's idea of what sub-tree(s) to seek for a particular topic should match that of the editors performing the classification. This quickly becomes challenging as the size of the taxonomy grows; the Yahoo! taxonomy tree surpassed 1000 distinct nodes fairly early on. Given these challenges, the popularity of taxonomies declined over time, even though variants (such as About.com and the Open Directory Project) sprang up with subject-matter experts collecting and annotating web pages for each category. The first generation of web search engines transported classical search techniques such as those in the preceding chapters to the web domain, focusing on the challenge of scale. The earliest web search engines had to contend with indexes containing tens of millions of documents, which was a few orders of magnitude larger than any prior information retrieval system in the public domain. Indexing, query serving and ranking at this scale required the harnessing together of tens of machines to create highly available systems, again at scales not witnessed hitherto in a consumer-facing search application. The first generation of web search engines was largely successful at solving these challenges while continually indexing a significant fraction of the Web, all the while serving queries with sub-second response times. However, the quality and relevance of web search results left much to be desired owing to the idiosyncrasies of content creation on the Web that we discuss in Section 19.2 . This necessitated the invention of new ranking and spam-fighting techniques in order to ensure the quality of the search results. While classical information retrieval techniques (such as those covered earlier in this book) continue to be necessary for web search, they are not by any means sufficient. A key aspect (developed further in Chapter 21 ) is that whereas classical techniques measure the relevance of a document to a query, there remains a need to gauge the authoritativeness of a document based on cues such as which website hosts it. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_19_2</id>
         <title>Web characteristics</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/web-characteristics-1.html</url>
         <file>web-characteristics-1.html</file>
         <text> Web characteristics What about the substance of the text in web pages? The democratization of content creation on the web meant a new level of granularity in opinion on virtually any subject. This meant that the web contained truth, lies, contradictions and suppositions on a grand scale. This gives rise to the question: which web pages does one trust? In a simplistic approach, one might argue that some publishers are trustworthy and others not - begging the question of how a search engine is to assign such a measure of trust to each website or web page. In Chapter 21 we will examine approaches to understanding this question. More subtly, there may be no universal, user-independent notion of trust; a web page whose contents are trustworthy to one user may not be so to another. In traditional (non-web) publishing this is not an issue: users self-select sources they find trustworthy. Thus one reader may find the reporting of The New York Times to be reliable, while another may prefer The Wall Street Journal. But when a search engine is the only viable means for a user to become aware of (let alone select) most content, this challenge becomes significant. While the question ``how big is the Web?'' has no easy answer (see Section 19.5 ), the question ``how many web pages are in a search engine's index'' is more precise, although, even this question has issues. By the end of 1995, Altavista reported that it had crawled and indexed approximately 30 million static web pages . Static web pages are those whose content does not vary from one request for that page to the next. For this purpose, a professor who manually updates his home page every week is considered to have a static web page, but an airport's flight status page is considered to be dynamic. Dynamic pages are typically mechanically generated by an application server in response to a query to a database, as show in Figure 19.1 . One sign of such a page is that the URL has the character "?" in it. Since the number of static web pages was believed to be doubling every few months in 1995, early web search engines such as Altavista had to constantly add hardware and bandwidth for crawling and indexing web pages. A dynamically generated web page.The browser sends a request for flight information on flight AA129 to the web application, that fetches the information from back-end databases then creates a dynamic web page that it returns to the browser.   Subsections The web graph Spam</text>
         <subsections>
            <section>
              <id>iir_19_2_1</id>
              <title>The web graph</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-web-graph-1.html</url>
              <file>the-web-graph-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_19_2_2</id>
              <title>Spam</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/spam-1.html</url>
              <file>spam-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_19_3</id>
         <title>Advertising as the economic model</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/advertising-as-the-economic-model-1.html</url>
         <file>advertising-as-the-economic-model-1.html</file>
         <text> Advertising as the economic model branding cost per mil  CPM impressions clicked on cost per click  CPC The pioneer in this direction was a company named Goto, which changed its name to Overture prior to eventual acquisition by Yahoo! Goto was not, in the traditional sense, a search engine; rather, for every query term it accepted bids from companies who wanted their web page shown on the query . In response to the query , Goto would return the pages of all advertisers who bid for , ordered by their bids. Furthermore, when the user clicked on one of the returned results, the corresponding advertiser would make a payment to Goto (in the initial implementation, this payment equaled the advertiser's bid for ). Several aspects of Goto's model are worth highlighting. First, a user typing the query into Goto's search interface was actively expressing an interest and intent related to the query . For instance, a user typing golf clubs is more likely to be imminently purchasing a set than one who is simply browsing news on golf. Second, Goto only got compensated when a user actually expressed interest in an advertisement - as evinced by the user clicking the advertisement. Taken together, these created a powerful mechanism by which to connect advertisers to consumers, quickly raising the annual revenues of Goto/Overture into hundreds of millions of dollars. This style of search engine came to be known variously as sponsored search or search advertising . Given these two kinds of search engines - the ``pure'' search engines such as Google and Altavista, versus the sponsored search engines - the logical next step was to combine them into a single user experience. Current search engines follow precisely this model: they provide pure search results (generally known as algorithmic search results) as the primary response to a user's search, together with sponsored search results displayed separately and distinctively to the right of the algorithmic results. This is shown in Figure 19.6 . Retrieving sponsored search results and ranking them in response to a query has now become considerably more sophisticated than the simple Goto scheme; the process entails a blending of ideas from information retrieval and microeconomics, and is beyond the scope of this book. For advertisers, understanding how search engines do this ranking and how to allocate marketing campaign budgets to different keywords and to different sponsored search engines has become a profession known as search engine marketing (SEM).  Search advertising triggered by query keywords.Here the query A320 returns algorithmic search results about the Airbus aircraft, together with advertisements for various non-aircraft goods numbered A320, that advertisers seek to market to those querying on this query. The lack of advertisements for the aircraft reflects the fact that few marketers attempt to sell A320 aircraft on the web. The inherently economic motives underlying sponsored search give rise to attempts by some participants to subvert the system to their advantage. This can take many forms, one of which is known as click spam . There is currently no universally accepted definition of click spam. It refers (as the name suggests) to clicks on sponsored search results that are not from bona fide search users. For instance, a devious advertiser may attempt to exhaust the advertising budget of a competitor by clicking repeatedly (through the use of a robotic click generator) on that competitor's sponsored search advertisements. Search engines face the challenge of discerning which of the clicks they observe are part of a pattern of click spam, to avoid charging their advertiser clients for such clicks. Exercises. The Goto method ranked advertisements matching a query by bid: the highest-bidding advertiser got the top position, the second-highest the next, and so on. What can go wrong with this when the highest-bidding advertiser places an advertisement that is irrelevant to the query? Why might an advertiser with an irrelevant advertisement bid high in this manner? Suppose that, in addition to bids, we had for each advertiser their click-through rate: the ratio of the historical number of times users click on their advertisement to the number of times the advertisement was shown. Suggest a modification of the Goto scheme that exploits this data to avoid the problem in Exercise 19.3 above.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_19_4</id>
         <title>The search user experience</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-search-user-experience-1.html</url>
         <file>the-search-user-experience-1.html</file>
         <text> The search user experience It is clear that the more user traffic a web search engine can attract, the more revenue it stands to earn from sponsored search. How do search engines differentiate themselves and grow their traffic? Here Google identified two principles that helped it grow at the expense of its competitors: (1) a focus on relevance, specifically precision rather than recall in the first few results; (2) a user experience that is lightweight, meaning that both the search query page and the search results page are uncluttered and almost entirely textual, with very few graphical elements. The effect of the first was simply to save users time in locating the information they sought. The effect of the second is to provide a user experience that is extremely responsive, or at any rate not bottlenecked by the time to load the search query or results page.  Subsections User query needs</text>
         <subsections>
            <section>
              <id>iir_19_4_1</id>
              <title>User query needs</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/user-query-needs-1.html</url>
              <file>user-query-needs-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_19_5</id>
         <title>Index size and estimation</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/index-size-and-estimation-1.html</url>
         <file>index-size-and-estimation-1.html</file>
         <text> Index size and estimation http://www.yahoo.com/any_string  spider traps 20 We could ask the following better-defined question: given two search engines, what are the relative sizes of their indexes? Even this question turns out to be imprecise, because: In response to queries a search engine can return web pages whose contents it has not (fully or even partially) indexed. For one thing, search engines generally index only the first few thousand words in a web page. In some cases, a search engine is aware of a page that is linked to by pages it has indexed, but has not indexed itself. As we will see in Chapter 21 , it is still possible to meaningfully return in search results. Search engines generally organize their indexes in various tiers and partitions, not all of which are examined on every search (recall tiered indexes from Section 7.2.1 ). For instance, a web page deep inside a website may be indexed but not retrieved on general web searches; it is however retrieved as a result on a search that a user has explicitly restricted to that website (such site-specific search is offered by most web search engines).   20  capture-recapture method Suppose that we could pick a random page from the index of and test whether it is in 's index and symmetrically, test whether a random page from is in . These experiments give us fractions and such that our estimate is that a fraction of the pages in are in , while a fraction of the pages in are in . Then, letting denote the size of the index of search engine , we have (245)   (246)    246   from outside the search engine To implement the sampling phase, we might generate a random page from the entire (idealized, finite) Web and test it for presence in each search engine. Unfortunately, picking a web page uniformly at random is a difficult problem. We briefly outline several attempts to achieve such a sample, pointing out the biases inherent to each; following this we describe in some detail one technique that much research has built on. Random searches: Begin with a search log of web searches; send a random search from this log to and a random page from the results. Since such logs are not widely available outside a search engine, one implementation is to trap all search queries going out of a work group (say scientists in a research center) that agrees to have all its searches logged. This approach has a number of issues, including the bias from the types of searches made by the work group. Further, a random document from the results of such a random search to is not the same as a random document from . Random IP addresses: A second approach is to generate random IP addresses and send a request to a web server residing at the random address, collecting all pages at that server. The biases here include the fact that many hosts might share one IP (due to a practice known as virtual hosting) or not accept http requests from the host where the experiment is conducted. Furthermore, this technique is more likely to hit one of the many sites with few pages, skewing the document probabilities; we may be able to correct for this effect if we understand the distribution of the number of pages on websites. Random walks: If the web graph were a strongly connected directed graph, we could run a random walk starting at an arbitrary web page. This walk would converge to a steady state distribution (see Chapter 21 , Section 21.2.1 for more background material on this), from which we could in principle pick a web page with a fixed probability. This method, too has a number of biases. First, the Web is not strongly connected so that, even with various corrective rules, it is difficult to argue that we can reach a steady state distribution starting from any page. Second, the time it takes for the random walk to settle into this steady state is unknown and could exceed the length of the experiment. Clearly each of these approaches is far from perfect. We now describe a fourth sampling approach, random queries. This approach is noteworthy for two reasons: it has been successfully built upon for a series of increasingly refined estimates, and conversely it has turned out to be the approach most likely to be misinterpreted and carelessly implemented, leading to misleading measurements. The idea is to pick a page (almost) uniformly at random from a search engine's index by posing a random query to it. It should be clear that picking a set of random terms from (say) Webster's dictionary is not a good way of implementing this idea. For one thing, not all vocabulary terms occur equally often, so this approach will not result in documents being chosen uniformly at random from the search engine. For another, there are a great many terms in web documents that do not occur in a standard dictionary such as Webster's. To address the problem of vocabulary terms not in a standard dictionary, we begin by amassing a sample web dictionary. This could be done by crawling a limited portion of the Web, or by crawling a manually-assembled representative subset of the Web such as Yahoo! (as was done in the earliest experiments with this method). Consider a conjunctive query with two or more randomly chosen words from this dictionary. Operationally, we proceed as follows: we use a random conjunctive query on and pick from the top 100 returned results a page at random. We then test for presence in by choosing 6-8 low-frequency terms in and using them in a conjunctive query for . We can improve the estimate by repeating the experiment a large number of times. Both the sampling process and the testing process have a number of issues. Our sample is biased towards longer documents. Picking from the top 100 results of induces a bias from the ranking algorithm of . Picking from all the results of makes the experiment slower. This is particularly so because most web search engines put up defenses against excessive robotic querying. During the checking phase, a number of additional biases are introduced: for instance, may not handle 8-word conjunctive queries properly. Either or may refuse to respond to the test queries, treating them as robotic spam rather than as bona fide queries. There could be operational problems like connection time-outs. A sequence of research has built on this basic paradigm to eliminate some of these issues; there is no perfect solution yet, but the level of sophistication in statistics for understanding the biases is increasing. The main idea is to address biases by estimating, for each document, the magnitude of the bias. From this, standard statistical sampling methods can generate unbiased samples. In the checking phase, the newer work moves away from conjunctive queries to phrase and other queries that appear to be better-behaved. Finally, newer experiments use other sampling methods besides random queries. The best known of these is document random walk sampling, in which a document is chosen by a random walk on a virtual graph derived from documents. In this graph, nodes are documents; two documents are connected by an edge if they share two or more words in common. The graph is never instantiated; rather, a random walk on it can be performed by moving from a document to another by picking a pair of keywords in , running a query on a search engine and picking a random document from the results. Details may be found in the references in Section 19.7 . Exercises. Two web search engines A and B each generate a large number of pages uniformly at random from their indexes. 30% of A's pages are present in B's index, while 50% of B's pages are present in A's index. What is the number of pages in A's index relative to B's? </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_19_6</id>
         <title>Near-duplicates and shingling</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/near-duplicates-and-shingling-1.html</url>
         <file>near-duplicates-and-shingling-1.html</file>
         <text> Near-duplicates and shingling 19.5 duplication The simplest approach to detecting duplicates is to compute, for each web page, a fingerprint that is a succinct (say 64-bit) digest of the characters on that page. Then, whenever the fingerprints of two web pages are equal, we test whether the pages themselves are equal and if so declare one of them to be a duplicate copy of the other. This simplistic approach fails to capture a crucial and widespread phenomenon on the Web: near duplication. In many cases, the contents of one web page are identical to those of another except for a few characters - say, a notation showing the date and time at which the page was last modified. Even in such cases, we want to be able to declare the two pages to be close enough that we only index one copy. Short of exhaustively comparing all pairs of web pages, an infeasible task at the scale of billions of pages, how can we detect and filter out such near duplicates? We now describe a solution to the problem of detecting near-duplicate web pages. The answer lies in a technique known as shingling . Given a positive integer and a sequence of terms in a document , define the -shingles of to be the set of all consecutive sequences of terms in . As an example, consider the following text: a rose is a rose is a rose. The 4-shingles for this text ( is a typical value used in the detection of near-duplicate web pages) are a rose is a, rose is a rose and is a rose is. The first two of these shingles each occur twice in the text. Intuitively, two documents are near duplicates if the sets of shingles generated from them are nearly the same. We now make this intuition precise, then develop a method for efficiently computing and comparing the sets of shingles for all web pages. Let denote the set of shingles of document . Recall the Jaccard coefficient from page 3.3.4 , which measures the degree of overlap between the sets and as ; denote this by . Our test for near duplication between and is to compute this Jaccard coefficient; if it exceeds a preset threshold (say, ), we declare them near duplicates and eliminate one from indexing. However, this does not appear to have simplified matters: we still have to compute Jaccard coefficients pairwise. To avoid this, we use a form of hashing. First, we map every shingle into a hash value over a large space, say 64 bits. For , let be the corresponding set of 64-bit hash values derived from . We now invoke the following trick to detect document pairs whose sets have large Jaccard overlaps. Let be a random permutation from the 64-bit integers to the 64-bit integers. Denote by the set of permuted hash values in ; thus for each , there is a corresponding value .  Let be the smallest integer in . Then Theorem. (247)  End theorem. Proof. We give the proof in a slightly more general setting: consider a family of sets whose elements are drawn from a common universe. View the sets as columns of a matrix , with one row for each element in the universe. The element if element is present in the set that the th column represents. Let be a random permutation of the rows of ; denote by the column that results from applying to the th column. Finally, let be the index of the first row in which the column has a . We then prove that for any two columns , (248)   Figure 19.9: Two sets and ; their Jaccard coefficient is . Consider two columns as shown in Figure 19.9 . The ordered pairs of entries of and partition the rows into four types: those with 0's in both of these columns, those with a 0 in and a 1 in , those with a 1 in and a 0 in , and finally those with 1's in both of these columns. Indeed, the first four rows of Figure 19.9 exemplify all of these four types of rows. Denote by the number of rows with 0's in both columns, the second, the third and the fourth. Then, (249)  249    249 End proof. Thus, our test for the Jaccard coefficient of the shingle sets is probabilistic: we compare the computed values from different documents. If a pair coincides, we have candidate near duplicates. Repeat the process independently for 200 random permutations (a choice suggested in the literature). Call the set of the 200 resulting values of the sketch of . We can then estimate the Jaccard coefficient for any pair of documents to be ; if this exceeds a preset threshold, we declare that and are similar. How can we quickly compute for all pairs ? Indeed, how do we represent all pairs of documents that are similar, without incurring a blowup that is quadratic in the number of documents? First, we use fingerprints to remove all but one copy of identical documents. We may also remove common HTML tags and integers from the shingle computation, to eliminate shingles that occur very commonly in documents without telling us anything about duplication. Next we use a union-find algorithm to create clusters that contain documents that are similar. To do this, we must accomplish a crucial step: going from the set of sketches to the set of pairs such that and are similar. To this end, we compute the number of shingles in common for any pair of documents whose sketches have any members in common. We begin with the list sorted by pairs. For each , we can now generate all pairs for which is present in both their sketches. From these we can compute, for each pair with non-zero sketch overlap, a count of the number of values they have in common. By applying a preset threshold, we know which pairs have heavily overlapping sketches. For instance, if the threshold were 80%, we would need the count to be at least 160 for any . As we identify such pairs, we run the union-find to group documents into near-duplicate ``syntactic clusters''. This is essentially a variant of the single-link clustering algorithm introduced in Section 17.2 (page ). One final trick cuts down the space needed in the computation of for pairs , which in principle could still demand space quadratic in the number of documents. To remove from consideration those pairs whose sketches have few shingles in common, we preprocess the sketch for each document as follows: sort the in the sketch, then shingle this sorted sequence to generate a set of super-shingles for each document. If two documents have a super-shingle in common, we proceed to compute the precise value of . This again is a heuristic but can be highly effective in cutting down the number of pairs for which we accumulate the sketch overlap counts. Exercises. Web search engines A and B each crawl a random subset of the same size of the Web. Some of the pages crawled are duplicates - exact textual copies of each other at different URLs. Assume that duplicates are distributed uniformly amongst the pages crawled by A and B. Further, assume that a duplicate is a page that has exactly two copies - no pages have more than two copies. A indexes pages without duplicate elimination whereas B indexes only one copy of each duplicate page. The two random subsets have the same size before duplicate elimination. If, 45% of A's indexed URLs are present in B's index, while 50% of B's indexed URLs are present in A's index, what fraction of the Web consists of pages that do not have a duplicate? Instead of using the process depicted in Figure 19.8 , consider instead the following process for estimating the Jaccard coefficient of the overlap between two sets and . We pick a random subset of the elements of the universe from which and are drawn; this corresponds to picking a random subset of the rows of the matrix in the proof. We exhaustively compute the Jaccard coefficient of these random subsets. Why is this estimate an unbiased estimator of the Jaccard coefficient for and ? Explain why this estimator would be very difficult to use in practice.</text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_19_7</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-19.html</url>
         <file>references-and-further-reading-19.html</file>
         <text> References and further reading Bush (1945) memex Berners-Lee et al. (1992) Kumar et al. (2000) Broder et al. (2000) McBryan (1994) 19.4 Broder (2002) 19.2.1 Kumar et al. (1999) Chakrabarti (2002) The estimation of web search index sizes has a long history of development covered by Bharat and Broder (1998), Lawrence and Giles (1998), Rusmevichientong et al. (2001), Lawrence and Giles (1999), Henzinger et al. (2000), Bar-Yossef and Gurevich (2006). The state of the art is Bar-Yossef and Gurevich (2006), including several of the bias-removal techniques mentioned at the end of Section 19.5 . Shingling was introduced by Broder et al. (1997) and used for detecting websites (rather than simply pages) that are identical by Bharat et al. (2000). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_20</id>
    <title>Web crawling and indexes</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/web-crawling-and-indexes-1.html</url>
    <file>web-crawling-and-indexes-1.html</file>
    <text> Web crawling and indexes   Subsections Overview Features a crawler must provide Features a crawler should provide Crawling Crawler architecture Distributing the crawler DNS resolution The URL frontier Distributing indexes Connectivity servers References and further reading </text>
    <subsections>
       <section>
         <id>iir_20_1</id>
         <title>Overview</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/overview-1.html</url>
         <file>overview-1.html</file>
         <text> Overview 19 19.7  web crawler  spider The goal of this chapter is not to describe how to build the crawler for a full-scale commercial web search engine. We focus instead on a range of issues that are generic to crawling from the student project scale to substantial research projects. We begin (Section 20.1.1 ) by listing desiderata for web crawlers, and then discuss in Section 20.2 how each of these issues is addressed. The remainder of this chapter describes the architecture and some implementation details for a distributed web crawler that satisfies these features. Section 20.3 discusses distributing indexes across many machines for a web-scale implementation.  Subsections Features a crawler must provide Features a crawler should provide</text>
         <subsections>
            <section>
              <id>iir_20_1_1</id>
              <title>Features a crawler must provide</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/features-a-crawler-must-provide-1.html</url>
              <file>features-a-crawler-must-provide-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_20_1_2</id>
              <title>Features a crawler should provide</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/features-a-crawler-should-provide-1.html</url>
              <file>features-a-crawler-should-provide-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_20_2</id>
         <title>Crawling</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/crawling-1.html</url>
         <file>crawling-1.html</file>
         <text> Crawling seed set 4 5 URL frontier 19 This seemingly simple recursive traversal of the web graph is complicated by the many demands on a practical web crawling system: the crawler has to be distributed, scalable, efficient, polite, robust and extensible while fetching pages of high quality. We examine the effects of each of these issues. Our treatment follows the design of the Mercator crawler that has formed the basis of a number of research and commercial crawlers. As a reference point, fetching a billion pages (a small fraction of the static Web at present) in a month-long crawl requires fetching several hundred pages each second. We will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate. Before proceeding to this detailed description, we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy: Only one connection should be open to any given host at a time. A waiting time of a few seconds should occur between successive requests to a host. Politeness restrictions detailed in Section 20.2.1 should be obeyed.   Subsections Crawler architecture Distributing the crawler DNS resolution The URL frontier</text>
         <subsections>
            <section>
              <id>iir_20_2_1</id>
              <title>Crawler architecture</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/crawler-architecture-1.html</url>
              <file>crawler-architecture-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_20_2_2</id>
              <title>DNS resolution</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/dns-resolution-1.html</url>
              <file>dns-resolution-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_20_2_3</id>
              <title>The URL frontier</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-url-frontier-1.html</url>
              <file>the-url-frontier-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_20_3</id>
         <title>Distributing indexes</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/distributing-indexes-1.html</url>
         <file>distributing-indexes-1.html</file>
         <text> Distributing indexes In Section 4.4 we described distributed indexing. We now consider the distribution of the index across a large computer cluster that supports querying. Two obvious alternative index implementations suggest themselves: partitioning by terms , also known as global index organization, and partitioning by documents , also know as local index organization. In the former, the dictionary of index terms is partitioned into subsets, each subset residing at a node. Along with the terms at a node, we keep the postings for those terms. A query is routed to the nodes corresponding to its query terms. In principle, this allows greater concurrency since a stream of queries with different query terms would hit different sets of machines. In practice, partitioning indexes by vocabulary terms turns out to be non-trivial. Multi-word queries require the sending of long postings lists between sets of nodes for merging, and the cost of this can outweigh the greater concurrency. Load balancing the partition is governed not by an a priori analysis of relative term frequencies, but rather by the distribution of query terms and their co-occurrences, which can drift with time or exhibit sudden bursts. Achieving good partitions is a function of the co-occurrences of query terms and entails the clustering of terms to optimize objectives that are not easy to quantify. Finally, this strategy makes implementation of dynamic indexing more difficult. A more common implementation is to partition by documents: each node contains the index for a subset of all documents. Each query is distributed to all nodes, with the results from various nodes being merged before presentation to the user. This strategy trades more local disk seeks for less inter-node communication. One difficulty in this approach is that global statistics used in scoring - such as idf - must be computed across the entire document collection even though the index at any single node only contains a subset of the documents. These are computed by distributed ``background'' processes that periodically refresh the node indexes with fresh global statistics. How do we decide the partition of documents to nodes? Based on our development of the crawler architecture in Section 20.2.1 , one simple approach would be to assign all pages from a host to a single node. This partitioning could follow the partitioning of hosts to crawler nodes. A danger of such partitioning is that on many queries, a preponderance of the results would come from documents at a small number of hosts (and hence a small number of index nodes). A hash of each URL into the space of index nodes results in a more uniform distribution of query-time computation across nodes. At query time, the query is broadcast to each of the nodes, with the top results from each node being merged to find the top documents for the query. A common implementation heuristic is to partition the document collection into indexes of documents that are more likely to score highly on most queries (using, for instance, techniques in Chapter 21 ) and low-scoring indexes with the remaining documents. We only search the low-scoring indexes when there are too few matches in the high-scoring indexes, as described in Section 7.2.1 . </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_20_4</id>
         <title>Connectivity servers</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html</url>
         <file>connectivity-servers-1.html</file>
         <text> Connectivity servers 21  connectivity server  connectivity queries which URLs link to a given URL? which URLs does a given URL link to? link analysis 21 Suppose that the Web had four billion pages, each with ten links to other pages. In the simplest form, we would require 32 bits or 4 bytes to specify each end (source and destination) of each link, requiring a total of (250)  5 We assume that each web page is represented by a unique integer; the specific scheme used to assign these integers is described below. We build an adjacency table that resembles an inverted index: it has a row for each web page, with the rows ordered by the corresponding integers. The row for any page contains a sorted list of integers, each corresponding to a web page that links to . This table permits us to respond to queries of the form which pages link to ? In similar fashion we build a table whose entries are the pages linked to by . This table representation cuts the space taken by the naive representation (in which we explicitly represent each link by its two end points, each a 32-bit integer) by 50%. Our description below will focus on the table for the links from each page; it should be clear that the techniques apply just as well to the table of links to each page. To further reduce the storage for the table, we exploit several ideas: Similarity between lists: Many rows of the table have many entries in common. Thus, if we explicitly represent a prototype row for several similar rows, the remainder can be succinctly expressed in terms of the prototypical row. Locality: many links from a page go to ``nearby'' pages - pages on the same host, for instance. This suggests that in encoding the destination of a link, we can often use small integers and thereby save space. We use gap encodings in sorted lists: rather than store the destination of each link, we store the offset from the previous entry in the row. In a lexicographic ordering of all URLs, we treat each URL as an alphanumeric string and sort these strings. Figure 20.5 shows a segment of this sorted order. For a true lexicographic sort of web pages, the domain name part of the URL should be inverted, so that www.stanford.edu becomes edu.stanford.www, but this is not necessary here since we are mainly concerned with links local to a single host.  Figure 20.5: A lexicographically ordered set of URLs. To each URL, we assign its position in this ordering as the unique identifying integer. Figure 20.6 shows an example of such a numbering and the resulting table. In this example sequence, www.stanford.edu/biology is assigned the integer 2 since it is second in the sequence. We next exploit a property that stems from the way most websites are structured to get similarity and locality. Most websites have a template with a set of links from each page in the site to a fixed set of pages on the site (such as its copyright notice, terms of use, and so on). In this case, the rows corresponding to pages in a website will have many table entries in common. Moreover, under the lexicographic ordering of URLs, it is very likely that the pages from a website appear as contiguous rows in the table.  Figure 20.6: A four-row segment of the table of links. We adopt the following strategy: we walk down the table, encoding each table row in terms of the seven preceding rows. In the example of Figure 20.6, we could encode the fourth row as ``the same as the row at offset 2 (meaning, two rows earlier in the table), with 9 replaced by 8''. This requires the specification of the offset, the integer(s) dropped (in this case 9) and the integer(s) added (in this case 8). The use of only the seven preceding rows has two advantages: (i) the offset can be expressed with only 3 bits; this choice is optimized empirically (the reason for seven and not eight preceding rows is the subject of Exercise 20.4) and (ii) fixing the maximum offset to a small value like seven avoids having to perform an expensive search among many candidate prototypes in terms of which to express the current row. What if none of the preceding seven rows is a good prototype for expressing the current row? This would happen, for instance, at each boundary between different websites as we walk down the rows of the table. In this case we simply express the row as starting from the empty set and ``adding in'' each integer in that row. By using gap encodings to store the gaps (rather than the actual integers) in each row, and encoding these gaps tightly based on the distribution of their values, we obtain further space reduction. In experiments mentioned in Section 20.5 , the series of techniques outlined here appears to use as few as 3 bits per link, on average - a dramatic reduction from the 64 required in the naive representation. While these ideas give us a representation of sizable web graphs that comfortably fit in memory, we still need to support connectivity queries. What is entailed in retrieving from this representation the set of links from a page? First, we need an index lookup from (a hash of) the URL to its row number in the table. Next, we need to reconstruct these entries, which may be encoded in terms of entries in other rows. This entails following the offsets to reconstruct these other rows - a process that in principle could lead through many levels of indirection. In practice however, this does not happen very often. A heuristic for controlling this can be introduced into the construction of the table: when examining the preceding seven rows as candidates from which to model the current row, we demand a threshold of similarity between the current row and the candidate prototype. This threshold must be chosen with care. If the threshold is set too high, we seldom use prototypes and express many rows afresh. If the threshold is too low, most rows get expressed in terms of prototypes, so that at query time the reconstruction of a row leads to many levels of indirection through preceding prototypes. Exercises. We noted that expressing a row in terms of one of seven preceding rows allowed us to use no more than three bits to specify which of the preceding rows we are using as prototype. Why seven and not eight preceding rows? (Hint: consider the case when none of the preceding seven rows is a good prototype.) We noted that for the scheme in Section 20.4 , decoding the links incident on a URL could result in many levels of indirection. Construct an example in which the number of levels of indirection grows linearly with the number of URLs. </text>
         <subsections>
         </subsections>
       </section>
       <section>
         <id>iir_20_5</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-20.html</url>
         <file>references-and-further-reading-20.html</file>
         <text> References and further reading Najork and Heydon, 2002 2001 Burner (1997) Brin and Page (1998) Cho et al. (1998) Hirai et al., 2000 Cho and Garcia-Molina (2002) http://www.robotstxt.org/wc/exclusion.html Boldi et al. (2002) Shkapenyuk and Suel (2002) Our discussion of DNS resolution (Section 20.2.2 ) uses the current convention for internet addresses, known as IPv4 (for Internet Protocol version 4) - each IP address is a sequence of four bytes. In the future, the convention for addresses (collectively known as the internet address space) is likely to use a new standard known as IPv6 (http://www.ipv6.org/). Tomasic and Garcia-Molina (1993) and Jeong and Omiecinski (1995) are key early papers evaluating term partitioning versus document partitioning for distributed indexes. Document partitioning is found to be superior, at least when the distribution of terms is skewed, as it typically is in practice. This result has generally been confirmed in more recent work (MacFarlane et al., 2000). But the outcome depends on the details of the distributed system; at least one thread of work has reached the opposite conclusion (Ribeiro-Neto and Barbosa, 1998, Badue et al., 2001). Sornil (2001) argues for a partitioning scheme that is a hybrid between term and document partitioning. Barroso et al. (2003) describe the distribution methods used at Google. The first implementation of a connectivity server was described by Bharat et al. (1998). The scheme discussed in this chapter, currently believed to be the best published scheme (achieving as few as 3 bits per link for encoding), is described in a series of papers by Boldi and Vigna (2004b;a). </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_21</id>
    <title>Link analysis</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/link-analysis-1.html</url>
    <file>link-analysis-1.html</file>
    <text> Link analysis The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search. In this chapter we focus on the use of hyperlinks for ranking web search results. Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query. We begin by reviewing some basics of the Web as a graph in Section 21.1 , then proceed to the technical development of the elements of link analysis for ranking. Link analysis for web search has intellectual antecedents in the field of citation analysis, aspects of which overlap with an area known as bibliometrics. These disciplines seek to quantify the influence of scholarly articles by analyzing the pattern of citations amongst them. Much as citations represent the conferral of authority from a scholarly article to others, link analysis on the Web treats hyperlinks from a web page to another as a conferral of authority. Clearly, not every citation or hyperlink implies such authority conferral; for this reason, simply measuring the quality of a web page by the number of in-links (citations from other pages) is not robust enough. For instance, one may contrive to set up multiple web pages pointing to a target web page, with the intent of artificially boosting the latter's tally of in-links. This phenomenon is referred to as link spam . Nevertheless, the phenomenon of citation is prevalent and dependable enough that it is feasible for web search engines to derive useful signals for ranking from more sophisticated link analysis. Link analysis also proves to be a useful indicator of what page(s) to crawl next while crawling the web; this is done by using link analysis to guide the priority assignment in the front queues of Chapter 20 . Section 21.1 develops the basic ideas underlying the use of the web graph in link analysis. and 21.3 then develop two distinct methods for link analysis, PageRank and HITS.   Subsections The Web as a graph Anchor text and the web graph PageRank Markov chains Definition: The PageRank computation Topic-specific PageRank Hubs and Authorities Choosing the subset of the Web References and further reading</text>
    <subsections>
       <section>
         <id>iir_21_1</id>
         <title>The Web as a graph</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-web-as-a-graph-1.html</url>
         <file>the-web-as-a-graph-1.html</file>
         <text> The Web as a graph 19.2.1 19.2 The anchor text pointing to page B is a good description of page B. The hyperlink from A to B represents an endorsement of page B, by the creator of page A. This is not always the case; for instance, many links amongst pages within a single website stem from the user of a common template. For instance, most corporate websites have a pointer from every page to a page containing a copyright notice - this is clearly not an endorsement. Accordingly, implementations of link analysis algorithms will typical discount such ``internal'' links.   Subsections Anchor text and the web graph </text>
         <subsections>
            <section>
              <id>iir_21_1_1</id>
              <title>Anchor text and the web graph</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/anchor-text-and-the-web-graph-1.html</url>
              <file>anchor-text-and-the-web-graph-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_21_2</id>
         <title>PageRank</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/pagerank-1.html</url>
         <file>pagerank-1.html</file>
         <text> PageRank  PageRank 6.3 7.2.2 15.4.1 Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of which there are three hyperlinks to nodes B, C and D; the surfer proceeds at the next time step to one of these three nodes, with equal probabilities 1/3.  Figure 21.1: The random surfer at node A proceeds with probability 1/3 to each of B, C and D. As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important. What if the current location of the surfer, the node A, has no out-links? To address this we introduce an additional operation for our random surfer: the teleport operation. In the teleport operation the surfer jumps from a node to any other node in the web graph. This could happen because he types an address into the URL bar of his browser. The destination of a teleport operation is modeled as being chosen uniformly at random from all web pages. In other words, if is the total number of nodes in the web graph, the teleport operation takes the surfer to each node with probability . The surfer would also teleport to his present position with probability . In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: (1) When at a node with no out-links, the surfer invokes the teleport operation. (2) At any node that has outgoing links, the surfer invokes the teleport operation with probability and the standard random walk (follow an out-link chosen uniformly at random as in Figure 21.1 ) with probability , where is a fixed parameter chosen in advance. Typically, might be 0.1. In Section 21.2.1 , we will use the theory of Markov chains to argue that when the surfer follows this combined process (random walk plus teleport) he visits each node of the web graph a fixed fraction of the time that depends on (1) the structure of the web graph and (2) the value of . We call this value the PageRank of and will show how to compute this value in Section 21.2.2 .   Subsections Markov chains Definition: The PageRank computation Topic-specific PageRank</text>
         <subsections>
            <section>
              <id>iir_21_2_1</id>
              <title>Markov chains</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/markov-chains-1.html</url>
              <file>markov-chains-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_21_2_2</id>
              <title>The PageRank computation</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/the-pagerank-computation-1.html</url>
              <file>the-pagerank-computation-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
            <section>
              <id>iir_21_2_3</id>
              <title>Topic-specific PageRank</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/topic-specific-pagerank-1.html</url>
              <file>topic-specific-pagerank-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_21_3</id>
         <title>Hubs and Authorities</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/hubs-and-authorities-1.html</url>
         <file>hubs-and-authorities-1.html</file>
         <text> Hubs and Authorities two  hub score  authority score This approach stems from a particular insight into the creation of web pages, that there are two primary kinds of web pages useful as results for broad-topic searches. By a broad topic search we mean an informational query such as "I wish to learn about leukemia". There are authoritative sources of information on the topic; in this case, the National Cancer Institute's page on leukemia would be such a page. We will call such pages authorities; in the computation we are about to describe, they are the pages that will emerge with high authority scores. On the other hand, there are many pages on the Web that are hand-compiled lists of links to authoritative web pages on a specific topic. These hub pages are not in themselves authoritative sources of topic-specific information, but rather compilations that someone with an interest in the topic has spent time putting together. The approach we will take, then, is to use these hub pages to discover the authority pages. In the computation we now develop, these hub pages are the pages that will emerge with high hub scores. A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages. We thus appear to have a circular definition of hubs and authorities; we will turn this into an iterative computation. Suppose that we have a subset of the web containing good hub and authority pages, together with the hyperlinks amongst them. We will iteratively compute a hub score and an authority score for every web page in this subset, deferring the discussion of how we pick this subset until Section 21.3.1 . For a web page in our subset of the web, we use to denote its hub score and its authority score. Initially, we set for all nodes . We also denote by the existence of a hyperlink from to . The core of the iterative algorithm is a pair of updates to the hub and authority scores of all pages given by Equation 262, which capture the intuitive notions that good hubs point to good authorities and that good authorities are pointed to by good hubs.  (262) (263)   262    What happens as we perform these updates iteratively, recomputing hub scores, then new authority scores based on the recomputed hub scores, and so on? Let us recast the equations Equation 262 into matrix-vector form. Let and denote the vectors of all hub and all authority scores respectively, for the pages in our subset of the web graph. Let denote the adjacency matrix of the subset of the web graph that we are dealing with: is a square matrix with one row and one column for each page in the subset. The entry is 1 if there is a hyperlink from page to page , and 0 otherwise. Then, we may write Equation 262 (264) (265)     264 264 264  (266) (267)   266 18.1   266    (268) (269)       This leads to some key consequences: The iterative updates in Equation 262 (or equivalently, Equation 264), if scaled by the appropriate eigenvalues, are equivalent to the power iteration method for computing the eigenvectors of and . Provided that the principal eigenvalue of is unique, the iteratively computed entries of and settle into unique steady-state values determined by the entries of and hence the link structure of the graph. In computing these eigenvector entries, we are not restricted to using the power iteration method; indeed, we could use any fast method for computing the principal eigenvector of a stochastic matrix. The resulting computation thus takes the following form: Assemble the target subset of web pages, form the graph induced by their hyperlinks and compute and . Compute the principal eigenvectors of and to form the vector of hub scores and authority scores . Output the top-scoring hubs and the top-scoring authorities.  HITS Hyperlink-Induced Topic Search Worked example. Assuming the query jaguar and double-weighting of links whose anchors contain the query word, the matrix for Figure 21.4 is as follows: (270)  The hub and authority vectors are:  (271)   (272)  Here, is the main authority - two hubs ( and ) are pointing to it via highly weighted jaguar links. End worked example. Since the iterative updates captured the intuition of good hubs and good authorities, the high-scoring pages we output would give us good hubs and authorities from the target subset of web pages. In Section 21.3.1 we describe the remaining detail: how do we gather a target subset of web pages around a topic such as leukemia?   Subsections Choosing the subset of the Web</text>
         <subsections>
            <section>
              <id>iir_21_3_1</id>
              <title>Choosing the subset of the Web</title>
              <url>http://nlp.stanford.edu/IR-book/html/htmledition/choosing-the-subset-of-the-web-1.html</url>
              <file>choosing-the-subset-of-the-web-1.html</file>
              <text></text>
              <subsections>
              </subsections>
            </section>
         </subsections>
       </section>
       <section>
         <id>iir_21_4</id>
         <title>References and further reading</title>
         <url>http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-21.html</url>
         <file>references-and-further-reading-21.html</file>
         <text> References and further reading Garfield (1955) is seminal in the science of citation analysis. This was built on by Pinski and Narin (1976) to develop a journal influence weight, whose definition is remarkably similar to that of the PageRank measure. The use of anchor text as an aid to searching and ranking stems from the work of McBryan (1994). Extended anchor-text was implicit in his work, with systematic experiments reported in Chakrabarti et al. (1998). Kemeny and Snell (1976) is a classic text on Markov chains. The PageRank measure was developed in Brin and Page (1998) and in Page et al. (1998). A number of methods for the fast computation of PageRank values are surveyed in Berkhin (2005) and in Langville and Meyer (2006); the former also details how the PageRank eigenvector solution may be viewed as solving a linear system, leading to one way of solving Exercise 21.2.3 . The effect of the teleport probability has been studied by Baeza-Yates et al. (2005) and by Boldi et al. (2005). Topic-specific PageRank and variants were developed in Haveliwala (2002), Haveliwala (2003) and in Jeh and Widom (2003). Berkhin (2006a) develops an alternate view of topic-specific PageRank. Ng et al. (2001b) suggests that the PageRank score assignment is more robust than HITS in the sense that scores are less sensitive to small changes in graph topology. However, it has also been noted that the teleport operation contributes significantly to PageRank's robustness in this sense. Both PageRank and HITS can be ``spammed'' by the orchestrated insertion of links into the web graph; indeed, the Web is known to have such link farms that collude to increase the score assigned to certain pages by various link analysis algorithms. The HITS algorithm is due to Kleinberg (1999). Chakrabarti et al. (1998) developed variants that weighted links in the iterative computation based on the presence of query terms in the pages being linked and compared these to results from several web search engines. Bharat and Henzinger (1998) further developed these and other heuristics, showing that certain combinations outperformed the basic HITS algorithm. Borodin et al. (2001) provides a systematic study of several variants of the HITS algorithm. Ng et al. (2001b) introduces a notion of stability for link analysis, arguing that small changes to link topology should not lead to significant changes in the ranked list of results for a query. Numerous other variants of HITS have been developed by a number of authors, the best know of which is perhaps SALSA (Lempel and Moran, 2000).   We use the following abbreviated journal and conference names in the bibliography: CACM Communications of the Association for Computing Machinery. IP&M Information Processing and Management. IR Information Retrieval. JACM Journal of the Association for Computing Machinery. JASIS Journal of the American Society for Information Science. JASIST Journal of the American Society for Information Science and Technology. JMLR Journal of Machine Learning Research. TOIS ACM Transactions on Information Systems. Proc. ACL Proceedings of the Annual Meeting of the Association for Computational Linguistics. Available from: http://www.aclweb.org/anthology-index/ Proc. CIKM Proceedings of the ACM CIKM Conference on Information and Knowledge Management. ACM Press. Proc. ECIR Proceedings of the European Conference on Information Retrieval. Proc. ECML Proceedings of the European Conference on Machine Learning. Proc. ICML Proceedings of the International Conference on Machine Learning. Proc. IJCAI Proceedings of the International Joint Conference on Artificial Intelligence. Proc. INEX Proceedings of the Initiative for the Evaluation of XML Retrieval. Proc. KDD Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Proc. NIPS Proceedings of the Neural Information Processing Systems Conference. Proc. PODS Proceedings of the ACM Conference on Principles of Database Systems. Proc. SDAIR Proceedings of the Annual Symposium on Document Analysis and Information Retrieval. Proc. SIGIR Proceedings of the Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval. Available from: http://www.sigir.org/proceedings/Proc-Browse.html Proc. SPIRE Proceedings of the Symposium on String Processing and Information Retrieval. Proc. TREC Proceedings of the Text Retrieval Conference. Proc. UAI Proceedings of the Conference on Uncertainty in Artificial Intelligence. Proc. VLDB Proceedings of the Very Large Data Bases Conference. Proc. WWW Proceedings of the International World Wide Web Conference. </text>
         <subsections>
         </subsections>
       </section>
    </subsections>
  </section>
  <section>
    <id>iir_22</id>
    <title>Bibliography</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/bibliography-1.html</url>
    <file>bibliography-1.html</file>
    <text></text>
    <subsections>
    </subsections>
  </section>
  <section>
    <id>iir_23</id>
    <title>Index</title>
    <url>http://nlp.stanford.edu/IR-book/html/htmledition/index-1.html</url>
    <file>index-1.html</file>
    <text></text>
    <subsections>
    </subsections>
  </section>
</toc>

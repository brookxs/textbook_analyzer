<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Evaluation of clustering</title> 
  <meta name="description" content="Evaluation of clustering" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="k-means-1.html" /> 
  <link rel="previous" href="problem-statement-1.html" /> 
  <link rel="up" href="flat-clustering-1.html" /> 
  <link rel="next" href="k-means-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html4211" href="k-means-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4205" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4199" href="cardinality---the-number-of-clusters-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4207" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4209" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4212" href="k-means-1.html">K-means</a> 
  <b> Up:</b> 
  <a name="tex2html4206" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4200" href="cardinality---the-number-of-clusters-1.html">Cardinality - the number</a> &nbsp; 
  <b> <a name="tex2html4208" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4210" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION002130000000000000000"></a> <a name="sec:clustereval"></a> <a name="p:clustereval"></a> <br /> Evaluation of clustering </h1> 
  <p> Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low inter-cluster similarity (documents from different clusters are dissimilar). This is an <a name="24251"></a> <i>internal criterion</i> for the quality of a clustering. But good scores on an internal criterion do not necessarily translate into good effectiveness in an application. An alternative to internal criteria is direct evaluation in the application of interest. For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms. This is the most direct evaluation, but it is expensive, especially if large user studies are necessary. </p>
  <p> As a surrogate for user judgments, we can use a set of classes in an evaluation benchmark or gold standard (see Section <a href="assessing-relevance-1.html#sec:test-collections">8.5</a> , page <a href="assessing-relevance-1.html#p:test-collections">8.5</a> , and Section <a href="evaluation-of-text-classification-1.html#sec:evalclass">13.6</a> , page <a href="evaluation-of-text-classification-1.html#p:evalclass">13.6</a> ). The gold standard is ideally produced by human judges with a good level of inter-judge agreement (see Chapter <a href="evaluation-in-information-retrieval-1.html#ch:evaluation">8</a> , page <a href="information-retrieval-system-evaluation-1.html#p:goldstandard">8.1</a> ). We can then compute an <a name="24259"></a> <i>external criterion</i> that evaluates how well the clustering matches the gold standard classes. For example, we may want to say that the optimal clustering of the search results for jaguar in Figure <a href="clustering-in-information-retrieval-1.html#fig:clustfg1">16.2</a> consists of three classes corresponding to the three senses car, animal, and operating system. In this type of evaluation, we only use the partition provided by the gold standard, not the class labels. </p>
  <p> This section introduces four external criteria of clustering quality. <i>Purity</i> is a simple and transparent evaluation measure. <i>Normalized mutual information</i> can be information-theoretically interpreted. The <i>Rand index</i> penalizes both false positive and false negative decisions during clustering. The <i>F&nbsp;measure</i> in addition supports differential weighting of these two types of errors. </p>
  <p> </p>
  <div align="CENTER"> 
   <p><a name="fig:clustfg3"></a><a name="p:clustfg3"></a></p>
   <img width="556" height="231" border="0" src="img1393.png" alt="\begin{figure}
% latex2html id marker 24270
\par
\psset{unit=0.75cm}
\par
\begin...
...$,
3
(cluster 3).
Purity is $(1/17) \times (5+4+3)
\approx 0.71$.
}
\end{figure}" /> 
  </div> 
  <p> To compute <a name="24299"></a> <i>purity</i> , each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by <img width="17" height="32" align="MIDDLE" border="0" src="img62.png" alt="$N$" />. Formally: <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\mbox{purity}(
\Omega,\mathbb{C}
) = 
\frac{1}{N}
\sum_k \max_j
|\omega_k \cap
c_j|
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="eqn:purity"></a><img width="242" height="48" border="0" src="img1394.png" alt="\begin{displaymath}
\mbox{purity}(
\Omega,\mathbb{C}
) =
\frac{1}{N}
\sum_k \max_j
\vert\omega_k \cap
c_j\vert
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (182)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> where 
  <!-- MATH
 $\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$
 --> 
  <img width="158" height="33" align="MIDDLE" border="0" src="img1395.png" alt="$\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$" />
  <a name="Omega-notation"></a>
  <a name="omega-notation"></a>is the set of clusters and 
  <!-- MATH
 $\mathbb{C} = \{ c_1, c_2, \ldots,
c_J \}$
 --> 
  <img width="137" height="33" align="MIDDLE" border="0" src="img855.png" alt="$\mathbb{C} = \{ c_1,c_2,\ldots,c_J \}$" /> is the set of classes. We interpret 
  <img width="22" height="32" align="MIDDLE" border="0" src="img1396.png" alt="$\omega_k$" /> as the set of documents in 
  <img width="22" height="32" align="MIDDLE" border="0" src="img1396.png" alt="$\omega_k$" /> and 
  <img width="16" height="32" align="MIDDLE" border="0" src="img1191.png" alt="$c_j$" /> as the set of documents in 
  <img width="16" height="32" align="MIDDLE" border="0" src="img1191.png" alt="$c_j$" /> in Equation&nbsp;
  <a href="#eqn:purity">182</a>. 
  <p> We present an example of how to compute purity in Figure <a href="#fig:clustfg3">16.4</a> .<a name="tex2html182" href="footnode.html#foot25225"><sup><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/footnote.png" /></sup></a> Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1 . Purity is compared with the other three measures discussed in this chapter in Table <a href="#tab:clmeascomp">16.2</a> . </p>
  <p> <br /></p>
  <p></p> 
  <div align="CENTER">
   <a name="25226"></a> 
   <table> 
    <caption>
     <strong>Table 16.2:</strong> The four external evaluation measures applied to the clustering in Figure 
     <a href="#fig:clustfg3">16.4</a> .
    </caption> 
    <tbody>
     <tr>
      <td>
       <table cellpadding="3" border="1"> 
        <tbody>
         <tr>
          <td align="LEFT">&nbsp;</td> 
          <td align="LEFT">purity</td> 
          <td align="LEFT">NMI</td> 
          <td align="LEFT">RI</td> 
          <td align="LEFT"><img width="19" height="32" align="MIDDLE" border="0" src="img1397.png" alt="$F_5$" /></td> 
         </tr> 
         <tr>
          <td align="LEFT">lower bound</td> 
          <td align="LEFT">0.0</td> 
          <td align="LEFT">0.0</td> 
          <td align="LEFT">0.0</td> 
          <td align="LEFT">0.0</td> 
         </tr> 
         <tr>
          <td align="LEFT">maximum</td> 
          <td align="LEFT">1</td> 
          <td align="LEFT">1</td> 
          <td align="LEFT">1</td> 
          <td align="LEFT">1</td> 
         </tr> 
         <tr>
          <td align="LEFT">value for Figure <a href="#fig:clustfg3">16.4</a></td> 
          <td align="LEFT">0.71</td> 
          <td align="LEFT">0.36</td> 
          <td align="LEFT">0.68</td> 
          <td align="LEFT">0.46</td> 
         </tr> 
        </tbody>
       </table> <a name="tab:clmeascomp"></a> <a name="p:clmeascomp"></a> </td>
     </tr> 
    </tbody>
   </table> 
  </div>
  <p></p> 
  <br /> 
  <p> High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. </p>
  <p> A measure that allows us to make this tradeoff is <a name="24325"></a> <i>normalized mutual information</i> or <a name="24327"></a> <i>NMI</i> : <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\mbox{NMI}(\Omega , \mathbb{C})
= 
\frac{
I(\Omega ; \mathbb{C})
}
{
[H(\Omega)+ H(\mathbb{C} )]/2
}
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="nmidef"></a><img width="232" height="45" border="0" src="img1398.png" alt="\begin{displaymath}
\mbox{NMI}(\Omega , \mathbb{C})
=
\frac{
I(\Omega ; \mathbb{C})
}
{
[H(\Omega)+ H(\mathbb{C} )]/2
}
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (183)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> 
  <img width="11" height="32" align="MIDDLE" border="0" src="img1399.png" alt="$I$" /> is 
  <a name="24336"></a>mutual information (cf. Chapter 
  <a href="text-classification-and-naive-bayes-1.html#ch:nbayes">13</a> , page 
  <a href="mutual-information-1.html#p:mutualinfo">13.5.1</a> ): 
  <br /> 
  <div align="CENTER">
   <a name="midef2"></a>
   <a name="midef2ml"></a> 
   <!-- MATH
 \begin{eqnarray}
I( \Omega ; \mathbb{C} )
&=&
\sum_k \sum_j     P(\omega_k \cap c_j) \log
\frac{P(\omega_k \cap c_j)}{P(\omega_k)P(c_j)}\\
&=&
\sum_k \sum_j     \frac{|\omega_k \cap c_j|}{N} \log
\frac{N|\omega_k \cap c_j|}{|\omega_k||c_j|}
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="56" height="33" align="MIDDLE" border="0" src="img1400.png" alt="$\displaystyle I( \Omega ; \mathbb{C} )$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="226" height="60" align="MIDDLE" border="0" src="img1401.png" alt="$\displaystyle \sum_k \sum_j P(\omega_k \cap c_j) \log
\frac{P(\omega_k \cap c_j)}{P(\omega_k)P(c_j)}$" /></td> 
      <td width="10" align="RIGHT"> (184)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="209" height="60" align="MIDDLE" border="0" src="img1402.png" alt="$\displaystyle \sum_k \sum_j \frac{\vert\omega_k \cap c_j\vert}{N} \log
\frac{N\vert\omega_k \cap c_j\vert}{\vert\omega_k\vert\vert c_j\vert}$" /></td> 
      <td width="10" align="RIGHT"> (185)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> where 
  <img width="47" height="33" align="MIDDLE" border="0" src="img1403.png" alt="$P(\omega_k)$" />, 
  <img width="39" height="33" align="MIDDLE" border="0" src="img1404.png" alt="$P(c_j)$" />, and 
  <!-- MATH
 $P(\omega_k
\cap c_j)$
 --> 
  <img width="76" height="33" align="MIDDLE" border="0" src="img1405.png" alt="$P(\omega_k
\cap c_j)$" /> are the probabilities of a document being in cluster 
  <img width="22" height="32" align="MIDDLE" border="0" src="img1396.png" alt="$\omega_k$" />, class 
  <img width="16" height="32" align="MIDDLE" border="0" src="img1191.png" alt="$c_j$" />, and in the intersection of 
  <img width="22" height="32" align="MIDDLE" border="0" src="img1396.png" alt="$\omega_k$" /> and 
  <img width="16" height="32" align="MIDDLE" border="0" src="img1191.png" alt="$c_j$" />, respectively. Equation 
  <a href="#midef2ml">185</a> is equivalent to Equation&nbsp;
  <a href="#midef2">184</a> for maximum likelihood estimates of the probabilities (i.e., the estimate of each probability is the corresponding relative frequency). 
  <p> <img width="18" height="32" align="MIDDLE" border="0" src="img317.png" alt="$H$" /> is <a name="24352"></a>entropy as defined in Chapter <a href="index-compression-1.html#ch:icompress">5</a> (page <a href="gamma-codes-1.html#p:entropy">5.3.2</a> ): <br /> </p>
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray}
H(\Omega) &=& -\sum_k P(\omega_k) \log P(\omega_k)\\
&=& -\sum_k \frac{|\omega_k|}{N} \log \frac{|\omega_k|}{N}
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="45" height="33" align="MIDDLE" border="0" src="img1406.png" alt="$\displaystyle H(\Omega)$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="153" height="48" align="MIDDLE" border="0" src="img1407.png" alt="$\displaystyle -\sum_k P(\omega_k) \log P(\omega_k)$" /></td> 
      <td width="10" align="RIGHT"> (186)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="132" height="56" align="MIDDLE" border="0" src="img1408.png" alt="$\displaystyle -\sum_k \frac{\vert\omega_k\vert}{N} \log \frac{\vert\omega_k\vert}{N}$" /></td> 
      <td width="10" align="RIGHT"> (187)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> where, again, the second equation is based on maximum likelihood estimates of the probabilities. 
  <p> 
   <!-- MATH
 $I( \Omega ; \mathbb{C} )$
 --> <img width="57" height="33" align="MIDDLE" border="0" src="img1409.png" alt="$I( \Omega ; \mathbb{C} )$" /> in Equation&nbsp;<a href="#midef2">184</a> measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are. The minimum of 
   <!-- MATH
 $I( \Omega ; \mathbb{C} )$
 --> <img width="57" height="33" align="MIDDLE" border="0" src="img1409.png" alt="$I( \Omega ; \mathbb{C} )$" /> is 0 if the clustering is random with respect to class membership. In that case, knowing that a document is in a particular cluster does not give us any new information about what its class might be. Maximum mutual information is reached for a clustering 
   <!-- MATH
 $\Omega_{exact}$
 --> <img width="45" height="32" align="MIDDLE" border="0" src="img1410.png" alt="$\Omega_{exact}$" /> that perfectly recreates the classes - but also if clusters in 
   <!-- MATH
 $\Omega_{exact}$
 --> <img width="45" height="32" align="MIDDLE" border="0" src="img1410.png" alt="$\Omega_{exact}$" /> are further subdivided into smaller clusters (Exercise <a href="exercises-3.html#ex:miclustering">16.7</a> ). In particular, a clustering with <img width="51" height="32" align="MIDDLE" border="0" src="img1411.png" alt="$K=N$" /> one-document clusters has maximum MI. So MI has the same problem as purity: it does not penalize large cardinalities and thus does not formalize our bias that, other things being equal, fewer clusters are better. </p>
  <p> The normalization by the denominator 
   <!-- MATH
 $[H(\Omega )+H(\mathbb{C}
)]/2$
 --> <img width="131" height="33" align="MIDDLE" border="0" src="img1412.png" alt="$[H(\Omega )+H(\mathbb{C}
)]/2$" /> in Equation&nbsp;<a href="#nmidef">183</a> fixes this problem since entropy tends to increase with the number of clusters. For example, <img width="45" height="33" align="MIDDLE" border="0" src="img1413.png" alt="$H(\Omega)$" /> reaches its maximum <img width="42" height="31" align="MIDDLE" border="0" src="img1414.png" alt="$\log N$" /> for <img width="51" height="32" align="MIDDLE" border="0" src="img1411.png" alt="$K=N$" />, which </p>
  <p> ensures that NMI is low for <img width="51" height="32" align="MIDDLE" border="0" src="img1411.png" alt="$K=N$" />. Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters. The particular form of the denominator is chosen because 
   <!-- MATH
 $[H(\Omega )+H(\mathbb{C}
)]/2$
 --> <img width="131" height="33" align="MIDDLE" border="0" src="img1412.png" alt="$[H(\Omega )+H(\mathbb{C}
)]/2$" /> is a tight upper bound on 
   <!-- MATH
 $I(\Omega; \mathbb{C})$
 --> <img width="57" height="33" align="MIDDLE" border="0" src="img1409.png" alt="$I( \Omega ; \mathbb{C} )$" /> (Exercise <a href="exercises-3.html#ex:nmibound">16.7</a> ). Thus, NMI is always a number between 0 and 1. </p>
  <p> An alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions, one for each of the <img width="91" height="33" align="MIDDLE" border="0" src="img1415.png" alt="$N(N-1)/2$" /> pairs of documents in the collection. We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A <a name="24372"></a> (FP) decision assigns two dissimilar documents to the same cluster. A <a name="24374"></a> (FN) decision assigns two similar documents to different clusters. The <a name="24376"></a> <i>Rand index</i> (<a name="24378"></a> ) measures the percentage of decisions that are correct. That is, it is simply accuracy (Section <a href="evaluation-of-unranked-retrieval-sets-1.html#sec:measuresperf">8.3</a> , page <a href="evaluation-of-unranked-retrieval-sets-1.html#p:accuracy">8.3</a> ). </p>
  <p></p> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray*}
\mbox{RI} = \frac{\mbox{TP}+\mbox{TN}}{\mbox{TP}+\mbox{FP}+\mbox{FN}+\mbox{TN}}
\end{eqnarray*}
 --> 
   <img width="182" height="41" border="0" src="img1416.png" alt="\begin{eqnarray*}
\mbox{RI} = \frac{\mbox{TP}+\mbox{TN}}{\mbox{TP}+\mbox{FP}+\mbox{FN}+\mbox{TN}}
\end{eqnarray*}" />
  </div> 
  <br clear="ALL" />
  <p></p> 
  <p> As an example, we compute RI for Figure <a href="#fig:clustfg3">16.4</a> . We first compute 
   <!-- MATH
 $\mbox{TP}+\mbox{FP}$
 --> <img width="61" height="32" align="MIDDLE" border="0" src="img1417.png" alt="$\mbox{TP}+\mbox{FP}$" />. The three clusters contain 6, 6, and 5 points, respectively, so the total number of ``positives'' or pairs of documents that are in the same cluster is: <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\mbox{TP}+\mbox{FP} = \left( \begin{array}{c} 6 \\2 \end{array} \right) +
\left( \begin{array}{c} 6 \\2 \end{array} \right) +
\left( \begin{array}{c} 5 \\2 \end{array} \right) = 40
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="300" height="45" border="0" src="img1418.png" alt="\begin{displaymath}
\mbox{TP}+\mbox{FP} = \left( \begin{array}{c} 6 \\ 2 \end{ar...
...ght) +
\left( \begin{array}{c} 5 \\ 2 \end{array} \right) = 40
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (188)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Of these, the x pairs in cluster&nbsp;1, the o pairs in cluster&nbsp;2, the 
  <img width="13" height="14" align="BOTTOM" border="0" src="img1419.png" alt="$\diamond$" /> pairs in cluster&nbsp;3, and the x pair in cluster&nbsp;3 are true positives: 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\mbox{TP}= \left( \begin{array}{c} 5 \\2 \end{array} \right) +
\left( \begin{array}{c} 4 \\2 \end{array} \right) +
\left( \begin{array}{c} 3 \\2 \end{array} \right) +
\left( \begin{array}{c} 2 \\2 \end{array} \right) = 20
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="330" height="45" border="0" src="img1420.png" alt="\begin{displaymath}
\mbox{TP}= \left( \begin{array}{c} 5 \\ 2 \end{array} \right...
...ght) +
\left( \begin{array}{c} 2 \\ 2 \end{array} \right) = 20
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (189)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Thus, 
  <!-- MATH
 $\mbox{FP}=40-20=20$
 --> 
  <img width="134" height="32" align="MIDDLE" border="0" src="img1421.png" alt="$\mbox{FP}=40-20=20$" />. 
  <p> <img width="26" height="32" align="MIDDLE" border="0" src="img1422.png" alt="$\mbox{FN}$" /> and <img width="27" height="32" align="MIDDLE" border="0" src="img1423.png" alt="$\mbox{TN}$" /> are computed similarly, resulting in the following contingency table: </p>
  <blockquote> 
   <table cellpadding="3" border="1"> 
    <tbody>
     <tr>
      <td align="CENTER" colspan="1">&nbsp;</td> 
      <td align="CENTER" colspan="1">Same cluster</td> 
      <td align="CENTER" colspan="1">Different clusters</td> 
     </tr> 
     <tr>
      <td align="LEFT">Same class</td> 
      <td align="CENTER">
       <!-- MATH
 $\mbox{TP} = 20$
 --> <img width="61" height="32" align="MIDDLE" border="0" src="img1424.png" alt="$\mbox{TP} = 20 $" /></td> 
      <td align="CENTER">
       <!-- MATH
 $\mbox{FN} = 24$
 --> <img width="64" height="32" align="MIDDLE" border="0" src="img1425.png" alt="$\mbox{FN} = 24$" /></td> 
     </tr> 
     <tr>
      <td align="LEFT">Different classes</td> 
      <td align="CENTER">
       <!-- MATH
 $\mbox{FP} = 20$
 --> <img width="60" height="32" align="MIDDLE" border="0" src="img1426.png" alt="$\mbox{FP} = 20$" /></td> 
      <td align="CENTER">
       <!-- MATH
 $\mbox{TN} = 72$
 --> <img width="65" height="32" align="MIDDLE" border="0" src="img1427.png" alt="$\mbox{TN} = 72$" /></td> 
     </tr> 
    </tbody>
   </table> 
  </blockquote> 
  <img width="20" height="32" align="MIDDLE" border="0" src="img1428.png" alt="$\mbox{RI}$" /> is then 
  <!-- MATH
 $(20+72)/(20+20+24+72) \approx 0.68$
 --> 
  <img width="265" height="33" align="MIDDLE" border="0" src="img1429.png" alt="$(20+72)/(20+20+24+72) \approx 0.68$" />. 
  <p> The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the <a name="24446"></a> <i>F&nbsp;measure</i> measuresperf to penalize false negatives more strongly than false positives by selecting a value <img width="44" height="31" align="MIDDLE" border="0" src="img527.png" alt="$\beta &gt; 1$" />, thus giving more weight to recall. </p>
  <p></p> 
  <div align="CENTER"> 
   <!-- MATH
 \begin{eqnarray*}
P = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}} \qquad
R = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FN}} \qquad
F_{\beta} = \frac{(\beta^2+1)PR}{\beta^2 P+R}
\end{eqnarray*}
 --> 
   <img width="375" height="47" border="0" src="img1430.png" alt="\begin{eqnarray*}
P = \frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}} \qquad
R = \frac{\mb...
...+\mbox{FN}} \qquad
F_{\beta} = \frac{(\beta^2+1)PR}{\beta^2 P+R}
\end{eqnarray*}" />
  </div> 
  <br clear="ALL" />
  <p></p> Based on the numbers in the contingency table, 
  <!-- MATH
 $P= 20/40
= 0.5$
 --> 
  <img width="120" height="31" align="MIDDLE" border="0" src="img1431.png" alt="$P= 20/40
= 0.5$" /> and 
  <!-- MATH
 $R= 20/44 \approx 0.455$
 --> 
  <img width="137" height="31" align="MIDDLE" border="0" src="img1432.png" alt="$R= 20/44 \approx 0.455$" />. This gives us 
  <!-- MATH
 $F_1
\approx 0.48$
 --> 
  <img width="69" height="32" align="MIDDLE" border="0" src="img1433.png" alt="$F_1
\approx 0.48$" /> for 
  <img width="44" height="31" align="MIDDLE" border="0" src="img521.png" alt="$\beta = 1$" /> and 
  <!-- MATH
 $F_5 \approx 0.456$
 --> 
  <img width="76" height="32" align="MIDDLE" border="0" src="img1434.png" alt="$F_5 \approx 0.456$" /> for 
  <img width="44" height="31" align="MIDDLE" border="0" src="img529.png" alt="$\beta=5$" />. In information retrieval, evaluating clustering with 
  <img width="14" height="32" align="MIDDLE" border="0" src="img1435.png" alt="$F$" /> has the advantage that the measure is already familiar to the research community. 
  <p> <b>Exercises.</b> </p>
  <ul> 
   <li>Replace every point <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" /> in Figure <a href="#fig:clustfg3">16.4</a> with two identical copies of <img width="12" height="31" align="MIDDLE" border="0" src="img354.png" alt="$d$" /> in the same class. (i) Is it less difficult, equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in Figure <a href="#fig:clustfg3">16.4</a> ? (ii) Compute purity, NMI, RI, and <img width="19" height="32" align="MIDDLE" border="0" src="img1397.png" alt="$F_5$" /> for the clustering with 34 points. Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings? <p> </p></li> 
  </ul> 
  <p> </p>
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html4211" href="k-means-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4205" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4199" href="cardinality---the-number-of-clusters-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4207" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4209" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4212" href="k-means-1.html">K-means</a> 
  <b> Up:</b> 
  <a name="tex2html4206" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4200" href="cardinality---the-number-of-clusters-1.html">Cardinality - the number</a> &nbsp; 
  <b> <a name="tex2html4208" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4210" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>References and further reading</title> 
  <meta name="description" content="References and further reading" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="previous" href="results-snippets-1.html" /> 
  <link rel="up" href="evaluation-in-information-retrieval-1.html" /> 
  <link rel="next" href="relevance-feedback-and-query-expansion-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html2555" href="relevance-feedback-and-query-expansion-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html2549" href="evaluation-in-information-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html2545" href="results-snippets-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html2551" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html2553" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html2556" href="relevance-feedback-and-query-expansion-1.html">Relevance feedback and query</a> 
  <b> Up:</b> 
  <a name="tex2html2550" href="evaluation-in-information-retrieval-1.html">Evaluation in information retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html2546" href="results-snippets-1.html">Results snippets</a> &nbsp; 
  <b> <a name="tex2html2552" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html2554" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION001380000000000000000"> References and further reading</a> </h1> 
  <p> Definition and implementation of the notion of relevance to a query got off to a rocky start in 1953. <a href="bibliography-1.html#swanson88historical">Swanson (1988)</a> reports that in an evaluation in that year between two teams, they agreed that 1390 documents were variously relevant to a set of 98 questions, but disagreed on a further 1577 documents, and the disagreements were never resolved. </p>
  <p> Rigorous formal testing of IR systems was first completed in the Cranfield experiments, beginning in the late 1950s. A retrospective discussion of the Cranfield test collection and experimentation with it can be found in (<a href="bibliography-1.html#cleverdon91cranfield">Cleverdon, 1991</a>). The other seminal series of early IR experiments were those on the SMART system by Gerard Salton and colleagues (<a name="tex2html2557" href="bibliography-1.html#salton71smart">Salton, 1971b</a>;<a name="tex2html2558" href="bibliography-1.html#salton91panel">1991</a>). The TREC evaluations are described in detail by <a href="bibliography-1.html#voorhees05experiment">Voorhees and Harman (2005)</a>. Online information is available at <tt><a name="tex2html84" href="http://trec.nist.gov/">http://trec.nist.gov/</a></tt>. Initially, few researchers computed the statistical significance of their experimental results, but the IR community increasingly demands this (<a href="bibliography-1.html#hull93using">Hull, 1993</a>). User studies of IR system effectiveness began more recently (<a name="tex2html2559" href="bibliography-1.html#saracevic88users">Saracevic and Kantor, 1988</a>;<a name="tex2html2560" href="bibliography-1.html#saracevic88">1996</a>). </p>
  <p> The notions of recall and precision were first used by <a href="bibliography-1.html#kent55operational">Kent et&nbsp;al. (1955)</a>, although the term <i>precision</i> did not appear until later. The <a name="10966"></a> (or, rather its complement <img width="74" height="32" align="MIDDLE" border="0" src="img567.png" alt="$E = 1 - F$" />) was introduced by <a href="bibliography-1.html#rij79">van&nbsp;Rijsbergen (1979)</a>. He provides an extensive theoretical discussion, which shows how adopting a principle of decreasing marginal relevance (at some point a user will be unwilling to sacrifice a unit of precision for an added unit of recall) leads to the harmonic mean being the appropriate method for combining precision and recall (and hence to its adoption rather than the minimum or geometric mean). </p>
  <p> <a href="bibliography-1.html#buckley00evaluating">Buckley and Voorhees (2000)</a> compare several evaluation measures, including precision at <img width="11" height="31" align="MIDDLE" border="0" src="img20.png" alt="$k$" />, MAP, and R-precision, and evaluate the error rate of each measure. <a name="10970"></a> was adopted as the official evaluation metric in the TREC HARD track (<a href="bibliography-1.html#allan05hard">Allan, 2005</a>). <a href="bibliography-1.html#aslam05geometric">Aslam and Yilmaz (2005)</a> examine its surprisingly close correlation to MAP, which had been noted in earlier studies (<a name="tex2html2561" href="bibliography-1.html#buckley00evaluating">Buckley and Voorhees, 2000</a>, <a name="tex2html2562" href="bibliography-1.html#tague-sutcliffe95statistical">Tague-Sutcliffe and Blustein, 1995</a>). A standard program for evaluating IR systems which computes many measures of ranked retrieval effectiveness is Chris Buckley's <a name="10975"></a><code>trec_eval</code> program used in the TREC evaluations. It can be downloaded from: <tt><a name="tex2html85" href="http://trec.nist.gov/trec_eval/">http://trec.nist.gov/trec_eval/</a></tt>. </p>
  <p> <a href="bibliography-1.html#kekalainen02graded">Kek&auml;l&auml;inen and J&auml;rvelin (2002)</a> argue for the superiority of graded relevance judgments when dealing with very large document collections, and <a href="bibliography-1.html#jarvelin02cumulated">J&auml;rvelin and Kek&auml;l&auml;inen (2002)</a> introduce cumulated gain-based methods for IR system evaluation in this context. <a href="bibliography-1.html#sakai07reliability">Sakai (2007)</a> does a study of the stability and sensitivity of evaluation measures based on graded relevance judgments from <a name="10980"></a> <i>NTCIR</i> tasks, and concludes that NDCG is best for evaluating document ranking. </p>
  <p> <a href="bibliography-1.html#schamber90re-examination">Schamber et&nbsp;al. (1990)</a> examine the concept of relevance, stressing its multidimensional and context-specific nature, but also arguing that it can be measured effectively. (<a href="bibliography-1.html#voorhees00variations">Voorhees, 2000</a>) is the standard article for examining variation in relevance judgments and their effects on retrieval system scores and ranking for the TREC Ad Hoc task. <a href="bibliography-1.html#voorhees00variations">Voorhees</a> concludes that although the numbers change, the rankings are quite stable. <a href="bibliography-1.html#hersh94ohsumed">Hersh et&nbsp;al. (1994)</a> present similar analysis for a medical IR collection. In contrast, <a href="bibliography-1.html#kekalainen05relevance">Kek&auml;l&auml;inen (2005)</a> analyze some of the later TRECs, exploring a 4-way relevance judgment and the notion of cumulative gain, arguing that the relevance measure used does substantially affect system rankings. See also <a href="bibliography-1.html#harter98relevance">Harter (1998)</a>. <a href="bibliography-1.html#zobel98reliable">Zobel (1998)</a> studies whether the <a name="10989"></a> <i>pooling</i> method used by TREC to collect a subset of documents that will be evaluated for relevance is reliable and fair, and concludes that it is. </p>
  <p> The <a name="10991"></a> and its use for language-related purposes is discussed by <a href="bibliography-1.html#carletta96kappa">Carletta (1996)</a>. Many standard sources (e.g., <a href="bibliography-1.html#siegel88nonparametric">Siegel and Castellan, 1988</a>) present pooled calculation of the expected agreement, but <a href="bibliography-1.html#dieugenio04kappa">Di Eugenio (2004)</a> argue for preferring the unpooled agreement (though perhaps presenting multiple measures). For further discussion of alternative measures of agreement, which may in fact be better, see <a href="bibliography-1.html#lombard02content">Lombard et&nbsp;al. (2002)</a> and <a href="bibliography-1.html#krippendorff03content">Krippendorff (2003)</a>. </p>
  <p> Text summarization has been actively explored for many years. Modern work on sentence selection was initiated by <a href="bibliography-1.html#kupiec95">Kupiec et&nbsp;al. (1995)</a>. More recent work includes (<a href="bibliography-1.html#barzilay97chains">Barzilay and Elhadad, 1997</a>) and (<a href="bibliography-1.html#jing00reduction">Jing, 2000</a>), together with a broad selection of work appearing at the yearly DUC conferences and at other NLP venues. <a href="bibliography-1.html#tombros98advantages">Tombros and Sanderson (1998)</a> demonstrate the advantages of dynamic summaries in the IR context. <a href="bibliography-1.html#turpin07fast">Turpin et&nbsp;al. (2007)</a> address how to generate snippets efficiently. </p>
  <p> Clickthrough log analysis is studied in (<a name="tex2html2563" href="bibliography-1.html#joachims02clickthrough">Joachims, 2002b</a>, <a name="tex2html2564" href="bibliography-1.html#joachims05clickthrough">Joachims et&nbsp;al., 2005</a>). </p>
  <p> In a series of papers, Hersh, Turpin and colleagues show how improvements in formal retrieval effectiveness, as evaluated in batch experiments, do not always translate into an improved system for users (<a name="tex2html2565" href="bibliography-1.html#hersh00further">Hersh et&nbsp;al., 2000b</a>, <a name="tex2html2566" href="bibliography-1.html#turpin02user">Turpin and Hersh, 2002</a>, <a name="tex2html2567" href="bibliography-1.html#hersh00batch">Hersh et&nbsp;al., 2000a</a>;<a name="tex2html2568" href="bibliography-1.html#hersh01challenging">2001</a>, <a name="tex2html2569" href="bibliography-1.html#turpin01why">Turpin and Hersh, 2001</a>). </p>
  <p> User interfaces for IR and human factors such as models of human information seeking and usability testing are outside the scope of what we cover in this book. More information on these topics can be found in other textbooks, including (<a href="bibliography-1.html#baezayates99">Baeza-Yates and Ribeiro-Neto, 1999</a>, ch.&nbsp;10) and (<a href="bibliography-1.html#korfhage97">Korfhage, 1997</a>), and collections focused on cognitive aspects (<a href="bibliography-1.html#spink05cognitive">Spink and Cole, 2005</a>). </p>
  <p> </p>
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html2555" href="relevance-feedback-and-query-expansion-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html2549" href="evaluation-in-information-retrieval-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html2545" href="results-snippets-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html2551" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html2553" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html2556" href="relevance-feedback-and-query-expansion-1.html">Relevance feedback and query</a> 
  <b> Up:</b> 
  <a name="tex2html2550" href="evaluation-in-information-retrieval-1.html">Evaluation in information retrieval</a> 
  <b> Previous:</b> 
  <a name="tex2html2546" href="results-snippets-1.html">Results snippets</a> &nbsp; 
  <b> <a name="tex2html2552" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html2554" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>
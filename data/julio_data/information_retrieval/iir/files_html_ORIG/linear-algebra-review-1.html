<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>Linear algebra review</title> 
  <meta name="description" content="Linear algebra review" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="term-document-matrices-and-singular-value-decompositions-1.html" /> 
  <link rel="previous" href="matrix-decompositions-and-latent-semantic-indexing-1.html" /> 
  <link rel="up" href="matrix-decompositions-and-latent-semantic-indexing-1.html" /> 
  <link rel="next" href="matrix-decompositions-1.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html4528" href="matrix-decompositions-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4522" href="matrix-decompositions-and-latent-semantic-indexing-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4516" href="matrix-decompositions-and-latent-semantic-indexing-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4524" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4526" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4529" href="matrix-decompositions-1.html">Matrix decompositions</a> 
  <b> Up:</b> 
  <a name="tex2html4523" href="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</a> 
  <b> Previous:</b> 
  <a name="tex2html4517" href="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</a> &nbsp; 
  <b> <a name="tex2html4525" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4527" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION002310000000000000000"></a> <a name="sec:linalg"></a> <a name="p:linalg"></a> <br /> Linear algebra review </h1> We briefly review some necessary background in linear algebra. Let 
  <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> be an 
  <!-- MATH
 $\lsinoterms \times
\lsinodocs$
 --> 
  <img width="53" height="32" align="MIDDLE" border="0" src="img1664.png" alt="$\lsinoterms\times \lsinodocs$" /> matrix with real-valued entries; for a term-document matrix, all entries are in fact non-negative. 
  <p> The <a name="28495"></a> <i>rank</i> of a matrix is the number of linearly independent rows (or columns) in it; thus, 
   <!-- MATH
 ${\mbox rank}(\lsimatrix)\leq \min\{\lsinoterms,\lsinodocs\}$
 --> <img width="163" height="33" align="MIDDLE" border="0" src="img1666.png" alt="${\mbox rank}(\lsimatrix)\leq \min\{\lsinoterms,\lsinodocs\}$" />. A square <img width="37" height="32" align="MIDDLE" border="0" src="img1667.png" alt="$r\times r$" /> matrix all of whose off-diagonal entries are zero is called a <em>diagonal matrix</em>; its rank is equal to the number of non-zero diagonal entries. If all <img width="10" height="32" align="MIDDLE" border="0" src="img28.png" alt="$r$" /> diagonal entries of such a diagonal matrix are <img width="12" height="32" align="MIDDLE" border="0" src="img291.png" alt="$1$" />, it is called the identity matrix of dimension <img width="10" height="32" align="MIDDLE" border="0" src="img28.png" alt="$r$" /> and represented by <img width="16" height="32" align="MIDDLE" border="0" src="img1668.png" alt="$I_r$" />. </p>
  <p> For a square 
   <!-- MATH
 $\lsinoterms\times \lsinoterms$
 --> <img width="56" height="32" align="MIDDLE" border="0" src="img1669.png" alt="$\lsinoterms\times \lsinoterms$" /> matrix <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> and a vector <img width="13" height="32" align="MIDDLE" border="0" src="img701.png" alt="$\vec{x}$" /> that is not all zeros, the values of <img width="14" height="31" align="MIDDLE" border="0" src="img830.png" alt="$\lambda$" /><a name="eigenvalue-notation"></a> satisfying <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\matrix{\lsimatrix}\vec{x} = \lambda \vec{x}
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="eqn:right-eigen"></a><img width="64" height="28" border="0" src="img1670.png" alt="\begin{displaymath}
\matrix{\lsimatrix}\vec{x} = \lambda \vec{x}
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (213)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> are called the 
  <a name="28507"></a> 
  <i>eigenvalues</i> of 
  <!-- MATH
 $\matrix{\lsimatrix}$
 --> 
  <img width="20" height="34" align="MIDDLE" border="0" src="img1671.png" alt="$\matrix{\lsimatrix}$" />. The 
  <img width="17" height="32" align="MIDDLE" border="0" src="img1672.png" alt="$\lsinodocs$" />-vector 
  <img width="13" height="32" align="MIDDLE" border="0" src="img701.png" alt="$\vec{x}$" /> satisfying Equation&nbsp;
  <a href="#eqn:right-eigen">213</a> for an eigenvalue 
  <img width="14" height="31" align="MIDDLE" border="0" src="img830.png" alt="$\lambda$" /> is the corresponding 
  <em>right eigenvector</em>. The eigenvector corresponding to the eigenvalue of largest magnitude is called the 
  <em>principal eigenvector.</em> In a similar fashion, the 
  <em>left eigenvectors</em> of 
  <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> are the 
  <img width="20" height="32" align="MIDDLE" border="0" src="img1673.png" alt="$\lsinoterms$" />-vectors 
  <img width="12" height="32" align="MIDDLE" border="0" src="img59.png" alt="$y$" /> such that 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\vec{y}^T\matrix{\lsimatrix} = \lambda \vec{y}^T.
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="eqn:left-eigen"></a><img width="87" height="28" border="0" src="img1674.png" alt="\begin{displaymath}
\vec{y}^T\matrix{\lsimatrix} = \lambda \vec{y}^T.
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (214)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> The number of non-zero eigenvalues of 
  <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> is at most 
  <!-- MATH
 $\mbox{rank}(\lsimatrix)$
 --> 
  <img width="61" height="33" align="MIDDLE" border="0" src="img1675.png" alt="$\mbox{rank}(\lsimatrix)$" />. 
  <p> The eigenvalues of a matrix are found by solving the <i>characteristic equation</i>, which is obtained by rewriting Equation&nbsp;<a href="#eqn:right-eigen">213</a> in the form 
   <!-- MATH
 $(\lsimatrix-\lambda
I_\lsinoterms)\vec{x}=0$
 --> <img width="116" height="33" align="MIDDLE" border="0" src="img1676.png" alt="$(\lsimatrix-\lambda
I_\lsinoterms)\vec{x}=0$" />. The eigenvalues of <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> are then the solutions of 
   <!-- MATH
 $|(\lsimatrix-\lambda I_\lsinoterms)|=0$
 --> <img width="118" height="33" align="MIDDLE" border="0" src="img1677.png" alt="$\vert(\lsimatrix-\lambda I_\lsinoterms)\vert=0$" />, where <img width="23" height="33" align="MIDDLE" border="0" src="img1678.png" alt="$\vert S\vert$" /> denotes the <a name="p:determinant-def"></a> determinant of a square matrix <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />. The equation 
   <!-- MATH
 $|(\lsimatrix-\lambda I_\lsinoterms)|=0$
 --> <img width="118" height="33" align="MIDDLE" border="0" src="img1677.png" alt="$\vert(\lsimatrix-\lambda I_\lsinoterms)\vert=0$" /> is an <img width="20" height="32" align="MIDDLE" border="0" src="img1673.png" alt="$\lsinoterms$" />th order polynomial equation in <img width="14" height="31" align="MIDDLE" border="0" src="img830.png" alt="$\lambda$" /> and can have at most <img width="20" height="32" align="MIDDLE" border="0" src="img1673.png" alt="$\lsinoterms$" /> roots, which are the eigenvalues of <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" />. These eigenvalues can in general be complex, even if all entries of <img width="15" height="32" align="MIDDLE" border="0" src="img1665.png" alt="$\lsimatrix$" /> are real. </p>
  <p> We now examine some further properties of eigenvalues and eigenvectors, to set up the central idea of singular value decompositions in Section <a href="term-document-matrices-and-singular-value-decompositions-1.html#sec:svd">18.2</a> below. First, we look at the relationship between matrix-vector multiplication and eigenvalues. </p>
  <p> <b>Worked example.</b> <a name="eg:3eigen"></a>Consider the matrix <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
S=\left(
      \begin{array}{ccc}
        30 & 0 & 0 \\
        0 & 20 & 0 \\
        0 & 0 & 1\\
      \end{array}
    \right).
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="153" height="64" border="0" src="img1680.png" alt="\begin{displaymath}
S=\left(
\begin{array}{ccc}
30 &amp; 0 &amp; 0 \\
0 &amp; 20 &amp; 0 \\
0 &amp; 0 &amp; 1\\
\end{array} \right).
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (215)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Clearly the matrix has rank 3, and has 3 non-zero eigenvalues 
  <img width="63" height="31" align="MIDDLE" border="0" src="img1681.png" alt="$\lambda_1=30,$" /> 
  <img width="59" height="31" align="MIDDLE" border="0" src="img1682.png" alt="$ \lambda_2=20$" /> and 
  <img width="51" height="31" align="MIDDLE" border="0" src="img1683.png" alt="$\lambda_3=1$" />, with the three corresponding eigenvectors 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\vec{x_1}=\left(
      \begin{array}{c}
        1 \\0 \\0 \\
      \end{array}
    \right),
    \vec{x_2}=\left(
      \begin{array}{c}
        0 \\1 \\0 \\
      \end{array}
    \right)\mbox{ and }
    \vec{x_3}=\left(
      \begin{array}{c}
        0 \\0 \\1 \\
      \end{array}
    \right).
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="323" height="64" border="0" src="img1684.png" alt="\begin{displaymath}
\vec{x_1}=\left(
\begin{array}{c}
1 \\ 0 \\ 0 \\
\end{a...
...left(
\begin{array}{c}
0 \\ 0 \\ 1 \\
\end{array} \right).
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (216)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> For each of the eigenvectors, multiplication by 
  <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" /> acts as if we were multiplying the eigenvector by a multiple of the identity matrix; the multiple is different for each eigenvector. Now, consider an arbitrary vector, such as 
  <!-- MATH
 $\vec{v}=\left(
                             \begin{array}{c}
                               2 \\
                               4 \\
                               6 \\
                             \end{array}
                           \right).$
 --> 
  <img width="94" height="75" align="MIDDLE" border="0" src="img1685.png" alt="$\vec{v}=\left(
\begin{array}{c}
2 \\
4 \\
6 \\
\end{array} \right).
$" /> We can always express 
  <img width="12" height="32" align="MIDDLE" border="0" src="img433.png" alt="$\vec{v}$" /> as a linear combination of the three eigenvectors of 
  <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />; in the current example we have 
  <br /> 
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
\vec{v}=\left(
       \begin{array}{c}
         2 \\
         4 \\
         6 \\
       \end{array}
     \right)=2\vec{x_1}+4\vec{x_2}+6\vec{x_3}.
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><img width="218" height="64" border="0" src="img1686.png" alt="\begin{displaymath}
\vec{v}=\left(
\begin{array}{c}
2 \\
4 \\
6 \\
\end{array} \right)=2\vec{x_1}+4\vec{x_2}+6\vec{x_3}.
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (217)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> Suppose we multiply 
  <img width="12" height="32" align="MIDDLE" border="0" src="img433.png" alt="$\vec{v}$" /> by 
  <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />: 
  <br /> 
  <div align="CENTER">
   <a name="eqn:superpose"></a> 
   <!-- MATH
 \begin{eqnarray}
S\vec{v} &=& S(2\vec{x_1}+4\vec{x_2}+6\vec{x_3}) \\
  &=& 2S\vec{x_1}+4S\vec{x_2}+6S\vec{x_3}  \\
  &=& 2\lambda_1 \vec{x_1}+4\lambda_2\vec{x_2}+6\lambda_3\vec{x_3}\\
  &=& 60\vec{x_1}+80\vec{x_2}+6\vec{x_3}.
\end{eqnarray}
 --> 
   <table align="CENTER" cellpadding="0" width="100%"> 
    <tbody>
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT"><img width="22" height="32" align="MIDDLE" border="0" src="img1687.png" alt="$\displaystyle S\vec{v}$" /></td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="136" height="33" align="MIDDLE" border="0" src="img1688.png" alt="$\displaystyle S(2\vec{x_1}+4\vec{x_2}+6\vec{x_3})$" /></td> 
      <td width="10" align="RIGHT"> (218)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="141" height="32" align="MIDDLE" border="0" src="img1689.png" alt="$\displaystyle 2S\vec{x_1}+4S\vec{x_2}+6S\vec{x_3}$" /></td> 
      <td width="10" align="RIGHT"> (219)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="163" height="32" align="MIDDLE" border="0" src="img1690.png" alt="$\displaystyle 2\lambda_1 \vec{x_1}+4\lambda_2\vec{x_2}+6\lambda_3\vec{x_3}$" /></td> 
      <td width="10" align="RIGHT"> (220)</td>
     </tr> 
     <tr valign="MIDDLE">
      <td nowrap="" align="RIGHT">&nbsp;</td> 
      <td align="CENTER" nowrap=""><img width="17" height="32" align="MIDDLE" border="0" src="img313.png" alt="$\textstyle =$" /></td> 
      <td align="LEFT" nowrap=""><img width="133" height="32" align="MIDDLE" border="0" src="img1691.png" alt="$\displaystyle 60\vec{x_1}+80\vec{x_2}+6\vec{x_3}.$" /></td> 
      <td width="10" align="RIGHT"> (221)</td>
     </tr> 
    </tbody>
   </table>
  </div> 
  <br clear="ALL" />
  <p></p> 
  <b>End worked example.</b> 
  <p> Example&nbsp;<a href="#eg:3eigen">18.1</a> shows that even though <img width="12" height="32" align="MIDDLE" border="0" src="img433.png" alt="$\vec{v}$" /> is an arbitrary vector, the effect of multiplication by <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" /> is determined by the eigenvalues and eigenvectors of <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />. Furthermore, it is intuitively apparent from Equation&nbsp;<a href="#eqn:superpose">221</a> that the product <img width="21" height="32" align="MIDDLE" border="0" src="img1692.png" alt="$S\vec{v}$" /> is relatively unaffected by terms arising from the small eigenvalues of <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />; in our example, since <img width="51" height="31" align="MIDDLE" border="0" src="img1683.png" alt="$\lambda_3=1$" />, the contribution of the third term on the right hand side of Equation&nbsp;<a href="#eqn:superpose">221</a> is small. In fact, if we were to completely ignore the contribution in Equation&nbsp;<a href="#eqn:superpose">221</a> from the third eigenvector corresponding to <img width="51" height="31" align="MIDDLE" border="0" src="img1683.png" alt="$\lambda_3=1$" />, then the product <img width="21" height="32" align="MIDDLE" border="0" src="img1692.png" alt="$S\vec{v}$" /> would be computed to be 
   <!-- MATH
 $\left(
       \begin{array}{c}
         60 \\
         80 \\
         0 \\
       \end{array}
     \right)$
 --> <img width="64" height="75" align="MIDDLE" border="0" src="img1693.png" alt="$ \left(
\begin{array}{c}
60 \\
80 \\
0 \\
\end{array} \right)$" /> rather than the correct product which is 
   <!-- MATH
 $\left(
       \begin{array}{c}
         60 \\
         80 \\
         6 \\
       \end{array}
     \right)$
 --> <img width="64" height="75" align="MIDDLE" border="0" src="img1694.png" alt="$ \left(
\begin{array}{c}
60 \\
80 \\
6 \\
\end{array} \right)$" />; these two vectors are relatively close to each other by any of various metrics one could apply (such as the length of their vector difference). </p>
  <p> This suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector product is small. We will carry forward this intuition when studying matrix decompositions and low-rank approximations in Section <a href="term-document-matrices-and-singular-value-decompositions-1.html#sec:svd">18.2</a> . Before doing so, we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us. </p>
  <p> For a <i>symmetric</i> matrix <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" />, the eigenvectors corresponding to distinct eigenvalues are <i>orthogonal</i>. Further, if <img width="13" height="32" align="MIDDLE" border="0" src="img1679.png" alt="$S$" /> is both real and symmetric, the eigenvalues are all real. </p>
  <p> <b>Worked example.</b> Consider the real, symmetric matrix <br /> </p>
  <div align="RIGHT"> 
   <!-- MATH
 \begin{equation}
S=\left(
      \begin{array}{cc}
        2 & 1 \\
        1 & 2 \\
      \end{array}
    \right).
\end{equation}
 --> 
   <table width="100%" align="CENTER"> 
    <tbody>
     <tr valign="MIDDLE">
      <td align="CENTER" nowrap=""><a name="eqn:examplematrix"></a><img width="109" height="45" border="0" src="img1695.png" alt="\begin{displaymath}
S=\left(
\begin{array}{cc}
2 &amp; 1 \\
1 &amp; 2 \\
\end{array} \right).
\end{displaymath}" /></td> 
      <td width="10" align="RIGHT"> (222)</td>
     </tr> 
    </tbody>
   </table> 
   <br clear="ALL" />
  </div>
  <p></p> From the characteristic equation 
  <!-- MATH
 $|S-\lambda I|=0$
 --> 
  <img width="90" height="33" align="MIDDLE" border="0" src="img1696.png" alt="$\vert S-\lambda I\vert=0$" />, we have the quadratic 
  <!-- MATH
 $(2-\lambda)^2-1=0$
 --> 
  <img width="119" height="36" align="MIDDLE" border="0" src="img1697.png" alt="$(2-\lambda)^2-1=0$" />, whose solutions yield the eigenvalues 
  <img width="12" height="32" align="MIDDLE" border="0" src="img1698.png" alt="$3$" /> and 
  <img width="12" height="32" align="MIDDLE" border="0" src="img291.png" alt="$1$" />. The corresponding eigenvectors 
  <!-- MATH
 $\left(
                                                \begin{array}{c}
                                                  1 \\
                                                  -1 \\
                                                \end{array}
                                              \right)$
 --> 
  <img width="65" height="54" align="MIDDLE" border="0" src="img1699.png" alt="$\left(
\begin{array}{c}
1 \\
-1 \\
\end{array} \right)
$" /> and 
  <!-- MATH
 $\left(
         \begin{array}{c}
           1 \\
           1 \\
         \end{array}
       \right)$
 --> 
  <img width="52" height="54" align="MIDDLE" border="0" src="img1700.png" alt="$\left(
\begin{array}{c}
1 \\
1 \\
\end{array} \right)
$" /> are orthogonal. 
  <b>End worked example.</b> 
  <p> <br /></p>
  <hr /> 
  <!--Table of Child-Links--> 
  <a name="CHILD_LINKS"><strong>Subsections</strong></a> 
  <ul> 
   <li><a name="tex2html4530" href="matrix-decompositions-1.html">Matrix decompositions</a> </li>
  </ul> 
  <!--End of Table of Child-Links--> 
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html4528" href="matrix-decompositions-1.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4522" href="matrix-decompositions-and-latent-semantic-indexing-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4516" href="matrix-decompositions-and-latent-semantic-indexing-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4524" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4526" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4529" href="matrix-decompositions-1.html">Matrix decompositions</a> 
  <b> Up:</b> 
  <a name="tex2html4523" href="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</a> 
  <b> Previous:</b> 
  <a name="tex2html4517" href="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</a> &nbsp; 
  <b> <a name="tex2html4525" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4527" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>
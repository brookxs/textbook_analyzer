<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<html>
 <head> 
  <title>References and further reading</title> 
  <meta name="description" content="References and further reading" /> 
  <meta name="keywords" content="irbook" /> 
  <meta name="resource-type" content="document" /> 
  <meta name="distribution" content="global" /> 
  <meta name="Generator" content="LaTeX2HTML v2002-2-1" /> 
  <meta http-equiv="Content-Style-Type" content="text/css" /> 
  <link rel="STYLESHEET" href="irbook.css" /> 
  <link rel="next" href="exercises-3.html" /> 
  <link rel="previous" href="model-based-clustering-1.html" /> 
  <link rel="up" href="flat-clustering-1.html" /> 
  <link rel="next" href="exercises-3.html" /> 
 </head> 
 <body> 
  <!--Navigation Panel--> 
  <a name="tex2html4266" href="exercises-3.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4260" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4254" href="model-based-clustering-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4262" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4264" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4267" href="exercises-3.html">Exercises</a> 
  <b> Up:</b> 
  <a name="tex2html4261" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4255" href="model-based-clustering-1.html">Model-based clustering</a> &nbsp; 
  <b> <a name="tex2html4263" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4265" href="index-1.html">Index</a></b> 
  <br /> 
  <br /> 
  <!--End of Navigation Panel--> 
  <h1><a name="SECTION002160000000000000000"></a> <a name="sec:flatclustref"></a> <a name="p:flatclustref"></a> <br /> References and further reading </h1> 
  <p> <a href="bibliography-1.html#berkhin06survey">Berkhin (2006b)</a> gives a general up-to-date survey of clustering methods with special attention to scalability. The classic reference for clustering in pattern recognition, covering both <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means and EM, is (<a href="bibliography-1.html#duda00pattern">Duda et&nbsp;al., 2000</a>). <a href="bibliography-1.html#rasmussen92">Rasmussen (1992)</a> introduces clustering from an information retrieval perspective. <a href="bibliography-1.html#anderberg73cluster">Anderberg (1973)</a> provides a general introduction to clustering for applications. In addition to <a name="25072"></a> <i>Euclidean distance</i> and <a name="25074"></a> <i>cosine similarity</i> , <a name="25076"></a> <i>Kullback-Leibler divergence</i> is often used in clustering as a measure of how (dis)similar documents and clusters are (<a name="tex2html4268" href="bibliography-1.html#xu99clusterbased">Xu and Croft, 1999</a>, <a name="tex2html4269" href="bibliography-1.html#muresan04topic">Muresan and Harper, 2004</a>, <a name="tex2html4270" href="bibliography-1.html#kurland04corpus">Kurland and Lee, 2004</a>). </p>
  <p> The cluster hypothesis is due to <a href="bibliography-1.html#jardine71hierarchic">Jardine and van&nbsp;Rijsbergen (1971)</a> who state it as follows: Associations between documents convey information about the relevance of documents to requests. <a name="tex2html4271" href="bibliography-1.html#croft78cluster">Croft (1978)</a>, <a name="tex2html4272" href="bibliography-1.html#can90concepts">Can and Ozkarahan (1990)</a>, <a name="tex2html4273" href="bibliography-1.html#voorhees85">Voorhees (1985a)</a>, <a name="tex2html4274" href="bibliography-1.html#salton75dynamic">Salton (1975)</a>, <a name="tex2html4275" href="bibliography-1.html#cacheda03optimization">Cacheda et&nbsp;al. (2003)</a>, <a name="tex2html4276" href="bibliography-1.html#salton71cluster">Salton (1971a)</a>, <a name="tex2html4277" href="bibliography-1.html#singitham04efficiency">Singitham et&nbsp;al. (2004)</a>, <a name="tex2html4278" href="bibliography-1.html#can04efficiency">Can et&nbsp;al. (2004)</a> and <a href="bibliography-1.html#altingovde08incremental">Alting&ouml;vde et&nbsp;al. (2008)</a> investigate the efficiency and effectiveness of cluster-based retrieval. While some of these studies show improvements in effectiveness, efficiency or both, there is no consensus that cluster-based retrieval works well consistently across scenarios. Cluster-based language modeling was pioneered by <a href="bibliography-1.html#liu04cluster">Liu and Croft (2004)</a>. </p>
  <p> There is good evidence that clustering of search results improves user experience and search result quality (<a name="tex2html4279" href="bibliography-1.html#hp96">Hearst and Pedersen, 1996</a>, <a name="tex2html4280" href="bibliography-1.html#zamir99grouper">Zamir and Etzioni, 1999</a>, <a name="tex2html4281" href="bibliography-1.html#kaki05findex">K&auml;ki, 2005</a>, <a name="tex2html4282" href="bibliography-1.html#toda05search">Toda and Kataoka, 2005</a>, <a name="tex2html4283" href="bibliography-1.html#tombros02effectiveness">Tombros et&nbsp;al., 2002</a>), although not as much as search result structuring based on carefully edited category hierarchies (<a href="bibliography-1.html#hearst06clustering">Hearst, 2006</a>). The Scatter-Gather interface for browsing collections was presented by <a href="bibliography-1.html#cutting92scattergather">Cutting et&nbsp;al. (1992)</a>. A theoretical framework for analyzing the properties of Scatter/Gather and other information seeking user interfaces is presented by <a href="bibliography-1.html#pirolli07information">Pirolli (2007)</a>. <a href="bibliography-1.html#schutze97projections">Sch&uuml;tze and Silverstein (1997)</a> evaluate LSI (Chapter <a href="matrix-decompositions-and-latent-semantic-indexing-1.html#ch:lsi">18</a> ) and truncated representations of centroids for efficient <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means clustering. </p>
  <p> The Columbia NewsBlaster system (<a href="bibliography-1.html#mckeown02news">McKeown et&nbsp;al., 2002</a>), a forerunner to the now much more famous and refined Google News (<tt><a name="tex2html186" href="http://news.google.com">http://news.google.com</a></tt>), used hierarchical clustering (Chapter <a href="hierarchical-clustering-1.html#ch:hierclust">17</a> ) to give two levels of news topic granularity. See <a href="bibliography-1.html#hatzivassiloglou00linguistic">Hatzivassiloglou et&nbsp;al. (2000)</a> for details, and <a href="bibliography-1.html#chen00multilingual">Chen and Lin (2000)</a> and <a href="bibliography-1.html#radev01interactive">Radev et&nbsp;al. (2001)</a> for related systems. Other applications of clustering in information retrieval are duplicate detection (<a href="bibliography-1.html#yang06near">Yang and Callan (2006)</a>, shingling), novelty detection (see references in hclstfurther) and <a name="25100"></a> <i>metadata</i> discovery on the semantic web (<a href="bibliography-1.html#alonso06gio">Alonso et&nbsp;al., 2006</a>). </p>
  <p> The discussion of external evaluation measures is partially based on <a href="bibliography-1.html#strehl02relationship">Strehl (2002)</a>. <a href="bibliography-1.html#dom02information">Dom (2002)</a> proposes a measure <img width="24" height="32" align="MIDDLE" border="0" src="img1535.png" alt="$Q_0$" /> that is better motivated theoretically than NMI. <img width="24" height="32" align="MIDDLE" border="0" src="img1535.png" alt="$Q_0$" /> is the number of bits needed to transmit class memberships assuming cluster memberships are known. The Rand index is due to <a href="bibliography-1.html#rand71objective">Rand (1971)</a>. <a href="bibliography-1.html#hubert85comparing">Hubert and Arabie (1985)</a> propose an <a name="25107"></a> <i>adjusted</i> that ranges between <img width="25" height="32" align="MIDDLE" border="0" src="img1267.png" alt="$-1$" /> and 1 and is 0 if there is only chance agreement between clusters and classes (similar to <img width="12" height="32" align="MIDDLE" border="0" src="img754.png" alt="$\kappa$" /><a name="25109"></a> in Chapter <a href="evaluation-in-information-retrieval-1.html#ch:evaluation">8</a> , page <a href="assessing-relevance-1.html#p:kappa">8.2</a> ). <a href="bibliography-1.html#basu04active">Basu et&nbsp;al. (2004)</a> argue that the three evaluation measures NMI, Rand index and F measure give very similar results. <a href="bibliography-1.html#stein03cluster">Stein et&nbsp;al. (2003)</a> propose <a name="25114"></a> <i>expected edge density</i> as an internal measure and give evidence that it is a good predictor of the quality of a clustering. </p>
  <p> <a href="bibliography-1.html#kleinberg02impossibility">Kleinberg (2002)</a> and <a href="bibliography-1.html#meila05clusterings">Meila (2005)</a> present axiomatic frameworks for comparing clusterings. </p>
  <p> Authors that are often credited with the invention of the <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means algorithm include <a href="bibliography-1.html#lloyd82least">Lloyd (1982)</a> (first distributed in 1957), <a href="bibliography-1.html#ball65data">Ball (1965)</a>, <a href="bibliography-1.html#macqueen67some">MacQueen (1967)</a>, and <a href="bibliography-1.html#hartigan79kmeans">Hartigan and Wong (1979)</a>. <a href="bibliography-1.html#arthur06worstcase">Arthur and Vassilvitskii (2006)</a> investigate the worst-case complexity of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means. <a name="tex2html4284" href="bibliography-1.html#bradley98refining">Bradley and Fayyad (1998)</a>, <a name="tex2html4285" href="bibliography-1.html#pelleg99accelerating">Pelleg and Moore (1999)</a> and <a href="bibliography-1.html#davidson03speeding">Davidson and Satyanarayana (2003)</a> investigate the convergence properties of <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means empirically and how it depends on initial seed selection. <a href="bibliography-1.html#dhillon01concept">Dhillon and Modha (2001)</a> compare <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means clusters with <a name="25130"></a> <i>SVD</i> -based clusters (Chapter <a href="matrix-decompositions-and-latent-semantic-indexing-1.html#ch:lsi">18</a> ). The K-medoid algorithm was presented by <a href="bibliography-1.html#kaufman90finding">Kaufman and Rousseeuw (1990)</a>. The EM algorithm was originally introduced by <a href="bibliography-1.html#dlr77">Dempster et&nbsp;al. (1977)</a>. An in-depth treatment of EM is (<a href="bibliography-1.html#mclachlan96em">McLachlan and Krishnan, 1996</a>). See Section&nbsp;<a href="references-and-further-reading-18.html#sec:furtherlsi">18.5</a> (page&nbsp;<a href="references-and-further-reading-18.html#p:furtherlsi"><img align="BOTTOM" border="1" alt="[*]" src="http://nlp.stanford.edu/IR-book/html/icons/crossref.png" /></a>) for publications on latent analysis, which can also be viewed as soft clustering. </p>
  <p> AIC is due to <a href="bibliography-1.html#akaike74new">Akaike (1974)</a> (see also <a href="bibliography-1.html#burnham02model">Burnham and Anderson (2002)</a>). An alternative to AIC is BIC, which can be motivated as a Bayesian model selection procedure (<a href="bibliography-1.html#schwarz78estimating">Schwarz, 1978</a>). <a href="bibliography-1.html#fraley98how">Fraley and Raftery (1998)</a> show how to choose an optimal number of clusters based on BIC. An application of BIC to <img width="15" height="32" align="MIDDLE" border="0" src="img30.png" alt="$K$" />-means is (<a href="bibliography-1.html#pelleg00xmeans">Pelleg and Moore, 2000</a>). <a href="bibliography-1.html#hamerly03kmeans">Hamerly and Elkan (2003)</a> propose an alternative to BIC that performs better in their experiments. Another influential Bayesian approach for determining the number of clusters (simultaneously with cluster assignment) is described by <a href="bibliography-1.html#cheeseman96bayesian">Cheeseman and Stutz (1996)</a>. Two methods for determining cardinality without external criteria are presented by <a href="bibliography-1.html#tibshirani01estimating">Tibshirani et&nbsp;al. (2001)</a>. </p>
  <p> We only have space here for classical completely unsupervised clustering. An important current topic of research is how to use prior knowledge to guide clustering (e.g., <a href="bibliography-1.html#ji06document">Ji and Xu (2006)</a>) and how to incorporate interactive feedback during clustering (e.g., <a href="bibliography-1.html#huang06text">Huang and Mitchell (2006)</a>). <a href="bibliography-1.html#fayyad98initialization">Fayyad et&nbsp;al. (1998)</a> propose an initialization for EM clustering. For algorithms that can cluster very large data sets in one scan through the data see <a href="bibliography-1.html#bradley98scaling">Bradley et&nbsp;al. (1998)</a>. </p>
  <p> The applications in Table <a href="clustering-in-information-retrieval-1.html#tab:clusttb1">16.1</a> all cluster documents. Other information retrieval applications cluster words (e.g., <a href="bibliography-1.html#crouch88cluster">Crouch, 1988</a>), contexts of words (e.g., <a href="bibliography-1.html#schuetze95information">Sch&uuml;tze and Pedersen, 1995</a>) or words and documents simultaneously (e.g., <a name="tex2html4286" href="bibliography-1.html#tishby00data">Tishby and Slonim, 2000</a>, <a name="tex2html4287" href="bibliography-1.html#zha01bipartite">Zha et&nbsp;al., 2001</a>, <a name="tex2html4288" href="bibliography-1.html#dhillon01coclustering">Dhillon, 2001</a>). Simultaneous clustering of words and documents is an example of <a name="25155"></a> <i>co-clustering</i> or <a name="25157"></a> <i>biclustering</i> . </p>
  <p> </p>
  <hr /> 
  <!--Navigation Panel--> 
  <a name="tex2html4266" href="exercises-3.html"> <img width="37" height="24" align="BOTTOM" border="0" alt="next" src="http://nlp.stanford.edu/IR-book/html/icons/next.png" /></a> 
  <a name="tex2html4260" href="flat-clustering-1.html"> <img width="26" height="24" align="BOTTOM" border="0" alt="up" src="http://nlp.stanford.edu/IR-book/html/icons/up.png" /></a> 
  <a name="tex2html4254" href="model-based-clustering-1.html"> <img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="http://nlp.stanford.edu/IR-book/html/icons/prev.png" /></a> 
  <a name="tex2html4262" href="contents-1.html"> <img width="65" height="24" align="BOTTOM" border="0" alt="contents" src="http://nlp.stanford.edu/IR-book/html/icons/contents.png" /></a> 
  <a name="tex2html4264" href="index-1.html"> <img width="43" height="24" align="BOTTOM" border="0" alt="index" src="http://nlp.stanford.edu/IR-book/html/icons/index.png" /></a> 
  <br /> 
  <b> Next:</b> 
  <a name="tex2html4267" href="exercises-3.html">Exercises</a> 
  <b> Up:</b> 
  <a name="tex2html4261" href="flat-clustering-1.html">Flat clustering</a> 
  <b> Previous:</b> 
  <a name="tex2html4255" href="model-based-clustering-1.html">Model-based clustering</a> &nbsp; 
  <b> <a name="tex2html4263" href="contents-1.html">Contents</a></b> &nbsp; 
  <b> <a name="tex2html4265" href="index-1.html">Index</a></b> 
  <!--End of Navigation Panel--> 
  <address> &copy; 2008 Cambridge University Press<br />This is an automatically generated page. In case of formatting errors you may want to look at the <a href="http://informationretrieval.org">PDF edition</a> of the book.<br /> 2009-04-07 </address>   
 </body>
</html>